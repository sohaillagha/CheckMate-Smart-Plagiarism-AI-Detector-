sentence1,sentence2
Associative Memories Hopfield Networks Hopfield neural network was invented by Dr. John J. Hopfield in 1982.,
It consists of a single layer which contains one or more fully connected recurrent neurons.,
The Hopfield network is commonly used for auto-association and optimization tasks.,
Discrete Hopfield Network Problem Continuous Hopfield Network Brain-State-in-a-Box Network,
MACHINE LEARNING (ECOMP722/IT722) - COMPREHENSIVE QUESTION BANK UNIT 1: Introduction & Getting to Know Data Machine Learning Basics & Fundamentals Q1.,
What is Machine Learning?,
Explain with the help of examples its applications in regression problems.,
(January 2023) [8 marks] Q2.,
Compare and contrast supervised and unsupervised learning techniques with suitable examples.,
(January 2023 OE) [7 marks] Q3.,
Consider a classification problem to decide if credit card is to be issued to a person based on whether he has a job.,
Identify the (i) Experience (ii) Performance (iii) Task with respect to Machine Learning terminology by providing suitable justification.,
(January 2023) [4 marks] Overfitting and Underfitting Q4.,
Define overfitting and underfitting.,
Provide examples to illustrate their effects on model performance.,
(November 2024) [6 marks] Q5.,
Explain the Bias-Variance trade-off in machine learning.,
How does model complexity impact this trade- off?,
(November 2021) [6 marks] Q6.,
What is the Bias-Variance tradeoff in machine learning and how does it affect model performance?,
(November 2024) [5 marks] Data Attributes and Types Q7.,
"Explain the difference between nominal, ordinal, and continuous attributes.",
Provide examples for each.,
(November 2024) [6 marks] Data Preprocessing & Statistical Measures Q8.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Suppose the following height values (in cm) are given for analysis (in ascending order): 150, 152, 155, 157, 160, 160, 162, 164, 165, 165, 168, 170, 172, 175, 175, 178, 180, 182, 185, 188, 190, 193, 195, 200, 205 a) Calculate the mean and median of the height values.",
How do these measures of central tendency differ in this dataset?,
b) Determine the mode(s) of the dataset.,
"Would you classify the dataset as unimodal, bimodal, or multimodal based on the mode(s)?",
c) Estimate the first quartile (Q1) and third quartile (Q3).,
How do these quartiles help in understanding the distribution of the data?,
"d) Provide the five-number summary (minimum, Q1, median, Q3, maximum) for the dataset.",
e) Create a boxplot for the given height values and describe any outliers.,
f) Calculate the midrange of the dataset.,
What does this value tell you about the central tendency of the dataset?,
(November 2024) [8 marks] Q9.,
[PROBLEM QUESTION - Refer to November 2021 Paper] A dataset contains several missing values on different features.,
"Describe a technique for handling missing values and demonstrate how you would apply one of them to the following dataset with Feature A, Feature B, Feature C with missing values.",
(November 2021) [8 marks] Q10.,
"Consider the two objects represented by attribute values (1,6,2,5,3) and (3,5,2,6,7): 1.",
Compute the Euclidean distance between two objects 2.,
Compute the Manhattan distance between two objects 3.,
"Compute the Minkowski distance between two objects, using q=3 (January 2023) [6 marks] Q11.",
[PROBLEM QUESTION - Refer to November 2021 Paper] Consider the following data points representing two objects.,
Compute the Manhattan distance and Euclidean distance between the two objects.,
(November 2021) [6 marks] Q12.,
Explain the following measures: 1.,
Euclidean distance 2.,
Manhattan distance 3.,
Minkowski distance 4.,
Cosine Similarity (December 2023) [6 marks] Q13.,
What is data normalization and why is it important in machine learning?,
Briefly describe two normalization techniques.,
(November 2021) [8 marks] UNIT 2: Basic Classification Methods & Model Evaluation Decision Tree Induction Q14.,
"[PROBLEM QUESTION - Refer to January 2023 Paper] Construct the ID3 decision tree for the following training data: Balloon dataset with Color, Size, Act, Age attributes.",
(January 2023) [10 marks] Q15.,
[PROBLEM QUESTION - Refer to May 2024 Paper] What is Machine Learning?,
"Construct the ID3 decision tree for the Sunlight attribute given: Height, Weight, Location, Sunlight data.",
(May 2024) [8 marks] Q16.,
"[PROBLEM QUESTION - Refer to December 2023 Paper] The following dataset is supposed to be a part of a transportation study regarding mode choice to select Bus, Car or Train in a city.",
"Construct a decision tree with ID3 for the target attribute ""TRANSPORTATION MODE"": Gender, Car ownership, Travel cost, Income, Transportation mode.",
(December 2023) [10 marks] Q17.,
[PROBLEM QUESTION - Refer to May 2024 Paper] Transportation study dataset - similar to Q16.,
(May 2024) [10 marks] Q18.,
"[PROBLEM QUESTION - Refer to January 2023 OE Paper] Consider the training examples for a binary classification problem with Price, Maintenance, Capacity, Airbag, Profitable attributes: a.",
"Considering ""profitable"" as the binary valued attribute to be predicted, which attribute would be selected as the root node in a decision tree with multi-way splits using the information gain measure?",
"For the same data set, suppose we decide to construct a decision tree using binary split and the Gini index impurity measure, which feature and split point combination would be the best to use as the root node?",
(January 2023 OE) [10 marks] Q19.,
"[PROBLEM QUESTION - Refer to November 2021 Paper] Apply the ID3 algorithm to write the rulebook for the given dataset: Humidity, Family, Play dataset.",
(November 2021) [8 marks] Naive Bayesian Classification Q20.,
"[PROBLEM QUESTION - Refer to January 2023 OE Paper] Using Naive Bayesian Classification for the following training data, predict whether the unknown sample with [Age:7, Vaccination:1, Tumor Size: Medium, Tumor Site: Shoulder] belongs to malignant tumor or not.",
"Dataset includes: Age, Vaccination, Tumor Size, Tumor Site, Malignant.",
(January 2023 OE) [6 marks] Q21.,
"What are Bayesian Belief Networks, and how are they used in classification?",
(December 2023) [6 marks] Q22.,
[PROBLEM QUESTION - Refer to January 2023 OE Paper] A Bayesian belief network for detecting heart disease and heartburn in patients with conditional probability tables: a.,
Determine the likelihood of a person to have heart diseases.,
"If the person has high blood pressure, calculate the probability that they have heart disease.",
"c. Given that the person having high blood pressure exercises regularly and eats a healthy diet, what is the posterior probability that the person has a heart disease?",
(January 2023 OE) [8 marks] Q23.,
Explain how Naive Bayes Classification works.,
Illustrate your answer with an example.,
(November 2021) [5 marks] Rule-Based Classification Q24.,
"[PROBLEM QUESTION - Refer to January 2023 OE Paper] Suppose a rule-based classifier produces the following rule set for Car Value with Mileage, Air Conditioner, Engine: R1: (Mileage: High)→(Car Value: Low) R2: (Mileage: Low)→(Car Value: High) R3: (Air Conditioner: Working)^(Engine: Good)→Car Value: High R4: (Air Conditioner: Working)^(Engine: Bad)→Car Value: Low R5: (Air Conditioner: Broken)→Car Value: Low a.",
Are the rules mutually exclusive?,
Is the rule set exhaustive?,
c. Is ordering needed for this set of rules?,
d. Do you need a default class for the rule set?,
e. Find the coverage and accuracy for each of the rules.,
(January 2023 OE) [7 marks] Q25.,
Explain the different types of associative classification used in machine learning.,
"(January 2023, May 2024) [6 marks] Support Vector Machines (SVM) Q26.",
Describe the working principle of the K-Nearest Neighbor (KNN) algorithm.,
Explain the working principle of the Support Vector Machine (SVM) algorithm.,
(December 2023) [7 marks each] Q27.,
"[PROBLEM QUESTION - Refer to January 2023 OE Paper] With the help of relevant sketches and mathematical equations, explain how Support Vector Machines determine the maximum marginal hyperplane in case of linearly separable data.",
"Consider positively labeled data points: {[3,1], [3,-1], [6,1], [6,-1]} and negatively labeled data points: {[1,0], [0,1], [0,-1], [-1,0]} a. Verify if the data points are linearly separable.",
Using SVM algorithm create a line or a hyperplane which separates the data into classes.,
(January 2023 OE) [10 marks] Q28.,
"[PROBLEM QUESTION - Refer to May 2024 Paper] Consider the following positively labeled data points: {(3,1),(3,-1),(6,1),(6,-1)} and negatively labeled data points: {(1,0),(0,1),(0,-1),(-1,0)}.",
Using SVM create a hyperplane that converts data into classes.,
(May 2024) [10 marks] Q29.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Construct a Support Vector Machine (SVM) classifier with positively labeled data points: {(3,1),(3,-1),(6,1),(6,-1)} and negatively labeled data points: {(1,0),(0,1),(0,-1),(-1,0)}.",
"Find the equation of separating hyperplane, identify support vectors, and verify classification.",
(November 2024) [8 marks] Q30.,
Explain the Support Vector Machine (SVM) algorithm.,
(January 2023) [8 marks] K-Nearest Neighbor (KNN) Q31.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Myntra's customer dataset includes height, weight, and T-shirt sizes ('M' or 'L').",
"Predict T-shirt size for Anurag (Height: 161 cm, Weight: 61 kg) using KNN with k=3.",
"Dataset provided with Height(cm), Weight(kg), T-Shirt Size for 10 customers.",
(November 2024) [8 marks] Q32.,
"Write an algorithm for k-nearest neighbor classification given k, the nearest number of neighbors, and n, the number of attributes describing each tuple.",
"In k-nearest neighbors' classifiers, how is the distance computed: a.",
"For attributes that are not numeric, but categorical?",
In case of missing values?,
(January 2023 OE) [6 marks] Q33.,
[PROBLEM QUESTION - Refer to November 2021 Paper] You are tasked with classifying using k-NN.,
Given distances between test point and 5 nearest neighbours with their class labels.,
"If k=3, classify the test point based on majority voting principle.",
(November 2021) [7 marks] Model Evaluation Metrics Q34.,
[PROBLEM QUESTION - Refer to January 2023 OE Paper] Environmental scientists solve two-class classification for genetic variant prediction.,
"500 samples, predicted 350 with variant, 150 without.",
"Actual: 305 with, 195 without.",
"Complete confusion matrix and calculate accuracy, misclassification rate, sensitivity, specificity, precision, recall and F1-score.",
(January 2023 OE) [6 marks] Q35.,
[PROBLEM QUESTION - Refer to November 2024 Paper] International Airlines decided 2 classifiers (A and B) to predict if flight from Pakistan arrives on time.,
Tested on 500 flights.,
Confusion matrix provided for both classifiers.,
(i) Which is preferable in terms of F1 score?,
(ii) Which is preferable in terms of accuracy?,
(November 2024) [7 marks] Q36.,
[PROBLEM QUESTION - Refer to December 2023 Paper] Calculate accuracy of Practo Health Tech's patient diagnosis model using confusion matrix.,
Calculate precision and recall for Class C. Explain implications of false positive and false negative values.,
Interpret true positive value.,
(December 2023) [4+4 marks] Q37.,
"Suppose I have 10,000 emails in my mailbox out of which 200 are spams.",
"The spam detection system detects 150 mails as spams, out of which 50 are actually spams.",
What is the precision and recall of my spam detection system?,
(May 2024) [4 marks] Q38.,
Explain the key metrics used to evaluate classifier performance.,
"Discuss precision, recall, F1-score, and accuracy with examples.",
(November 2021) [6 marks] Cross-Validation Q39.,
What is k-fold cross-validation?,
Explain how it works and why it is preferred over the holdout method in certain situations.,
(November 2024) [7 marks] Q40.,
Differentiate between holdout and cross validation.,
Why is cross-validation preferred over hold-out?,
How does stratified cross validation work?,
(January 2023 OE) [4 marks] Q41.,
[PROBLEM QUESTION] A dataset is split into training and test sets using 10-fold cross-validation.,
"Accuracy scores for each fold: 85%, 78%, 90%, 82%, 88%, 80%, 92%, 85%, 84%, 83%.",
Calculate mean accuracy and standard deviation.,
(November 2021) [7 marks] Q42.,
Explain the following: i.,
Cross validation ii.,
Bootstrap (May 2024) [4 marks] Q43.,
Describe the Holdout method and Cross-Validation technique for evaluating machine learning models.,
How do they differ in terms of performance estimation?,
(November 2021) [5 marks] Ensemble Methods Q44.,
Explain the concept of bagging and boosting?,
State with suitable justification why it may improve the accuracy of decision tree induction.,
"(December 2023, January 2023) [8 marks each] Q45.",
What makes Random Forests robust?,
Explain how the Random Forest algorithm works.,
(November 2021) [5 marks] Q46.,
Explain in brief the techniques used to improve classification accuracy.,
"(May 2024, November 2024) [4-7 marks] Lazy vs Eager Learners Q47.",
Explain the difference between Eager Learner and Lazy Learner classification algorithms.,
Give examples of each.,
(November 2024) [5 marks] Q48.,
Distinguish between Lazy learners & Eager learners?,
"(January 2023, May 2024, November 2021) [4 marks each] ROC Curves Q49.",
[PROBLEM QUESTION - Refer to January 2023 OE Paper] Data tuples sorted by decreasing probability values returned by classifier.,
"For each tuple, compute TP, FP, TN, FN.",
Compute TPR and FPR for ROC curve.,
"Table with 10 tuples, class (P/N), and probability values provided.",
(January 2023 OE) [6 marks] Q50.,
Consider a binary classification problem where performance is evaluated using ROC curve.,
"Draw and explain ROC curve, and illustrate how AUC can be interpreted in terms of classifier performance.",
(November 2021) [5 marks] Logistic Regression Q51.,
Write a note on logistic regression.,
(May 2024) [4 marks] Q52.,
Distinguish between logistic regression and linear regression?,
(January 2023) [4 marks] Q53.,
Explain how logistic regression can be used for binary classification.,
What is the role of the logistic function in this context?,
(November 2024) [7 marks] Other Classification Topics Q54.,
Outline methods for addressing the class imbalance problem.,
Suppose a bank wants to develop a classifier that guards against fraudulent credit card transactions.,
Illustrate how you can induce a quality classifier based on large set of non-fraudulent examples and very small set of fraudulent cases.,
(January 2023 OE) [6 marks] Q55.,
Define (i) Prior Probability (ii) Conditional Probability (iii) Posterior Probability (May 2024) [3 marks] UNIT 3: Cluster Analysis Clustering Basics Q56.,
What is clustering.,
Discuss the major requirements of Clustering Analysis.,
(May 2024) [6 marks] Q57.,
Explain what are clusters and write requirements for cluster analysis.,
(November 2021) [7 marks] Q58.,
Compare the properties of a good clustering algorithm.,
(November 2021) [7 marks] Q59.,
Explain the basic requirements for performing cluster analysis.,
(November 2024) [6 marks] Partitioning Methods - K-Means Q60.,
[PROBLEM QUESTION - Refer to January 2023 Paper] Consider clustering points into 2 clusters.,
Distance function is Euclidean.,
"Points A1-A8 with x,y coordinates.",
"Initial cluster centers A1, A4, A7.",
Use K- Means algorithm.,
(January 2023) [12 marks] Q61.,
[PROBLEM QUESTION - Refer to May 2024 Paper] Use K-means clustering to divide data into two clusters.,
Euclidean distance.,
"Points A1-A8 with x,y coordinates provided.",
(May 2024) [11 marks] Q62.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Cluster points (x,y) into three clusters: A1(2,10), A2(2,5), A3(8,4), B1(5,8), B2(7,5), B3(6,4), C1(1,2), C2(4,9).",
Euclidean distance.,
"Initially assign A1, B1, C1 as centers.",
Use k-means to show: (i) Three cluster centers after first round (ii) Final three clusters (November 2024) [10 marks] Q63.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Given data points: x1(4,5), x2(2,2), x3(4,5), x4(4,7), x5(3,1), x6(9,2), x7(6,2), x8(7,4), x9(8,4), x10(7,3).",
"Perform: (i) Apply CURE algorithm with Euclidean distance, threshold ε=1.5, Maximum 2 (ii) Create C# tree (iii) Explain structure of C# tree after clustering (November 2024) [8 marks] Q64.",
[PROBLEM QUESTION] Cluster the following data points using K-Means algorithm with k=3.,
(November 2024) [8 marks] Q65.,
"[PROBLEM QUESTION - Refer to November 2024 Paper] Consider points P1, P2, P3, P4, P5.",
Use distance matrix given below to perform hierarchical clustering using single link.,
Show results by drawing dendrogram.,
The dendrogram should clearly show the order in which points are merged.,
Distance matrix provided.,
(November 2024) [7 marks] K-Medoids Q66.,
[PROBLEM QUESTION - Refer to December 2023 Paper] Apply K-Medoid algorithm and cluster dataset for k=2.,
X-coordinate and Y-coordinate data table provided.,
(December 2023) [12 marks] Q67.,
"[PROBLEM QUESTION] Consider dataset clustered using k-medoids: P1(1,1), P2(3,3), P3(10,10), P4(11,11).",
Calculate medoids after one iteration assuming P1 and P3 are initial medoids.,
(November 2021) [5 marks] Hierarchical Clustering - AGNES Q68.,
[PROBLEM QUESTION - Refer to January 2023 OE Paper] Apply Agglomerative Nesting (AGNES) algorithm.,
Draw dendogram using single link and complete link approach.,
"Distance matrix for items A,B,C,D,E provided.",
(January 2023 OE) [6 marks] Q69.,
[PROBLEM QUESTION - Refer to May 2024 Paper] Use distance matrix to perform hierarchical clustering using single link and divisive hierarchical clustering technique.,
Distance matrix provided.,
(May 2024) [8 marks] Q70.,
Explain the concept of hierarchical clustering and discuss the various types of hierarchical clustering schemes.,
(January 2023) [8 marks] Q71.,
Discuss why there are more agglomerative methods than divisive methods?,
(January 2023 OE) [8 marks] Q72.,
Differentiate between partitioning and hierarchical clustering methods.,
(December 2023) [8 marks] Q73.,
Write short notes on the following Hierarchical Clustering Algorithms: 1.,
Chameleon Clustering 2.,
Probabilistic Clustering (January 2023 OE) [8 marks] Q74.,
Explain Chameleon - Hierarchical based clustering algorithm.,
"(January 2023, May 2024) [8 marks] Density-Based Clustering - DBSCAN Q75.",
[PROBLEM QUESTION - Refer to January 2023 OE Paper] Discuss need for density-based clustering.,
"Given 12 two-dimensional dataset points with x,y coordinates, apply DBSCAN (ε=1.9, minimum points=4) to decide core, border, outlier points.",
(January 2023 OE) [12 marks] Q76.,
"[PROBLEM QUESTION - Refer to December 2023 Paper] Given 8 data points with (X,Y) coordinates.",
"Apply DBSCAN (Epsilon=0.5, MinPts=3): i.",
Identify and label clusters ii.,
Provide description of clusters (December 2023) [12 marks] Q77.,
"[PROBLEM QUESTION - Refer to November 2021 Paper] Given data points, apply DBSCAN (Eps=3, minPts=4).",
"Identify Core points, border points, noise points.",
Data points P1-P5 with coordinates provided.,
(November 2021) [17 marks] Q78.,
Explain the following terms with reference to DBSCAN: (i) Core points (ii) Noise points (iii) Border points (November 2024) [6 marks] Q79.,
Differentiate between DBSCAN & OPTICS density based clustering algorithm.,
"(January 2023, January 2023 OE) [4 marks each] Grid-Based Clustering Q80.",
Elaborate the following grid-based clustering methods: i. STING: Statistical Information Grid ii.,
CLIQUE: Clustering in Quest (January 2023 OE) [8 marks] Q81.,
Explain any one grid based clustering Technique.,
(May 2024) [4 marks] Q82.,
Explain the following clustering algorithm: (i) Chameleon 2.,
STING (December 2023) [6 marks] Other Clustering Methods Q83.,
What are fuzzy clusters?,
Discuss situations in which fuzzy clustering techniques would be preferred over traditional clustering methods.,
(January 2023 OE) [6 marks] Q84.,
Describe how the Expectation-Maximization (EM) algorithm works for probabilistic clustering.,
Explain the main steps involved.,
(November 2021) [6 marks] Q85.,
"While assessing the feasibility of clustering analysis, how is the number of clusters in a data set determined?",
(January 2023 OE) [6 marks] High-Dimensional Data Clustering Q86.,
Explain the necessity of clustering high-dimensional data?,
"(January 2023, December 2023) [4-6 marks] Q87.",
With respect to clustering of high-dimensional data: 1.,
"Why are traditional distance measures, used in low-dimensional cluster analysis, not effective on high-dimensional data?",
Discuss kinds of clusters that are meaningful on high-dimensional data.,
"(January 2023 OE) [4 marks each] UNIT 4: Regression, Outlier Analysis & Dimensionality Reduction Linear Regression Q88.",
Differentiate between: i.,
Simple and Multiple Regression ii.,
Linear and Non-Linear Regression.,
With help of mathematical expressions explain simple linear regression model.,
(January 2023 OE) [7+6 marks] Q89.,
Explain the concept multiple linear regression for prediction.,
(December 2023) [6 marks] Q90.,
Explain the difference between simple and multiple linear regression.,
Provide examples of scenarios where each would be applied.,
(November 2021) [7 marks] Q91.,
"[PROBLEM QUESTION - Refer to November 2021 Paper] Given dataset with three variables V1, V2, Y.",
"Perform multiple linear regression to predict Y from X1 and X2, provide regression coefficients.",
(November 2021) [7 marks] Logistic Regression Q92.,
Distinguish between logistic regression and linear regression?,
(January 2023) [4 marks] Q93.,
Explain how logistic regression can be used for binary classification.,
What is the role of the logistic function in this context?,
(November 2024) [7 marks] Outlier Detection Q94.,
What are outliers?,
Can noise objects be Outliers?,
With an example explain the collective outliers.,
(January 2023 OE) [4 marks] Q95.,
What is an outlier?,
Briefly describe distance based outlier detection method?,
"(January 2023, May 2024) [4 marks each] Q96.",
What are the key challenges involved in outlier detection?,
Discuss how these challenges affect the performance of machine learning models.,
(November 2024) [7 marks] Q97.,
Discuss the challenges of Outlier detection.,
(January 2023 OE) [4 marks] Q98.,
Explain with the help of example the different types of outliers.,
(January 2023) [8 marks] Q99.,
"[PROBLEM QUESTION] Given dataset representing ages: [15, 24, 25, 33, 26, 31, 40, 51] (i) Use Z-score method to detect outlier.",
Assume threshold=2.,
(ii) Calculate Z-scores and identify outliers.,
(November 2021) [7 marks] Q100.,
"Define and discriminate between point outliers, contextual outliers, and collective outliers.",
Provide examples for each.,
(November 2021) [5 marks] Dimensionality Reduction - PCA Q101.,
Explain the concept of Principal Component Analysis (PCA) and its use in dimensionality reduction.,
How does PCA transform the original variables?,
(November 2024) [6 marks] Q102.,
Explain the procedure used in PCA to enable dimension reduction.,
(January 2023) [8 marks] Q103.,
"Using Principal Components Analysis (PCA), describe how dimensionality reduction is performed.",
What are main advantages of using PCA in high-dimensional data?,
(November 2021) [6 marks] Q104.,
"[PROBLEM QUESTION - Refer to November 2021 Paper] Given data matrix X with A1, A2 values: (i) Perform PCA and calculate first principal component (ii) Explain how PCA helps in reducing dimensionality (November 2021) [8 marks] Q105.",
"Discuss the curse of dimensionality, model overfitting, and how reducing dimensions can improve model performance and interpretability.",
(November 2024) [4 marks] Linear Discriminant Analysis (LDA) Q106.,
[PROBLEM QUESTION - Refer to January 2023 OE Paper] Discuss Curse of Dimensionality.,
How LDA helps in dimensionality reduction?,
"Apply LDA to project feature into smaller subspace: X1: [(4,1), (2,4), (2,3), (3,6), (4,4)], X2: [(9,10), (6,8), (9,5), (8,7), (10,8)] (January 2023 OE) [10 marks] Q107.",
"[PROBLEM QUESTION - Refer to November 2024 Paper] Compute linear Discriminant projection for two-dimensional dataset: Class w1: X1={(4,1),(2,4),(2,3),(3,6),(4,4)} Class w2: X2={(9,10),(6,8),(9,5),(8,7),(10,8)} (i) Calculate mean vectors m1 and m2 (ii) Compute within-class scatter matrix SW (iii) Find optimal linear discriminant direction w (iv) Project samples and interpret results (November 2024) [10 marks] Q108.",
Explain how linear discriminant analysis (LDA) helps in reducing dimensionality of a data set.,
"(January 2023, December 2023) [6 marks each] Similarity Measures Q109.",
Explain how Geodesic Distance is used to measure similarity between two vertices in a graph.,
Does it suit to measure similarity between people in social network?,
(January 2023 OE) [6 marks] UNIT 5: Neural Networks (Additional/Advanced Topics) Artificial Neural Networks Q110.,
"[PROBLEM QUESTION - Refer to December 2023 Paper] Consider feedforward neural network with Input layer, Hidden layer, Output layer with initial weights and bias values.",
"Target output=0.7, learning rate η=0.1 Perform one forward pass for input (x1=0.6, x2=0.9), calculate predicted output with calculations for each neuron Calculate error (E) using MSE (December 2023) [12 marks] Q111.",
[PROBLEM QUESTION - Refer to May 2024 Paper] Consider feedforward neural network with weights provided.,
Network has good activation function.,
"Perform forward pass, calculate error & backpropagate using error backpropagation algorithm.",
"Actual output y=0.8, learning rate η=1.",
(May 2024) [10 marks] Q112.,
Explain the importance of activation function and explain the sigmoid activation functions.,
"(January 2023, December 2023) [8 marks] QUESTIONS REQUIRING REFERENCE TO ORIGINAL PAPERS (Questions with Detailed Data Tables/Diagrams/Matrices) 1.",
Q8 - Height values complete statistical analysis with 27 data points 2.,
Q9 - Missing values handling with Feature table 3.,
Q11 - Distance calculation between two multi-dimensional objects 4.,
"Q14 - Balloon dataset with Color, Size, Act, Age attributes 5.",
"Q15 - Sunlight prediction with Height, Weight, Location attributes 6.",
"Q16 & Q17 - Transportation mode dataset with Gender, Car ownership, Travel cost, Income 7.",
"Q18 - Car profitability with Price, Maintenance, Capacity, Airbag - full data table 8.",
"Q19 - Humidity, Family, Play dataset for ID3 9.",
"Q20 - Tumor malignancy dataset with Age, Vaccination, Tumor Size/Site 10.",
Q22 - Bayesian network diagram with conditional probability tables 11.,
Q24 - Car value rules with complete data table showing records 12.,
"Q27, Q28, Q29 - SVM problems with 8 data points each",
"IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
"1, JANUARY 2024 263 Improving Semiconductor Device Modeling for Electronic Design Automation by Machine Learning Techniques Zeheng Wang , Member, IEEE, Liang Li, Ross C. C. Leon, Jinlin Yang, Junjie Shi, Timothy van der Laan, and Muhammad Usman Abstract— The semiconductors industry benefits greatly from the integration of machine learning (ML)-based tech- niques in technology computer-aided design (TCAD) meth- ods.",
"The performance of ML models, however, relies heavily on the quality and quantity of training datasets.",
They can be particularly difficult to obtain in the semiconductor industry due to the complexity and expense of the device fabrication.,
"In this article, we propose a self-augmentation strategy for improving ML-based device modeling using variational autoencoder (VAE)-based techniques.",
These techniques require a small number of experimental data points and do not rely on TCAD tools.,
"To demonstrate the effectiveness of our approach, we apply it to a deep neural network (DNN)-based prediction task for the ohmic resistance value in gallium nitride (GaN) devices.",
A 70% reduction in mean absolute error (MAE) when predicting experimental results is achieved.,
"The inherent flexibility of our approach allows easy adaptation to various tasks, thus making it highly relevant to many applications of the semiconductor industry.",
"Index Terms— Data augmentation, electronic design automation (EDA), gallium nitride (GaN), machine learning (ML), semiconductor devices.",
INTRODUCTION E LECTRONIC design automation (EDA) has been crucial in advancing the semiconductors industry by simplifying Manuscript received 17 July 2023; accepted 17 August 2023.,
Date of publication 30 August 2023; date of current version 2 January 2024.,
This work was supported in part by CSIRO’s Impossible Without You Program.,
The review of this article was arranged by Editor H. Agarwal.,
(Corresponding authors: Zeheng Wang; Timothy van der Laan; Muhammad Usman.),
"Zeheng Wang is with Data61, CSIRO, Clayton, VIC 3168, Australia, and also with CSIRO Manufacturing, Lindfield, NSW 2070, Australia (e-mail: zenwang@outlook.com).",
"Liang Li is with the Academy for Advanced Interdisciplinary Studies, Peking University, Beijing 100871, China.",
"Ross C. C. Leon is with Quantum Motion, N7 9HJ London, U.K. Jinlin Yang is with the Department of Chemistry, National University of Singapore, Singapore 117543.",
"Junjie Shi is with the School of Materials Science and Engineering, University of New South Wales, Sydney, NSW 2052, Australia.",
"Timothy van der Laan is with CSIRO Manufacturing, Lindfield, NSW 2070, Australia (e-mail: tim.vanderlaan@csiro.au).",
"Muhammad Usman is with Data61, CSIRO, Clayton, VIC 3168, Australia (e-mail: muhammad.usman@csiro.au).",
Color versions of one or more figures in this article are available at https://doi.org/10.1109/TED.2023.3307051.,
Digital Object Identifier 10.1109/TED.2023.3307051 design tasks and reducing their time consumption [1].,
"One particular EDA technique, technology computer-aided design (TCAD), has been especially useful in the area of semiconduc- tor devices.",
"TCAD solves basic physics equations using the finite element method, such as the Poisson and Schrödinger equations, which provides easy access to simulated results that would be difficult to solve manually [2], [3], [4].",
"In addition, TCAD has significantly reduced the cost of experiments during device design by avoiding them altogether [5].",
"Nevertheless, simulating complex 3-D device structures requires significant computational resources.",
"While many models and methods have been developed to reduce resource consumption, exploring novel methodologies of TCAD remains a pressing issue to balance the accuracy and time consumption of sophisticated physics simulations.",
"So far, machine learning (ML)-based solutions have been success- fully employed in many device modeling cases and offer the advantage of low-resource consumption after model train- ing [6], [7], [8], [9], [10].",
"However, with expanding size of the ML models, there is an increasing need for input data to fully complete model training [11].",
"TCAD-based data augmentation, a technique that has gar- nered significant attention in the semiconductor industry since 2019 [9], [12], [13], [14], has been employed to generate artifi- cial data that can be fed into deep neural network (DNN)-based models.",
This approach could provide an expanded dataset and then significant boost to DNN-based modeling within the TCAD industry’s development.,
"However, many problems in the semiconductor industry cannot be directly solved by TCAD tools, such as the simulation of the formation of ohmic contacts in gallium nitride (GaN) devices, which imposes a formidable challenge on the TCAD-based augmentation technique.",
"Recently, a study by Sheelvardhan et al.",
[15] highlighted the potential of knowledge-based ML algorithms in overcom- ing the limitations of traditional ML-based approaches for semiconductor device modeling.,
"By leveraging prior knowl- edge, these algorithms offer a promising solution to address the complexities associated with establishing and training ML models.",
This research represents a significant advancement toward the development of next-generation ML-based TCAD toolkits.,
© 2023 The Authors.,
This work is licensed under a Creative Commons Attribution 4.0 License.,
"For more information, see https://creativecommons.org/licenses/by/4.0/ 264 IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
"1, JANUARY 2024 Fig.",
Sketch of the whole procedure of augmentation-enhanced ML-based semiconductor device modeling–The data were extracted from literature containing experimental data and then were augmented by the augmentation model (VAE).,
The original data and the artificially augmented data were measured in a DNN-based prediction task.,
The data flows between each step are indicated by arrows.,
Note that the validation process was carried out with the experimental data that were extracted from the recently reported literature.,
"This article proposes a novel data self-augmentation strategy for expanding the size of semiconductor device datasets used for DNN-based modeling tasks, without requiring calibration of TCAD tools.",
"Although several studies have made good attempts in using autoencoders as an unsupervised ML strategy to improve the classification tasks in the semiconductor indus- try [16], [17], using variational autoencoder (VAE) in boosting TCAD’s performance of semiconductor devices is still unclear to the best of our knowledge.",
"Our approach is based on the use of VAE [18], a type of generative model that can learn the underlying probability distribution of the dataset, and then generate new, synthetic data samples that are statistically similar to the original data.",
"Specifically, we apply the data from our proposed strategy to the DNN-based modeling task— predicting the ohmic contacts of GaN devices, a challenging problem that cannot be solved directly using TCAD.",
"To validate the effectiveness of our approach, we used experimental data extracted from the literature to train the VAE, which was then used to generate augmented data that were combined with the experimental data for training the DNN model (for dataset details, see our previous work [19]).",
"The results demonstrate that our data augmentation strategy significantly reduces the mean absolute error (MAE) of the prediction by up to 70% for AlGaN/GaN devices, when compared with the model using experimental data only.",
This finding highlights the potential of our approach for enhancing the accuracy and robustness of device simulation tools in the semiconductor industry.,
"METHODS FOR DATA AUGMENTATION AND VERIFICATION In this article, we propose a data self-augmentation strategy based on a VAE to improve the performance of ML-based modeling.",
"Generally, ML-based device modeling aims to link device features, such as gate length, drain voltage, and anneal- ing temperature, to performance, such as surface potential, saturation current, and ON/OFF ratio [20], [21], [22].",
"Our proposed data augmentation framework is divided into three main parts: a feature preprocessing module (sectors I and II), a data augmentation module (sector III), and an ML-based modeling module (sector IV), as shown in Fig.",
"The first step of modeling is to extract parameter data, i.e., device features, from experimental results, as illustrated in sector I.",
"The extracted data are then transformed into vectors by the feature preprocessing module, as shown in sector II.",
The experimental data are split into a training set (67% of the data) and a test set (33% of the data) before establishing the VAE model.,
Each dataset’s data points were chosen randomly.,
"The training set is used to build the generative models for augmentation, as depicted in sector III.",
The generated artificial data are combined with the training set and fed into the DNN-based model for semiconductor device performance modeling (sector IV).,
The test set (only contains experimental data) is used to evaluate the generalization ability of the DNN and the augmentation model.,
It is noteworthy that we adopted a linear unit structure of VAE with minimal parameters in this study.,
"By doing so, only the crucial parameters of the real distribution are learned through backpropagation, akin to principal component analysis.",
"This approach allows us to partially relax the size requirements of the training dataset, but meanwhile, it may introduce inaccuracy, which requires further study and is not the scope of this study.",
"The steps of the proposed data augmentation strategy and verification are described as follows, with technical details provided in the subsequent subsections.",
1) Extract feature data from experiments and split the data into a test set and a training set.,
2) Train the VAE-based model using the training set.,
3) Generate artificial data and combine it with the training set.,
4) Train the corresponding DNN models using the com- bined dataset.,
5) Test the DNN model using the test set and calculate the MAE.,
6) Repeat steps 2)–5) five times and calculate the average MAE for further discussion.,
"Feature Preprocessing To minimize the influence of missing data points, all experimental data were filtered by a “data cleaning” process, this deletes whole vectors if any vacancy exists.",
"Then, the WANG et al.",
: IMPROVING SEMICONDUCTOR DEVICE MODELING FOR EDA BY ML TECHNIQUES 265 Fig.,
Schematic procedure of artificial data generation.,
"The typical VAE model was adopted for generating the features, while for generating the corresponding labels, the nearest neighbor algorithm was used.",
"The green frames indicate the process of data transformation in latent spaces, while the frames with gray backgrounds show the structure of the augmentation model.",
experimental data were divided into two parts for the next step: the numerical part and the text-based part.,
"These two parts are handled by different processes: for the text-based input, such as the name of material layers, the data were then transformed into a numerical vector using one-hot encoding [19], [23].",
"For the numerical input, the item x of the data was directly standardized by z-score to ensure that the numerical input was centered to 0 with a standard deviation of 1, following the equation given next: z = x −µx σx (1) where µx is the mean of x and σx is the standard deviation of x.",
"Note that, for simplicity, the ohmic value data described in the following sections are all standardized values, not original values.",
B. Generative Model The primary goal of a generative model is to learn the joint probability distribution of a given dataset through unsuper- vised learning.,
This enables successful data augmentation from experimental data by interpolating variations into the trained generative model (shown in panels I and II of Fig.,
"In this study, we used a two-component generator consisting of an artificial feature generator and an artificial label generator.",
"To overcome the challenge of insufficient training data in the augmentation task for the artificial feature generator, we utilized the VAE, which is a simple yet powerful deep generative model.",
"In addition, we applied the Kth near- est neighbor (KNN) regressor to generate the corresponding labels.",
This approach offers an effective solution to the prob- lem of insufficient training data in data augmentation and represents a significant contribution to the field of generative modeling.,
1) Artificial Feature Generator: The artificial feature genera- tor is realized by the VAE.,
This is a variant of the automatic encoder combining variational inference with a conventional autoencoder framework.,
"Thus, the VAE consists of two parts: an encoder and a decoder.",
"The encoding–decoding process efficiently realizes dimensionality reduction, which empha- sizes the preferred features of the input and suppresses the less-important features to minimize interference [24], [25].",
"The encoder encodes, denoted as Enc(x), input x into a latent representation z with a parameter θ, which is denoted by qθ(z | x).",
"The decoder reconstructs, denoted as Dec(z), the data distribution ˜x from the given z, which is depicted as follows: z = Enc(x) ∼qθ(z | x) (2) ˜x = Dec(z) ∼p(x | z).",
"(3) Given a dataset x = {x1, .",
", xN}, where N is the number of samples, the target of the generative model is to maximize the probability p(X) p(X) = N X i=1 p(xi | z)p(z) (4) in which p(z) is the probability distribution of the encoded latent representations, which is unknown.",
The purpose of the VAE is to infer p(z) from the ideal posterior prob- ability p(z | x).,
It could be replaced by a simpler normal distribution qθ(z | x).,
"Then, the problem is converted into minimizing the difference between those two distributions 266 IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
"1, JANUARY 2024 using Kullback–Leibler (KL) divergence [26] KL(qθ(z | x)||p(z | x)) = E  log qθ(z | x) −log p(x | z) −log p(z)  .",
(5) By applying the following function named evidence lower bound (ELBO) [27] ELBO(θ) = −E  log p(x | z) + log p(z) −log qθ(z | x)  = E  log p(x | z)  −KL(qθ(z | x)||p(z)).,
(6) Equation (4) can be rewritten as a log-likelihood function log p(x) = KL(qθ(z | x)||p(z | x)) + ELBO(θ).,
"(7) Note that x is known, log p(x) is constant.",
"Besides, the KL divergence is always greater than or equal to zero according to Jensen’s inequality [28].",
"Therefore, minimizing the KL divergence is equivalent to maximizing ELBO(θ).",
"Since no datapoint shares its latent z with another datapoint in VAE, we can write this function for a single datapoint as follows: ELBO(θ)i = E  log p(xi | zi)  −KL(qθ(zi | xi)||p(zi)).",
"(8) Thus, the training target of the VAE is actually approaching the maximum of the ELBO function shown next, which is also labeled in Fig.",
2 between panels I and III arg max θ ELBO(θ) = X i ELBO(θ)i.,
"(9) According to [25], p(zi) = N(0, I) is assumed as a standard normal distribution, while qθ(zi | xi) = N(µ(xi), 6(xi)), where N represents a normal distribution, I is an identity matrix, and µ and 6 are arbitrary deterministic functions that can be learned from data.",
"Thus, this can be explicitly expressed by µ and 6 as follows: X i,k  xk i −˜xk i 2 + 1 2  6(xi)k + µ2(xi)k −1 −log 6(xi)k  (10) where ˜xk i is the kth element of reconstructed data vectors ˜xi and µ(xi)k and 6(xi)k denote the kth element of vectors µ(xi) and 6(xi), respectively.",
"Considering the encoder and decoder processes, this ˜xi can be formulated as the following form: ˜xi = Dec(Enc(xi)).",
"(11) In this proposed strategy, we used a linear neural network to construct the encoder of the VAE model by which the input xi is encoded into the latent representation zi.",
"Thereafter, another linear neural network can be constructed for decoding zi into ˜xi.",
"Mathematically, for Enc(xi), we have Enc(xi) = qθ(zi | xi) ∼N  ˜µ(xi), ˜6(xi)  .",
"(12) According to (2) and (3), ˜µ(xi) and ˜6(xi) should therefore be encoded through the following forms by the linear neural network ( ˜µ(xi) = w2σ(xiw1) ˜6(xi) = w3σ(xiw1) (13) where w1 is the weight of the first layer in the encoder neural network, w2 and w3 are the weight of the second layer in the encoder neural network, and σ is the nonlinear activation function.",
The latent representation zi can then be expressed as follows: zi = ˜µ(xi) + ˜6(xi).,
"(14) Similarly, for Dec(zi), we have Dec(zi) = p(xi | zi) ∼N(µ(xi), 6(xi)).",
"(15) According to (2) and (13), µ(xi) and 6(xi) should be decoded as the following form by the neural network: µ(xi) = w5σ(ziw4) = w5σ((w2σ(xiw1) + w3σ(xiw1))w4) (16) 6(xi) = w6σ(ziw4) = w6σ((w2σ(xiw1) + w3σ(xiw1))w4) (17) where w4 is the weight of the first layer in the decoder neural network, w5 and w6 are the weight of second layer in the decoder neural network, and the activation function σ here is the same activation function as that of the encoder neural network.",
"The training target function arg maxθ ELBO(θ) for our model was obtained by combining (10), (11), (16), and (17) X i,k    xk i −Dec  Enc  xk i 2 +1 2    w6σ  w2σ(xiw1) +w3σ(xiw1)  w4  k +  w5σ  w2σ(xiw1) +w3σ(xiw1)  w4  k 2 −1 −log  w6σ  w2σ(xiw1) +w3σ(xiw1)  w4  k     .",
(18) The VAE is a neural network used for generating artificial data that have a similar distribution to a given dataset.,
The training process involves iterating the maximization of ELBO: arg maxθ ELBO(θ) [as expressed in (18)] until the network produces appropriate weights wi for the training dataset.,
"Dur- ing this process, the weights record information about the data distribution.",
This allows the VAE to generate artificial data (represented by ˜zi) with a distribution similar to the original dataset x [as shown in (16) and (17)].,
This process ensures that the VAE can successfully generate artificial data that closely match the original dataset.,
"Once trained, the artificial feature generator can create artificial device features, and the artificial label generator can generate artificial device performance.",
"For a better understanding of the training process and usage of the VAE model, refer to Fig.",
"2) Artificial Label Generator: The artificial label generator is a critical component of the data generation process, respon- sible for assigning device performances to the data generated by the artificial feature generator.",
"Since the artificial and real features occupy the same space, the artificial performance data can be assumed to be in the same neighborhood as the real performance data.",
"To generate performance values for artificial features, the nearest neighbor algorithm is employed WANG et al.",
: IMPROVING SEMICONDUCTOR DEVICE MODELING FOR EDA BY ML TECHNIQUES 267 to analyze the real performance data.,
This algorithm compares the artificial features with the real feature dataset to find the most similar real features and then assigns the corresponding performance value to the artificial features.,
"In this way, the artificial label generator ensures that the generated artificial device performance closely matches the performance values of real devices.",
The detailed procedure for using the nearest neighbor algorithm to assign performance values to artificial features is explained as follows.,
"First, we calculate all the distances between artificial fea- tures and real features by an Euclidean metric [29] d  `xi, x j  = M X k=1 (`xik −x jk) 2 (19) where d(`xi, x j) denotes the distance between the ith artificial feature `xi and the jth real feature x j and M is the dimension of x j.",
"Then, we introduce this distance into the performance data space.",
"The nearest neighbor algorithm is applied to calculate the outputs of artificial features using the distance of features as follows: `yi = PS r=1 1 d(`xi,xr) yr PS r=1 1 d(`xi,xr) (20) where `yi is the generated output of the ith artificial feature `xi, S denotes the top S nearest neighbors to `xi, and yr is the real output value of the rth nearest real feature.",
The artificial label generator first calculates the distance between an artificial vector and its nearest real data vectors.,
"Then, the generator creates the corresponding artificial label by evaluating the real labels of its nearest neighbors (regarding the calculated distances) as expressed in (20).",
"Note that the VAE models for AlGaN/GaN, n-GaN, and p-GaN data were trained separately in this article, considering the intrinsic differences in the materials.",
C. Methods for Verification A DNN-based regression model is adopted in this article to verify the artificial augmented data (Fig.,
"1, sector IV).",
"The model can be described in the following form: ( y = σ(Hwh + bh) H = σ(Xwi + bi) (21) where y ∈Rm are the measured values, wh ∈Rn are the weight of the hidden layer, bh ∈Rn is the bias of hidden layer, wi ∈Rt are the weight of input layers, bi ∈Rt is the bias of input layer, X ∈Rm×n is the data matrix combined with the real samples and artificial samples, and σ is the nonlinear activation function.",
"This model contains four layers, including an input layer for inputting the real features or artificial features, two hidden layers, and an output layer.",
The input layer has the same number of neurons as the length of those device features.,
The output layer contains only one neuron for predicting the device’s electric performance.,
The two hidden layers that have Fig.,
Typical ohmic contact structure of GaN device and the simplified fabrication process flow.,
more than 50 units are designed to fit the complex relationships between device features and their performance.,
"We used an experimental dataset of metal–semiconductor ohmic contact resistance, extracted from fabricated n-type GaN, p-type GaN, and AlGaN/GaN heterojunction devices (the dataset details can be found in our previous work [19]).",
The device structure and the process are represented in Fig.,
"The dataset includes resistance values and their corresponding fabrication recipes, such as metal layers, annealing temperature, annealing time, and annealing gas.",
This dataset is ideal for verifying our proposed self- augmentation model.,
"This is because the dataset has few data points, a complicated fabrication process (also consuming significant foundry time), and low fabrication recipe variation and is difficult to simulate in TCAD.",
"Before training the DNN for the prediction task, we gen- erated ten different scales of augmentations individually.",
These ranged from the same number of data points as the training dataset to ten times more.,
We then trained a DNN-based network for ohmic resistance prediction using a batch of random combinations of experimental and arti- ficial data.,
"We then tested its performance against the test data, which exclusively consisted of experimental data.",
"Each DNN model was trained using augmented data at differ- ent scales, and we performed this process five times to ensure the accuracy.",
"For comparison, we used Gaussian noise augmentation as a control group.",
These noise-based data are generated by adding noise from the standard Gaussian distribution to the experimental data.,
"Note that the DNN models for AlGaN/GaN, n-GaN, and p-GaN data were trained separately.",
"To evaluate the performance of the augmented dataset and the model, we measured the MAE of the prediction using a well-trained DNN-based ML model.",
"The test set is from experimental data only, so we believe that benchmarking the final MAE provides a reasonable strategy for assessing the VAE model’s performance.",
"Validation of results could be improved by comparison to new experimental data, which will be the focus of future work.",
"To avoid bias toward the augmented data, we only used real experimental data for testing.",
"There is, however, no standard index or figure of merit to evaluate the artificially generated data.",
"Therefore, we used three steps to evaluate our augmentation strategy as follows.",
1) We mapped the augmented data into a lower dimension with experimental data for intuitive visualization.,
"268 IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
"1, JANUARY 2024 Fig.",
"Visualization of experimental data and augmented data, projected to a 2-D plane through a UMAP algorithm, for the resistance value of ohmic contacts on (a) AlGaN/GaN heterojunction, (b) n-type GaN, and (c) p-type GaN substrates.",
All data for training the VAE were extracted from experiments (for dataset details see our previous work [19]).,
"Pearson r of the predicted and experimental ohmic resistance values of (a) AlGaN/GaN, (b) n-type GaN, and (c) p-type GaN substrates.",
The augmented data are ten times larger than the experimental data.,
"Note: fitting lines are from individual prediction process, which is not averaged.",
"For the details of the mean values, kindly refer to Fig.",
2) We analyzed the Pearson r to evaluate the similarity between the prediction results from the augmented data and the experimental data.,
3) We evaluated the MAE of the prediction task using augmented data.,
RESULTS AND DISCUSSION A. Augmented Data Visualization Fig.,
4 shows the kernel density plots that organize the dis- tributions of the generated data with the real data.,
The uniform manifold approximation and projection (UMAP) algorithm was used to project the data from a high-dimensional parame- ter space into a 2-D plane [30].,
"The artificially generated data are shown as a density color map, and the real data are shown as circles.",
It is observed that the artificially generated data are located in proximity to the real data.,
This indicates that it carries realistic information similar to the real data.,
"In addition, in Fig.",
"4, the high-concentration positions of the real data do not largely overlap the augmented data.",
This implies that the augmented data do not repeat the real data’s pattern but (to some extent) compensates for the insufficient real data.,
"The augmented data therefore can extend the occupied area in the data space, with a deliberate pattern, providing more comprehensive sampling points for ML-based tasks.",
"Furthermore, it can be observed that the kernel density of the augmented data is not condensed altogether in a small range but is dispersed over a large region.",
This observation suggests that the established augmentation model has suc- cessfully extracted the realistic patterns from the experimental data.,
It also suggests that it has reasonably generated a more comprehensive artificial pattern containing sufficient additional realistic information.,
"B. Pearson r of Augmented Data To investigate and evaluate the proposed augmentation model, we plotted the ohmic resistance values obtained from predictions and experiments in Fig.",
These predicted values were generated by the DNN-based prediction model.,
We dis- cuss the evaluation of this model and its training process in the next section.,
Note that in Fig.,
"5, both the VAE-augmented WANG et al.",
: IMPROVING SEMICONDUCTOR DEVICE MODELING FOR EDA BY ML TECHNIQUES 269 Fig.,
"Mean values of Pearson r (reflecting the correlation between the real data and the predicted data) using VAE-based model, noise-based model, and duplicating the experimental data (the test set) with different augmentation scales for (a) AlGaN/GaN heterojunction, (b) n-type GaN, and (c) p-type GaN substrates.",
Note: each data point is averaged from fivefold cross validation.,
"MAE of the prediction after five times testing, using pure experimental data, and different sizes of VAE- and noise-based data on (a) AlGaN/GaN, (b) n-type GaN, and (c) p-type GaN substrates.",
Each bar represents five repeated testing processes.,
and Gaussian noise augmented datasets have ten times more data points than the training data.,
"5, we observe that for all three types of sub- strates, the Pearson correlation coefficient of the model using VAE-augmented data is higher than that using Gaussian noise augmented data.",
The Gaussian noise can even result in a negative slope of the correlation sometimes (see n-GaN data).,
"This is because augmenting the data with Gaussian noise may introduce patterns that are opposite to the actual data, and these patterns are subsequently learned by the ANN.",
"Moreover, the Pearson correlation coefficient of the augmented data in all three groups is similar to real experiment data, indicating that the generated augmentation data provide sufficient information similar to the real data.",
The Pearson correlation coefficient of VAE-based data and noise-based data versus the augmentation scales (in multi- ples) is shown in Fig.,
"6 (mean values), where all but one point in the p-GaN group have higher r-index values for VAE-based data than for noise-based data.",
We observe a trend in Fig.,
"6(b) and (c), where the Pearson correlation coefficient decreases with an increase in noise-based data points.",
"This indicates that more noise-based data points lead to lower relativity, as the noise dilutes the data pool and fades the realistic information.",
This trend is not apparent in Fig.,
"6(a), possibly due to the relatively sophisticated experimental data pool of the AlGaN/GaN group, where the augmented noise cannot significantly alter the data pattern.",
"On the contrary to the noise-based data, the Pearson correlation coefficient of VAE-based data remains stable during ten times of multipli- cation, indicating that the proposed augmentation model does not inject any negative influence on the data pattern during augmentation.",
"Thus, the data pool is not observed to be diluted.",
C. MAE of Prediction Task Fig.,
7 shows the MAE of the VAE- and noise-based augmented data.,
The VAE-based model outperforms the noise-based model in all three device groups.,
"For n-type GaN and p-type GaN, the MAE of the VAE-based model decreases initially and then flattens as the size of the augmented data increases.",
"The MAE level of the noise-based model remains the same as the pure experimental data (without any augmenta- tion, gray dashed lines) except for showing small fluctuations.",
This difference in behavior is attributed to the different levels of contribution of additional realistic information provided by VAE- and noise-based data.,
"The VAE-based data successfully exhibit the realistic data pattern, leading to a decrease in 270 IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
"1, JANUARY 2024 Fig.",
MAE improvement in test process of different augmentation scales.,
"This pattern does, however, feature an accuracy limit (or systematic error), which eventually flattens the MAE at large augmentation scales.",
Increasing the amount of experimental data used to train the VAE would reduce this error further.,
"Comparatively, the noise-based model can only contribute a random pattern to the DNN model, resulting in a shift in MAE.",
"Notably, the error bar of the MAE of the noise-based data spans a huge range, whereas the VAE-based data provide a more confined MAE distribution in each multiple.",
This suggests that the augmented data from the VAE-based model are more patterned and less random than the noise-based data.,
"Interestingly, in Fig.",
"7(a), the MAE of the VAE-based data does not show the same trend as its counterparts in Fig.",
7(b) and (c).,
"Instead, it exhibits similar features to the noise-based data, although the mean MAE remains lower than the noise group.",
"The reason for this could be that the size of the exper- imental data in this group is larger than in the other groups, and the augmentation model is not robust enough to extract sufficient patterns from such a large dataset.",
"Alternatively, the augmentation model may only be able to extract partial information from the training data.",
"This could especially be the case when the experimental data of this group are following several different patterns, as can be seen in Fig.",
4(a) where the circles are more dispersed than in the other two groups.,
8 provides more intuitive results of the MAE improve- ment provided by the augmentation.,
"The augmented data significantly improve the prediction performance in the n-GaN and p-GaN groups, with an improvement of over 70%.",
"Although the proposed augmentation significantly improves the MAE of the modeling, the factors that contribute most to the enhancement are still not clear and require further explo- ration.",
It is understood that the size of the experimental data will strongly influence the modeling results.,
"Also, this study suggests that the nature of the distribution of the experimental data may play a key role in this regard.",
"Moreover, the source of the performance difference between AlGaN/GaN-type and other types of data could also be attributed to the features of the experimental data distribution, and this warrants further investigation.",
"Furthermore, exploring the in-depth mechanisms behind the superior performance of the VAE is indeed an impor- tant aspect.",
"However, this study’s aim is to showcase the advantages of generating more data at a lower cost, as it reduces the reliance on the limited old dataset.",
"In this context, achieving higher accuracy using lower cost data already fulfills the expectations of this study.",
CONCLUSION We have proposed and tested a VAE-based data self- augmentation strategy to relieve the contradiction between the accuracy and the insufficient training data in ML-based semiconductor device modeling.,
"In this strategy, no additional TCAD simulation is required and only a few experimental data points are needed for functionality.",
"The testing suggests that the established augmentation model could successfully extract realistic patterns from the experimental data, leading to a set of high-quality augmented data that were able to be seamlessly fed into the DNN model used.",
"As a result, this strategy could significantly improve the performance of the DNN model, where a maximum of more than a 70% drop of MAE was obtained.",
We therefore believe that this strategy could benefit the next-generation EDA simulations and modeling in the semiconductor industry.,
ACKNOWLEDGMENT Zeheng Wang would like to thank the support for the preliminary results of this work from Prof. Arne Laucht and the University of New South Wales.,
"REFERENCES [1] D. Macmillen, R. Camposano, D. Hill, and T. W. Williams, “An indus- trial view of electronic design automation,” IEEE Trans.",
Comput.-Aided Design Integr.,
"Circuits Syst., vol.",
"1428–1448, Dec. 2000, doi: 10.1109/43.898825.",
"[2] T. Ma, V. Moroz, R. Borges, and L. Smith, “TCAD: Present state and future challenges,” in IEDM Tech.",
"Dig., San Francisco, CA, USA, Dec. 2010, pp.",
"15.3.1–15.3.4, doi: 10.1109/IEDM.2010.5703367.",
"[3] S. Selberherr, Analysis and Simulation of Semiconductor Devices.",
"Vienna, Austria: Springer, 1984.",
"Available: https://www.google.com.au/books/edition/Analysis_and_Simulation_of_ Semiconductor/EE4HlRZTYi4C?hl=en [4] Y.-C. Wu and Y.-R. Jhan, “Introduction of synopsys sentaurus TCAD simulation,” in 3D TCAD Simulation for CMOS Nanoeletronic Devices.",
"Singapore: Springer, 2018, pp.",
"1–17, doi: 10.1007/978-981-10-3066- 6_1.",
"[5] K. Mehta, S. S. Raju, M. Xiao, B. Wang, Y. Zhang, and H. Y. Wong, “Improvement of TCAD augmented machine learn- ing using autoencoder for semiconductor variation identification and inverse design,” IEEE Access, vol.",
"143519–143529, 2020, doi: 10.1109/ACCESS.2020.3014470.",
"[6] H. Carrillo-Nuñez, N. Dimitrova, A. Asenov, and V. Georgiev, “Machine learning approach for predicting the effect of statistical variability in Si junctionless nanowire transistors,” IEEE Electron Device Lett., vol.",
"1366–1369, Sep. 2019, doi: 10.1109/LED.2019.2931839.",
"[7] N. Hari, M. Ahsan, S. Ramasamy, P. Sanjeevikumar, A. Albarbar, and F. Blaabjerg, “Gallium nitride power electronic devices modeling using machine learning,” IEEE Access, vol.",
"119654–119667, 2020, doi: 10.1109/ACCESS.2020.3005457.",
WANG et al.,
": IMPROVING SEMICONDUCTOR DEVICE MODELING FOR EDA BY ML TECHNIQUES 271 [8] A.-D. Huang, Z. Zhong, W. Wu, and Y.-X.",
"Guo, “An artifi- cial neural network-based electrothermal model for GaN HEMTs with dynamic trapping effects consideration,” IEEE Trans.",
"Theory Techn., vol.",
"2519–2528, Aug. 2016, doi: 10.1109/TMTT.2016.2586055.",
"[9] M. Usman, Y.",
"Z. Wong, C. D. Hill, and L. C. L. Hollenberg, “Frame- work for atomic-level characterisation of quantum computer arrays by machine learning,” npj Comput.",
"Mater., vol.",
"1, p. 19, Mar.",
"2020, doi: 10.1038/s41524-020-0282-0.",
[10] K. Mehta and H.-Y.,
"Wong, “Prediction of FinFET current-voltage and capacitance-voltage curves using machine learning with autoencoder,” IEEE Electron Device Lett., vol.",
"136–139, Feb. 2021, doi: 10.1109/LED.2020.3045064.",
"[11] L. Zhang and M. Chan, “Artificial neural network design for compact modeling of generic transistors,” J. Comput.",
"Electron., vol.",
"825–832, Sep. 2017, doi: 10.1007/s10825-017-0984-9.",
"[12] Y. S. Bankapalli and H. Y. Wong, “TCAD augmented machine learning for semiconductor device failure troubleshooting and reverse engineer- ing,” in Proc.",
"Simulation Semiconductor Processes Devices (SISPAD), Sep. 2019, pp.",
"1–4, doi: 10.1109/SISPAD.2019.8870467.",
"[13] S. S. Raju, B. Wang, K. Mehta, M. Xiao, Y. Zhang, and H.-Y.",
"Wong, “Application of noise to avoid overfitting in TCAD augmented machine learning,” in Proc.",
"Semiconductor Pro- cesses Devices (SISPAD), Sep. 2020, pp.",
"351–354, doi: 10.23919/SIS- PAD49475.2020.9241654.",
"[14] H. Dhillon, K. Mehta, M. Xiao, B. Wang, Y. Zhang, and H. Y. Wong, “TCAD-augmented machine learning with and without domain exper- tise,” IEEE Trans.",
"Electron Devices, vol.",
"5498–5503, Nov. 2021, doi: 10.1109/TED.2021.3073378.",
"[15] K. Sheelvardhan, S. Guglani, M. Ehteshamuddin, S. Roy, and A. Dasgupta, “Machine learning augmented compact modeling for simultaneous improvement in computational speed and accuracy,” IEEE Trans.",
"Electron Devices, early access, Mar.",
"9, 2023, doi: 10.1109/TED.2023.3251296.",
[16] S.-K.-S.,
"Hsu, C.-H. Jen, K.-L. Chen, and L.-T. Juan, “Defec- tive wafer detection using a denoising autoencoder for semiconductor manufacturing processes,” Adv.",
"Informat., vol.",
"46, Oct. 2020, Art.",
"101166, doi: 10.1016/j.aei.2020.101166.",
"Liao, C.-Y.",
"Chen, W.-P. Tsai, H.-T. Chen, Y.-T. Wu, and S.-C. Chang, “Anomaly detection for semiconductor tools using stacked autoencoder learning,” in Proc.",
Semiconductor Manuf.,
"(ISSM), Dec. 2018, pp.",
"1–4, doi: 10.1109/ISSM.2018.865 1179.",
"[18] D. P. Kingma and M. Welling, “An introduction to variational autoen- coders,” Found.",
Trends Mach.,
"Learn., vol.",
"307–392, 2019, doi: 10.1561/2200000056.",
"[19] Z. Wang, L. Li, and Y. Yao, “A machine learning-assisted model for GaN ohmic contacts regarding the fabrication processes,” IEEE Trans.",
"Electron Devices, vol.",
"2212–2219, May 2021, doi: 10.1109/TED.2021.3063213.",
"[20] J. Xu et al., “A review on AI for smart manufacturing: Deep learning challenges and solutions,” Appl.",
"16, p. 8239, Aug. 2022, doi: 10.3390/app12168239.",
"[21] E. Afacan, N. Lourenço, R. Martins, and G. Dündar, “Review: Machine learning techniques in analog/RF integrated circuit design, synthesis, layout, and test,” Integration, vol.",
"113–130, Mar.",
"2021, doi: 10.1016/j.vlsi.2020.11.006.",
"[22] T.-L. Wu and S. B. Kutub, “Machine learning-based statistical approach to analyze process dependencies on threshold voltage in recessed gate AlGaN/GaN MIS-HEMTs,” IEEE Trans.",
"Electron Devices, vol.",
"5448–5453, Dec. 2020, doi: 10.1109/TED.2020.3032634.",
"[23] A. V. Uriarte-Arcia, I. López-Yáñez, and C. Yáñez-Márquez, “One-hot vector hybrid associative classifier for medical data classification,” PLoS ONE, vol.",
"[24] Y. Wang, H. Yao, and S. Zhao, “Auto-encoder based dimensionality reduction,” Neurocomputing, vol.",
"232–242, Apr.",
"2016, doi: 10.1016/j.neucom.2015.08.104.",
"[25] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” 2013, arXiv:1312.6114.",
"[26] F. Perez-Cruz, “Kullback–Leibler divergence estimation of continu- ous distributions,” in Proc.",
"Theory, Jul.",
"1666–1670, doi: 10.1109/ISIT.2008.4595271.",
"[27] M. D. Hoffman and M. J. Johnson, “ELBO surgery: Yet another way to carve up the variational evidence lower bound,” in Proc.",
Workshop Adv.,
"Bayesian Inference, NIPS, 2016, p. 2.",
"[28] J. J. Ruel and M. P. Ayres, “Jensen’s inequality predicts effects of environmental variation,” Trends Ecol.",
"Evol., vol.",
"361–366, 1999, doi: 10.1016/S0169-5347(99)01664-X.",
"[29] L. Wang, Y. Zhang, and J. Feng, “On the Euclidean distance of images,” IEEE Trans.",
Pattern Anal.,
"Intell., vol.",
"1334–1339, Aug. 2005, doi: 10.1109/TPAMI.2005.165.",
"[30] R. M. Parra-Hernández, J. I. Posada-Quintero, O. Acevedo-Charry, and H. F. Posada-Quintero, “Uniform manifold approximation and projection for clustering taxa through vocalizations in a neotropical passerine (rough-legged tyrannulet, Phyllomyias burmeisteri),” Animals, vol.",
"8, p. 1406, Aug. 2020, doi: 10.3390/ani10081406.",
"Biomedical Signal Processing and Control 46 (2018) 67–75 Contents lists available at ScienceDirect Biomedical Signal Processing and Control journal homepage: www.elsevier.com/locate/bspc Stacked Autoencoder for classiﬁcation of glioma grade III and grade IV Supriya Patil a,b,∗, Gourish Naik b, Radhakrishna Pai a, Rajendra Gad b a Padre Conceicao College of Engineering, Goa, India b Electronics Dept., Goa University, Goa, India a r t i c l e i n f o Article history: Received 4 July 2017 Received in revised form 12 October 2017 Accepted 10 July 2018 Keywords: Resilient Back Propagation algorithm Conjugate Gradient Back Propagation with Fletcher–Reeves Update algorithm Conjugate Gradient Back Propagation with Polak–Ribire Update Conjugate Gradient Back Propagation with Powell–Beale Restarts algorithm Levenberg Marquardt algorithm Stacked Autoencoder algorithm a b s t r a c t Invention of the microarray technology has rendered it possible to inspect the whole genome at once in cancer classiﬁcation.",
"However, in order to curtail the computational complexity and augment the accuracy of cancer classiﬁcation, it is essential to sift the vast microarray data for the informative genes.",
"In this paper, Thresholding and Ratio methods are presented, individually as well as conjointly (hybrid method) to choose optimal gene subset from the microarray data.",
"Moreover, Discrete Wavelet Transform (DWT) is deployed to pare the size of microarray data still further.",
The classiﬁcation is accomplished by using various neural network algorithms and Stacked Autoencoder algorithm.,
"The results of classiﬁcation are compared for number of thresholds, ratios, wavelets and classiﬁcation algorithms.",
It is observed that the Stacked Autoencoder network trained by Back Propagation algorithm delivers the best results in terms of classiﬁcation accuracy and number of genes.,
© 2018 Elsevier Ltd. All rights reserved.,
Introduction Cancer develops when certain genes (cancer causing or can- cer suppressing) in the human body start mutating.,
These gene mutations may be hereditary or acquired.,
"As a result of the gene mutations, cells multiply rapidly and cause cancer.",
Knowing the subtype of a cancer plays a vital role in determining the progno- sis and planning the treatment.,
"However, conventional techniques of cancer diagnosis depend largely on the experience and skill of the physician.",
Microarray technology atomizes the process of can- cer identiﬁcation and assists in accurate cancer diagnosis.,
There is enormous number of genes in the microarray data.,
"Besides, the intensity values of these genes if given directly as input to the classi- ﬁer, will increase the computations and hinder the speed of cancer identiﬁcation.",
"In other words, to enhance the speed of accurate cancer classiﬁcation, it is essential that the dimensions of microar- ray data be diminished to the maximum extent.",
This is achieved by ﬁltering the informative genes from microarray data or con- verting the microarray data in different domain or using fusion of these methods [1].,
"To choose the required genes ﬁlter [2–4], wrapper [5–7], ensemble [8–10], hybrid [11–13] and embedded methods [14–16] are used.",
"Wrapper, ensemble, hybrid and embed- ∗Corresponding author at: Electronics Dept., Goa University, Goa, India.",
E-mail address: supriya@pccegoa.org (S. Patil).,
"ded methods are difﬁcult to understand and are computationally more expensive while the ﬁlter methods are advantageous in terms of easy implementation, speed and computational complexity [17].",
Zhang et al.,
[18] implemented distance based feature selection using Bhattacharya distance along with Support Vector Machine (SVM) classiﬁer for colon and leukemia datasets.,
The average mis- classiﬁcation rate obtained is more than 3% with optimum number of genes.,
Tarek et al.,
"[19] compared performance of Backward Elim- ination Hilberts Schmidt Independence Criterion, Extreme Value Distribution and Singular Value Deposition Entropy based gene selection methods.",
"The classiﬁcation is performed using ﬁve dif- ferent ensemble classiﬁcation algorithms for leukemia, colon and breast cancer datasets.",
The average classiﬁcation accuracy achieved is less than 100% for larger number of genes.,
"However, on account of using ensemble approach for classiﬁcation, the method appears to be more complex and computationally expensive.",
[20] compressed the size of microarray datasets using DWT.,
"With approximation as well as detailed coefﬁcients and SVM algorithm, classiﬁcation accuracy of 100% for 100 genes and 93.5% for 250 genes is obtained for leukemia and colon datasets, respectively.",
Abusamra et al.,
"[21] implemented eight different gene selection methods like Information Gain, Twoing Rule, Sum Minority, Gini Index, Sum of Variances, One Dimensional SVM and t-statistics.",
"The classiﬁcation algorithms employed are SVM, K-mean clustering and Random Forest.",
"For GDS1975 and GDS1976 datasets, the maximum classiﬁcation accuracy attained is 94.59%, 90.81%, respectively, with https://doi.org/10.1016/j.bspc.2018.07.002 1746-8094/© 2018 Elsevier Ltd. All rights reserved.",
68 S. Patil et al.,
/ Biomedical Signal Processing and Control 46 (2018) 67–75 twenty genes and Random Forest algorithm.,
Shen et al.,
"[22] com- pared the performance of Naive SVM on GDS1976 dataset, 400 top ranked genes selected by t-statistics along with SVM classiﬁer and simultaneous gene and sample selection by Modiﬁed Particle Swarm Optimization method.",
The maximum classiﬁcation accu- racy of 92.67% is achieved for GDS1976 dataset with 41 genes.,
Many a times the cancer marker genes are used for screening of the cancer but there are evidences of failure of this method [23].,
"Researchers implemented plenteous ways for diminishing the size of the microarray data, however, there are many open prospects for further improvement in terms of achieving 100% classiﬁcation accuracy for less number of genes [17].",
Therefore we are motivated to design a method to obtain 100% classiﬁca- tion accuracy with optimum number of genes.,
"We propose to perform the selection of features using Thresholding method and Ratio method, individually as well as conjointly (hybrid method), while the transformation of the signal is accomplished using DWT.",
"The proposed classiﬁcation algorithms are Resilient Back Propaga- tion algorithm (RPROP), Conjugate Gradient algorithms, Levenberg Marquardt algorithm (LM) and Stacked Autoencoder algorithm (SAEN).",
The Thresholding method and the Stacked Autoencoder provides major contribution towards achieving 100% classiﬁcation accuracy for optimum number of genes.,
The Thresholding method is computationally less expensive and has an ability to diminish the gene subset size by eliminating the genes which have a very large variation in their intensity value that may not be useful for classiﬁ- cation.,
Based on the threshold ranges selected it is possible to obtain more than one gene subset to conﬁrm the result of classiﬁcation.,
"In absence of the Thresholding method in the proposed work, 100% classiﬁcation accuracy may be obtained due to some noisy genes (cross hybridized) with less maximum to minimum gene intensity ratio.",
This result will miss lead the classiﬁer.,
"Further, as compared to the other neural network algorithms such as Error Back Propa- gation (EBP), RPROP, Conjugate Gradient, LM etc., SAEN algorithm tends to outperform as a consequence of pre-training of the Sparse Autoencoder stages and the Softmax layer.",
"The sparsity constraint of Sparse Autoencoder during its unsupervised training leads to sig- niﬁcant reduction in the reconstruction error of the signal which in turn increases the classiﬁcation accuracy [24,25].",
"Materials and methods The details of the databases, system ﬂow chart, different feature selection methods, feature extraction method and various classiﬁ- cation algorithms are explained in the following subsections.",
"Database In the proposed work, classiﬁcation is implemented for glioma grade III/grade IV datasets- GDS1975, GDS1976, GDS1815 and GDS1816 from Gene Expression Omnibus Database [26–28].",
"Orig- inally, Phillip and Freije obtained the datasets using Affymetrix Human Genome U133 Array.",
"GDS1975, GDS1976 datasets contain 26 samples of glioma grade III and 59 samples of glioma grade IV while GDS1815, GDS1816 contain 24 samples of glioma grade III and 76 samples of glioma grade IV.",
"Every dataset contains 22,283 features.",
From every dataset 70% samples are used for training and 15% are used for testing and validation each.,
Before using the datasets the duplicate features are eliminated by averaging and the data is normalized to have zero mean and unit standard deviation.,
The proposed work is implemented using MATLAB R2017a soft- ware.,
System ﬂow chart Fig.,
1 shows the system ﬂow chart for individual microarray data set.,
"Finally, to derive the identical gene subset across glioma datasets, considering optimal gene subsets, the genes common to the glioma datasets are mined.",
The classiﬁcation is implemented using these genes.,
"Gene subset selection For glioma grade III and grade IV classiﬁcation, the various gene selection methods to obtain the required gene subset from the glioma datasets, Thresholding method, Ratio method and combi- nation of two methods are explained in following subsections.",
Thresholding method Thresholding method is a simple method to select the con- sistently varying gene subset for cancer classiﬁcation.,
It helps to narrow down the search space for selection of informative genes by discarding the genes with inconsistent intensity variation within the selected threshold range across the cancer samples.,
"To ﬁlter the informative subset of genes for cancer classiﬁcation, various values of threshold range are selected based on gene intensity variation in a speciﬁc data set.",
"The microarray gene expression datasets for GDS1975, GDS1976, GDS1815 and GDS1816 are generated from 16 bit microarray image.",
"As per the general observation of gene inten- sity values of glioma datasets, gene intensity values predominantly lie in 0 to 2000 range.",
"If threshold ranges are considered as per the standard 1–2–5 sequence (500–1000, 1000–2000, 2000–5000, 5000–10,000, etc.",
"), most of the threshold ranges either include no genes or very less number of genes.",
"Therefore, the threshold ranges THD1 (500, 2000), THD2 (2000, 10,000) and THD3 (10,000, 100,000) are selected as a result of combination of two consecutive thresh- old ranges till 10,000 and thereafter from 10,000–100,000.",
The gene intensity values below 500 are not considered as it mostly includes noise.,
"For each dataset, the genes with intensity value within these threshold ranges for both the classes of glioma are extracted.",
It gives three subsets of genes for every dataset that are used for classiﬁcation of glioma grade III and grade IV.,
Ratio method Ratio method limits the search space for the selection of infor- mative genes by eliminating the genes with inconsistent intensity variation across the cancer samples.,
"In Ratio method, for every gene in both cancer classes, the ratio of maximum to mini- mum intensity is calculated.",
"For GDS1975, GDS1976, GDS1815 and GDS1816 datasets various ratios considered are ratio <= 4, ratio <= 3.5, ratio <= 3, ratio <= 2.5.",
"For each dataset, the genes hav- ing intensity value within the chosen ratio for both the classes of glioma are extracted and used for classiﬁcation.",
Fusion of Thresholding method and Ratio method The fusion of Threshold and Ratio method helps us to obtain the subset of genes with less ratio within the speciﬁc range of thresh- old.,
"For GDS1975, GDS1976, GDS1815 and GDS1816 datasets the threshold range and ratio that gives 100% classiﬁcation accuracy almost by every algorithm are selected.",
"For each of the dataset, the subset of genes that is common to the chosen thresholding range and the ratio are mined from both the classes of glioma and consid- ered for classiﬁcation.",
"To get the optimal subset of genes, resultant genes are ﬁltered depending on the difference in the average inten- sity of the genes for both the classes of glioma.",
S. Patil et al.,
/ Biomedical Signal Processing and Control 46 (2018) 67–75 69 Fig.,
System ﬂow chart.,
"Feature extraction The feature extraction is implemented using the DWT due to its ability to provide multiresolution analysis, localized time and frequency information while dealing with stationary and non- stationary signals.",
"Due to higher energy compaction of DWT, less number of wavelet coefﬁcients are required for classiﬁcation.",
"Moreover, availability of the several of types of wavelets like Haar, Daubechies, Coiﬂet, Symlet, Bio-orthogonal, Meyer, Mexican Hat etc.",
offer more ﬂexibility for the selection of the wavelet basis func- tion and ease of comparison.,
"Hence, it can be safely concluded that, DWT performs best among all other feature extraction methods.",
"In the case of DWT, a time scale representation of the digital signal is computed using digital ﬁltering techniques.",
The DWT of a signal is calculated by successive high pass and low pass ﬁltering of the discrete time-domain signal.,
"As a result of dyadic decimation of ﬁltered data at each level, detailed and approximation coefﬁcients are obtained.",
"The approximation coefﬁcients qj(k) and the detailed coefﬁ- cients pj(k) are given as below: qj(k) = 2k+N−1  m−2k r(m − 2k)qj+1(m), (1) pj(k) = 2k+N−1  m−2k s(m − 2k)pj+1(m), (2) where s(n) = impulse response of high pass ﬁlter; r(n) = impulse response of low pass ﬁlter; k = parameter related to time shift; j = decomposition level; N = number of wavelet coefﬁcients.",
Approximation coefﬁcients constitute the low frequency part of the signal and the detailed coefﬁcients constitute the high fre- quency part of the signal.,
"Either the approximation coefﬁcients or the detailed coefﬁcients or both can be used for the purpose of classiﬁcation [20,29].",
"For proposed work, the approximation coefﬁ- cients obtained by using Db2, Db4, Sym2, Sym4, Bior1.3 and Bior2.4 wavelets are used for classiﬁcation.",
Classiﬁcation algorithms Artiﬁcial neural network processes information in parallel and it learns by examples.,
This distinctive characteristic makes the neural network appealing for solving problems with non-linear relation- ship between the input and the output.,
For a speciﬁc application the multi-layer neural network can be trained using number of exam- ples.,
"In the present work the classiﬁcation is implemented using RPROP, Conjugate Gradient, LM and SAEN algorithm as explained in the following subsections.",
Error Back Propagation algorithm One of the most commonly used classiﬁcation algorithm for multilayer perceptron is EBP algorithm [30].,
The ﬂow chart for EBP algorithm is as shown in Fig.,
The EBP algorithm makes the individual weight change pro- portional to the slope of error curve.,
"The slope of error curve is proportional to the learning constant, difference between input and output and the derivative of the output of corresponding neuron.",
"The equation for the individual weight update of the output layer neuron is given as, w′ kj = wkj + (ek − yk)y′ kxj, (3) where wkj′ = modiﬁed weight that connects output of jth neuron in the hidden layer to kth neuron in output layer; wkj = weight that connects output of jth neuron in hidden layer to kth neuron in the output layer;  = learning constant; ek = expected output of the neu- ron; yk = actual output of the neuron; y′ k = differentiation of actual output of the neuron; xj = input to the neuron.",
"For larger inputs, as the actual output of neuron increases, derivative of the output drops off.",
"As a consequence of reduction in weight change, the classiﬁcation accuracy gets affected with 70 S. Patil et al.",
/ Biomedical Signal Processing and Control 46 (2018) 67–75 Fig.,
Flow chart for Error Back Propagation algorithm.,
increased difference between input and output.,
"However, to avoid the effect of the magnitude of gradient on the weight update, RPROP algorithm can be used [31,32].",
"Resilient Back Propagation algorithm For weight update, EBP algorithm considers the magnitude of the gradient of the error while, RPROP algorithm considers the sign of the gradient of the error.",
"The size of weight update is increased, if the sign of slope of the error curve in two consecutive iterations remains same and vice-versa.",
"Further, the weight update is retained if the slope of the error curve becomes zero [31,32].",
"Conjugate Gradient Back Propagation algorithms EBP algorithm performs a linear search, to arrive at the global minimum of error curve.",
The next search direction is orthogonal to the former search direction.,
"In the case of Conjugate Gradient algo- rithms, the new search direction is A-orthogonal to the previous search direction [33].",
It increases the speed of convergence of Con- jugate Gradient algorithms.,
"The new search direction is determined as, a = (yc) + d, (4) where a = new search direction; y = multiplicative factor; c = previous search direction; d = the direction of steepest descent.",
The multiplicative factor ‘y’ is calculated in different ways for various Conjugate Gradient algorithms.,
"Conjugate Gradient Back Propagation with Fletcher–Reeves Update algorithm (CGFR) calcu- lates ‘y’ as given by the equation below [32,34], y = EC/EP, (5) where EC = energy in the current gradient; EP = energy in the previ- ous gradient.",
"Conjugate Gradient Back Propagation with Polak–Ribire Update algorithm (CGPR) calculates ‘y’ as given by the equation below [32,35] y = (EP − EC)/EP.",
"(6) When the number of iterations equal to the number of net- work parameters, Conjugate Gradient algorithms converge.",
"If the algorithms do not converge within the number of iterations equal to the number of neural network parameters, the search direc- tion is reset.",
"In the case of Conjugate Gradient Back Propagation with Powell–Beale Restarts (CGPB) algorithm, the search direction is reset, when there is very little orthogonality left between the current gradient and a previous gradient [32,36,37].",
Levenberg Marquardt algorithm LM algorithm is one of the most competent algorithms for training multilayer perceptron algorithm.,
"It performs like an EBP algorithm near the areas of complex part of the error curves while, near the areas of quadratic part of the error curve it performs like Newton’s algorithm.",
"The weight update rule for LM algorithm is given as [32,38] w′ kj = wkj − (JT k Jk + I)JTek, (7) where Jk = Jacobian matrix; u = combination coefﬁcient; I = identity matrix; ek = error vector.",
Stacked Autoencoder Marquardt algorithm An Autoencoder type of neural network makes use of an unsu- pervised Back Propagation training algorithm.,
It consists of an encoder and a decoder.,
An encoder converts the input into a hid- den representation in order to extract the features from the input data.,
The decoder converts the hidden representation back to the S. Patil et al.,
/ Biomedical Signal Processing and Control 46 (2018) 67–75 71 Fig.,
Autoencoder.,
Stacked Autoencoder.,
"To get the best possible representation of the input, the error between the original input and reconstructed input is used to update the weights.",
An Autoencoder is as shown in Fig.,
The Stacked Autoencoder network [39] consists of one or more Autoencoders and Softmax layer.,
The Stacked Autoencoder net- work is as shown in Fig.,
4 The steps in training the Stacked Autoencoder network are as given below: • Train the ﬁrst Autoencoder to minimize the error between the original input and reconstructed input.,
"• Considering the output of hidden layer of ﬁrst Autoencoder as input to second Autoencoder, train the second Autoencoder.",
• Repeat the procedure for subsequent Autoencoders.,
• Consider output of hidden layer of last Autoencoder as input to Softmax layer and train the Softmax layer using the supervised Back Propagation learning algorithm with the labeled data.,
• Train the entire network using supervised Back Propagation algo- rithm to ﬁne tune the weights and bias.,
"Results For GDS1975, GDS1976, GDS1815, GDS1816 datasets, Figs.",
"5 and 6 present the results of Thresholding method and Ratio method, respectively.",
"Application of threshold THD1, results into 574, 330, 493 and 521 number of genes for GDS1975, GDS1976, GDS1815 and GDS1816 datasets, respectively.",
"The number of genes chosen are 1301, 1413, 147 and 118 for GDS1975, GDS1976, GDS1815 and GDS1816 datasets, respectively using THD2.",
"Threshold THD3 implementation results into 274, 185, 32 and 41 genes for GDS1975, GDS1976, GDS1815 and GDS1816 datasets, respectively.",
Db2 wavelet appears to be most suitable for the analysis of above men- tioned datasets.,
"The accuracy obtained by the fusion of Thresholding and Ratio method along with u1–u2 (difference in mean intensity values of genes from both the classes of glioma) using common genes across GDS1975, GDS1976, GDS1815 and GDS1816 datasets is as shown in Fig.",
"Discussion Cancer classiﬁcation reported in the literature, vary widely in respect of microarray datasets as well as methods employed to measure parameters deﬁning and evaluating types of cancers.",
"In this paper accuracy of classiﬁcation and optimum number of genes obtained are compared with the results of the authors [21,22], who employed some of the same dataset as ours.",
"8 and 9 demonstrates the variations in 30 average gene intensity values of malignant, benign and glioma grade III, grade IV samples, respectively.",
As shown in Figs.,
"8 and 9, the cancerous and non-cancerous sam- ples of brain tumor have gene intensity values wide apart from each other, making it easy to differentiate between them.",
"However, with increase in the level of malignancy, genes become less differentially expressed, making the classiﬁcation an uphill battle.",
Grade III and grade IV glioma brain tumors are the subtypes of the brain tumor at higher malignancy level.,
"Consequently, it is a real challenge to singularize them.",
"In the proposed work, the threshold range THD2 (2000, 10,000) gives 100% classiﬁcation accuracy with/without wavelet transform for GDS1975, GDS1976 and GDS1815 dataset using RPROP, Con- jugate Gradient and Stacked Autoencoder algorithm while, THD1 (500, 2000) performs better for GDS1816 dataset.",
"To effectively classify the samples using neural network, it is necessary to have smaller number of genes with considerable difference between them for both classes or else, more number of the genes with lesser difference in the intensity values.",
"Threshold range THD1 (500, 2000) partially satisﬁes this criterion and at times gives 100% clas- siﬁcation accuracy with wavelet transform for GDS1975, GDS1976 and GDS1815 datasets.",
The combination of the type of wavelet and neural network algorithm that gives the best result depends on the nature of variation of classiﬁcation data and network parameters.,
"Threshold range THD3 (10,000, 100,000) contains less number of genes with higher values of intensity.",
"Since intensity values are very large, a small change often remains unnoticed especially if number of genes is lesser.",
"However, SAEN algorithm, having the advantage of pre training and ﬁne tuning of its stages gives 100% classiﬁcation accuracy without wavelet transform for THD3 (10,000, 100,000) of all above mentioned datasets.",
Thresholding method is advan- 72 S. Patil et al.,
/ Biomedical Signal Processing and Control 46 (2018) 67–75 Fig.,
"Comparison of accuracy obtained for GDS1975, GDS1976, GDS1815, GDS1816 datasets using Thresholding method.",
"Best of the results of the Ratio method for GDS1975, GDS1976, GDS1815 and GDS1816 datasets.",
"Comparison of the accuracy achieved by fusion of Thresholding method and Ratio method for GDS1975, GDS1976, GDS1815 and GDS1816 datasets.",
tageous in terms of providing the alternative subset of genes to conﬁrm the result of classiﬁcation.,
Genes with very large value of the intensity ratio exhibits inconsistent variation of intensity across the samples of that particular class.,
Ratio method elimi- nates such unreliable genes from the whole gene set.,
"The hybrid of Thresholding method and Ratio method gives a subset of genes that are common and small in number in comparison with both Thresholding and Ratio method, carried out individually.",
Further ﬁltering of genes on the basis of difference in the aver- age gene intensity with or without wavelet transform leads to 100% classiﬁcation accuracy with less number of genes.,
"While dealing with large size and number of samples, the increased memory requirement of LM algorithm, makes the implementation inef- ﬁcient.",
For smaller network LM algorithm performs better than that of Conjugate Gradient algorithms.,
"Stacked Autoencoder net- work trained with Back Propagation algorithm gives the best result as compare to Conjugate Gradient algorithms and LM algorithm owing to the pre training of Autoencoder stages, ﬁne tuning of Softmax layer and ﬁnally ﬁne tuning of entire Stacked Autoencoder.",
S. Patil et al.,
/ Biomedical Signal Processing and Control 46 (2018) 67–75 73 Fig.,
Variation in 30 average gene intensity values of malignant and benign samples.,
Variation in 30 average gene intensity values of glioma grade III and grade IV samples.,
Comparative results of proposed method with the existing methods.,
"The comparative analysis of four datasets in proposed study utilizes commonly transcribed genes, such as AKT3, MORF4L2, ANKRD17, SRP14 and ZNF550.",
"AKT3 gene coding for ser- ine/threonine protein kinase is involved in cell proliferation, differentiation and apoptosis.",
"Further, AKT3 gene expression gets down regulated from grade III to grade IV [40].",
"It may be noted that, MORF4L2 is a vital component of NuA4 HAT and has signif- icant role in transcriptional activation of several genes including oncogenes and proto-oncogenes [41].",
An alteration in the gene expression of ANKRD17 observed from glioma grade III to grade IV may be attributed to G1/S transition [42].,
SRP14 along with SRP9 and Alu RNA constitute elongation arrest domain signal recogni- tion particle and plays a crucial role in targeting secretary protein to endoplasmic reticulum.,
Down regulation of SRP14 would alter signal recognition particle mediated vernacular protein transport system leading to cancer progression [43].,
The uniportKB database has reviewed and annotated ZNF550 to be involved in transcrip- tional regulation.,
An Alteration in ZNF550 expression may lead to remodeling in expression pattern of cancer related genes promot- ing oncogenesis.,
The common transcriptions among four datasets and related functions of these genes leads to direct or indirect correlation of mutations in the above genes with the develop- ment of glioma grade III and IV.,
Hence it is opined that proposed computational approach facilitates the easy selection and precise classiﬁcation of commonly transcribed genes among different tran- scription datasets with highest accuracy with moderate time.,
The glioma grade III and grade IV classiﬁcation using the fusion of Thresholding and Ratio method along with SAEN algorithm is advantageous in number of ways.,
It uses simple and computa- tionally less expensive feature selection method.,
A classiﬁcation accuracy of 100% is obtained using all the samples in the dataset as opposed to the method suggested by Shen et al.,
Threshold- ing method results into an alternative subset of genes to conﬁrm the result of classiﬁcation.,
It can be clearly seen that the optimal subset of genes obtained by proposed method is much smaller as compared to Abusamra et al.,
[21] and Shen et al.,
[22] for GDS1975 and GDS1976 datasets.,
"Therefore, the proposed work provides a good trade-off between the number genes selected for classiﬁcation 74 S. Patil et al.",
/ Biomedical Signal Processing and Control 46 (2018) 67–75 Table 1 Comparison of computational time of proposed method and existing methods.,
Method Computational time in seconds 1 Abusamra et al.,
3 2 Shen et al.,
258 3 Patil et al.,
(proposed approach) 17 and the computational time for achieving maximum classiﬁcation accuracy with respect to existing methods.,
"However, the proposed method has certain disadvantages.",
The gene selection method does not consider interdependency between the genes.,
Also it offers moderate speed but considering the state of art it appears to be insigniﬁcant.,
10 illustrates the comparison of the accuracy and the num- ber of genes obtained by proposed method with the existing methods.,
Table 1. demonstrates the computational time of the proposed method with the existing method using Intel(R) Core(TM) i3 CPU M380 @2.53 GHz and MATLAB R2017a.,
"Conclusion To select the informative subset of genes, Thresholding method considers the genes within the certain range of threshold values, while Ratio method considers entire gene set.",
"From the results, it can be concluded that Thresholding method appears to have an edge, in the sense, it provides an alternative subset of genes for obtaining 100% classiﬁcation accuracy.",
"However, fusion of the two methods along with difference between mean intensity values of the both the classes of glioma gives optimum subset of required genes with less ratio within the certain threshold range.",
"Alterna- tively, ratio can be chosen ﬁrst and thresholding can be applied later, yielding the same subset of genes.",
The Stacked Autoen- coder network along with the combination of Threshold and Ratio method outperforms the methods suggested by Abusamra et al.,
[21] and Shen et al.,
"[22] giving 100% classiﬁcation accuracy using only ﬁve common genes for GDS1975, GDS1976, GDS1815 and GDS1816 datasets.",
"Acknowledgements We would like to thank Dr. Sanjeev C. Ghadi, Professor, Depart- ment of Biotechnology, Goa University, for his timely guidance in correlating the proposed study with the Genomics.",
References [1] T.R.,
"Golub, D.K.",
"Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, et al., Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring, Science 286 (1999) 531–537, http://dx.doi.",
org/10.1126/science.286.5439.531.,
"Ferreira, M.A.",
"Figueiredo, Efﬁcient feature selection ﬁlters for high-dimensional data, Pattern Recognit.",
"33 (2012) 1794–1804, http:// dx.doi.org/10.1016/j.patrec.2012.05.019.",
"[3] Y. Zheng, C.K.",
"Kwoh, A feature subset selection method based on high-dimensional mutual information, Entropy 13 (2011) 860–901, http://dx.",
doi.org/10.3390/e13040860.,
"[4] D. Mishra, B. Sahu, Feature selection for cancer classiﬁcation: a signal-to-noise ratio approach, Int.",
2 (2011) 1–7.,
"[5] Q. Liu, Z. Zhao, Y.-x.",
"Li, X. Yu, Y. Wang, A novel method of feature selection based on SVM, JCP 8 (2013) 2144–2149, http://dx.doi.org/10.4304/jcp.8.8.",
"[6] A. Sharma, S. Imoto, S. Miyano, A top-r feature selection algorithm for microarray gene expression data, IEEE/ACM Trans.",
"9 (2012) 754–764, http://dx.doi.org/10.1109/TCBB.2011.151.",
"Witten, R. Tibshirani, A framework for feature selection in clustering, J.",
"105 (2010) 713–726, http://dx.doi.org/10.1016/j.ins.2010.08.",
"[8] S. Maldonado, R. Weber, J. Basak, Simultaneous feature selection and classiﬁcation using kernel-penalized support vector machines, Inf.",
"181 (2011) 115–128, http://dx.doi.org/10.1016/j.ins.2010.08.047.",
"[9] M. Hindawi, K. Benabdeslem, Local-to-global semi-supervised feature selection, ACM International Conference on Information and Knowledge Management (CIKM 2013) (2013) 2159–2168, http://dx.doi.org/10.1145/ 2505515.2505542.",
"[10] S. Xiang, F. Nie, G. Meng, C. Pan, C. Zhang, Discriminative least squares regression for multiclass classiﬁcation and feature selection, IEEE Trans.",
Neural Netw.,
"23 (2012) 1738–1754, http://dx.doi.org/10.1109/ TNNLS.2012.2212721.",
"[11] P. Shi, S. Ray, Q.E.A.",
"Zhu, Top scoring pairs for feature selection in machine learning and applications to cancer outcome prediction, BMC Bioinf.",
"12 (2011) 375, http://dx.doi.org/10.1186/1471-2105-12-375.",
"[12] S.-W. Chang, S. Abdul-Kareem, A.F.E.A.",
"Merican, Oral cancer prognosis based on clinicopathologic and genomic markers using a hybrid of feature selection and machine learning methods, BMC Bioinf.",
"14 (2013) 170, http://dx.doi.org/ 10.1186/1471-2105-14-170.",
"Kim, J. Gao, Unsupervised gene selection for high dimensional data, Sixth IEEE Symposium on BioInformatics and BioEngineering, BIBE 2006 (2006) 227–234, http://dx.doi.org/10.1109/BIBE.2006.253339.",
"Gaafar, N.A.",
"Yousri, M.A.",
"Ismail, A novel ensemble selection method for cancer diagnosis using microarray datasets, in: 2012 IEEE 12th International Conference on Bioinformatics & Bioengineering (BIBE), IEEE, 2012, pp.",
"368–373, http://dx.doi.org/10.1109/BIBE.2012.6399652.",
"[15] H. Liu, L. Liu, H. Zhang, Ensemble gene selection by grouping for microarray data classiﬁcation, J. Biomed.",
"43 (2010) 81–87, http://dx.doi.org/10.1016/ j.jbi.2009.08.010.",
"[16] T. Abeel, T. Helleputte, Y.D.",
"Van de Peer, et al., Robust biomarker identiﬁcation for cancer diagnosis with ensemble feature selection methods, Bioinformatics 26 (2009) 392–398, http://dx.doi.org/10.1093/bioinformatics/btp630.",
"[17] J.C. Ang, A.H. Mirzal, et al., Supervised, unsupervised, and semi-supervised feature selection: a review on gene selection, IEEE/ACM Trans.",
"13 (2016) 971–989, http://dx.doi.org/10.1109/TCBB.2015.2478454.",
[18] J.W.W.,
"Zhang, X. Lu, Feature selection for cancer classiﬁcation using microarray gene expression data, Biostat.",
Biometrics Open Access J.,
1 (2017) 555557.,
[19] M.S.S.,
"Tarek, R. Elwahab, Gene expression based cancer classiﬁcation, Egypt.",
18 (2017) 151–159.,
"[20] S. Li, C. Liao, J.T.",
"Kwok, Wavelet-based feature extraction for microarray data classiﬁcation, International Joint Conference on Neural Networks, IJCNN’06 (2006) 5028–5033, http://dx.doi.org/10.1109/IJCNN.2006.247208.",
"[21] H. Abusamra, A comparative study of feature selection and classiﬁcation methods for gene expression data of glioma, Proc.",
"23 (2013) 5–14, http://dx.doi.org/10.1016/j.procs.2013.10.003.",
"[22] Q. Shen, Z. Mei, B.-X.",
"Ye, Simultaneous genes and training samples selection by modiﬁed particle swarm optimization for gene expression data classiﬁcation, Comput.",
"39 (2009) 646–649, http://dx.doi.org/10.",
1016/j.compbiomed.2009.04.008.,
"[23] Tumor markers, 2011.",
URL https://www.cancer.gov/about-cancer/diagnosis- staging/diagnosis/tumor-markers-fact-sheet.,
(Accessed 20 December 2011).,
"Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deep belief nets, Neural Comput.",
18 (7) (2006) 1527–1554.,
[25] Deep learning.,
"Sparse autoencoders, 2014.",
URL http://www.ericlwilkinson.,
com/blog/2014/11/19/deep-learning-sparse-autoencoders.,
(Accessed 1 October 2017).,
"Freije, F.E.",
"Castro-Vargas, Z.H.",
"Fang, et al., [Dataset] gene expression proﬁling of gliomas strongly predicts survival, Cancer Res.",
"64 (2004) 6503–6510, http://dx.doi.org/10.1158/0008-5472.CAN-04-0452.",
"Phillips, S. Kharbanda, R.F.",
"Chen, et al., [Dataset] molecular subclasses of high-grade glioma predict prognosis, delineate a pattern of disease progression, and resemble stages in neurogenesis, Cancer Cell 9 (2006) 157–173, http://dx.doi.org/10.1016/j.ccr.2006.02.019.",
"Costa, J.S.",
"Smith, Y.C.",
"Chen, et al., Reversing hoxa9 oncogene activation by pi3k inhibition: epigenetic mechanism and prognostic signiﬁcance in human glioblastoma, Cancer Res.",
"70 (2010) 453–462, http://dx.doi.org/10.",
1158/0008-5472.,
"[29] K. Soman, Insight into Wavelets: From Theory to Practice, PHI Learning Pvt.",
"Ltd., 2010.",
"Zurada, Introduction to Artiﬁcial Neural Systems, vol.",
"8, West St. Paul, 1992.",
"[31] M. Riedmiller, H. Braun, A direct adaptive method for faster backpropagation learning: the RPROP algorithm, IEEE International Conference on Neural Networks (1993) 586–591, http://dx.doi.org/10.1109/ICNN.1993.298623.",
"[32] Matlab neural network toolbox, 2016.",
"[33] J.R. Shewchuk, An introduction to the conjugate gradient method without the agonizing pain, 2009, URL https://www.cs.cmu.edu/quake-papers/painless- conjugate-gradient.pdf.",
(Accessed 3 February 2014).,
"[34] R. Fletcher, C.M.",
"Reeves, Function minimization by conjugate gradients, Comput.",
"7 (1964) 149–154, http://dx.doi.org/10.1093/comjnl/7.2.149.",
"[35] L. Scales, Introduction to Non-Linear Optimization, Springer-Verlag New York, Inc., 1985.",
[36] M.J.D.,
"Powell, Restart procedures for the conjugate gradient method, Math.",
"12 (1977) 241–254, http://dx.doi.org/10.1007/BF01593790.",
"[37] E. Beale, A derivation of conjugate gradients, Numer.",
Methods Nonlinear Optim.,
(1972) 39–43.,
"Wilamowski, H. Yu, Neural network learning without backpropagation, IEEE Trans.",
Neural Netw.,
"21 (2010) 1793–1803, http://dx.doi.org/10.1109/ TNN.2010.2073482.",
S. Patil et al.,
"/ Biomedical Signal Processing and Control 46 (2018) 67–75 75 [39] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, M.S.",
"Lew, Deep learning for visual understanding: a review, Neurocomputing 187 (2016) 27–48, http://dx.doi.",
org/10.1016/j.neucom.2015.09.116.,
"[40] H. Mure, K. Matsuzaki, K.T.",
"Kitazato, Y. Mizobuchi, K. Kuwayama, T. Kageji, S. Nagahiro, Akt2 and akt3 play a pivotal role in malignant gliomas, Neuro-oncology 12 (3) (2009) 221–232, http://dx.doi.org/10.1093/neuonc/ nop026.",
"Yochum, D.E.",
"Ayer, Role for the mortality factors morf4, mrgx, and mrg15 in transcriptional repression via associations with pf1, msin3a, and transducin-like enhancer of split, Mol.",
22 (22) (2002) 7868–7876.,
"[42] M. Deng, F. Li, B.A.",
"Ballif, S. Li, X. Chen, L. Guo, X. Ye, Identiﬁcation and functional analysis of a novel cyclin e/cdk2 substrate ankrd17, J. Biol.",
284 (12) (2009) 7875–7888.,
"[43] K. Strub, P. Walter, Assembly of the alu domain of the signal recognition particle (srp): dimerization of the two protein components is required for efﬁcient binding to srp rna, Mol.",
10 (2) (1990) 777–784.,
"Supriya Patil is an Associate professor in Electronics and Telecommunication Engineering Department at Padre Conceicao College of Engineering, Verna, Goa.",
"Presently, she is pursuing her Ph.D at Goa University.",
"She has obtained B.E degree from DKTE Textile and Engineering Institute, Ichalkaranji, India and M.E degree from Walc- hand College of Engineering, Sangli, India.",
She has 20 years of teaching experience in the ﬁeld of Electronics and communication.,
Her research interests are Digital Signal Processing and Artiﬁcial Neural Network.,
t Prof. Gourish M. Naik is presently working as Professor and Head of Electronics department at Goa University.,
"He obtained his Ph.D from Indian Institute of Science, Bangalore in 1987.",
He has guided six Ph.D’s.,
He is also coordinator of DEITI (an educational broadcast studio sup- ported by Indian Space Research).,
"His other commitments are regulating digitization Center at Goa University to sup- port the various Digital repository projects like DIGITAP (Digital Repository for Fighter Aircrafts of Indian Navy) Million Book project of Ministry of Information Technol- ogy, New Delhi and Antarctica Study Center (NCAOR), Govt.",
"He has to his credit around 50 research papers, many invited talks and keynote addresses.",
He has authored two books on Embedded Systems published by Springer (Holland).,
"He is a member of Goa State Rural Development Authority, member of advisory board, Goa Police Cyber Crimes and also advisor for Directorate of Technical Education.",
He was the chairman of Goa University Technical Advisory Committee.,
Pai has retired as Professor and Head in Electronics and Telecommunication Engineering at P.C.,
"College of Engineering, Goa.",
"He received B.E degree in Electrical Engineering from NITK, Surathkal, M.Tech degree from IISC, Bangalore and Ph.D from IIT, Mumbai.",
He has 35 years of teaching experience with special- ization in communication and signal processing.,
He has authored and coauthored more than 25 journal and con- ference papers in the area of digital signal processing.,
He has guided three Ph.Ds in the ﬁeld of Signal Pro- cessing and Biomedical Engineering.,
He has been referee for Ph.D thesis and Examiner for many Ph.D comprehen- sive and defense viva-voce at various universities.,
He has chaired/co-chaired several technical sessions in National and International confer- ences.,
Dr. Rajendra S. Gad is Associate Professor of Electronics at Goa University.,
He received B.Sc (Physics) and M.Sc (Electronics) degrees from Goa University in 1995 and 1997 respectively.,
He worked for his Ph.D in areas of non- invasive measurements to understand problem of human blood glucose measurement in the year 2009.,
His team was judge winner at Mentor Graphics University Design Contest to design LC3 Processor in the year 2010.,
"He established MOU with AlTERA Inc., USA under Univer- sity program to develop FPGA SoC laboratory.",
"His research interest is multivariate signal processing having applica- tion in the areas of multispectral and 3D biometrics, MIMO communication systems and Autonomic Neural System.",
UNIT 1 & 2 NOTES Structure of a Biological Neuron (6-marks answer – concise) A biological neuron is the basic information-processing unit of the nervous system.,
"In neural networks and encoders, the artiﬁcial neuron is modeled based on this biological structure.",
The main structural components are: 1.,
Dendrites – Branched structures that receive signals from other neurons through synapses.,
They act as input terminals.,
Cell Body (Soma) – Contains the nucleus and cytoplasm; integrates/sums all incoming signals from dendrites.,
Nucleus – Present inside the soma; controls metabolic activities and functioning of the neuron.,
Axon – A long ﬁber that carries the processed electrical impulse away from the cell body toward other neurons.,
Myelin Sheath – Fatty insulating layer around the axon (in many neurons) that increases speed of impulse conduction.,
Synapse – Junction between axon terminal of one neuron and dendrite of another; enables electro- chemical signal transmission using neurotransmitters.,
"Link to neural networks: dendrites = inputs, synaptic strength = weights, soma = summation + activation, axon = output.",
Artiﬁcial Neuron Model – McCulloch–Pitts (MCP) Neuron (6-marks answer – concise) The McCulloch–Pitts neuron is a simple mathematical model of a biological neuron used in neural networks.,
It converts multiple inputs into a single output using weighted summation and thresholding.,
"Inputs (x₁, x₂, …, xₙ) External signals are applied to the neuron, analogous to dendrites of biological neurons.",
"Weights (w₁, w₂, …, wₙ) Each input is multiplied with a weight representing synaptic strength or importance of that input.",
Summation Function (∑) The neuron computes the net input net = ෍𝑤௜ ௡ ௜ୀଵ 𝑥௜+ 𝑏 where b is bias (or threshold adjustment).,
Threshold / Bias (bk) A constant value used to shift the activation level; controls when the neuron should ﬁre.,
Activation / Transfer Function T The net value is passed through a threshold activation function producing output: o If net ≥ θ → neuron ﬁres (output = 1) o If net < θ → neuron does not ﬁre (output = 0) 6.,
Output f(net) Produces a binary decision (0/1) or (+1/−1) depending on activation function; used in logic gates and perceptrons.,
Linear Separability A problem is linearly separable when a single straight line (or hyperplane) can separate two classes completely.,
" Classes can be divided by one straight boundary  Examples: AND, OR logic functions  Single perceptron/single neuron can solve it Nonlinear Separability A problem is nonlinearly separable when no straight line can separate classes.",
 Requires curved or complex boundary  Example: XOR problem  Single neuron cannot solve it — needs multilayer network Key limitation Single MCP neuron/perceptron → only solves linearly separable problems Multilayer neural networks → solve nonlinear problems Steps of Hebbian learning rule: 1.,
Initialize all weights to small random values or zero.,
Apply an input vector (x) to the neuron.,
Compute the neuron output (y).,
Update each weight using ( \Delta w_i = \eta x_i y ).,
Add weight change to old weight: ( w_i = w_i + \Delta w_i ).,
Repeat the above process for all training patterns.,
Advantages 1.,
Simple learning rule; easy to implement 2.,
Biologically realistic and supports associative learning Disadvantages 1.,
Weights may grow indeﬁnitely (no upper limit) 2.,
Does not include error-correction; poor for complex tasks Perceptron Learning Rule Steps 1.,
Initialize weights and bias 2.,
Apply input vector and compute output 𝑦 3.,
Compare with target output 𝑡 4.,
"If error exists, update 𝑤௜= 𝑤௜+ 𝜂(𝑡−𝑦)𝑥௜ 5.",
Repeat for all patterns until error becomes zero Advantages 1.,
Simple and easy to implement 2.,
Guarantees convergence for linearly separable problems Disadvantages 1.,
Works only for linearly separable data 2.,
Produces binary output only (hard-limited activation) Delta Learning Rule (Gradient Descent Rule) Steps 1.,
Initialize weights and bias 2.,
Apply inputs and compute output 𝑦 3.,
Compute error 𝑒= 𝑡−𝑦 4.,
Update weights Δ𝑤௜= 𝜂𝑒𝑥௜ 5.,
Repeat for all patterns to minimize mean square error Advantages 1.,
Can be used for continuous-valued outputs 2.,
Forms basis of backpropagation in multilayer networks Disadvantages 1.,
May get stuck in local minima 2.,
Convergence can be slow and depends on learning rate Widrow–HoƯ Learning Rule (LMS Rule / ADALINE) Steps 1.,
Initialize weights 2.,
Apply input and compute ADALINE output (before activation) 3.,
Calculate error using desired output 4.,
Update weights using Δ𝑤௜= 𝜂(𝑡−𝑦)𝑥௜ 5.,
Repeat until mean squared error is minimized Advantages 1.,
Minimizes mean square error optimally 2.,
More stable and accurate than perceptron rule Disadvantages 1.,
Assumes linear activation (works for ADALINE only) 2.,
Computationally heavier due to error minimization EBPA (Error Backpropagation Algorithm) is a supervised learning algorithm used to train multilayer feedforward neural networks.,
"It works by propagating inputs forward through the network to produce outputs, comparing them with the desired outputs, computing the error, and then back-propagating this error layer by layer to adjust the connection weights using gradient descent so that future outputs become more accurate.",
"In short: EBPA adjusts network weights in the direction that reduces output error, using forward pass + backward pass.",
Key points  used for multilayer perceptrons  requires continuous activation functions  minimizes mean square error  based on gradient descent  backbone of modern deep learning training ✔ Working of Error Backpropagation Algorithm (EBPA) 1.,
Initialize weights Random small values are assigned to all connection weights of the hidden and output layers.,
Forward pass – hidden layer Inputs are applied and outputs of hidden layer neurons are computed using the current hidden layer weights.,
"Forward pass – output layer Using hidden layer outputs and output layer weights, outputs of output neurons are calculated.",
Compute error The diƯerence between desired output and actual network output is found.,
Backward pass – update output layer weights Output-layer weights are updated ﬁrst using the gradient descent rule.,
Update hidden layer weights Hidden-layer weights are then updated using the backpropagated error and gradient descent.,
Iterate until convergence Steps 2–6 are repeated until the total error becomes minimum / within tolerance.,
"the 8 steps from the EBPA ﬂow diagram, in short points: 1.",
"Initialize weights (W, V) with small random values.",
Present an input pattern (z) to the network.,
Compute hidden layer outputs (y = f(Vz)).,
Compute output layer outputs (o = f(Wy)).,
Calculate error for the pattern: (E = \frac{1}{2}\lVert d - o \rVert^2).,
Compute error signals for output and hidden layers (δ terms).,
Update weights o Output layer: (W = W + \eta \delta_o y^T) o Hidden layer: (V = V + \eta \delta_y z^T) 8.,
Check stopping condition o if (E < E_{\text{max}}) → stop o else → repeat for remaining patterns/cycles SETTING OF DESIGN CONSIDERATIONS AND PARAMETER VALUES Initialization of weights  Weights are initialized to small random values  Common ranges: (+0.5 to −0.5) or (+1 to −1)  Very large initial weights → neuron saturation → slow learning  Unequal scaling of inputs may bias learning  Goal: similar eƯective input to each hidden node Frequency of weight updates Per-pattern (online) updating  Weights updated after every sample  Requires more computation  Suitable for real-time / streaming applications Per-epoch (batch) updating  Weights updated after full training set  Computationally cheaper  Not suitable for online systems Choice of learning rate (η)  Controls step size of learning  Large η → fast but oscillatory/unstable  Small η → slow but stable  Typical range: 0.1 – 0.9 Improvement strategies  Start large then decrease  Start small then increase when helpful  Use 2nd derivative of error to adapt step size Momentum  Added term in weight update to smooth learning  Prevents oscillation and helps escape local minima  Typical momentum factor α ≈ 0.9  EƯective weight change depends on: o current gradient o previous weight change Generalizability  Network must perform well on unseen test data  Overtraining reduces test accuracy  Monitor validation error  Stop when: o training error ↓ but test error ↑ (overﬁtting point) Network size  Too few nodes → underﬁtting / weak learning  Too many nodes → overﬁtting / high computation  Input nodes = number of input features  Output nodes = number of classes  Hidden nodes chosen by trial and error Sample size  More weights require more training samples Thumb rule Samples = 5–10 × number of weights Necessary condition (Baum–Haussler) 𝑃> ∣𝑊∣ 1 −𝑎 SuƯicient condition 𝑃> ∣𝑊∣ 𝑎 log ቀ 𝑛 1 −𝑎ቁ Non-numeric inputs  EBPA requires numeric inputs  Non-numeric data must be encoded ﬁrst: o one-hot encoding o label encoding o embedding representations  Scaling/normalization improves learning � � � Performance Evaluation of Error Backpropagation Algorithm (EBPA) 꾆 1.,
"Processing Time The total training time of EBPA depends on:  number of weights to be trained  value of learning rate (η)  number of nodes in the network  number of input samples  complexity of activation function used  number of epochs/iterations More nodes, more weights, and complex activation functions → longer training time.",
Error Measurement (a) Error per sample / per neuron 𝐸= 1 2 (𝑑௞−𝑜௞)ଶ where 𝑑௞= desired output 𝑜௞= actual output (b) Error for all samples and neurons 𝐸= ා෎1 2 ௄ ௞ୀଵ ௉ ௣ୀଵ (𝑑௞−𝑜௞)ଶ (c) Error based on Euclidean distance 𝐸= 1 𝑃𝐾ා෎1 2 ௄ ௞ୀଵ ௉ ௣ୀଵ (𝑑௞−𝑜௞)ଶ (d) Error based on Hamming distance 𝐸= 1 𝑃𝐾ා෎1 2 ௄ ௞ୀଵ ௉ ௣ୀଵ ∣𝑑௞−𝑜௞∣ 꾆 3.,
"Implementation Hardware implementation Software implementation Very high speed Slower Hardware implementation Software implementation Cannot support all weight magnitudes Can support any weight magnitude DiƯicult to implement & debug for large networks Easy to implement & debug Suitable for small, high-speed systems Suitable for large networks where speed is less critical 꾆 4.",
"Generalizability (Very Important)  Network must perform well on unseen test data, not only training data  Excessive training → overﬁtting  Overﬁtting eƯect: o training error ↓ continues decreasing o test error ↑ starts increasing Solution  continuously monitor test error  stop training when: test error increases even if training error decreases (early stopping principle).",
"룅 룆 Local Minima Problem in EBPA  Ideal error curve should be single smooth parabola  In practice, the error surface contains many local minima  Training using gradient descent may get stuck in one of these  At a local minimum, gradient = 0, so algorithm stops updating  Training is assumed complete, but global minimum is not reached Goal: Reach global minimum, not get trapped in local minima.",
� � � � � � � Why it happens?,
" Error surface is non-convex in multilayer networks  Weight space is high-dimensional  Gradient descent follows steepest local slope only ✔ Solution — Momentum Term To avoid getting stuck and reduce oscillations: Δ𝑤(𝑡) = 𝜂𝛿+ 𝛼Δ𝑤(𝑡−1) Where  𝜂= learning rate  𝛼= momentum factor (typically 0.9)  Δ𝑤(𝑡−1)= previous weight change EƯect of momentum  Acts like averaging of error curve (without expensive computation)  Helps system roll past shallow local minima  Prevents drastic oscillations  Speeds convergence Intuition  If gradient direction stays same → momentum increases step size  If gradient direction ﬂips → momentum reduces step size Point of DiƯerence Supervised Learning Unsupervised Learning Reinforcement Learning Availability of output Target/desired output is provided No target output is provided No target output; reward/penalty given Presence of teacher Teacher present No teacher Environment acts as teacher via rewards Data type used Labeled data Unlabeled data Interaction data (state– action–reward) Main goal Minimize error between actual and desired output Discover hidden structure/patterns in data Maximize cumulative reward Error signal Explicit error signal computed No error signal Reward signal instead of error Output type Classiﬁcation or regression Clusters or feature groups Policy or optimal action strategy Learning method Error-correction learning Self-organization learning Trial-and-error learning Example applications Handwriting recognition, spam detection Market segmentation, anomaly detection Game playing, robotics, self-driving cars Algorithms used Perceptron, Backpropagation, SVM, k-NN K-means, PCA, SOM Q-learning, TD learning, Policy gradients Feedback type Direct and explicit None Delayed and evaluative feedback Simple Learning Algorithm Deﬁnition The simple learning algorithm is the earliest learning rule used for a single neuron.",
Weights are increased when the output is wrong and kept same when the output is correct.,
Explanation  Inputs are applied to the neuron  Output is compared with target output  If output is correct → no change in weights  If output is incorrect → weights are adjusted in the direction of error  Process is repeated for all patterns until correct classiﬁcation is obtained Steps 1.,
Initialize weights and bias 2.,
Apply an input pattern 3.,
Compute neuron output 4.,
Compare with desired output 5.,
If output wrong → modify weights 6.,
If output correct → keep weights same 7.,
Repeat for all patterns Weight update formula 𝑤௜(𝑛𝑒𝑤) = 𝑤௜(𝑜𝑙𝑑) + 𝜂(𝑡−𝑦)𝑥௜ ✔ Advantages 1.,
Simple and easy to implement (no complex error backpropagation) 2.,
Learns clusters automatically without target output (unsupervised) 3.,
Requires fewer computations since only the winning neuron is updated ✖ Disadvantages 1.,
Sensitive to initial weights and may converge to poor clusters 2.,
Only one neuron wins — may lead to dead neurons (some neurons never learn) 3.,
"Cannot handle overlapping or complex class structures eƯectively Learning Vector Quantization (LVQ) Deﬁnition Learning Vector Quantization is a supervised competitive learning algorithm in which prototype vectors (codebook vectors) represent classes, and learning occurs by moving prototypes toward correctly classiﬁed samples and away from misclassiﬁed samples.",
Explanation  Each class is represented by one or more reference/prototype vectors  An input vector is compared with all prototypes  The nearest prototype is found (winner neuron)  If winner’s class = input class → move prototype towards input  If winner’s class ≠ input class → move prototype away from input  Repeated until classiﬁcation accuracy improves Steps 1.,
Initialize prototype/codebook vectors for each class 2.,
Present an input sample with known class label 3.,
Find the prototype vector closest to the input (minimum distance) 4.,
If prototype class = input class → move prototype towards input 5.,
If prototype class ≠ input class → move prototype away from input 6.,
"Repeat for all training samples Update formulas If correctly classiﬁed 𝑤௡௘௪= 𝑤௢௟ௗ+ 𝜂(𝑥−𝑤௢௟ௗ) If misclassiﬁed 𝑤௡௘௪= 𝑤௢௟ௗ−𝜂(𝑥−𝑤௢௟ௗ) Where  𝑤= prototype vector , 𝑥= input vector, 𝜂= learning rate ✔ Advantages 1.",
Simple and intuitive; easy to implement 2.,
Directly works with class labels (supervised competitive learning) 3.,
Decision boundaries are easy to interpret ✖ Disadvantages 1.,
Sensitive to initial prototype selection 2.,
May misclassify overlapping classes 3.,
"Performance depends on learning rate choice Self-Organizing Map (SOM) Deﬁnition Self-Organizing Map is an unsupervised competitive learning algorithm that maps high-dimensional input data to a low-dimensional (usually 2-D) grid, preserving the topological structure of the data.",
"Explanation  No target/teacher is used (unsupervised)  Each neuron on the grid has an associated weight (prototype) vector  For each input, the best matching unit (BMU) (nearest neuron) is found  BMU and its neighbors move closer to the input vector  Over time, the map organizes itself so that similar inputs lie close together Steps 1.",
Initialize weights of all neurons randomly 2.,
Present an input vector to the network 3.,
Compute distance between input vector and all neuron weight vectors 4.,
Select the Best Matching Unit (BMU) (minimum distance) 5.,
Update BMU and its neighboring neurons towards the input 6.,
Decrease learning rate and neighborhood size gradually 7.,
"Repeat for all input samples over many iterations Update formula 𝑤௡௘௪= 𝑤௢௟ௗ+ 𝜂ௗℎ(𝑡)ௗ(𝑥−𝑤௢௟ௗ) Where 𝑤= weight vector, 𝑥= input vector, 𝜂= learning rate, ℎ(𝑡)= neighborhood function (larger at start, shrinks with time) ✔ Advantages 1.",
Works without target output (unsupervised) 2.,
Preserves topology — similar inputs map close together 3.,
Useful for visualizing high-dimensional data ✖ Disadvantages 1.,
No direct control on number of clusters formed 2.,
Training time may be high for large data 3.,
Quality depends on neighborhood and learning-rate schedule Adaptive Resonance Theory (ART) Deﬁnition Adaptive Resonance Theory (ART) is an unsupervised learning neural network that performs stable clustering of input patterns while allowing the network to learn new patterns without forgetting old ones.,
"Explanation  ART groups similar input patterns into clusters (categories)  Each category has a prototype vector  When a new input is presented: o it is compared with existing categories o if similarity is above vigilance level, it is assigned to that category o if not, a new category is created  Thus, ART avoids catastrophic forgetting while still learning new data Steps 1.",
Present an input vector to the network 2.,
Compare input with each category prototype 3.,
Select the best matching category 4.,
Check vigilance parameter (similarity test) 5.,
If similarity ≥ vigilance → update that category 6.,
If similarity < vigilance → create new category 7.,
"Repeat for remaining inputs Update idea (qualitative)  For accepted category, weights move towards input  Vigilance parameter controls: o high vigilance → many small clusters o low vigilance → few broad clusters ✔ Advantages 1.",
Stable learning — does not forget previously learned patterns 2.,
Can learn new patterns dynamically 3.,
Vigilance parameter allows control over cluster granularity ✖ Disadvantages 1.,
Performance highly sensitive to vigilance parameter 2.,
May create too many categories at high vigilance 3.,
"Implementation is more complex than SOM/LVQ Hopﬁeld Network 膆  Deﬁnition A Hopﬁeld Network is a recurrent neural network used as associative memory, where stored patterns correspond to stable states (attractors) of the network.",
"It is fully connected, has symmetric weights, and no self-connections.",
"膆  Explanation  Every neuron is connected to every other neuron  Neuron outputs are binary or bipolar  When an input pattern (possibly noisy/incomplete) is applied, neurons update their states  With each update, the energy function decreases  Finally, the network settles into a stable minimum-energy state  This stable state corresponds to one of the stored patterns 踰 踱 踲 踳 Thus, Hopﬁeld networks can retrieve complete patterns from partial/noisy inputs (content-addressable memory).",
膆  Steps (Working) 1.,
Initialize symmetric weights; set diagonal weights to zero 2.,
"Store patterns by adjusting weights (e.g., Hebbian rule) 3.",
Present an input pattern to the network 4.,
Compute neuron net input using weighted sum 5.,
Update neuron states asynchronously or synchronously using activation rule 6.,
Recompute energy; network keeps updating 7.,
Stop when neuron states stop changing (stable state reached) 膆  Advantages 1.,
Works as associative memory — retrieves stored patterns from noisy inputs 2.,
Guaranteed convergence to a stable state due to energy minimization 3.,
Simple implementation; binary/bipolar neurons and symmetric weights 膆  Disadvantages 1.,
Low storage capacity (~0.138N patterns for N neurons) 2.,
May converge to spurious states/local minima instead of correct memory 3.,
"Not suitable for large network sizes; sensitive to noise and weight errors Brain-State-in-a-Box (BSB) Network 膆  Deﬁnition The BSB network is a recurrent auto-associative neural network used for pattern storage and recall, similar to Hopﬁeld but with continuous-valued neuron activations constrained inside a box-shaped region (typically between –1 and +1).",
"膆  Explanation  It stores patterns as stable equilibrium states  Neurons are recurrently connected  Neuron activations are continuous (not just binary)  After each update, the state is projected back into a bounded region  When a noisy or partial input is applied, the network trajectory moves toward the nearest stored pattern  Convergence happens within a state box (hence the name) 踰 踱 踲 踳 BSB therefore performs auto-associative memory with continuous activation values.",
膆  Steps (Working) 1.,
Initialize weight matrix using stored training patterns 2.,
Present an input pattern (possibly noisy/incomplete) 3.,
Compute new state using: 𝑥(𝑡+ 1) = 𝑥(𝑡) + 𝜂(𝑊𝑥(𝑡) + 𝑏−𝑥(𝑡)) 4.,
"Clip/limit each neuron output to lie within bounds (e.g., –1 to +1) 5.",
Repeat state updates iteratively 6.,
Stop when states stop changing → stable pattern recalled 膆  Advantages 1.,
"Stores and recalls patterns like Hopﬁeld, but with continuous activations 2.",
Better convergence properties than binary Hopﬁeld networks 3.,
Handles analog/noisy input data more eƯectively 膆  Disadvantages 1.,
Training and weight design are more complex than Hopﬁeld 2.,
"Sensitive to choice of parameters (learning rate, bounds) 3.",
Storage capacity is still limited and network may converge to spurious states,
"Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces André Silva andreans@kth.se KTH Royal Institute of Technology Gustav Thorén gthor@kth.se KTH Royal Institute of Technology Martin Monperrus monperrus@kth.se KTH Royal Institute of Technology Abstract Automatic program repair seeks to generate correct code from buggy programs, with most approaches searching the correct program in a discrete, symbolic space of source code tokens.",
This symbolic search is fundamentally limited by its inability to directly reason about program behavior.,
"We introduce Gradient-Based Program Repair (GBPR), a new paradigm that reframes program repair as continuous optimization in a differentiable numerical program space.",
"Our core insight is to compile symbolic programs into differentiable numerical representations, enabling search in the numerical program space directly guided by program behavior.",
"To evaluate GBPR, we present RaspBugs, a new benchmark of 1,466 buggy symbolic RASP programs and their respective numerical representations.",
"Our experiments demonstrate that GBPR can effectively repair buggy symbolic programs by gradient-based optimization in the numerical program space, with convincing repair trajectories.",
"To our knowledge, we are the first to state program repair as continuous optimization in a numerical program space.",
"Our work establishes a new direction for program repair research, bridging two rich worlds: continuous optimization and program behavior.",
"1 Introduction Program repair, or automatic bug fixing, promises to generate corrective patches for faulty code (Monperrus, 2018).",
"Recent years have seen dramatic improvements in the quality and complexity of patches thanks to learning based program repair, with the most complex bugs being repaired by frontier LLMs (Yang et al., 2024).",
"Progress on benchmarks like SWE-Bench (Jimenez et al., 2024) and RepairBench (Silva and Monperrus, 2025) has demonstrated that real-world bugs can be fixed automatically.",
"The fundamental limitation of asking a language model to generate a patch is that it does so by reasoning about token distributions, and not by reasoning about the expected behavior.",
"In other words, optimizing for next token prediction captures very little of the difference between buggy behavior and expected correct behavior from the specification.",
"For the same reason, it is hard to repair completely new programs as language models fail to generalize to unseen problems (Chollet et al., 2024).",
"In this paper, we completely reframe learning based program repair.",
We propose to embed the specification and the incorrect behavior to be repaired as a first-class concept in a loss function that is used to directly optimize the program.,
"We describe Gradient-Based Program Repair (GBPR), a novel paradigm that is founded on expressing programs as numerical representations, such as embeddings or neural network weights.",
"With this numerical program representation associated with a loss that captures the expected behavior, GBPR repairs the bug by searching in the numerical program space.",
This core originality of GBPR is that it considers program 1 arXiv:2505.17703v2 [cs.PL] 28 Nov 2025 Figure 1: The key insight of Gradient-Based Program Repair is that program search can be done in a numerical space by employing gradient-based optimization.,
"a) Symbolic program computing the reverse function, written in RASP, and the difference between the expected and buggy behavior; b) Compilation of the symbolic program into a numerical program, encoded as a Transformer; c) Numerical program, equivalent to the symbolic program; d) GBPR optimizes the numerical program via the correctness loss, starting from the buggy program.",
"The program is iteratively optimized, moving towards correct behavior.",
"As the correctness loss decreases, the program correctness increases, with some incorrect behavior now corrected.",
"At the end of the optimization, the repaired program correctly implements the reverse function.",
"As opposed to LLM-based bug fixing, GBPR directly reasons about the expected behavior as a first-class optimizable concept.",
"behavior, the expected correct one and the buggy one, as a first-class concept in the learning pipeline, directly expressed in the loss function to be optimized.",
"To sum up, we propose to 1) transform symbolic programs into numerical programs, 2) design loss functions that capture correct behavior, and 3) optimize numerical programs with gradient descent in the program space until a repair is found.",
"To rigorously evaluate our approach, we introduce RaspBugs, a new benchmark of buggy symbolic programs and their corresponding numerical representation.",
"Those programs are written in RASP (Weiss et al., 2021), a class of sequence-processing programs that can be analytically represented as Transformer models.",
"By systematically applying mutations to base RASP programs, we create a diverse collection of meaningful bugs that can be analyzed both at the symbolic and numerical levels.",
"RaspBugs contains 1,466 bugs over 6 programs, and provides the first-ever controlled environment for researching program repair as continuous optimization.",
Our results demonstrate that gradient-based program repair is feasible.,
"First, we observe proper convergence of the optimization problem, with the correctness of the considered programs improving.",
"Second, we are able to repair the majority of buggy programs for 5 out of the 6 considered base programs.",
"Third, the analysis of the repair trajectories and the correctness landscape confirms that incorrect behavior on buggy input points is gradually fixed.",
"To summarize, our main contributions are: • Gradient-Based Program Repair, a novel paradigm for program repair that performs gradient-based program search, driven by a loss function that captures correct program behavior.",
"2 • RaspBugs, a curated benchmark for evaluating research on continuous program repair, with 1,466 pairs of buggy RASP programs, available as either symbolic or numerical programs.",
• An empirical evaluation demonstrating the feasibility and effectiveness of GBPR for repairing RASP programs as continuous optimization in the numerical program space.,
2 Background Program Repair.,
"Program repair (Monperrus, 2018) automatically finds a correct program from a buggy one, changing incorrect behavior to correct behavior according to a specification (e.g., an input-output test suite), typically via search or mutation over the program space.",
"Most program repair research considers repairing imperative programs, in particular Python or Java.",
Symbolic Program Space.,
"In the context of traditional program repair, programs are symbolic artifacts, represented using discrete structures like source code token sequences, abstract syntax trees (ASTs), or control-flow graphs (CFGs).",
"Program repair on symbolic programs relies on symbolic methods operating directly on these structures according to rules based on language syntax and semantics (e.g., program transformations, static analysis, symbolic execution).",
"Large Language Models (LLMs) are used for program repair (Vasic et al., 2019; Yasunaga and Liang, 2021; Yang et al., 2024) by considering code as a sequence of textual tokens.",
Numerical Program Space.,
"A numerical program is a program 1) whose behavior is encoded as continuous, real-valued parameters and 2) can be executed.",
"These can be either neural networks or vectors in latent spaces with execution semantics, such as in Bonnet and Macfarlane (2024).",
"Unlike traditional symbolic programs, which are constrained by discrete structures, the behavior of numerical programs can be adjusted smoothly via optimization techniques like gradient descent.",
RASP Language.,
"RASP (Restricted Access Sequence Processing) is a domain-specific programming language for sequence processing (Weiss et al., 2021).",
Its key caharacteristic is that the language primitives align with the transformer-encoder model.,
"RASP programs operate on token and position streams using selectors (n×n relations that hold when a predicate over per-position values is true), per-position aggregation (reductions along selectors), and elementwise maps.",
We adopt the bounded setting of RASP which assumes fixed vocabulary and a maximum sequence length.,
Tracr Compiler.,
"Tracr (Lindner et al., 2023) is an analytical, deterministic compiler from RASP to encoder-style transformer models.",
It programmatically constructs attention from RASP selectors (select →query/key scores high exactly where the predicate holds; values route information) and implements aggregation and maps via attentiona and MLP steps respectively.,
"For a fixed vocabulary and sequence length, the compiled model is strictly equivalent to the RASP program (D(x) = P(x) on the supported domain).",
3 Gradient-Based Program Repair All previous research has done program repair as a search in a symbolic space.,
Our core insight is that one can do program repair by searching programs in a numerical space instead.,
"In that numerical space, the program semantics are encoded into a numerical representation.",
"Gradient-Based Program Repair (GBPR) leverages gradient descent to search the numerical program space, minimizing a loss that directly measures deviations from correct behavior.",
The program zeroing the loss is considered the repaired program.,
3.1 Compilation of Symbolic Programs to Differentiable Numerical Programs The first step of numerical repair is to translate the initial symbolic program into a numerical representation where a correctness gradient can be computed with respect to its parameters.,
"Let Pf be a symbolic program (e.g., source code text in Python) that implements the target function f : X →Y, mapping inputs from space X to outputs in space Y.",
3 Compilation.,
"We require a compiler function, denoted C, that transforms Pf into a numerical representation Df,θ.",
"This representation Df,θ is parameterized by a set of numerical parameters θ, such that executing the numerical representation on an input x ∈X yields the program’s output.",
"Crucially, the compiler C must ensure that the numerical parameters θ completely encode the semantics of the original program Pf.",
"In other words, θ ‘is’ the numerical program.",
Numerical Execution.,
"The execution of Df,θ must match the input-output behavior of Pf on the supported domain, guaranteeing that program semantics agree in both the symbolic and numerical spaces.",
"Df,θ ≡Pf =⇒Df,θ(x) = Pf(x) ∀x ∈X.",
Differentiation.,
"We require Df,θ to be differentiable over X with respect to θ.",
"This means we need to compute the gradient of a loss function, in order to change the parameters θ to improve the correctness of the output for x.",
"If the gradient captures correctness, this means that gradient descent is actually optimizing the program towards more correct behavior, which is the fundamental goal of program repair (section 2).",
"Alternatives for Df,θ.",
"In section 6, we will discuss a few appropriate representations for Df,θ.",
"At this point, we focus on neural networks as our numerical representation.",
"The neural network input, resp.",
"output, is the program input, resp.",
"This is a natural choice as 1) neural networks are inherently differentiable via backpropagation, 2) their parameters form the continuous space θ we seek to optimize, and 3) they are executable via forward passes.",
3.2 Gradient-Based Program Repair (GBPR) Let us assume a buggy symbolic program Pb implementing an incorrect function b.,
"The ideal correct function is called f, and is defined by a specification that describes the behavior of the ideal program.",
"In this paper, we assume specifications in the form of input-output examples: {(xi, yi)}n i=1, where each input xi is mapped to its correct output yi by the ideal function f. Symbolic repair means directly changing Pb with e.g., symbolic repair templates or repair operators that manipulate symbols.",
"GBPR means repairing the numerical representation Df,θ instead.",
"For this, we first compile Pb using C to obtain its differentiable representation Db,θb.",
Both the initial parameters θb and the structure of the numerical program are given by the compiler.,
The goal of GBPR is to adjust these parameters θb to find a new set of parameters θ∗such that the behavior of Dθ∗(x) matches the specification.,
Dθ∗(x) = f(x) ∀x ∈X.,
Correctness Loss.,
"Next, we need a loss function L that measures how far the current program behavior deviates from the specification.",
"The total loss is an aggregation of a local loss function ℓcomputed over a subset of the specification: L(θ, {(xi, yi)}n i=1) = n X i=1 ℓ(Dθ(xi), yi) .",
Consider the space of all possible parameter values θ for our differentiable numerical program Dθ.,
Each point in this space corresponds to a slightly different program behavior.,
"The loss function L creates a landscape over this space, where lower values indicate behavior closer to the correct program Pf.",
"The repair process is then a classical optimization problem: finding the parameters θ∗that minimize the correctness loss: θ∗= arg min θ L(θ, {(xi, yi)}).",
Repair as Gradient Descent.,
Gradient descent acts like rolling a ball down this landscape.,
The initial parameters θb place the ball somewhere corresponding to the buggy program’s behavior.,
The gradient ∇θL points uphill towards higher loss (more incorrect behavior).,
"By moving in the opposite direction (−∇θL), we iteratively adjust the parameters θ, effectively improving the program’s behavior step-by-step towards the 4 desired correct functionality defined by the input-output specification.",
"Starting from the initial parameters θ(0) = θb obtained from compiling the buggy program, we iteratively update the parameters in the direction opposite to the gradient of the loss: θ(t+1) = θ(t) −η∇θL(θ(t)), where η is the learning rate.",
"The main difference between symbolic repair and repair as gradient descent is that, because the representation Dθ is continuous and differentiable, small improvements are possible and efficiently guided by the gradient.",
"This sharply contrasts with symbolic repair, which entirely consists of discrete jumps in the program space.",
"3.3 Repair Acceptance Criterion Minimizing loss on training examples is insufficient for successful repair, as optimization might overfit, leading to a program Dθ∗(x) that performs well on training data but fails to generalize to unseen inputs and thus hasn’t captured f’s true semantics.",
"Therefore, we need a repair acceptance criterion based on the performance of the optimized program Dθ∗on a separate, held-out set of test examples {(x′ j, y′ j)} that were not used during the gradient descent optimization.",
"We consider the program repaired if its correctness on this held-out set exceeds 1 −ϵ of the held-out test cases, for some small ϵ ≥0, ensuring that: Dθ∗(x) ≈f(x) ∀x ∈X.",
This ensures that the repair generalizes beyond the training data and the program likely corresponds to the intended function.,
3.4 Summary of Key Novel Concepts Differentiable Numerical Programs.,
"Symbolic programs translated to continuous, differentiable forms (e.g., neural networks) with parameters (θ) encoding semantics; a novel concept in the program repair literature.",
Numerical Repair Search Space.,
"Viewing the repair search space θ as a continuous landscape where program behavior can be smoothly varied, as opposed to the irregular, discrete symbolic search space.",
Correctness Loss.,
A differentiable function L quantifying the difference between the current program’s behavior Dθ(x) and the expected behavior y.,
We cast classical optimization loss into a behavioral semantics conceptual framework.,
Correctness Gradient.,
"∇θL, indicating the direction in numerical program space towards correct behavior.",
Gradient-Based Program Repair.,
"Iteratively adjusting program parameters θ via gradient descent on the correctness loss (θ(t+1) = θ(t) −η∇θL), optimizing towards functional correctness.",
"This is the first framing of program repair as continuous optimization, in contrast to traditional discrete symbolic search.",
"4 RaspBugs: A Benchmark of Buggy Transformer Programs To evaluate GBPR, we need buggy symbolic programs and their equivalent differentiable numerical coun- terparts.",
"We thus build RaspBugs, a novel benchmark of buggy transformer programs.",
"We choose to consider RASP programs (Weiss et al., 2021), which have the property to be representable symbolically or as Transformer models (see section 2) Programs.",
We rely on previous work by Weiss et al.,
(2021) and six of their reference RASP programs.,
"These programs perform various sequence processing operations, including sorting, reversing, histogram computation, frequency-based sorting, and validating Dyck language expressions.",
"5 def hist(input) -> list: """""" Returns the number of times each token occurs in the input.",
"Example usage: hist(a b a c) >> 2 1 2 1 """""" same_tok = rasp.Select( rasp.tokens, rasp.tokens, rasp.Comparison.GEQ # bug: should be rasp.Comparison.EQ ) hist_op = rasp.SelectorWidth(same_tok) return hist_op(input) # correct behavior hist(a c d b a d) = 2 1 2 1 2 2 # buggy behavior hist(a c d b a d) = 6 3 2 4 6 2 Figure 2: Example of a buggy RASP program in Rasp- Bugs, synthesized from the reference hist program using mutation.",
"The reference program selects only equal tokens, while the mutated program selects tokens greater than or equal to, resulting in buggy program behavior.",
Input-Output Specifications.,
"For each RASP program, we generate an input-output specification by randomly sampling from the input space and com- puting the corresponding outputs using the ground- truth symbolic implementation.",
"Each program speci- fication is composed of 50,000 I/O pairs.",
The lengths of the input samples are randomly sampled between 2 and 10.,
"Each specification is split into train (80%), validation (10%), and test (10%) sets.",
Mutating Transformer Programs.,
We create RaspBugs by applying a suite of mutation operators to the original RASP programs.,
The mutations are meant to introduce semantic changes to the program.,
We consider generic mutation operators that act on programming language operators such as arithmetic operations and comparison operations.,
We also de- sign and implement nine RASP-specific mutation operators that target constructs of the RASP lan- guage.,
"In total, we utilize 15 mutation operators.",
These mutation operators are employed individually or combined with others to generate higher-order mutants - mutated programs with several changed locations.,
We set the limit of mutations per order per program to 200.,
Properties of Mutated Programs.,
"Buggy programs must: 1) be symbolically buggy (at least one input-output pair is incorrect), 2) compile to Transformer models via Tracr (Lindner et al., 2023), 3) be executable numerically (forward pass), and 4) be numerically buggy (incorrect on the same input-output pairs).",
"Validation outcomes include: FAILED_MUTATION (symbolic interpretation errors), UNCOMPILABLE (Tracr compilation failure), CORRECT_MODEL (semantically equivalent mutations), and BUGGY_MODEL (programs for repair, considered hereafter).",
Descriptive Statistics.,
"RaspBugs is composed of 1,466 buggy RASP programs, seeded from six reference programs and 15 mutation operators, their corresponding input-output specifications (split into train, validation, and test sets), and their numerical representations as Transformer models.",
"The buggy programs are broken to a different extent, as demonstrated by their different test set accuracies: min = 0.00% (completely broken), median = 2.00%, average = 36.69%, max = 98.00% (a corner-case bug).",
The numerical representations range from 2k (hist program) to 1M (dyck2 program) parameters.,
Full details about RaspBugs can be found in Appendix A.,
5 Experiments 5.1 Training and Evaluation.,
"Each buggy transformer program is fine-tuned via supervised learning on its train split (section 4), minimizing cross-entropy correctness loss between predicted and ground-truth output sequences.",
"We use batch size 256, learning rate 1 × 10−4, and train up to 10k epochs with early stopping (validation loss improvement < 1 × 10−4 for 10 epochs).",
"Repaired programs are evaluated on the test set via greedy decoding (temperature 0), reporting accuracy as exact output match percentage.",
"Experiments used multi-instance NVIDIA A100 GPUs (1/7th A100 compute, 10GB VRAM, 2 CPUs, 32GB RAM per instance/run).",
"6 0 20 40 60 80 100 Accuracy (%) 0 10 20 30 40 Count hist (Models: Before=46, After=46) Before GBPR Median (Before): 0.00% After GBPR Median (After): 53.02% 0 20 40 60 80 100 Accuracy (%) 0 25 50 75 100 125 150 175 Count most_freq (Models: Before=284, After=284) Before GBPR Median (Before): 0.00% After GBPR Median (After): 35.58% 0 20 40 60 80 100 Accuracy (%) 0 50 100 150 200 250 Count reverse (Models: Before=268, After=268) Before GBPR Median (Before): 0.00% After GBPR Median (After): 98.71% 0 20 40 60 80 100 Accuracy (%) 0 50 100 150 200 250 300 Count shuffle_dyck (Models: Before=331, After=331) Before GBPR Median (Before): 94.00% After GBPR Median (After): 98.54% 0 20 40 60 80 100 Accuracy (%) 0 50 100 150 200 250 300 Count shuffle_dyck2 (Models: Before=288, After=288) Before GBPR Median (Before): 96.00% After GBPR Median (After): 99.34% 0 20 40 60 80 100 Accuracy (%) 0 25 50 75 100 125 150 175 200 Count sort (Models: Before=249, After=249) Before GBPR Median (Before): 0.00% After GBPR Median (After): 99.80% Distribution of Correctness Accuracy by Program (Before and After GBPR) Figure 3: Accuracy distribution before (red) and after (green) Gradient-Based Program Repair for each program in RaspBugs.",
The majority of buggy variants for five programs can be repaired with GBPR (as demonstrated by the rightmost green bars).,
5.2 Repairing Transformer Program.,
"To evaluate the effectiveness of Gradient-Based Program Repair, we apply it to the entire RaspBugs benchmark.",
Our goal is to determine whether gradient-based optimization can reliably repair a wide variety of buggy transformer programs.,
Figure 3 shows the correctness accuracy over the test sets for the buggy programs before and after Gradient- Based Program Repair.,
"Here, correctness accuracy is defined as the percentage of test samples for which the model’s output exactly matches the ground-truth output.",
"For example, the top-left figure shows the correctness accuracy distribution over 46 buggy hist programs from RaspBugs.",
The red distribution shows that most buggy programs are completely broken with a correctness accuracy of close to 0%.,
The green distribution represents the correctness accuracy after repair.,
"We see that a large number of hist programs have higher correctness after Gradient-Based Program Repair (green distribution shifted to the right), with the majority achieving near perfect correctness (right-most bar).",
"Before repair (red bars), for four of the six program types, the majority of buggy numerical programs start with near-zero correctness accuracy (red bars clustered at 0%).",
"This indicates that the mutations introduce substantial semantic errors, resulting in programs that almost never produce correct outputs.",
"After repair (green bars), the accuracy distribution shifts dramatically to the right for five out of six program types.",
"In all these cases, the majority of repaired programs achieve near-perfect correctness, demonstrating that Gradient-Based Program Repair can repair incorrect behavior even for severe bugs (i.e., those with initial accuracies near 0% as detailed in section 4).",
"7 0 100 200 300 400 500 Epochs 0 20 40 60 80 100 % fixed programs Mutation Order: 1 (322 buggy mutants) GP GBPR BFS 0 100 200 300 400 500 Epochs 0 20 40 60 80 100 % fixed programs Mutation Order: 2 (481 buggy mutants) GP GBPR BFS 0 100 200 300 400 500 Epochs 0 20 40 60 80 100 % fixed programs Mutation Order: 3 (311 buggy mutants) GP GBPR BFS 0 100 200 300 400 500 Epochs 0 20 40 60 80 100 % fixed programs Mutation Order: 4 (218 buggy mutants) GP GBPR BFS 0 100 200 300 400 500 Epochs 0 20 40 60 80 100 % fixed programs Mutation Order: 5 (134 buggy mutants) GP GBPR BFS Figure 4: Repair success rates over evaluation steps, stratified by mutation order.",
"In GBPR, an epoch is a full forward+backward pass over the I/O samples of the training dataset; in symbolic baselines, an epoch is the symbolic evaluation of one mutated program against the same I/O samples.",
"The idea is that all three approaches are exposed to the same amount of information per epoch, in order to drive the search.",
"Each panel shows accuracy trajectories for GBPR, GP, and BFS (higher better).",
"For simple bugs (orders 1-2), symbolic methods achieve higher final performance since the number of potential repairs to explore is small.",
"For complex bugs (orders 4-5), GBPR surpasses symbolic baselines, showing how gradient-based optimization can traverse complex program spaces more efficiently than symbolic search methods.",
"For the most-freq program, while correctness clearly improves, most programs do not achieve perfect accuracy after repair.",
"This suggests inherent difficulties for gradient-based methods with certain programs, possibly due to complex loss landscapes or significant architectural changes (a point further discussed in section 6).",
"Overall, our experiments over 1,466 buggy transformer programs clearly demonstrate the viability of Gradient- Based Program Repair.",
It is effective to use gradient optimization and an input-output specification to repair a broken symbolic program.,
This is a paradigm shift in the field of automatic program repair.,
Takeaway GBPR successfully repairs buggy programs in the numerical program space.,
"Experiments show it restores correctness for the majority of bugs across 5 out of 6 base RASP programs, achieving near-perfect repair even for initially completely broken programs.",
5.3 Comparison with Symbolic Baselines.,
"To contextualize the effectiveness of Gradient-Based Program Repair, we compare it against two baselines that repair buggy RASP programs through search in the symbolic program space, as opposed to GBPR which operates in the numerical program space.",
"The first baseline employs a Genetic Programming (GP) approach with an evolutionary strategy with population size µ = 16 and offspring generation λ = 16 per generation, operating directly on RASP program source code represented as Abstract Syntax Trees (ASTs).",
"Starting from the buggy program, the initial population is seeded with the original bug plus random single-step mutations.",
"Each generation proceeds 8 as follows: (1) parent selection via tournament selection with k = 3 (i.e., we select 3 programs at random and keep the one with the highest fitness), (2) mutation of selected parents to generate offspring according the mutation operators, and (3) replacement where the top µ individuals by fitness are retained from the combined pool of parents and offspring.",
"The fitness function is defined as accuracy on the training dataset, computed by executing each candidate program on all input-output pairs and measuring sequence-level exact matches (ignoring the BOS token).",
The search terminates either when achieving 100% accuracy or after evaluating 500 programs (this corresponds to ≈15 generations if all of them are complete).,
The second baseline implements exhaustive breadth-first search (BFS) that systematically explores all possible mutations paths from the buggy program.,
"Critically, both baselines use the same mutation operators employed to generate the buggy programs in RaspBugs (section 4).",
"This represents an optimistic baseline: in realistic repair scenarios, one would not have prior knowledge of the exact fault operators that introduced the bugs, whereas our baselines benefit from this oracle information.",
Figure 4 shows repair success rates (test accuracy ≥99%) over epochs.,
"In GBPR, an epoch is a full forward+backward pass over the I/O samples of the training dataset; in symbolic baselines, an epoch is the symbolic evaluation of one mutated program against the same I/O samples.",
"The idea is that all three approaches are exposed to the same amount of information per epoch, in order to drive the search.",
"Figure 4 is stratified by mutation order (one panel per order, the top left figure is for simple bugs based on single mutations, the bottom right is for complex bugs stacking five different mutations).",
"Each panel plots pass rate trajectories for all three methods, revealing both asymptotic dynamics and final performance.",
Asymptotic behavior.,
"First, we see that all three methods exhibit asymptotic behavior after some time.",
"GBPR asymptotically reaches a performance level after approximately 75 training epochs, while the two symbolic baselines continue improving through their respective evaluation budgets but at slower rates.",
This reflects fundamentally different search dynamics.,
"Gradient-based optimization is able to quickly make use of the training signal via gradient, combining the information of the behavior for multiple I/O samples at once.",
"On the contrary, symbolic methods do not have this powerful joint convergence over I/O samples, but instead are able to steadily explore the search space.",
Performance by bug complexity.,
Examining convergence patterns across mutation orders reveals how different paradigms scale with bug complexity.,
"For simple bugs (mutation orders 1-2), symbolic methods achieve substantially higher pass rates at convergence (top left and middle figures).",
"BFS fixes all single- mutation bugs (order 1, top left) as it simply explores all single-step mutations from the buggy program, which is to be expected and also acts as a sanity check for our pipeline.",
"As the number of mutations increases, the number of potential repairs grows exponentially, meaning that symbolic search becomes ever harder.",
"Indeed, at mutation order ≥3, GBPR starts to be competitive compared to symbolic search, with faster optimization but still lower final performance (top right).",
"For complex bugs (orders 4-5), GBPR surpasses both symbolic baselines at their respective convergence points.",
"This clearly validates the intuition that GBPR enables a joint optimization over multiple faulty locations, where a single move along the gradient in the numerical space is equivalent to multiple mutations in the symbolic space.",
This pattern reflects fundamental differences in how those two search paradigms (symbolic versus numerical) scale with bug complexity.,
"Symbolic methods are able to systematically explore the search space of small modifications, excelling at bugs requiring one ot two local changes.",
"However, they face combinatorial challenges as the number of potential repairs grows exponentially with the number of locations to fix.",
"Continuous optimization navigates high-dimensional parameter spaces via gradients, trading performance on local, simple bugs for the ability to handle complex, multilocation bugs via moves in the numerical space.",
"Takeaway While symbolic search methods (GP, BFS) excel at simple bugs, GBPR outperforms them on complex, multi-location bugs.",
Gradient-based optimization scales better with bug complexity by enabling joint updates across the entire program structure in the numerical program space.,
"9 0 2 4 6 8 10 sort(4 5 6 1 3 2) = 1 1 1 1 1 1 ( ) sort(3 3 5 1 2 2) = 2 2 2 2 2 2 ( ) sort(5 2 6 4 6 6) = 6 6 6 6 6 6 ( ) sort(4 5 6 1 3 2) = 1 2 3 4 5 6 ( ) sort(3 3 5 1 2 2) = 1 2 3 3 5 5 ( ) sort(5 2 6 4 6 6) = 2 4 6 6 6 6 ( ) sort(4 5 6 1 3 2) = 1 2 3 4 5 6 ( ) sort(3 3 5 1 2 2) = 1 2 2 3 3 5 ( ) sort(5 2 6 4 6 6) = 2 5 5 6 6 6 ( ) sort(4 5 6 1 3 2) = 1 2 3 4 5 6 ( ) sort(3 3 5 1 2 2) = 1 2 2 3 3 5 ( ) sort(5 2 6 4 6 6) = 2 4 5 6 6 6 ( ) Repair Trajectory Buggy Program Repaired Program 0.0 1.5 3.0 4.5 6.0 7.5 9.0 Correctness Loss Figure 5: Repair trajectory for a buggy sort program, in the numerical program space.",
"The red cross marks the initial buggy program, and the trajectory shows the path taken by gradient descent towards a repaired program.",
"Gradient-Based Program Repair iteratively updates the numerical representation of the program using the gradient defined by the correctness loss landscape, until the program behavior is repaired (L ≈0).",
Left: Surface plot of the correctness loss landscape along the two principal components of the numerical program space.,
"Right: Contour plot of the same landscape, the input-output behavior changing along the trajectory.",
5.4 Repair Trajectories through the Correctness Landscape.,
"To provide further insight into how Gradient-Based Program Repair operates, we visualize repair trajectories from buggy programs to repaired ones across the numerical program space.",
"Figure 5 shows the repair trajectory for a buggy sort program, and the surrounding correctness landscape.",
"In this landscape, higher loss means more buggy behavior.",
"The left panel presents the surface plot while the right panel shows a contour plot for the same trajectory, augmented with input-output behavior sampled from the trajectory.",
"The red cross indicates the starting point of the search, i.e., the buggy program encoded as a numerical program, which has high loss and low correctness.",
"As GBPR proceeds, the program is iteratively updated, following the steepest descent in the loss landscape.",
"The trajectory ultimately converges to a minimum, where the program is successfully repaired and near-perfect correctness on the test set.",
"From an execution perspective, at the beginning, the buggy sort program (red cross) is not capable of sorting any of the three input examples.",
"For example, an incorrect output lists the same element multiple times.",
"During repair, the program gradually improves.",
"At the second highlighted point, the program already correctly sorts the first example.",
"However, at this point, the repair is only partial – the remaining two examples are not correctly sorted – which is reflected by the relatively high loss.",
"At the third highlighted point, the program correctly sorts two of the examples, with the loss now closer to 0.",
"As the loss landscape is explored, the program eventually converges to a minimum where the loss is minimized and the accuracy maximized.",
This means that the program is successfully repaired and behaves according to the provided specification.,
"This visualization highlights the core novelty of our approach: by representing programs as differentiable objects, we exploit the topology of the loss landscape to guide the repair process via gradient descent towards correct behavior.",
"This is in sharp opposition to relying on discrete, combinatorial search for repairing symbolic programs.",
"In summary, our experimental results demonstrate that Gradient-Based Program Repair is feasible, it can reliably repair a wide range of buggy transformer programs, often achieving near-perfect correctness.",
The 10 approach is robust across different RASP programs and various bugs seeded via different mutations.,
Repair trajectories clearly demonstrate the repair dynamics happening in the numerical program space.,
Takeaway Repair trajectory visualizations confirm that GBPR meaningfully navigates the numerical program space.,
"By following the gradient of the correctness loss, the optimization process steadily moves the program from buggy to correct behavior, with intermediate steps being progressively closer to specification.",
6 Discussion Specification Types in the Loss Function.,
A key concept of gradient-based program repair is that it expresses the specification in the loss.,
"Hence, the gradient directly captures the program’s incorrect behavior.",
"In our experiments, we have used the cross-entropy loss over an input-output specification, appropriate for the considered RASP programs.",
"Ultimately, GBPR opens the door to incorporating other rich behavioral information into the loss function, such as formal specifications or invariants.",
Differentiable Numerical Program Representations.,
Programs can be represented numerically in several ways.,
"In our experiments, we focus on neural networks, specifically Transformer models, as the numerical representation, compiled from symbolic RASP programs (Weiss et al., 2021; Lindner et al., 2023; Shaw et al., 2024).",
"Other approaches include embedding programs as points in a continuous latent space, so-called latent programs, which also support efficient search and repair via continuous optimization methods (Bonnet and Macfarlane, 2024).",
Execution of these numerical programs is performed by an auxiliary interpreter model.,
Future work will focus on the design of advanced differentiable numerical representations that are ever more expressive.,
Decompilation to the Symbolic Space.,
A key future direction is decompiling repaired numerical programs into human-readable symbolic code.,
Symbolic representation is both 1) more interpretable and amenable to human review and 2) appropriate for traditional verification techniques with guarantees.,
"However, this decompilation process is nontrivial: mapping the optimized parameters of a numerical representation back to structured, high-level code is an open research challenge, akin to decompilation.",
"Recent work has begun to address this problem in the context of Transformer models by discretizing the model (Friedman et al., 2023) or by training a meta-model to decompile weights into symbolic programs (Thurnherr and Riesen, 2024; Langosco et al., 2024).",
"However, robust and general decompilation from neural programs to symbolic programs remains an unsolved research problem.",
Limitations.,
"Our evaluation is limited to RaspBugs, our benchmark of RASP programs; broader experi- mentation with other programs (e.g., with Thurnherr and Scheurer (2024)) and languages is left to future work.",
"Additionally, GBPR can only optimize parameters within the initial model architecture obtained after Tracr compilation (see scope of Tracr in section 2).",
"If repairing a bug requires changing the structure itself (e.g., adding a new attention head), our prototype could not repair the bug.",
Future work is needed on symbolic-to-numerical compilation to maximize the expressivity of the numerical program space.,
A further practical limitation is the reliance on large input-output datasets to define the correctness loss and drive repair (we use up to 50k pairs per program; see section 5).,
"Real-world test suites may be smaller and incomplete, limiting applicability and increasing overfitting risk under sparse specifications.",
7 Related Work Latent Programs.,
"Latent programs are represented in a latent space, a compressed feature space preserving meaningful data features and placing similar points adjacently.",
Neelakantan et al.,
(2015) train a Neural Programmer to recursively select operations and data sources via latent representations at each execution step.,
Hong et al.,
(2021) find that generating discrete latent codes representing high-level operations improves program synthesis accuracy when compared with token-level generation.,
"Bonnet and Macfarlane (2024) learn a latent program space for ARC-AGI programs, and use gradient-based search to find correct programs.",
11 Liskowski et al.,
"(2020) train an autoencoder to embed programs in a latent space, mapping them back with an evolutionary algorithm.",
None of these works do program repair.,
"Beyond our focus on RASP in this paper, Gradient-Based Program Repair is conceptually applicable to other latent program representations such as the ones from this related work.",
"Lastly, Balog et al.",
"(2020) introduce a neural synthesizer with a differentiable fixer that iteratively revises symbolic programs using latent program representation intermediates; unlike GBPR, their method relies on the discretization of each edit step rather than optimizing the program fully in a continuous space.",
Learning Program Execution.,
"Related work explores how neural networks can understand (Reed and De Freitas, 2016; Shin et al., 2018; Yan et al., 2020; Chen et al., 2021) or benefit from (Ye et al., 2022; Liu et al., 2023) program execution.",
"For example, Zaremba and Sutskever (2014) learn LSTM networks to execute short Python programs.",
(2024) teach models to inspect and reason about code execution by bootstrapping a synthetic training set of execution-aware reasoning traces.,
"In contrast to these works, which simulate execution with a black-box network, GBPR expresses program behavior as a first-class concept within a numerical framework.",
Symbolic vs Numerical Program Spaces.,
"The mapping between symbolic and numerical program spaces is a key component in gradient-based program repair, and a topic of earlier research.",
"Neural surrogates (Esmaeilzadeh et al., 2012; Renda et al., 2021) are neural networks designed to approximate complex programs and are typically trained on a subset of the input-output space.",
Weber et al.,
"(2024) learn to compile source programs directly into neural surrogates, bypassing the need for generating input-output examples.",
"Smooth interpretation (Chaudhuri and Solar-Lezama, 2010) takes a different route: it smooths the semantics of a symbolic program to expose continuous parameters amenable to gradient-based optimization.",
Other works focus on mapping from numerical to symbolic program spaces.,
Cranmer et al.,
(2020) propose symbolic regression for extracting explicit symbolic models from learned models by relying on inductive biases matching to nature of the problem domain.,
Thurnherr and Riesen (2024); Langosco et al.,
(2024); Upadhyay et al.,
(2025) study the decompilation of Transformer models into symbolic programs by learning meta-decompiler models.,
"Given such as mapping, the core novelty of GBPR is to explore the numerical program space for finding patches, using gradient descent.",
Learning-based Program Repair.,
"Several works have proposed using machine learning to repair programs (Long and Rinard, 2016; Vasic et al., 2019; Chen et al., 2019).",
"In particular, LLMs are used to repair programs both in single-turn (Xia et al., 2023; Jiang et al., 2023) and agentic (Yang et al., 2024; Wang et al., 2024) setups.",
"Our work is different in that we focus on repairing programs in a numerical space, using gradient-based optimization in search of the correct program, rather than searching exclusively in the token space.",
"8 Conclusion We introduced Gradient-Based Program Repair (GBPR), casting program repair as continuous optimization.",
"By compiling symbolic programs to differentiable numerical representations and applying gradient descent to a correctness loss, GBPR effectively repairs buggy programs.",
"This work pioneers expression program execution semantics in a continuous, optimizable space, opening new avenues for tackling fundamental programming problems via numerical optimization.",
"References Matej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton.",
Neural program synthesis with a differentiable fixer.,
"arXiv preprint arXiv:2006.10924, 2020.",
Clément Bonnet and Matthew V Macfarlane.,
Searching latent program spaces.,
"arXiv preprint arXiv:2411.08706, 2024.",
Swarat Chaudhuri and Armando Solar-Lezama.,
Smooth interpretation.,
"ACM Sigplan Notices, 45(6):279–291, 2010.",
"12 Xinyun Chen, Dawn Song, and Yuandong Tian.",
Latent execution for neural program synthesis beyond domain-specific languages.,
"Advances in Neural Information Processing Systems, 34:22196–22208, 2021.",
"Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus.",
Sequencer: Sequence-to-sequence learning for end-to-end program repair.,
"IEEE Transactions on Software Engineering, 47(9):1943–1959, 2019.",
"Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers.",
Arc prize 2024: Technical report.,
"arXiv preprint arXiv:2412.04604, 2024.",
"Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, and Shirley Ho.",
Discovering symbolic models from deep learning with inductive biases.,
"Advances in neural information processing systems, 33:17429–17442, 2020.",
"Hadi Esmaeilzadeh, Adrian Sampson, Luis Ceze, and Doug Burger.",
Neural acceleration for general-purpose approximate programs.,
"In 2012 45th annual IEEE/ACM international symposium on microarchitecture, pages 449–460.",
"IEEE, 2012.",
"Dan Friedman, Alexander Wettig, and Danqi Chen.",
Learning transformer programs.,
"Advances in Neural Information Processing Systems, 36:49044–49067, 2023.",
"Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, and Manzil Zaheer.",
Latent programmer: Discrete latent codes for program synthesis.,
"In International Conference on Machine Learning, pages 4308–4318.",
"PMLR, 2021.",
"Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan.",
Impact of code language models on automated program repair.,
"In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 1430–1442.",
"IEEE, 2023.",
"Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan.",
Swe-bench: Can language models resolve real-world github issues?,
"In ICLR, 2024.",
"Lauro Langosco, Neel Alex, William Baker, David John Quarel, Herbie Bradley, and David Krueger.",
"Towards meta-models for automated interpretability, 2024.",
"David Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik.",
Tracr: Compiled transformers as a laboratory for interpretability.,
"Advances in Neural Information Processing Systems, 36:37876–37899, 2023.",
"Paweł Liskowski, Krzysztof Krawiec, Nihat Engin Toklu, and Jerry Swan.",
Program synthesis as latent continuous optimization: Evolutionary search in neural embeddings.,
"In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pages 359–367, 2020.",
"Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan.",
Code execution with pre-trained language models.,
"In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.",
Fan Long and Martin Rinard.,
Automatic patch generation by learning correct code.,
"In Proceedings of the 43rd annual ACM SIGPLAN-SIGACT symposium on principles of programming languages, pages 298–312, 2016.",
Martin Monperrus.,
Automatic software repair: A bibliography.,
"ACM Computing Surveys (CSUR), 51(1): 1–24, 2018.",
"Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever.",
Neural programmer: Inducing latent programs with gradient descent.,
"CoRR, abs/1511.04834, 2015.",
"Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin.",
Next: Teaching large language models to reason about code execution.,
"In International Conference on Machine Learning, pages 37929–37956.",
"PMLR, 2024.",
13 Scott Reed and Nando De Freitas.,
Neural programmer-interpreters.,
"In International Conference on Learning Representations, 2016.",
"Alex Renda, Yi Ding, and Michael Carbin.",
Programming with neural surrogates of programs.,
"In Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, pages 18–38, 2021.",
"Peter Shaw, James Cohan, Jacob Eisenstein, Kenton Lee, Jonathan Berant, and Kristina Toutanova.",
Alta: Compiler-based analysis of transformers.,
"In The First Workshop on System-2 Reasoning at Scale, NeurIPS’24, 2024.",
"Eui Chul Shin, Illia Polosukhin, and Dawn Song.",
Improving neural program synthesis with inferred execution traces.,
"Advances in Neural Information Processing Systems, 31, 2018.",
André Silva and Martin Monperrus.,
Repairbench: Leaderboard of frontier models for program repair.,
"In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pages 9–16.",
"IEEE, 2025.",
Hannes Thurnherr and Kaspar Riesen.,
Neural decompiling of tracr transformers.,
"In IAPR Workshop on Artificial Neural Networks in Pattern Recognition, pages 25–36.",
"Springer, 2024.",
Hannes Thurnherr and Jérémy Scheurer.,
Tracrbench: Generating interpretability testbeds with large language models.,
"In ICML 2024 Workshop on Mechanistic Interpretability, 2024.",
"Shriyash Upadhyay, Benjamin Keigwin, Chaithanya Bandi, Luka Samkharadze, Andrei Romanov, Artem Bakuta, and Aleksandr Zverianskii.",
Model mapping: Can we turn transformers into programs?,
"Available at SSRN 5497818, 2025.",
"Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh singh.",
Neural program repair by jointly learning to localize and repair.,
"In International Conference on Learning Representations, 2019.",
"Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al.",
Openhands: An open platform for ai software developers as generalist agents.,
"In The Thirteenth International Conference on Learning Representations, 2024.",
"Logan Weber, Jesse Michel, Alex Renda, and Michael Carbin.",
Learning to compile programs to neural networks.,
"In Proceedings of the 41st International Conference on Machine Learning, pages 52428–52471, 2024.",
"Gail Weiss, Yoav Goldberg, and Eran Yahav.",
Thinking like transformers.,
"In International Conference on Machine Learning, pages 11080–11090.",
"PMLR, 2021.",
"Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang.",
Automated program repair in the era of large pre-trained language models.,
"In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 1482–1494.",
"IEEE, 2023.",
"Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi.",
Neural execution engines: Learning to execute subroutines.,
"Advances in Neural Information Processing Systems, 33:17298–17308, 2020.",
"John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press.",
Swe-agent: Agent-computer interfaces enable automated software engineering.,
"Advances in Neural Information Processing Systems, 37:50528–50652, 2024.",
Michihiro Yasunaga and Percy Liang.,
Break-it-fix-it: Unsupervised learning for program repair.,
"In Interna- tional conference on machine learning, pages 11941–11952.",
"PMLR, 2021.",
"He Ye, Matias Martinez, and Martin Monperrus.",
Neural program repair with execution-based backpropagation.,
"In Proceedings of the 44th international conference on software engineering, pages 1506–1518, 2022.",
Wojciech Zaremba and Ilya Sutskever.,
Learning to execute.,
"arXiv preprint arXiv:1410.4615, 2014.",
"14 A RaspBugs: Benchmark Details This appendix provides a detailed overview of the RaspBugs benchmark, designed to facilitate research in gradient-based program repair of Transformer programs.",
"RaspBugs consists of a collection of buggy RASP (Restricted Access Sequence Processing) programs, their corresponding correct versions, input-output specifications, and their compiled Transformer model representations.",
"The benchmark is built upon six base RASP programs, originally presented by Weiss et al.",
"These programs cover a set of sequence-processing tasks, ranging from simple operations like sorting and reversing sequences to more complex tasks such as histogram computation and Dyck language validation.",
"Table 1 lists these base programs, along with a brief description and illustrative input-output examples for each.",
Program Description Example Input Example Output sort Returns the input tokens sorted in ascending order.,
"[1,5,3,4,3] [1,3,3,4,5] reverse Returns the input sequence in the reverse order.",
"[a,b,b,e,d] [d,e,b,b,a] hist Returns the histogram count for each token in the input.",
"[a,b,b,e,d] [1,2,2,1,1] most-freq Returns the input sorted according to the token frequency in descending order.",
Only the first occurrence of each token in the output list is considered.,
"[2,3,4,3,2,5] [2,3,3,2,4,5] dyck-1 Returns a sequence of ones if the sequence is balanced in regards to open and closed parenthesis, otherwise returns zeros.",
"[(,),(,(,)] [0,0,0,0,0] dyck-2 Returns a sequence of ones if the sequence is balanced in regards to open and closed parenthesis and curly brackets, otherwise returns zeros.",
"[{,(,{,},},)] [1,1,1,1,1,1] Table 1: Base RASP programs from Weiss et al.",
(2021) used to build RaspBugs.,
"RASP programs handle lists of tokens as input and output, computing different sequence-processing tasks.",
Buggy program variants were systematically generated by applying a suite of 15 mutation operators to the correct base RASP programs.,
"These operators include both generic mutations, which alter common programming constructs (e.g., replacing binary operators, modifying numerical constants), and RASP- specific mutations, tailored to the unique features of the RASP language (e.g., negating selector outputs, incrementing/decrementing RASP indices).",
Mutations are applied individually or in combination to create a rich set of buggy programs with varying levels of semantic deviation from the original correct programs.,
"Details of these mutation operators, including descriptions, examples, the number of occurrences in the generated bugs, and their mean and median accuracy impact, are provided in Table 2.",
15 Type Name Description Example # Occurrences Mean Acc.,
Median Acc.,
"Generic replace-binary-operator Replaces a generic bi- nary operator (e.g., ""+"" to ""-"") in an ex- pression.",
"x + ... → x - ... 1055 0.17 0.00 Custom replace-rasp-comparison Changes a RASP comparison operator (e.g., EQ to LT) in a select statement.",
"Comparison.EQ → Comparison.LT 896 0.26 0.00 Generic replace-comparison-operator Replaces a generic comparison operator (e.g., ""<"" to ""<="") in a condition.",
"x < 0 →x <= 0 330 0.84 0.96 Custom negate-rasp-sop-select Negates a RASP Se- lect operator (e.g., multiplies by -1).",
rasp.tokens → rasp.tokens * -1 288 0.19 0.00 Generic number-replacer Replaces a numeric constant with an- other value.,
-1 * ... →-0 * ... 283 0.64 0.90 Custom negate-rasp-sop-constructor Negates the result of a RASP SOp con- structor.,
SelectorWidth(...) → SelectorWidth(...) * -1 160 0.24 0.00 Custom decrement-integer Decrements an inte- ger constant.,
min_key=1 → min_key=0 140 0.57 0.82 Custom increment-integer Increments an inte- ger constant.,
min_key=1 → min_key=2 131 0.69 0.96 Custom decrement-rasp-indices Decrements a RASP indices expression in a select statement.,
rasp.indices → rasp.indices - 1 130 0.22 0.00 Custom increment-rasp-indices Increments a RASP indices expression.,
rasp.indices → rasp.indices + 1 127 0.25 0.00 Custom negate-rasp-sop-return-stmt Negates the return value of a RASP SOp in the return state- ment.,
"return ... → return ... * -1 108 0.33 0.00 Generic replace-unary-operator Changes a unary op- erator (e.g., ""-"" to ""+"") in an expres- sion.",
"-1 * ... →+1 * ... 74 0.45 0.04 Generic zero-iteration-for-loop Replaces a for-loop’s range with an empty list, skipping the loop.",
for x in xs: → for x in []: 19 0.93 0.96 Custom negate-rasp-sop-aggregate-value Negates the value ar- gumernt of a RASP Aggregate SOp.,
"Aggregate(..., sop) → Aggregate(..., sop * -1) 14 0.67 0.96 Generic add-not Adds a not operator to a condition.",
x < 0 →not (x < 0) 4 0.96 0.96 Table 2: Summary and distribution of mutation operators used in RaspBugs.,
"This table details generic and RASP-specific mutation operators, including their descriptions, examples, the number of occurrences in the generated bugs, and their mean and median accuracy impact.",
16 B Repairing Higher-Order Mutants Fixing bugs increases in difficulty as the number of buggy locations increases.,
"In RaspBugs, we generate higher-order mutants by applying multiple mutation operators to the same program.",
We evaluate the effectiveness of Gradient-Based Program Repair on such mutants in RaspBugs by analyzing repair accuracy as a function of mutation order.,
"As shown in Figure 6, Gradient-Based Program Repair consistently repairs both single and higher-order mutants, with post-repair accuracy distributions remaining unimodal and concentrated near 100%.",
This suggests that Gradient-Based Program Repair is robust to increasing bug complexity and can successfully fix programs even when multiple faults are present.,
"0 20 40 60 80 100 Accuracy (%) 0 50 100 150 200 Count Mutation Order 1 (Models: Before=322, After=322) Before GBPR Median (Before): 78.00% After GBPR Median (After): 98.54% 0 20 40 60 80 100 Accuracy (%) 0 50 100 150 200 250 300 Count Mutation Order 2 (Models: Before=481, After=481) Before GBPR Median (Before): 0.00% After GBPR Median (After): 98.54% 0 20 40 60 80 100 Accuracy (%) 0 25 50 75 100 125 150 175 Count Mutation Order 3 (Models: Before=311, After=311) Before GBPR Median (Before): 0.00% After GBPR Median (After): 98.54% 0 20 40 60 80 100 Accuracy (%) 0 20 40 60 80 100 120 Count Mutation Order 4 (Models: Before=218, After=218) Before GBPR Median (Before): 0.00% After GBPR Median (After): 98.54% 0 20 40 60 80 100 Accuracy (%) 0 10 20 30 40 50 60 70 Count Mutation Order 5 (Models: Before=134, After=134) Before GBPR Median (Before): 0.00% After GBPR Median (After): 98.54% Distribution of Correctness Accuracy by Mutation Order (Before and After GBPR) Figure 6: Accuracy distribution before (red) and after (green) Gradient-Based Program Repair per mutation order in RaspBugs.",
"Gradient-Based Program Repair is effective even for higher-order mutants, as shown by the post-repair distributions clustering near 100% accuracy.",
"1 Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem Royer David Estrada-Esponda, royer.estrada@correounivalle.edu.co, Universidad del Valle, Colombia Gerardo Matturro, matturro@fi365.ort.edu.uy, Universidad ORT Uruguay, Uruguay José Reinaldo Sabogal-Pinilla, jose.sabogal@correounivalle.edu.co, Universidad del Valle, Colombia Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups.",
It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team.,
"This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows.",
"A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting.",
"The most valued soft skills are typically communication, leadership, and teamwork.",
"The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.",
"Keywords: software startups, entrepreneurial team, technical knowledge, soft skills, Colombia's entrepreneurial ecosystem.",
"Introduction A software startup is a recently created company with little or no operational history, focused on creating and developing an innovative software-intensive product or service as a basis for creating business value (Unterkalmsteiner, 2016), (Giardino, 2014).",
"Among the main challenges of software startups are their scarcity of resources, being highly reactive, being made up of small teams with little experience, relying on a single product, and starting to operate under conditions of uncertainty, rapid evolution, time pressure, and high risks (Giardino, 2014).",
"These conditions require empirical studies aimed at identifying what software development knowledge and skills are necessary to overcome these challenges, as well as to understand how these knowledge and skills requirements change throughout the evolution of new software companies (Unterkalmsteiner, 2016).",
"Several studies have shown that the probability of success of a new venture in a dynamic industry increases if the venture includes professionals trained in the various disciplines essential to competing in a complex global economy (Hallam, 2018).",
Tanner believes that the success or failure of a business venture is related to the quality of the people who make up the initial team of founders.,
"The new company will flourish or fail depending on how well it recruits, builds, and retains the team (Tanner, 2 2008), as well as the knowledge, technical abilities, and skills that the members of that team have in relation to the needs and challenges of the venture.",
"A similar opinion is held by Seppänen and colleagues, who believe that it is crucial that a venture obtains the knowledge, abilities, and capabilities necessary to create a product based on innovation (Seppänen, 2017).",
"This article aims to report on a study focused on identifying the technical knowledge and soft skills most valued in the founding team at the beginning of a software startup, and how these needs evolve as the venture progresses.",
The remainder of this paper is organized as follows.,
Section 2 presents a brief review of the general literature on the concepts of technical knowledge and soft skills and the knowledge and skills considered explicitly for this work.,
This section also outlines the key characteristics of Colombia's entrepreneurial ecosystem.,
"Section 3 describes this study's methodological design, while Section 4 presents the main findings.",
"Section 5 discusses the results, while Section 6 analyzes the threats to the study's validity.",
"Section 7 outlines some considerations about the relevance of the findings for software entrepreneurs, incubators, and researchers.",
"Finally, Section 8 presents the conclusions.",
"Background This section outlines the key features of Colombia’s entrepreneurial ecosystem and provides a brief review of the literature on technical knowledge and soft skills, including the specific types of technical knowledge and soft skills examined in this study.",
"2.1 The Entrepreneurial Ecosystem in Colombia Colombia's entrepreneurial ecosystem has experienced significant growth in recent years, positioning itself as a vibrant hub for innovation in Latin America.",
"This expansion is fueled by a confluence of several factors, including government initiatives, increasing access to capital, and a burgeoning tech-savvy population.",
"According to the Colombia Tech Report 2023 (KPMG, 2024), the 1,720 startups across the country included in the report are distributed among 30 sectors; however, 51% of these startups are concentrated in the top six: Fintech (17%), Software as a Service (10%), Business Management (6%), and EdTech, HealthTech, and ProTech, each with 6%, rounding out this top six.",
"Although entrepreneurial activity is widespread across the nation, the primary cities responsible for the most significant volume of startups in Colombia are three: Bogotá (55%), Medellín (25%), and Cali (8%).",
"Bogotá, the capital, serves as the country's economic and political center, attracting a significant portion of venture capital and fostering a diverse range of startups.",
"Its robust infrastructure, established universities, and concentration of large corporations create a fertile ground for innovation.",
"According to the Global Startup Ecosystem Index 2024 (https://www.startupblink.com/startup-ecosystem/bogota-co), Bogotá boasts the highest-ranked startup ecosystem in Colombia, holds the second position in South America, and is ranked 63rd worldwide.",
"Sectors like fintech, e-commerce, and software development are particularly prominent.",
"3 Medellín, once known for its challenges, has undergone a remarkable transformation, emerging as a leading innovation hub.",
"The city's focus on technology and education, epitomized by initiatives like Ruta N (https://rutanmedellin.org/), has significantly contributed to its entrepreneurial resurgence.",
"Medellín's strengths lie in digital media, biotechnology, and advanced manufacturing sectors.",
"Cali, the third-largest city, is establishing itself as a rising star in Colombia's entrepreneurial landscape.",
"Traditionally known for its agricultural sector, Cali is diversifying its economy, with a growing focus on technology and innovation.",
"Thanks to its reputable universities, the city possesses a strong talent pool, particularly in engineering and computer science.",
Colombia's entrepreneurial ecosystem is characterized by a strong sense of community and collaboration.,
"Events and conferences like Open Innovation and Investor Summit Colombia (https://www.oisummit.co/) and the various hackathons held throughout the year provide platforms for entrepreneurs to connect, learn, and pitch their ideas.",
"While each city offers distinct advantages, they collectively contribute to the nation's burgeoning innovation landscape.",
"The government's continued support, coupled with the increasing availability of funding and talent, suggests that Colombia's entrepreneurial ecosystem will continue to flourish in the future.",
"More details about the Colombian entrepreneurial ecosystem can be found in (Garcia Carvajal, 2022).",
"2.2 Technical knowledge and soft skills The concept of “technical knowledge” refers to the technical capacity and factual knowledge necessary to do the job and are the technical competencies an individual possesses, acquired through educational learning and its practical application (Bhatnaga, 2012), which are generally associated with the knowledge necessary for the understanding and execution of tasks and processes (Prince, 2013).",
"On the other hand, the so-called ""soft skills"" are defined as the combination of skills, attitudes, habits, and personality traits that allow people to perform better in the workplace, complementing the technical knowledge necessary to do their job and influencing the way they behave and interact with others (Matturro, 2019).",
2.3 Knowledge and skills in the founding team Entrepreneurs must have “some knowledge” about the industry and markets in which they are involved and about the technology that is relevant to the projected success of entrepreneurial activities.,
"Although entrepreneurs can hire people to fill gaps in their own “skill set”, they cannot rely on others for the industry and technology knowledge that is crucial to setting the right course during the entrepreneurial process (Shane, 2003).",
"When we mention “knowledge in the founding team”, we refer to the collective knowledge shared between team members.",
"According to Faulkner (Faulkner, 2022), “collective knowledge” refers to the shared understanding and information that arises 4 from individuals collaborating within a group.",
"It is characterized by the idea that knowledge can be achieved collectively and often depends on contributions from multiple members, as observed in scientific research or team navigation tasks.",
"2.4 Technical Knowledge and competencies considered for this study Regarding technical knowledge, literature emphasizes that to start and evolve a new venture, the founding team must have specific categories of technical knowledge and competencies, such as management capacity (Riyanti, 2003), (Gianesini, 2018), (Pauceanu, 2026), business competencies (Riyanti, 2003), production capacity (Riyanti, 2003), and financial competencies (Riyanti, 2003), (Pauceanu, 2026), (Gianesini, 2018).",
"The technical knowledge and competencies categories considered for this study were selected from the referred articles above, and are shown in Table 1.",
"Regarding “production” knowledge, this refers to the ability to process products, from raw materials to finished products, and includes the technical capabilities required to develop the production process.",
"For software startups, this “production capacity” corresponds to the knowledge necessary for the “creation and development of an innovative software-intensive product or service as a basis for creating business value,” according to the definition given at the beginning of this article.",
"With this definition, the technical knowledge in this category was selected from (Cavalcante, 2018), (Klotins, 2019), and (Berg, 2018).",
The set of technical knowledge and competencies of the above categories considered for this study is shown in Table 1.,
"Technical knowledge and competencies Category Technical knowledge and competencies Production Requirements engineering, Prototyping, Systems modeling, User experience design, Architectural design, Coding, Software testing, Wireframing, Software processes, Infrastructure.",
"Management Agile methodologies, Traditional methodologies, Project planning and management, Quality management, Change management.",
"Business SWOT analysis, Market analysis, Environmental analysis, Business model definition, Business plan definition, Marketing.",
"Financial Basic accounting, Budgeting, Cash flows analysis, Investment analysis.",
Tables 2 to 5 describe the meaning of the technical knowledge and competencies elements shown in Table 1.,
"Meaning of Production technical knowledge Technical knowledge and competencies Production Meaning Requirements engineering The process of defining, documenting, and managing the needs and specifications of a system or product.",
"Prototyping Creating a simplified early version of software to explore ideas, gather feedback, and refine requirements.",
"Systems modeling The process of creating abstract representations of a system to understand, design, analyze, or communicate its structure, behavior, and interactions.",
"User experience design UX design involves creating and optimizing user interactions with a product by understanding needs, behaviors, and preferences.",
"Architectural design The process of defining a software system's high-level structure involves making key decisions about components, their relationships, and interactions to meet requirements.",
"Coding Writing instructions in a programming language creates software by translating design specifications and algorithms into code that computers can run, enabling specific tasks.",
"Software testing The process of evaluating a software application or system to identify defects, verify functionality, and ensure it meets specified requirements.",
"Wireframing The process of creating a low-fidelity visual representation of a user interface to outline structure, layout, and functionality without focusing on design details.",
"Software processes Refers to structured activities, methods, and practices used to develop, maintain, and manage software systems throughout their lifecycle, including planning, designing, coding, testing, deployment, and maintenance.",
"Infrastructure In the context of technology, it refers to the foundational hardware, software, networks, and facilities that support the development, deployment, and operation of software systems and applications.",
"Meaning of Management technical knowledge Technical knowledge and competencies Management Meaning Agile methodologies They are iterative, collaborative software development approaches emphasizing flexibility, customer feedback, and continuous improvement.",
"They focus on working software, adaptive planning, and team collaboration over rigid processes, following the Agile Manifesto.",
"Traditional methodologies Refer to structured approaches like Waterfall and V- model, where phases (planning, design, implementation, testing) are completed sequentially.",
"These methods emphasize documentation, upfront planning, and strict process control.",
"Project planning and management The process of defining project goals, scope, and deliverables while organizing and coordinating resources, tasks, and timelines to ensure successful software development.",
"Quality management The process of ensuring products, services, or processes meet standards and customer expectations.",
"It includes quality planning, assurance, control, and continuous improvement to boost efficiency, reduce defects, and maintain consistency.",
"Change management The process of managing changes to software systems and projects by identifying, documenting, evaluating, and controlling them to minimize disruption, maintain quality, and ensure effective implementation.",
Meaning of Business technical knowledge Technical knowledge and competencies Business Meaning SWOT analysis It is a strategic planning tool used to evaluate the internal and external factors affecting a business or project.,
"It identifies Strengths, Weaknesses, Opportunities, and Threats to provide a clear picture of current conditions and inform decision-making.",
"Market analysis The process of evaluating a market to understand its size, trends, customer needs, competition, and potential opportunities or risks.",
"Business Environment analysis The assessment of internal and external factors that impact a business, including economic, political, social, technological, legal, and environmental influences.",
"Business model definition The process of outlining how a company creates, delivers, and captures value.",
"It describes the key components of a business, including its value proposition, target customers, revenue streams, cost structure, and operational processes.",
"Business plan definition The process of creating a structured document that outlines a company's goals, strategies, market analysis, financial projections, and operational plan.",
"Marketing The process of identifying, creating, communicating, and delivering value to meet customer needs and drive business growth.",
"It encompasses market research, branding, product positioning, pricing, promotion, and distribution to attract and retain customers.",
"Meaning of financial technical knowledge Technical knowledge and competencies Financial Meaning Basic accounting Refers to fundamental accounting principles and processes used to record and report financial transactions, including tracking income and expenses, managing accounts, preparing financial statements, and ensuring accuracy.",
"Budgeting Budgeting entails planning and managing financial resources by estimating income and expenses, setting goals, allocating funds, and monitoring spending to control costs, and making informed decisions for short- and long-term objectives.",
"Cash flows analysis It examines cash movement over a period, tracking inflows from sales, investments, and financing, and outflows for expenses, liabilities, or investments.",
"Investment analysis It involves assessing financial metrics, market conditions, and economic factors to see if the investment aligns with their financial goals.",
"2.5 Soft skills considered for this study Regarding “soft skills”, various authors have tried to define and characterize this term.",
"However, the general understanding is that it is a difficult task to accomplish because it is a broad concept, covering many dimensions of personal development and involving a combination of emotional, behavioral, and cognitive components.",
"Because of this, it is difficult to determine what to include or exclude in its definition.",
"As a way of broadening the base of field study, the soft skills considered for this study were taken from (Komarkova, 2015), (Wrobel, 2018), (Jones, 2021), and (Marr, 2022), and are listed in Table 6.",
"Meaning of soft skills Soft skill Meaning Motivation and perseverance The ability to stay enthusiastic, determined, and committed to long-term goals despite obstacles.",
It combines internal motivation with resilience and persistence to keep effort until goals are achieved.,
"Adaptability The ability to adjust effectively to changing environments, situations, or challenges.",
"It involves being open to new ideas, being flexible in thinking and behavior, and being capable of quickly learning or applying new skills.",
"Learning through experience The ability to gain knowledge, skills, and insights by reflecting on and analyzing past actions, successes, and mistakes.",
"Leadership The ability to inspire, guide, and influence others toward shared goals involves effective communication, decision-making, and the capacity to motivate and empower team members.",
"Resilience The ability to recover quickly from setbacks, adapt to difficult situations, and maintain a positive attitude in the face of adversity.",
"Value creation The ability to generate ideas, solutions, or innovations that contribute to the success of a project, organization, or customer.",
"Resources utilization The ability to effectively manage and allocate available resources (such as time, money, materials, or personnel) to achieve goals efficiently.",
"Communication The ability to clearly and effectively convey ideas, information, and emotions to others.",
"It involves active listening, verbal and nonverbal expressions, and adapting messages to different audiences and contexts.",
"Creative problem solving The ability to creatively approach challenges by combining analytical skills, adaptability, and innovation to develop solutions.",
"Developing and using contact networks The ability to build, maintain, and leverage relationships with individuals and organizations to access information, opportunities, and support.",
Opportunity recognition The ability to identify and assess potential advantages or gaps can lead to beneficial outcomes.,
"It involves being observant, proactive, and strategic in spotting trends, needs, or problems that can be turned into opportunities for growth, innovation, or improvement.",
"Self-efficacy Confidence in one’s ability to execute tasks, overcome challenges, and achieve goals involves personal competence, resilience, and initiative.",
"10 Teamwork The ability to work collaboratively to achieve shared goals by leveraging individual strengths, fostering open communication, and promoting mutual respect.",
"Tenacity The ability to persist through challenges, setbacks, or obstacles with focus and determination to achieve a goal.",
"It involves resilience, self-motivation, and a strong work ethic, helping individuals overcome difficulties, adapt, and keep striving despite adversity.",
Convey a compelling vision The ability to articulate an inspiring long-term goal that motivates and aligns others.,
"It involves persuasive communication to create a shared purpose, helping people see the bigger picture and their role in achieving it.",
"2.6 Related works Although the study of general entrepreneurial skills is extensive, very few works were found in the literature referring specifically to the technical knowledge and soft skills most valued for software ventures.",
"In this area, Santisteban and Mauricio identified, through a systematic review of the literature on success factors in information technology ventures, the following categories of knowledge and skills within the founding team: experience in the industry, academic training, technological and business knowledge, communication and negotiation skills, and leadership (Santisteban, 2017).",
"Conversely, Seppänen and colleagues focused their study on technical knowledge, highlighting software development and application domain knowledge as the most relevant categories (Seppänen, 2017).",
"Matturro and colleagues (Matturro, 2020) conducted a study to identify the most valued technical and soft skills within the founding teams of software startups in Uruguay.",
"Through semi-structured interviews with the founding partners from ten software ventures, they discovered that conducting market analysis, defining the business model, and developing the business plan are the most essential technical skills.",
"In contrast, motivation, leadership, and the ability to learn from experience emerged as the most important soft skills.",
"Additionally, as the startups grow, other knowledge and skills, such as financial acumen and securing investments, become increasingly valued, along with soft skills like opportunity recognition and resilience.",
"3 Research Design The study's methodological design includes formulating research questions, selecting methods and instruments for data collection, and defining data analysis and interpretation procedures.",
11 3.1 Research questions The research questions posed for the study are: • RQ1: What technical skills are most valued in the founding team of a software startup?,
• RQ2: What soft skills are most valued in the founding team of a software startup?,
• RQ3: How do the needs for new technical knowledge and soft skills required by the founding team evolve as the venture progresses?,
"3.2 Data collection Data collection was conducted in two stages, each serving distinct purposes and having unique characteristics, as explained below.",
3.2.1 Survey The first stage involved a survey of founding partners or representatives from software startups in various regions of Colombia.,
This survey aims to gather quantitative data that reflects the diversity of Colombia's entrepreneurial ecosystem in relation to software startups.,
"For this survey, a questionnaire was developed with a series of closed questions aimed at collecting information relevant to the research questions posed, as well as demographic data from the respondents.",
The questionnaire was previously validated through pilot interviews with the founding partners of two software ventures.,
The survey form was organized into the following sections: 1.,
"Demographics data: In this section, respondents are asked whether their startup qualifies as a “software” startup.",
"If so, they are requested to provide additional information, including the startup's name, the year it began operations, the name and email address of the respondent, and their job position within the startup.",
Technical knowledge assessment: This section presents the lists of technical knowledge shown in Tables 2 to 5.,
"Respondents must evaluate each item of knowledge based on a Likert scale with the following options: ""Not necessary"", ""Slightly necessary"", ""Necessary"", ""Very necessary"", ""Essential"".",
Soft skills assessment: This section presents the list of soft skills shown in Table 6.,
"Respondents must evaluate each soft skill based on a Likert scale with the following options: ""Not necessary"", ""Slightly necessary"", ""Necessary"", ""Very necessary"", ""Essential"".",
"Evolution of required technical knowledge: In this section, respondents are asked to differentiate between the technical knowledge they had sufficiently acquired or developed before starting the business and the knowledge they needed to acquire or enhance during its development.",
"""Sufficiently acquired or developed"" refers to having a level of knowledge that enables individuals to 12 perform their job adequately and consistently contribute to the company's objectives.",
The same lists of technical knowledge items shown in Tables 2 to 5 are presented.,
"The answer options were “Before”, “During”.",
"Evolution of required soft skills: In this section, respondents are asked to differentiate between the soft skills they had sufficiently developed at the beginning of the venture and those they needed to acquire or further develop as the startup grows.",
"The same list of soft skills shown in Table 6 is presented, and the options to answer for each one are “Before”, “During”.",
"Availability for an interview: In this final section, we ask the respondent if he/she is available for a follow-up interview on a date to be determined.",
"To conclude this section, Table 7 illustrates the connection between the research questions posed for this study and the survey sections where the data needed to answer them is gathered.",
"Relationship between research questions and survey sections Research question Survey section RQ1 2 RQ2 3 RQ3 4, 5 3.2.2 Interviews The second stage of data collection consisted of a series of semi-structured interviews with founding partners or representatives of software startups who had responded to the survey in the previous stage.",
"To achieve this, the last question in the survey inquired whether the respondent was willing to participate in an interview.",
"The purpose of these interviews was to gather qualitative information about the reasons behind the survey responses, as well as the opinions and perspectives of a small group of interviewees regarding the importance of technical knowledge and soft skills in creating and developing a software startup.",
4 Results This section presents the results obtained from analyzing the data collected in the survey.,
The presentation of these results follows the order of the research questions outlined in section 3.1.,
"4.1 Software startups taking part in the survey To connect with software startups to participate in the survey, we contacted business incubators and other organizations within Colombia's entrepreneurial ecosystem to ease 13 communication and distribute the online form.",
This approach allowed us to reach a substantial number of software startups from different regions of the country.,
"As explained above, to ensure that the responding startups were categorized as ""software startups,"" a discriminatory question (Yes/No) was included at the beginning of the survey.",
"Only if the response was ""Yes"" would the subsequent survey questions be displayed.",
Table 8 shows the geographical distribution of the 74 software startups that ultimately participated in the survey.,
"Number of participating startups by geographical region Region Quantity Antioquia 18 Atlántico 1 Caldas 2 Cundinamarca 22 Quindío 2 Risaralda 3 Valle del Cauca 26 Total 74 Since it is challenging to ascertain the total number of software startups in the country and the number that received the survey form, the survey conducted is considered ""exploratory"".",
Conducting an exploratory survey in a scenario where the population size and number of potential respondents are unknown is essential for gaining initial insights about the research topic.,
This type of survey allows researchers to explore emerging patterns and identify key themes without requiring a predefined sampling frame.,
"Since exploratory surveys do not aim for statistical generalization, they can effectively gather valuable data from available participants, even when the total population is uncertain.",
"For those who answered the survey on behalf of their startups, we requested them to serve as a founding partner or a representative with a senior management position.",
"This requirement is essential for obtaining responses from individuals with a global perspective on their startups and with ""first-hand"" knowledge and experience regarding the aspects explored in this study.",
"Thus, Table 9 shows the distribution of respondents based on their roles in the respective startup.",
14 Table 9.,
Distribution of respondents by their roles or positions.,
Role or position Quantity Founding partner 31 CEO 15 CIO 11 CTO 10 Other 7 Total 74 4.2 Most valued technical knowledge (RQ1) The questions in section 2 of the survey questionnaire enabled us to gather data on the technical knowledge most valued within the entrepreneurial team.,
"To differentiate and rank the relative importance assigned by respondents to each element of technical knowledge, the method employed involved assigning 1 point for each “Necessary” response, 2 points for “Very Necessary,” and 3 points for each “Essential” response.",
"Conversely, “Not Necessary” responses received a score of -2, while “Slightly Necessary” responses were assigned -1.",
"For every knowledge element included in the questionnaire, the total of weighted responses is summed up, according to the following formula: Weighted responses = 3*(Number of answers “Essential”) + 2*(Number of answers “Very Necessary) + 1*(Number of answers “Necessary) – 1(Number of answers “Slightly Necessary”) – 2* (Number of answers “Not Necessary”).",
"To establish the ranking, tables were organized from highest to lowest based on the weighted response value.",
Table 10 presents the ranking of most valued knowledge items in the “Production” category.,
15 Table 10.,
Weighted responses for Production knowledge.,
"Knowledge item Production Weighted responses Software testing 123 Requirements engineering 121 Prototyping 119 Coding 115 Software process 104 Systems modelling 91 Infrastructure 87 User experience design 77 Wireframing 64 Architectural design 61 For the knowledge items in the “Management” category, Table 11 illustrates how respondents ranked them.",
Weighted responses for Management knowledge.,
Knowledge item Management Weighted responses Project planning and management 158 Change management 145 Agile methodologies 144 Quality management 110 Traditional methodologies 62 Table 12 presents the ranking of most valued knowledge in the “Business” category.,
Weighted responses for Business knowledge.,
"Knowledge item Business Weighted responses Model business definition 185 Business plan definition 178 Marketing 163 Market analysis 162 Business Environment analysis 158 SWOT analysis 125 16 Finally, in Table 13, the ranking of most valued knowledge in the “Financial” category is presented.",
Weighted responses for Financial knowledge.,
Knowledge item Financial Weighted responses Budgeting 160 Investment analysis 152 Cash flows analysis 139 Basic accounting 138 4.3 Most valued soft skills (RQ2) Section 3 of the survey questionnaire presented questions about the soft skills most valued within the entrepreneurial team.,
"In all instances, the respondents indicated their preferences using the terms ""Necessary,"" “Very necessary,"" or ""Essential""; the respondents never selected the options ""Not necessary"" or ""Slightly necessary"" to refer to these skills.",
"Therefore, to differentiate and assess the relative importance attributed to each soft skill, the weighting method involved assigning 1 point to each “Necessary” response, 2 points to “Very necessary,"" and 3 points to each “Essential” response.",
"In this case, the formula for calculating the figures in the “Weighted responses” column is: Weighted responses = 3*(Number of answers “Essential”) + 2*(Number of answers “Very Necessary) + 1*(Number of answers “Necessary”).",
"Table 14 presents the ranking of the most valued soft skills, arranged according to the ""weighted responses"" values.",
17 Table 14.,
Weighted responses for most valued soft skills.,
"Soft skills Weighted responses Communication 194 Leadership 191 Value creation 188 Teamwork 187 Resilience 185 Adaptability 181 Motivation and perseverance 179 Creative problem solving 179 Convey a compelling vision 179 Learning through experience 174 Opportunity recognition 173 Developing and using contact networks 170 Resources utilization 168 Tenacity 168 Self-efficacy 154 4.4 Evolution of technical knowledge and soft skills requirements (RQ3) The questions in sections 4 and 5 of the survey gathered the necessary data to address the research question concerning the technical knowledge and soft skills that entrepreneurs typically possess before starting their ventures, as well as those they need to acquire or substantially improve during the venture's development.",
"For the knowledge elements in the Production category, Table 15 shows the number of responses indicating their acquisition or improvement before the venture began and during its development.",
18 Table 15.,
Moment of acquiring or improving Production knowledge.,
Knowledge item Production Before During Difference Coding 65 9 56 Software process 51 23 28 Requirements engineering 45 29 16 Software testing 41 33 8 Prototyping 39 35 4 Wireframing 32 42 -10 Systems modelling 30 44 -14 Infrastructure 27 47 -20 Architectural design 26 48 -22 User experience design 25 49 -24 The values in the “Difference” column were calculated as: “Difference = Before – During”.,
"These values in Table 15 indicate that software entrepreneurs are generally well- prepared in coding (+56), software processes (+28), and requirements engineering (+16) before launching their ventures.",
"However, they often need to acquire or significantly enhance their knowledge in user experience design (-24), architectural design (-22), and infrastructure (-20).",
The respondents' responses concerning the knowledge elements of the Management category are presented in Table 16.,
Moment of acquiring or improving Management knowledge.,
"Knowledge item Management Before During Difference Traditional methodologies 51 23 28 Project planning and management 43 31 12 Quality management 38 36 2 Change management 30 44 -14 Agile methodologies 28 46 -18 According to the meaning of the ""Difference"" column explained above, most entrepreneurs consider themselves well-versed in project planning and management (+12) as well as traditional methodologies (+28).",
"In comparison, they often need to acquire or improve their knowledge in change management (-14) and agile methodologies (-18).",
Table 17 presents the responses regarding the knowledge elements of the Business category.,
19 Table 17.,
Moment of acquiring or improving Business knowledge.,
"Knowledge item Business Before During Difference SWOT analysis 44 30 14 Business model definition 38 36 2 Business Environment analysis 37 37 0 Business plan definition 36 38 -2 Market analysis 34 40 -6 Marketing 33 41 -8 The values in the Difference column of Table 17 indicate that, except for the SWOT analysis (+14), most respondents generally need to acquire or enhance their knowledge in almost all other areas.",
"Finally, the results of the responses for the knowledge elements in the Financial category are shown in Table 18.",
Moment of acquiring or improving Financial knowledge.,
"Knowledge item Financial Before During Difference Budgeting 36 38 -2 Cash flows analysis 33 41 -8 Basic accounting 32 42 -10 Investment analysis 27 47 -20 In the Financial category, the values in the Difference column of Table 18 suggest that most entrepreneurs typically need to acquire or improve their knowledge across all areas during the development of their ventures.",
Table 19 presents the number of responses based on when the surveyed entrepreneurs acquired or developed their soft skills.,
20 Table 19.,
Moment of acquiring or improving the most valued soft skills.,
"Soft skills Before During Difference Motivation and perseverance 53 21 32 Teamwork 51 23 28 Communication 47 27 20 Adaptability 44 30 14 Tenacity 44 30 14 Resilience 43 31 12 Leadership 41 33 8 Self-efficacy 40 34 6 Creative problem solving 38 36 2 Learning through experience 37 37 0 Opportunity recognition 33 41 -8 Value creation 33 41 -8 Resources utilization 32 42 -10 Convey a compelling vision 30 44 -14 Developing and using contact networks 27 47 -20 Again, based on the values in the Difference column, most entrepreneurs consider themselves well-prepared in the skills of Motivation and Perseverance (+32), Teamwork (+28), and Communication (20) before starting their business.",
"On the other hand, they most often need to acquire or develop the skills of Developing and Using Contact Networks (-20), Conveying a Compelling Vision (-14), and Resources utilization (-10) during venture development.",
"4.5 The qualitative interviews As explained in Section 3, besides the survey as the primary data collection method, interviews were conducted with some respondents to explore the reasons for their responses and understand their perspectives on the study's objective.",
"Four of the 74 survey respondents agreed to be interviewed; three are founding partners of their respective software startups, and one serves as the CEO at another software venture.",
The interviews were conducted online through Google Meet and lasted an average of 50 minutes.,
Selected excerpts from the interviews are included throughout the Discussion section to complement and illustrate the analysis of the study's findings with the interviewees' opinions and perspectives.,
21 5 Discussion The key results presented in section 4 are discussed and analyzed in this section.,
"5.1 Overview of the participating software ventures and their representatives Of the 74 software startups surveyed (Table 8), 89% are in the Antioquia, Cundinamarca, and Valle del Cauca regions.",
"These departments include, respectively, Medellín, Bogotá, and Cali, cities highlighted in section 2.1 as the most dynamic in Colombia's entrepreneurial ecosystem.",
"Although representation from all regions has not been achieved, the three mentioned regions host most of the country's startups.",
"Regarding the representatives of the ventures that responded to the survey, as shown in Table 9, 90% hold executive roles in their respective ventures, including positions such as founding partner, Chief Executive Officer, Chief Information Officer, or Chief Technology Officer.",
"The “Other” option in Table 9 refers to positions like Team leader, Product Manager, or Sales representative.",
The executive roles held by most respondents indicate that they possess enough knowledge and authority to respond appropriately to the survey and to trust the accuracy and relevance of their answers.,
"5.2 On the most valued technical knowledge From Table 10, four software production activities directly related to product engineering emerge as the most valued by software entrepreneurs: requirements engineering, prototyping, coding, and software testing.",
These activities form the essential backbone of successful software development.,
"In the dynamic and often resource-constrained environment of software startups, their relevance becomes even more pronounced and critical for survival and growth.",
Requirements Engineering in a software startup involves identifying the core value and translating it into a Minimum Viable Product (MVP).,
"Melegati and colleagues (Melegati, 2020) highlight that the idea of ""product"" is key to understanding MVP among entrepreneurs.",
They believe MVP is the smallest version with minimal features that provide customer value.,
"Startups must quickly validate ideas, and prioritized requirements ensure initial efforts target the most crucial functionalities for a clear user need.",
"According to interviewee 4, “Honestly, we see requirements engineering as highly important because, in the early days, you really don’t have a lot of room for guesswork.",
"We needed to be sure we were building something people actually want, and that means taking the time to understand the problem, not just jumping into code.”.",
Knowledge about prototyping is exceptionally valuable for an entrepreneurial team in the early stages of a software startup because it provides a rapid and cost-effective way to visualize and test their core ideas before committing significant resources to full- scale development.,
"In words of Interviewee 1, “Prototyping is central to our process because it allows us to validate our assumptions quickly and cost-effectively.",
A prototype 22 lets us put something tangible in front of potential users or stakeholders early on.,
"It sparks real feedback, not just opinions or guesses.”.",
Coding in a startup requires technical skill and speed.,
"Building a solid, scalable system is vital, but startups often need to release a functional product quickly to secure funding, attract users, and stay competitive.",
"This calls for pragmatic coding, emphasizing clean, maintainable code for fast iteration and growth.",
"Poor practices early on can cause technical debt, jeopardizing future development and scalability for a growing startup.",
"Regarding software testing, it is often seen as a luxury in startups with limited resources, but it is essential to prevent costly failures.",
"Releasing buggy software can damage reputation, erode trust, and risk the startup's survival.",
"Even with limited resources, testing helps find and fix critical defects early, from basic unit tests to user acceptance testing with early users.",
"Regarding knowledge of management, a distinct valuation emerges as shown in Table 15.",
"In the early stages, founding teams tend to place a higher value on knowledge related to project planning and management, agile methodologies, and change management, while seemingly valuing quality management and traditional software methodologies to a lesser extent.",
"Founding teams are usually small, with members handling multiple roles, focusing on core product delivery over specialized roles or detailed documentation.",
The main goal is often to find product-market fit amidst uncertainty about the target audience and features.,
"Agile methods are favored for their flexibility, allowing rapid adaptation via short sprints and frequent releases.",
"A study by Mkpojiogu (2019) highlights that key motivations for agile include faster delivery, managing changing priorities, and improving predictability.",
Strong project planning is essential for defining the MVP and guiding development to validate the product's potential.,
"When comparing agile approaches to traditional ones, Interviewee 3's opinion is revealing: “We value agile way more than traditional methods because, honestly, in a startup, things change all the time—your users give unexpected feedback, your priorities shift, you discover better ideas mid-way.",
Traditional methods just do not handle that kind of flexibility well.”.,
The significant importance of knowledge in change management is influenced by the startup landscape's dynamic and often unpredictable nature.,
"In this context, change management knowledge and skills are essential for navigating uncertainties, adapting the product roadmap as necessary, and effectively communicating these changes within the small internal team and to the early user base.",
"Concerning business knowledge, the values in Table 16 show a notable similarity in the high ratings given by respondents.",
"Additionally, relatively higher values are noted compared to those for knowledge of production and management.",
"By placing a high value on business knowledge, the founding teams of software startups acknowledge that their technical (production) knowledge, while essential for product development, is only one aspect of a successful startup equation.",
"23 Interviewee 3 mentions, “Yeah, we put a lot of value on those areas (business plan, business model, marketing) because building a great product isn’t enough if you don’t know how to turn it into a real business.”.",
"A key reason for focusing on business acumen is to bridge the ""product-market fit"" gap.",
Startup teams must validate if their software's problem resonates enough with the target audience to justify payment.,
"Business model definition explains how they will capture value, while marketing helps understand target needs and craft effective messages about their software's value.",
"Regarding marketing, interviewee 2 said “…that’s how we get people to even ‘know’ we exist.",
It’s what connects us to our users and helps us learn what works and what doesn’t.”.,
A well-defined business model and thorough market research and analysis can effectively prevent the costly mistake of investing significant time and resources in developing a product that lacks market demand or a clear path to monetization.,
This emphasis on business knowledge strategically leverages their technical expertise.,
A clear business plan highlights opportunities for technical solutions that support growth.,
"In this regard, interviewee 1 stated: “…we see that business knowledge as just as important as the tech side—because one without the other isn’t really a startup, it’s just a project.”.",
"The final set of knowledge elements examined in this study relates to financial knowledge, with the respondents' valuations shown in Table 17.",
This table emphasizes the high valuation of knowledge concerning budgeting and investments.,
This prioritization arises from the fundamental need to manage scarce resources effectively and make informed decisions about allocating capital.,
"Early-stage startups often rely on personal savings, angel investors, or venture capital to fuel their initial growth.",
"While their technical skills are crucial for product development, the entrepreneurial teams recognize that without sound financial planning, the startup's runway can be severely limited, hindering its ability to reach key milestones and achieve sustainability.",
"In this regard, interviewee 3 said “Yeah, we definitely value budgeting and investment knowledge highly, and that’s because money decisions can make or break a startup.",
"In the early stage, you’re constantly juggling limited resources, so knowing how to budget properly helped us stretch every dollar and stay focused on what really makes the difference.”.",
"A strong grasp of investment analysis helps the founding team evaluate funding options, negotiate terms, and present a compelling financial case.",
"Pattyn (2023) advises that, with investors becoming more discerning and focusing beyond growth metrics, it's essential to consider metrics like cash flow, ROI, and NPV.",
"Regarding investment analysis, interviewee 4 said “…on the investment side, understanding how funding works—when to raise, how much to raise, what kind of investors to approach—gives us more control over our future.”.",
"24 5.3 On the most valued soft skills Among the soft skills assessed by the survey respondents, four are distinguished as the most valued within the entrepreneurial team, as shown in Table 18: interpersonal communication, leadership, value creation, and teamwork.",
Interpersonal communication becomes relevant in a usually small founding team.,
"Clear and concise communication minimizes misunderstandings, facilitates the rapid exchange of ideas, and ensures everyone is aligned on goals and progress.",
"Effective communication also extends outwards, playing a vital role in early customer interactions, pitching to potential investors, and building crucial relationships with early adopters and partners.",
"Regarding leadership, even in the absence of formal hierarchies, it is crucial from the outset.",
"Each founding team member needs to take ownership, inspire confidence, and guide others, especially as the team begins to grow.",
"Early leadership sets the tone for the company culture, fosters a sense of shared purpose, and empowers individuals to contribute their best.",
The emphasis on value creation as a soft skill highlights the importance of the team's ability to collectively understand and articulate the unique benefits their software brings to the market.,
This goes beyond simply building a functional product; it involves deeply understanding customer needs and translating technical capabilities into tangible value for the end-user.,
"Finally, teamwork is essential in the resource-limited and high-pressure environment of an early-stage software startup.",
"The ability to collaborate effectively, share responsibilities, leverage individual strengths, and support one another through setbacks is crucial for productivity and resilience (yet another highly regarded soft skill).",
"Regarding communication, leadership, and teamwork, interviewee 2 expressed “Good communication keeps everyone aligned—no mixed signals, no wasted effort.",
"Leadership, even if it's informal, is what keeps people motivated and focused when things get messy, which they always do.",
And teamwork?,
No one’s working in a silo in a startup.,
"Everyone’s wearing multiple hats, so collaborating, giving and taking feedback, and supporting each other is huge.”.",
"5.4 On the evolution of required knowledge and skills Startup founders generally possess a strong foundation in core production (software engineering) areas before launching, with coding having the highest prior knowledge (Table 15).",
"Teams often also have initial expertise in software development lifecycles, including software process, requirements engineering, testing, and prototyping.",
"However, as ventures mature, there's a significant shift in the knowledge needed, moving from core coding to other areas.",
"Entrepreneurial teams often need to acquire extensive new knowledge during their venture's development, such as user experience design, architectural design, infrastructure, systems modeling, and wireframing.",
"Many teams gain significant understanding in these areas on the job, often surpassing those with prior knowledge.",
"25 Regarding management knowledge (Table 16), founders often bring foundational management principles into the venture, reporting extensive prior knowledge in traditional areas like project planning and management and traditional methodologies.",
"Despite this, the results show a substantial need for knowledge acquisition during the venture's development.",
"A considerable number of teams had to acquire expertise in all listed management knowledge items while building their startups, indicating that the practical realities and specific demands of a new software venture often necessitate learning on the job.",
"The landscape for business knowledge is mixed, showing both pre-existing expertise and a necessity for on-the-job learning (Table 17).",
"A notable portion of teams possessed extensive knowledge of several fundamental business analysis tools and frameworks before launching, suggesting they bring basic business acumen.",
"Concepts like SWOT analysis and business model definition show a slightly stronger base of prior knowledge, likely because they are foundational concepts taught in business education.",
"Conversely, a significant trend of acquiring business knowledge during the startup’s development is evident, with substantial numbers of teams having to learn and develop their understanding while building their companies.",
This highlights the nuanced nature of applying business principles in a new software venture.,
The data suggests a shift towards more learning during the venture for areas directly related to market engagement and strategic planning.,
"For instance, market analysis and marketing show more teams acquiring knowledge during the venture than having it beforehand.",
"For financial knowledge, there's a clear trend where more expertise is acquired during the venture's development than is possessed beforehand (Table 18).",
"Across all listed financial knowledge items, the number of teams that had to learn these skills on their startup journey exceeds the number that had extensive prior knowledge.",
This suggests that practical financial management in a new software business often necessitates significant on-the-job learning.,
"Areas like cash flow and investment analysis reveal a large gap between prior knowledge and what was learned during the venture, reflecting the complexities of managing finances in a growing startup.",
"Finally, the survey reveals that many crucial soft skills for entrepreneurial success were not well-developed in software startup teams prior to launching (Table 19).",
"The data suggests that skills traditionally associated with individual traits or foundational team dynamics, such as motivation and perseverance, teamwork, and interpersonal communications, are often developed before entrepreneurial pursuits.",
"These skills are built early through education, social settings, and early work experiences.",
"However, the intense demands of a startup environment necessitate further development.",
The data shows that many teams had to actively acquire or significantly improve skills as their startups grew.,
"The higher numbers for development during the venture for skills like opportunity recognition, value creation, and resource utilization indicate that these competencies become critical as the startup navigates the market and strives for sustainability.",
A notable crossover point occurs where the number of teams developing a skill during the venture surpasses those who felt it was well-developed before.,
"This shift 26 happens for skills including opportunity recognition, value creation, resource utilization, conveying a compelling vision, and developing and using contact networks.",
"6 Threats to validity When conducting an exploratory survey where the population size and number of potential respondents are unknown, several threats to validity can arise.",
"These threats impact the study's findings' credibility, reliability, and generalizability.",
One of the key threats is selection bias.,
"Since our exploratory survey relied on non-probability sampling (e.g., purposive or snowball sampling), the respondents may not represent the broader population.",
"This can lead to biased results, as only certain groups may be included, while others remain unrepresented.",
"For this study, access to potential respondents was obtained through various business incubators and organizations within the Colombian entrepreneurial ecosystem.",
"However, some software startups may not be associated with these incubators or organizations and, as a result, were not included in the distribution of the survey form.",
Another threat corresponds to sampling bias.,
"Without knowing the total population size, it is not easy to ensure that the sample adequately represents the diversity of the population.",
The 74 individuals who responded may possess characteristics or experiences that differ systematically from those who did not participate.,
"The sample might be too small or skewed toward startups who are easier to reach, leading to overrepresentation or underrepresentation of particular perspectives.",
A third threat to validity we want to comment on regards to response bias.,
"Participants may provide socially desirable answers or be influenced by their personal experiences, rather than reflecting broader trends.",
"In the context of our study targeting software startups affiliated with specific business incubators or organizations, response bias can affect the validity of the study’s findings.",
"Since respondents are drawn from specific incubators or entrepreneurial organizations, their views may be shaped by the culture, mentorship, and funding opportunities of those institutions.",
"This can lead to biased responses that reflect the incubator’s best practices and philosophies, rather than the broader reality of software startups outside these ecosystems.",
"To address this threat, we tried to collect responses from startups located in various geographical regions of Colombia, and affiliated with different incubators, accelerators, or independent startups to reduce institutional bias.",
The last threat we want to reflect on is external validity.,
Findings from our exploratory survey cannot be statistically generalized to a larger population.,
The primary limitation stems from the unknown number of software startups within Colombia and the potential for a non-representative sampling method.,
"While the survey reached software entrepreneurs in various regions of Colombia, the undetermined distribution method makes it difficult to ascertain if the 74 respondents accurately reflect the diverse landscape of Colombian software startups.",
"Furthermore, the exploratory nature of the survey itself can impact external validity.",
"Exploratory studies often aim to identify initial trends and insights rather than establish definitive, generalizable conclusions.",
"Implications for practitioners and researchers This section outlines key considerations about the relevance of the findings for software entrepreneurs, incubators, and researchers in entrepreneurship and software startups.",
The survey results guide future software entrepreneurs by highlighting the most critical technical knowledge for early-stage ventures.,
"Aspiring founders can focus on skill development in requirements engineering, testing, prototyping, agile methods, project management, business modeling, marketing, budgeting, and investment analysis.",
"This approach helps them prepare for initial challenges and increase success by addressing key operational, strategic, and financial areas early.",
Understanding valued knowledge areas also aids in hiring decisions and identifying where external expertise is needed.,
"For business incubators, these survey findings offer insights into the needs and gaps of early-stage software startup teams.",
"Incubators can then refine their programs with targeted workshops, mentorship, and resources to develop key technical, managerial, business, and financial skills.",
"This enhances support effectiveness, accelerating startup growth and viability.",
"The findings can also guide selection, prioritizing startups with foundational knowledge or commitment to essential skills, leading to better resource use and outcomes.",
Researchers in entrepreneurship and software startups can use these survey results as empirical data to explore the link between specific technical knowledge types and early startup success.,
"The categories and key knowledge items can help develop more detailed research questions and hypotheses on how different knowledge domains affect performance, survival, and growth.",
"This data deepens understanding of essential skills for successful software entrepreneurship, guiding better education, support, and models.",
Longitudinal studies could examine how the importance of these knowledge areas changes as startups grow.,
"Conclusions In the early stages of software startups, specific technical knowledge and soft skills are especially valued by entrepreneurial teams as they enable startups to move and learn quickly while maximizing limited resources in a challenging business environment.",
"A survey of founding partners and representatives of software startups from various regions of Colombia identified a set of soft skills and technical knowledge in software production, management, business, and finance that are deemed the most valuable in the early stages of these ventures.",
"The most valued technical knowledge encompasses requirements engineering, software testing, prototyping, coding, project planning and management, change management, agile methodologies, marketing, business model definition, investment analysis, and budgeting.",
"The most valued soft skills include teamwork, interpersonal communication, adaptability, and motivation.",
"As ventures grow, demand for new or enhanced technical knowledge and soft skills rises.",
Knowledge and skills that were unavailable or underdeveloped prior to the venture's launch must be obtained or improved by entrepreneurial teams during the venture's growth.,
"Online and in-person courses and hiring experts are preferred methods 28 for acquiring or improving technical knowledge, whereas experiential learning, coaching, and mentoring are favored for developing soft skills.",
"CRediT authorship contribution statement Royer David Estrada-Esponda: Conceptualization, Investigation, Data Curation, Formal analysis, Funding acquisition.",
"Gerardo Matturro: Conceptualization, Methodology, Investigation, Formal analysis, Writing - Original Draft, Writing - Review & Editing.",
"Reinaldo Sabogal: Conceptualization, Writing - Review & Editing, Supervision, Funding acquisition.",
Acknowledgement The authors would like to express their sincere gratitude to the Vicerrectoría de Investigación of Universidad del Valle for their financial support of this research project.,
"References (Berg, 2018) V. Berg, J. Birkeland, A. Nguyen-Duc, I. Pappas and L. Jaccheri, Software startup engineering: A systematic mapping study, The Journal of Systems & Software, No.",
"255-274, 2018.",
"(Bhatnaga, 2012) N. Bhatnaga, Effective communication and soft skills.",
"New Delhi: Dorling Kindersley, 2012.",
"(Cavalcante, 2018) B. Cavalcante, G. Lapasini, R. Balancieri and I. de Farias, Technical Aspects of Software Development in Startups: A Systematic Mapping, XLIV Latin American Computing Conference (CLEI 2018), 2018.",
"(Choma, 2022) J. Choma, E. Guerra, A. Alvaro, R. Pereira, and L. Zaina, Influences of UX factors in the Agile UX context of software startups, Information and Software Technology 152, 2022.",
"(Faulkner, 2022) P. Faulkner, Collective and extended knowledge, Philosophical Issues, 32(1), 200–213, 2022.",
"(Garcia Carvajal, 2022) S. Garcia Carvajal, Entrepreneurship in Colombia, In: L. Dana, C. Keen, V. Ramadani, Entrepreneurship in South America.",
"Context, Diversity, Constraints, Opportunities and Prospects, Springer, Cham, 2022.",
"(Gianesini, 2018) G. Gianesini, S. Cubico, G. Favretto and J. Leitão, Entrepreneurial Competences: Comparing and Contrasting Models and Taxonomies, in S. Cubico, G. Favretto, J. Leitão, and U. Cantner (Eds.)",
Entrepreneurship and the Industry Life Cycle.,
"Studies on Entrepreneurship, Berlin, Springer, 2018.",
"29 (Giardino, 2014) C. Giardino, M. Unterkalmsteiner, N. Paternoster, T. Gorschek and P. Abrahamsson, What Do We Know about Software Development in Startups?, IEEE Software, vol.",
"31, No 5, pp.",
"28-32, 2014.",
"(Guerino, 2024) G. Guerino, S. Martinelli, J. Choma, G. Leal, R. Balancieri, and L. Zaina, Investigating UX work in Software Startups: A Survey about Attitudes, Methods, and Key Challenges, Journal of the Brazilian Computer Society, 30:1, 2024.",
"(Hallam, 2018) C. Hallam and W. Flannery, Engineering the High Tech Start Up: Fundamentals and Theory.",
"New York: Momentum Press, 2018.",
"(Jones, 2021) D. Jones, Own your tech career, Manning, Shelter Island, 2021.",
"(Klotins, 2019) E. Klotins, M. Unterkalmsteiner and T. Gorschek, Software engineering in start-up companies: An analysis of 88 experience reports, Empirical Software Engineering, vol.",
"68-102, 2019.",
"(Komarkova, 2015) I. Komarkova, D. Gagliardi, J. Conrads and A. Collado, Entrepreneurship Competence: An Overview of Existing Concepts, Policies and Initiatives, Sevilla, 2015.",
"(KPMG, 2024) Colombia Tech Report 2023-2024; https://colombia.home.kpmg/ colombia-tech-report-2023-2024, 2024.",
"(Marr, 2022), B. Marr, Future skills.",
"The 20 skills and competencies everyone needs to succeed in a digital world, Wiley, Hoboken, 2022.",
"(Matturro, 2019) G. Matturro, F. Raschetti and C. Fontán, A Systematic Mapping Study on Soft Skills in Software Engineering, Journal of Universal Computer Science, vol.",
"25, No 1, pp.",
"16-41, 2019.",
"(Matturro, 2020) G. Matturro, M. Solari, A. Buffa, and D. Febbles, Technical knowledge and soft skills in the founding teams of software startups, Proceedings of the XXIII Iberoamerican Conference on Software Engineering (CIbSE 2020), Curitiba, pp.",
"476-489, 2020.",
"(Melegati, 2020) J. Melegati, R. Chanin, A.",
"Sales, R. Prikladnicki and X. Wang, MVP and experimentation in software startups: a qualitative survey, 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), Kranj, 2020.",
"30 (Mkpojiogu, 2019) E. Mkpojiogu, N. Hashim, A. Al-sakkaf and A. Hussain, Software Startups: Motivations for Agile Adoption, International Journal of Innovative Technology and Exploring Engineering, 8 (8), 2019.",
"(Pattyn, 2023) F. Pattyn, Improving Software Startup Viability: Addressing Requirements Prioritization Challenges amid Increasing Interest Rates, 14th International Conference on Software Business (ICSOB 2023), Lahti, 2023.",
"(Pauceanu, 2026) A. Pauceanu, The basics of business start-up, Routledge, New York, 2026.",
"(Prince, 2013) E. S. Prince, The advantage.",
The 7 soft skills you need to stay one step ahead.,
"Financial Times Press, 2013.",
"(Riyanti, 2003) B. Riyanti, C. Sandroto and M. Warmiyati, Soft Skill Competencies, Hard Skill Competencies, and Intention to Become Entrepreneur of Vocational Graduates, International research journal of business studies, vol.",
"9, No 2, pp.",
"119-132, 2016.",
"(Santisteban, 2017) J. Santisteban and D. Mauricio, Systematic Literature Review of Critical Success Factors of Information Technology Startups, Academy of Entrepreneurship Journal, Vol.",
"(Shane, 2003) S. Shane, E. Locke, and Ch.",
"Collins, Entrepreneurial motivation, Human Resource Management Review, vol.",
"13, No 2, pp.",
"257-279, 2003.",
"(Seppänen, 2017) P. Seppänen, K. Liukkunen, and M. Oivo, Little Big Team: Acquiring Human Capital in Software Startups, in 18th International Conference on Product- Focused Software Process Improvement (PROFES 2017), 2017, pp.",
"(Tanner, 2008) K. Tanner, The entrepreneur’s guide to hiring and building the team.",
"West-port: Praeger Publishers, 2008.",
"(Unterkalmsteiner, 2016) M. Unterkalmsteiner, P. Abrahamsson, X. Wang and A. Nguyen-Duc, Software Startups: A Research Agenda, e-Informatica Software Engineering Journal, vol.",
"89-123, 2016.",
"(Wrobel, 2018) M. Wrobel, Do You Have What It Takes to Become an Internet Entrepre- neur?",
"The Key Competencies of Successful Founders, in N. Richter, P. Jackson and Th.",
Schildhauer (Eds.),
"Entrepreneurial Innovation and Leadership, Berlin, Palgrave Macmillan, 2018, pp.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis KEN SAKAYORI, The University of Tokyo, Japan ANDREA COLLEDAN, University of Bologna, Italy and Centre Inria d’Université Côte d’Azur, France UGO DAL LAGO, University of Bologna, Italy and Centre Inria d’Université Côte d’Azur, France In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language.",
The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect.,
"In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled.",
"Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations.",
CCS Concepts: • Theory of computation →Denotational semantics; Program analysis; • Software and its engineering →Domain specific languages; • Hardware →Quantum computation.,
1 Introduction Quantum computing promises to revolutionize various sub-fields within computer science by solv- ing complex problems exponentially faster than classical computing [Shor 1994].,
"This technology leverages the concept of quantum bits (or qubits), a unit of information whose dynamics is governed by the rules of quantum mechanics, thus enabling superposition and entanglement, keys to the aforementioned speedup.",
"To harness this potential, several programming languages have been developed specifically for quantum computing, such as Q# [Svore et al.",
"2018], Qiskit [Javadi-Abhari et al.",
"2024], and Cirq [2025].",
"In turn, fields like program verification have adapted well-known techniques like abstract interpretation [Perdrix 2008; Yu and Palsberg 2021], type systems [Amy 2019; Colledan and Dal Lago 2025], and Hoare logic [Liu et al.",
2019; Ying et al.,
2017; Zhou et al.,
2019] to these languages.,
We can identify at least two ways to design a quantum programming language.,
"On the one hand, we could simply allow programs written in traditional programming languages to access not only classical data but also quantum data.",
"The latter cannot be used the same way as the former, and is rather supported by specific initialization, modification, and reading operations.",
"As an example, the reading of a qubit value, often called a measurement, can alter its value and has, in general, a probabilistic outcome, thus being substantially different from the corresponding classical operation.",
"In programming languages of this kind, the quantum data are assumed to be stored in an external device accessible interactively through the aforementioned operations.",
"This model, often indicated with the acronym QRAM [Knill 2022], is adopted by a multitude of proposals in the literature (see e.g.",
[Bettelli et al.,
2003; Sanders and Zuliani 2000; Selinger 2004; Selinger and Valiron 2005]).,
"In theory, QRAM languages are the natural adaptation of classical programming languages to the quantum world.",
"In practice, however, quantum hardware architectures can hardly be programmed interactively: not only is the number of qubits available very small, but the time within which computation must be completed should itself be minimized, given that the useful lifespan of a qubit is short.",
"Consequently, quantum architectures typically take as input a whole quantum circuit, i.e.",
a precise description of all the necessary qubits and the operations to be performed on them.,
"This circuit must therefore be available in its entirety, preferably already subjected to an Authors’ Contact Information: Ken Sakayori, The University of Tokyo, Japan; Andrea Colledan, University of Bologna, Italy and Centre Inria d’Université Côte d’Azur, France; Ugo Dal Lago, University of Bologna, Italy and Centre Inria d’Université Côte d’Azur, France.",
"arXiv:2511.22419v1 [cs.PL] 27 Nov 2025 2 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago aggressive optimization process.",
"In such a context, so-called circuit description languages (CDLs for short) are to be preferred, and most mainstream languages in the field, including Qiskit and Cirq, are of this nature.",
"CDLs are high-level languages used to describe and generate circuits, a quintessential example being the quantum circuit.",
"Circuits are typically seen like any other ordinary data structure, with specific operations on them available through, e.g., methods or subroutines.",
Directly manipulating circuits from within a classical program offers the advantage of having more direct control over their shape and size.,
"This is crucial given the state of quantum hardware architectures today, which provide a limited number of error-prone qubits, and for which not all operations can be implemented at the same cost.",
A peculiar circuit description language is Quipper [Green et al.,
"In Quipper, circuits are not just like any other data structure.",
"Rather, they are seen as the by-product of certain effect-producing computations that modify some underlying quantum circuit when executed.",
"This, combined with the presence of higher-order functions and operations meant to turn any term (of an appropriate type) into a circuit, makes Quipper a very powerful and flexible idiom.",
"Its metatheory has been the subject of quite some investigations by the programming language community in the last years, with contributions ranging from advanced type systems [Fu et al.",
"2020, 2022a] to fancy features like dynamic lifting [Colledan and Dal Lago 2023; Fu et al.",
"2022b, 2023] to denotational semantics [Fu et al.",
"2022b, 2023, 2022a; Lindenhovius et al.",
2018; Rios and Selinger 2017].,
"This last aspect of Quipper, in particular, has been studied by providing semantic models for some of the languages of the so-called Proto-Quipper family, which includes various calculi, such as Proto- Quipper-M [Rios and Selinger 2017], Proto-Quipper-D [Fu et al.",
"2022a], Proto-Quipper-Dyn [Fu et al.",
"2023], etc.",
"In these cases, such semantics are built around concepts such as that of a presheaf and turn out to take the shape of a LNL model [Benton 1994].",
A by-product of the use of presheaves is that the interpretation of the term and the underlying circuit are somehow merged into a single mathematical object.,
"As a result, it is difficult to read interesting features of the underlying circuit from the interpretation of a term or closure: what the circuit does and what the program does to produce the circuit are inextricably coupled.",
"This coupling, in turn, prevents those models from adequately accounting for variants of the Proto-Quipper family that are specifically designed to control the shape of the produced circuits, and more specifically to derive upper bounds on the size of the latter [Colledan and Dal Lago 2024, 2025].",
"The correctness of these systems has been proved by purely operational means, and a denotational semantics for them is still missing.",
This ultimately makes such systems somewhat rigid and complicates their definition.,
The aim of this paper is precisely to give a denotational semantics to languages in the Proto- Quipper family in which the interpretation of terms is kept separate from that of the produced circuit.,
"This is achieved by seeing circuit building as an indexed monad [Atkey 2009].1 Remarkably, this new point of view allows us to give semantics to languages such as Colledan and Dal Lago’s Proto-Quipper-R, and even allows us to justify some of their peculiarities.",
"The introduced semantic framework suggests a natural way to unify so-called local and global circuit metrics, at the same time allowing the definition of metrics that go substantially beyond those proposed by Colledan and Dal Lago, in particular accounting for simple forms of circuit optimization.",
"The contributions of this paper can be thus summarized as follows: • First, we give a simple type system for Proto-Quipper.",
"The introduced system is a slight variation on the theme of Proto-Quipper-M [Rios and Selinger 2017], whereas the input to the circuit produced by each effectful functional term needs to be exposed in its type 1Indexed monads are also known as and were originally called parameterized monads.",
"We avoid this name since other “parameters”, such as parameters of circuits or grades of a monad, appear in this work.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 3 and thus becomes an integral part of the arrow type, turning it into a closure type.",
"This change, as we will explain in the next section, seems inevitable since, without it, it would not be possible to know even the nature, i.e.",
"the type, of the circuit produced by the term in question.",
"Noticeably, closure types are present in Colledan and Dal Lago’s [2025] most recent contribution.",
We call this calculus with closure type Proto-Quipper-C. • We then show that Atkey’s indexed monad is an appropriate framework for giving denota- tional semantics to Proto-Quipper-C. By treating circuits as (pre)monoidal morphisms and considering the category-action indexed monad—a many-sorted generalization of the writer monad—we maintain a clear separation between the value a term evaluates to and the circuit produced alongside the evaluation.,
"Proto-Quipper-C is interpreted in the parameterized Freyd category induced by this indexed monad, making explicit how “parameters”, compu- tations, and circuits interact as these three notions are interpreted in different categories.",
The semantics is proved both sound and computationally adequate.,
"• We then move to richer type systems, where simple types are enriched by a form of effect typing.",
"The model based on indexed monads remains adequate, and suggests an abstract notion of circuit algebra, through which it is possible to capture various circuit metrics (including all those considered by Dal Lago and Colledan), but also new forms of metrics induced by assertion-based circuit optimization schemes [Häner et al.",
• We briefly discuss how dependent types might be incorporated into both the syntax and semantics of the variant of Proto-Quipper-C (without effect typing) introduced earlier.,
"On the semantic side, this is achieved by applying the families construction to the denotational model of Proto-Quipper-C in the spirit of fibered adjunction models [Ahman et al.",
The rest of this paper is structured as follows.,
"After the next section, which serves to frame the problem without going into the technical details, we move on to Section 3, in which Proto- Quipper-C, i.e.",
"a slight variation of the Proto-Quipper-M calculus, is introduced and endowed with an operational semantics.",
"In Section 4, then, a monadic denotational semantics for Proto-Quipper-C is introduced and proved adequate.",
"In Section 5, an extension of these results to a calculus with effect typing, along the lines of Proto-Quipper-R, is presented.",
Section 6 discusses the possibility of extending our framework to incorporate dependent types.,
"Section 7 discusses related work, and Section 8 concludes the paper.",
2 A Monadic Semantics for CDLs: Why and How?,
"In this section, we will describe in a little more detail the problem of giving a monadic denotational semantics to CDLs, focusing on how this can be done, but also on why such an effort might be worth it.",
"Typically, a program written in a CDL is just a program in a mainstream programming language (e.g.",
"Python, Haskell, or dialects thereof) whose purpose is that of facilitating the construction of (quantum) circuits which, once built, can then be sent to quantum hardware for their evaluation, or merely simulated through high-performance classical hardware.",
"As already mentioned, the CDL we are mainly concerned with is Quipper, which is embedded in Haskell.",
"A program written in any CDL, and particularly in Quipper, does not describe a single circuit but a family of circuits, depending on some parameters, e.g.",
"a number 𝑛representing the number of input qubits, or, in the case of Shor’s algorithm, the size of the natural number to be factored.",
"The ability to describe families of circuits enables Quipper to succinctly and elegantly describe quantum algorithms, thus having a pragmatic impact and attracting the attention of the programming language community [Fu et al.",
"2023, 2022a; Lindenhovius et al.",
"2018; Rios and Selinger 2017], which 4 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago proposed idealized languages capturing the essence of Quipper.",
"Such formal calculi come equipped with an operational semantics, type system, and often with a denotational semantics.",
"Most forms of denotational semantics for the Proto-Quipper family are based on presheaves, and enjoy a constructive property, which states that the interpretation of a judgment in a certain form is indeed a parameterized family of circuits.",
"For example, in the categorical semantics of Proto- Quipper-M [Rios and Selinger 2017], the judgment 𝑛: nat,𝑥: Qubit ⊢𝑀: Qubit is interpreted as a function J𝑀K: N →M(Qubit, Qubit), where M(Qubit, Qubit) is the set of circuits whose input and output interfaces both consist of a single qubit.",
"In the above, the type nat can be replaced by any “classical data type” and Qubit can be replaced by any “wire type”.",
"However, some judgments cannot be interpreted as a family of circuits in the same way, including terms with free variables with a function type.",
"As an example, a term of type 𝑛: nat,𝑥: (Qubit ⊸Qubit) ⊢𝑁: Qubit (1) which evaluates to a qubit type value while generating a circuit, would be interpreted as J𝑁K: N → Nat(JQubit →QubitK, JQubitK) where 𝛼∈Nat(JQubit →QubitK, JQubitK) is a natural transfor- mation (i.e.",
a morphism in the presheaf category).,
"Each component 𝛼𝑇has a type JQubit → QubitK(𝑇) →M(𝑇, Qubit) because JQubitK is defined via the Yoneda embedding ょ: M → [Mop, Set] and JQubitK(𝑇) =ょ(Qubit)(𝑇) = M(𝑇, Qubit).",
"In other words, and more informally, 𝑁is interpreted as a family of polymorphic functions {𝑓𝑖}𝑖∈N, each of them having type ∀(𝑇: WireType).",
"Clos(Qubit, Qubit)[𝑇] →M(𝑇, Qubit) The type Clos(Qubit, Qubit)[𝑇] represents a closure type disclosing the type 𝑇of the data that the closure captures.",
"Given a “closure” as input, 𝑓𝑖returns a circuit whose input and output interfaces are 𝑇and Qubit, respectively.",
"In a sense, then, not even the input type of the generated circuit can be read from J𝑁K(𝑛) because we need to know the actual data that will be passed to 𝑥to determine the interface of the circuit.",
"This implies that modular reasoning about the produced circuits cannot be easily performed within the model, in which it would be hard to interpret type systems specifically built for intensional analysis [Colledan and Dal Lago 2024].",
This paper introduces a denotational semantics for a CDL in which every judgment is interpreted as a family of circuits whose types are uniquely defined.,
"More specifically, any judgment Γ ⊢𝑀: 𝐴 is interpreted as a function J𝑀K: J♭ΓK →J♭𝐴K × M(J♯ΓK, J♯𝐴K), (2) where ♯and ♭are operations extracting the “circuit part” and “parametric part” of any type, respectively.",
"The mathematical object J𝑀K, in other words, is a family of pairs {(𝑣𝑖,𝐶𝑖)}𝑖∈J♭ΓK indexed by J♭ΓK where 𝑣𝑖is a “value” in J♭𝐴K and 𝐶𝑖is a circuit in M(J♯ΓK, J♯𝐴K); a family of circuits is a special case where 𝑣𝑖is a unit value.",
The fact that every judgment is interpreted as a family of circuits is important since it allows us to compositionally reason about the family of circuits generated by a program.,
"For example, upper bounds to the width of the circuits generated by any program can be computed by looking at the interpretation of the subprograms.",
"The just sketched construction evidently has the form of a monad [Moggi 1991], structurally very similar to a writer monad.",
"There is, however, one important difference: the type of circuit produced during the execution of a term is not fixed a priori.",
"As a consequence, the classical notion of monad, being somehow monomorphic, cannot be applied directly.",
"Instead, indexed monads [Atkey 2009] can be fruitfully employed to model the circuit generated by the underlying program.",
"In fact, the interpretation (2) can be rewritten as J𝑀K: J♭ΓK →T (J♯ΓK, J♯𝐴K, J♭𝐴K), (3) On Circuit Description Languages, Indexed Monads, and Resource Analysis 5 where T (𝑇,𝑈,𝑋) is an indexed monad defined as 𝑋× M(𝑇,𝑈).",
"Using such a monadic approach allows us to structure the interpretation of languages in the Proto-Quipper family in a new way, fundamentally different from that considered in the literature on the subject: every term is inter- preted as a mathematical object in which the value produced and the underlying circuit are kept separate.",
"Technically, we interpret terms in a parameterized Freyd category obtained by applying an indexed version of the well-known Kleisli construction to the indexed monad above.",
"If we look at Equation (3) in more detail, we soon realize that a monadic interpretation like the one we are discussing requires knowing J♯ΓK whenever the effectful term 𝑀is a value 𝜆𝑥.𝑀 (where 𝑥is any of the variables in Γ).",
"In other words, the “circuit portion” of Γ must become part of the (functional) type of 𝜆𝑥.𝑀.",
This last observation justifies the small discrepancies between Proto-Quipper-C (the language we present in Section 3 below) and Proto-Quipper-M and provides a denotational reading to some of the type-theoretical tricks in [Colledan and Dal Lago 2024].,
"As an example, the typing judgment (1) becomes 𝑛: nat,𝑥: (Qubit ⊸𝑇Qubit) ⊢𝑁: Qubit, where 𝑇 is the type of circuit variables the argument function captures.",
The advantage of moving to a monadic view like the one just described is that we have now exposed the space M of circuits in the interpretation.,
"As we will see in Section 5, in fact, this naturally suggests a way to control the size of the generated circuits through a form of effect typing: any well-behaved functor from M to a category E which captures the relevant characteristics of the underlying circuit, e.g.",
"its size, induces a form of effect typing which is sound by construction.",
"This is what denotational semantics is good for: not only is the programming language in question interpreted compositionally, but the interpretation naturally suggests what in the language might be subject to modification or adaptation while, at the same time, indicating what the underlying axiomatics should be, and factoring out most proofs.",
"3 Simple Types In this section, we introduce the syntax and operational semantics of Proto-Quipper-C, our dialect of Proto-Quipper-M [Rios and Selinger 2017].",
"We will point out the differences with the language Proto-Quipper-M, still trying to keep the presentation as self-contained as possible.",
3.1 Type and Syntax We first introduce the simple type system of Proto-Quipper-C.,
"The grammar for types is as follows: Types 𝐴, 𝐵F 𝑃| 𝑇| 𝐴⊗𝐵| 𝐴⊸𝑇𝐵 Parameter Types 𝑃, 𝑅F 1 | Nat | !𝐴| 𝑃⊗𝑅| Circ(𝑇,𝑈) Bundle Types 𝑇,𝑈F 𝐼| 𝑤| 𝑇⊗𝑈 There are three kinds of types.",
"In addition to generic types, which are intended for (not necessarily duplicable) terms, there are parameter types whose inhabitants are freely duplicable and which include circuits and values of type !𝐴.",
"There are the unit and natural number type as base types, but the specific choice of base types is not that important.",
"We also need bundle types, which are used to give a type to circuit wires.",
"Observe that the tensor product operator ⊗is available in the three kinds of types, while the construction of functions is not available in parameter and bundle types.",
Typing is almost the same as in Proto-Quipper-M as defined in [Rios and Selinger 2017].,
"There is a significant difference in the definition of the arrow type, however.",
"We annotate the arrow type with a bundle type 𝑇, which describes the types of the free variables captured by the function, effectively turning it into a form of closure type.",
Note that we only care about the bundle type variables that are captured by the function and drop the information of variables with parameter types.,
"A similar kind of annotation was used by Colledan and Dal Lago [2024], although in their 6 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago work the label was rather a natural number abstracting the type 𝑇.",
"We have already argued about the need for this change to the type system, and will come back to that in the next section.",
We might write 𝐴⊸𝐵for 𝐴⊸𝐼𝐵for the sake of simplifying the notation.,
"Another difference compared to Proto-Quipper-M (or Proto-Quipper-R, as defined in [Colledan and Dal Lago 2024]) is that we do not have a type for lists.",
We removed lists as they cannot be directly interpreted in our semantic model.,
"However, as we shall see in Section 6, we can extend our language with a vector type (i.e.",
a list with specified length).,
The syntax of Proto-Quipper-C terms is defined as follows.,
We use a fine-grained call-by-value style syntax [Levy et al.,
"2003] as done by Colledan and Dal Lago [2024].2 Terms 𝑀, 𝑁F 𝑉𝑊| let ⟨𝑥,𝑦⟩= 𝑉in 𝑀| ifz 𝑉then 𝑀else 𝑁 | force𝑉| box𝑇𝑉| apply(𝑉,𝑊) | return 𝑉| let 𝑥= 𝑀in 𝑁 Values 𝑉,𝑊F ∗| 𝑛| 𝑥| ℓ| 𝜆𝑥𝐴.𝑀| lift 𝑀| ( ¯ℓ,𝐶, ¯𝑘) | ⟨𝑉,𝑊⟩ Wire Bundles ¯ℓ, ¯𝑘F ∗| ℓ| ⟨¯ℓ, ¯𝑘⟩.",
"The informal behavior for terms is in line with that of Proto-Quipper-M, which adds to the usual constructs of a call-by-value linear lambda-calculus (abstractions, applications, let bindings, pairs, etc.)",
"specific operators for circuit manipulation: • ℓis a label, that is, a pointer to a wire in the underlying circuit.",
"• ( ¯ℓ,𝐶, ¯𝑘) is a boxed circuit and represents the circuit 𝐶as a value in the language.",
"Wire bundles ¯ℓand ¯𝑘represent the input and output interfaces of 𝐶, respectively.",
"• apply(𝑉,𝑊) appends a boxed circuit 𝑉to the wires identified by 𝑊among the outputs of the underlying circuit.",
• box𝑇𝑉evaluates a circuit building function 𝑉in isolation and returns the result as a boxed circuit.,
"At this point, we should make it clear what we mean by a “circuit”.",
"We do not fix what a circuit is (except when considering concrete examples), as is often the case for Proto-Quipper calculi.",
"Usually, when giving a semantics to Proto-Quipper, circuits are seen as morphisms in a monoidal category M and the semantics is parametric to the choice of M. In this work, we assume that the category M of circuits is a premonoidal category.",
"Roughly, a premonoidal category is a monoidal category without the interchange law 𝐶 𝐷 = 𝐶 𝐷 which is too strong in a cost sensitive scenario.",
"(For example, we may say that the two circuits above are different because the width of the circuit on the left-hand-side is 4 whereas the width of the circuit on the right-hand-side is 5.)",
Definition 1 (Premonoidal Category [Power and Robinson 1997]).,
"A binoidal category is a category A equipped with, for each 𝑎∈Obj(A), endofunctors 𝑎⋊−and −⋉𝑎from A to A such that 2It is easy to extend the language with effect-free constants such as arithmetic operations or or meta-operations on circuits as in [Rios and Selinger 2017], but we we omit these, since they are immaterial to our main semantic results.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 7 𝑎⋉𝑏= 𝑎⋊𝑏 def= 𝑎⊗𝑏for every pair of objects (𝑎,𝑏) of A.",
"A morphism 𝑓: 𝑎→𝑏in A is central if it interchanges with any morphism 𝑔: 𝑎′ →𝑏′: (𝑓⋉𝑎′); (𝑏⋊𝑔) = (𝑎⋊𝑔); (𝑓⋉𝑏′) and (𝑎′ ⋊𝑓); (𝑔⋉𝑏) = (𝑔⋉𝑎); (𝑏′ ⋊𝑓).3 In case two composites agree, we write 𝑓⊗𝑔and 𝑔⊗𝑓, respectively.",
"A premonoidal category is a binoidal category equipped with an object 𝐼, the “monoidal unit”, central natural (separately at each given component) isomorphisms for associativity and right and left units satisfying the standard pentagon and triangle equations.",
"A premonoidal category is strict if all the coherence morphisms are identities, and is symmetric if it has a central isomorphism 𝑎⊗𝑏 𝑏⊗𝑎that is natural (at each component) and satisfies the usual axioms of a symmetry.",
"◁ Typically, the category of circuits M is defined by giving a syntactic description of circuits.",
"In other words, it is the free category generated by some collection of base types and gates.",
"While we do not fix the category M, we assume that M has some distinguished objects Q and B that are used to interpret qubits and classical bits, respectively.",
"We also assume that M is small and a strict symmetric premonoidal category.4 A typing judgment for terms is of the form Γ ⊢𝑐𝑀: 𝐴, and intuitively means that 𝑀is well-typed under the typing context Γ; similarly, we have a typing judgment for values of the form Γ ⊢𝑣𝑉: 𝐴.",
A typing context Γ is a finite sequence of bindings each of which is either of the form 𝑥: 𝐴or ℓ: 𝑤.,
"The reason for using finite sequences rather than finite sets is to simplify the definition of the semantics; if two contexts Γ1 and Γ2 are equal up to permutation, then we write Γ1 𝜎Γ2.",
"We use metavariables 𝑎,𝑏, .",
to denote variables or labels.,
"A typing context is a label context if it is of the form ℓ1 : 𝑤1, .",
"Label contexts are denoted by 𝑄, 𝐿.",
"A parameter context, written Φ, is a typing context that only contains variables with parameter types.",
"Typing rules are in Figure 1, and most of them are self-explanatory.",
"In the typing rules, when we write Φ, Γ, we stipulate that Γ does not contain any variable with a parameter type.",
"Rules circ, box and apply are specific to a CDL, and thus warrant discussion.",
"A boxed circuit ( ¯ℓ,𝐶, ¯𝑘) is well-typed if the labels ¯ℓand ¯𝑘acting as language-level interfaces to 𝐶have types that match with the (co)domain of 𝐶.",
"The notation 𝐶: 𝑄→𝐿means that 𝐶is a morphism from J𝑄K to J𝐿K in M; J𝑄K is the obvious interpretation, which we formally define in Section 4.",
"The box rule says that if 𝑉 is a circuit building function that, once applied to an input of type 𝑇, builds a circuit of output type 𝑈, then 𝑉can be turned into a circuit whose interface has types 𝑇and 𝑈.",
Note that the box rule requires the typing context to be Φ so as to ensure that the function is not capturing any variable with a bundle type.,
"The rule apply, on the other hand, can be read as a special version of the typing rule for function application.",
"Once again, the main novelty with respect to Proto-Quipper-M has to do with the arrow type.",
"The operation ♯used in the abs rule extracts a bundle type from any type 𝐴.5 Formally, the operation ♯is inductively defined as follows: ♯(𝑃) def= 𝐼 ♯(𝐼) def= 𝐼 ♯(𝑤) def= 𝑤 ♯(𝐴⊸𝑇𝐵) def= 𝑇 ♯(𝑃⊗𝑅) def= ♯(𝑃) ⊗♯(𝑅) ♯(𝑇⊗𝑈) def= ♯(𝑇) ⊗♯(𝑈).",
"We let the operator ♯act on typing contexts by ♯(𝑎1 : 𝐴1, .",
𝑎𝑛: 𝐴𝑛) def= ♯(𝐴1) ⊗· · · ⊗♯(𝐴𝑛).,
It should now be clear that the rule abs does nothing more than inserting the bundle type of the variables free in the abstraction we are typing into its arrow type.,
3We use diagrammatic order for compositions in this paper.,
4Every premonoidal category is equivalent to a strict one since the coherence theorem holds [Power and Robinson 1997].,
5A similarly looking operation was called wire count in [Colledan and Dal Lago 2024].,
We do not use this name since we extract the whole type and not just a natural number counting “how many wires are used”.,
"8 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago unit Φ ⊢𝑣∗: 1 nat Φ ⊢𝑣𝑛: Nat lab Φ, ℓ: 𝑤⊢𝑣ℓ: 𝑤 var Φ,𝑥: 𝐴⊢𝑣𝑥: 𝐴 abs Γ,𝑥: 𝐴⊢𝑐𝑀: 𝐵 Γ ⊢𝑣𝜆𝑥𝐴.𝑀: 𝐴⊸#(Γ) 𝐵 app Φ, Γ1 ⊢𝑣𝑉: 𝐴⊸𝑇𝐵 Φ, Γ2 ⊢𝑣𝑊: 𝐴 Φ, Γ1, Γ2 ⊢𝑐𝑉𝑊: 𝐵 lift Φ ⊢𝑐𝑀: 𝐴 Φ ⊢𝑣lift 𝑀: !𝐴 force Φ ⊢𝑣𝑉: !𝐴 Φ ⊢𝑐force𝑉: 𝐴 circ 𝐶: 𝑄→𝐿 𝑄𝜎𝑄′ 𝐿𝜎𝐿′ 𝑄′ ⊢𝑣¯ℓ: 𝑇 𝐿′ ⊢𝑣¯𝑘: 𝑈 Φ ⊢𝑣( ¯ℓ,𝐶, ¯𝑘) : Circ(𝑇,𝑈) box Φ ⊢𝑣𝑉: 𝑇⊸𝐼𝑈 Φ ⊢𝑐box𝑇𝑉: Circ(𝑇,𝑈) apply Φ, Γ1 ⊢𝑣𝑉: Circ(𝑇,𝑈) Φ, Γ2 ⊢𝑣𝑊: 𝑇 Φ, Γ1, Γ2 ⊢𝑐apply(𝑉,𝑊) : 𝑈 dest Φ, Γ1 ⊢𝑣𝑉: 𝐴⊗𝐵 Φ, Γ2,𝑥: 𝐴,𝑦: 𝐵⊢𝑐𝑀: 𝐶 Φ, Γ2, Γ1 ⊢𝑐let ⟨𝑥,𝑦⟩= 𝑉in 𝑀: 𝐶 ifz Φ ⊢𝑣𝑉: Nat Φ, Γ ⊢𝑐𝑀: 𝐴 Φ, Γ ⊢𝑐𝑁: 𝐴 Φ, Γ ⊢𝑐ifz 𝑉then 𝑀else 𝑁: 𝐴 pair Φ, Γ1 ⊢𝑣𝑉: 𝐴 Φ, Γ2 ⊢𝑣𝑊: 𝐵 Φ, Γ1, Γ2 ⊢𝑣⟨𝑉,𝑊⟩: 𝐴⊗𝐵 return Γ ⊢𝑣𝑉: 𝐴 Γ ⊢𝑐return 𝑉: 𝐴 let Φ, Γ1 ⊢𝑐𝑀: 𝐴 Φ, Γ2,𝑥: 𝐴⊢𝑐𝑁: 𝐵 Φ, Γ2, Γ1 ⊢𝑐let 𝑥= 𝑀in 𝑁: 𝐵 ex Γ1,𝑎: 𝐴,𝑏: 𝐵, Γ2 ⊢𝑐𝑀: 𝐶 Γ1,𝑏: 𝐵,𝑎: 𝐴, Γ2 ⊢𝑐𝑀: 𝐶 Fig.",
Typing Rules for Proto-Quipper-C.,
"The box rule is slightly different from the one commonly seen in the Proto-Quipper family, in which box is a coercion from !",
"(𝑇⊸𝑈) to Circ(𝑇,𝑈).",
"In our rule, instead, we drop the !",
"Intuitively, !",
is needed to ensure that the function with type 𝑇⊸𝑈does not capture any variable with bundle type.,
"But this information is already explicit in our type system by the subscript 𝐼, and there is no reason to additionally require the of-course modality.",
We will later also give a semantic explanation against this design choice (see Proposition 1 and the remark after it).,
3.2 Operational Semantics The operational semantics is defined as a big-step evaluation relation on configurations.,
"A config- uration is a pair (𝐶, 𝑀), where 𝐶is a circuit being generated and 𝑀is the term being evaluated.",
The definition of the big-step evaluation relation ⇓is in Figure 2.,
(The rule for evaluating the else branch of ifz is omitted.),
"The box rule relies on the freshlabels function, which is used to produce a fresh label context 𝑄 and a wire bundle ¯ℓsuch that 𝑄⊢𝑣¯ℓ: 𝑇.",
"On the other hand, the apply rule relies on the append function, which attaches the circuit 𝐷to the wires identified by ¯𝑡among the outputs of 𝐶.",
"This operation often requires a renaming of the labels in 𝐷, so that its input interface ¯ℓmatches ¯𝑡.",
"More formally, we say that two boxed circuits ( ¯ℓ, 𝐷, ¯𝑘) and ( ¯ℓ′, 𝐷′, ¯𝑘′) are equivalent, and we write ( ¯ℓ, 𝐷, ¯𝑘)  ( ¯ℓ′, 𝐷′, ¯𝑘′), if they only differ by a renaming of labels.",
"What append does, then, is find On Circuit Description Languages, Indexed Monads, and Resource Analysis 9 app (𝐶, 𝑀[𝑉/𝑥]) ⇓(𝐷,𝑊) (𝐶, (𝜆𝑥𝐴.𝑀) 𝑉) ⇓(𝐷,𝑊) dest (𝐶, 𝑀[𝑉/𝑥][𝑊/𝑦]) ⇓(𝐷,𝑋) (𝐶, let ⟨𝑥,𝑦⟩= ⟨𝑉,𝑊⟩in 𝑀) ⇓(𝐷,𝑋) if-zero (𝐶, 𝑀) ⇓(𝐷,𝑉) (𝐶, ifz 0 then 𝑀else 𝑁) ⇓(𝐷,𝑉) force (𝐶, 𝑀) ⇓(𝐷,𝑉) (𝐶, force(lift 𝑀)) ⇓(𝐷,𝑉) apply (𝐸, ¯𝑞) = append(𝐶, ¯𝑡, ( ¯ℓ, 𝐷, ¯𝑘)) (𝐶, apply(( ¯ℓ, 𝐷, ¯𝑘), ¯𝑡)) ⇓(𝐸, ¯𝑞) box (𝑄, ¯ℓ) = freshlabels(𝑇) (𝑖𝑑𝑄,𝑉¯ℓ) ⇓(𝐷, ¯𝑘) (𝐶, box𝑇𝑉) ⇓(𝐶, ( ¯ℓ, 𝐷, ¯𝑘)) return (𝐶, return 𝑉) ⇓(𝐶,𝑉) let (𝐶, 𝑀) ⇓(𝐸,𝑉) (𝐸, 𝑁[𝑉/𝑥]) ⇓(𝐷,𝑊) (𝐶, let 𝑥= 𝑀in 𝑁) ⇓(𝐷,𝑊) Fig.",
Proto-Quipper-C big-step operational semantics.,
"(¯𝑡, 𝐷′, ¯𝑞)  ( ¯ℓ, 𝐷, ¯𝑘) and return ¯𝑞, along with a circuit 𝐸defined as follows: 𝐶 ¯𝑡 𝐷′ ¯𝑞 Overall, this semantics is the same as the one given in [Colledan and Dal Lago 2024], except for the box rule, which is modified to align with the modifications made in the typing rules.",
"3.3 Type Preservation We write 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′ and say that the configuration (𝐶, 𝑀) is well-typed under 𝑄and 𝑄′ if 𝐶: 𝑄→𝐿,𝑄′ and 𝐿⊢𝑐𝑀: 𝐴for some label context 𝐿disjoint from 𝑄′.",
"Similarly, we write 𝑄⊢(𝐶,𝑉) : 𝐴;𝑄′ if 𝑄⊢(𝐶, return 𝑉) : 𝐴;𝑄′.",
We have the following type preservation theorem.,
Theorem 2 (Type Preservation).,
"If 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′ and (𝐶, 𝑀) ⇓(𝐷,𝑉), then 𝑄⊢(𝐷,𝑉) : 𝐴;𝑄′.",
"By induction of the derivation of (𝐶, 𝑀) ⇓(𝐷,𝑉).",
"□ 4 A Monadic Semantics for Proto-Quipper-C Here we define a new monadic denotational semantics for Proto-Quipper-C, introduced in Section 3.",
We first explain the categorical structure we use such as the indexed monad for circuits.,
"We then define the interpretation, and prove its soundness and adequacy.",
"4.1 The Circuit Monad We now review the notion of indexed monads [Atkey 2009] (aka parameterized monads), which plays a key role in our model.",
"Intuitively, an indexed monad is a “multi-sorted generalization” of a monad.",
Its formal definition is given as follows.,
Definition 3 (Indexed Monad [Atkey 2009]).,
Let C be any cartesian category and S be a category.,
"A (strong) S-indexed monad on C is a quadruple (T,𝜂, 𝜇,𝜏) where • T : Sop × S × C →C is a functor • the unit 𝜂is a family of morphisms 𝜂𝑆,𝑋: 𝑋→T (𝑆,𝑆,𝑋) natural in 𝑋and dinatural in 𝑆 10 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago • the multiplication 𝜇is a family of morphisms 𝜇𝑆1,𝑆2,𝑆3,𝑋: T (𝑆1,𝑆2, T (𝑆2,𝑆3,𝑋)) →T (𝑆1,𝑆3,𝑋) natural in 𝑆1, 𝑆3 and 𝑋and dinatural in 𝑆2.",
"• the strength 𝜏is a family of morphisms 𝜏𝑋,𝑆1,𝑆2,𝑌: 𝑋×T (𝑆1,𝑆2,𝑌) →T (𝑆1,𝑆2,𝑋×𝑌) natural in 𝑋, 𝑆1, 𝑆2 and 𝑌.",
The unit and multiplication must obey the evident monad laws and the axiom for strength (cf.,
Definition 9).,
◁ The indexed monad we are interested in is the circuit monad TM : Mop × M × Set →Set.,
"The circuit monad is defined as follows6: TM(𝑇,𝑈,𝑋) def= 𝑋× M(𝑇,𝑈) 𝜂𝑇,𝑋(𝑥) def= (𝑥, id𝑇) 𝜇𝑇1,𝑇2,𝑇3,𝑋((𝑥, 𝑓),𝑔) def= (𝑥, 𝑓;𝑔) 𝜏𝑋,𝑇,𝑈,𝑌(𝑥, (𝑦, 𝑓)) def= ((𝑥,𝑦), 𝑓).",
"So, the unit augments a value with the identity circuit and multiplication is just a sequential compo- sition of circuits.",
"Since we assumed that the category of circuits is premonoidal, this premonoidal structure lifts to TM (in the sense of [Atkey 2009, Def.",
"That is, there is a natural transformation (𝑇⋊−)† 𝑈1,𝑈2,𝑋: TM(𝑈1,𝑈2,𝑋) →TM(𝑇⊗𝑈1,𝑇⊗𝑈2,𝑋) satisfying certain desired properties.",
"In elementary terms, this is merely the map that associates (𝑥,𝐶) to (𝑥,𝑇⋊𝐶).",
"As one might expect, we will interpret terms in the Kleisli category of TM, which we now briefly explain.",
"Given an S-indexed monad T on C, its Kleisli category CT is a category whose objects are pairs of C and S objects and homsets CT((𝑋,𝑇), (𝑌,𝑈)) def= C(𝑋, T (𝑇,𝑈,𝑌)).",
The identity morphisms and composition of morphisms are defined using units and multiplication as the obvious generalization of those in the Kleisli category of an ordinary monad.,
"It is known that the Kleisli category induces a parameterized Freyd category [Atkey 2009] 𝐽: C × S →CT as is the case for the ordinary monad and Freyd category; here 𝐽is an identity on objects functor that strictly preserves the premonoidal structure of C. Since Freyd categories are known to have a better match with the fine-grained call-by-value syntax, our semantical model will be based on the parametrized Freyd category induced by TM.",
"The circuit monad TM also has Kleisli exponentials: there is a functor 𝑋⇒𝑇−: SetTM →Set for every objects 𝑋,𝑇, and there is a natural isomorphism Λ𝑌,(𝑋,𝑇),(𝑍,𝑈) : SetTM ((𝑌× 𝑋,𝑇), (𝑍,𝑈))  Set(𝑌,𝑋⇒𝑇(𝑍,𝑈)).",
This means that the parameter- ized Freyd category induced by TM is a closed parameterized Freyd category.,
"We note that the object 𝑋⇒𝑇(𝑍,𝑈) is just 𝑋⇒Set 𝑍× M(𝑈,𝑇), the set of functions from 𝑋to 𝑍× M(𝑈,𝑇).",
"The counit of this adjunction is written as ev: ((𝑋⇒𝑇(𝑌,𝑈)) × 𝑋,𝑇) →(𝑌,𝑈).",
"The Kleisli category SetTM is a premonoidal category, where (𝑋,𝑇) ⊗(𝑌,𝑈) = (𝑋×𝑌,𝑇⊗𝑈) and the binoidal functors are defined in a way analogous to how the premonoidal structure was lifted to TM.",
"7 4.2 Interpreting Types, Programs, and Configurations We now give the denotational semantics of Proto-Quipper-C using the Freyd category induced by TM.",
"In Figure 3, we summarize the overall structure of the interpretation.",
"Terms will be inter- preted in the Kleisli category SetTM, and values will be interpreted in Set × disc(Obj(M)), where disc(Obj(M)) is the discrete category whose objects are those of M. 6This is just an indexed monad for category actions [Atkey 2009] and we do not claim any novelty in this definition.",
"7This should not be confused with the premonoidal structure of a parameterized Freyd category with respect to the cartesian category, which exist even if S is not premonoidal.",
See also Appendix C for a review on parameterized Freyd categories.,
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 11 𝜄𝑇 𝐽 𝑋⇒𝑇− Set × disc(Obj(M)) = V Set Set × M SetTM Fig.",
"Overview of the model for Proto-Quipper-M. J𝐴K ∈Set × M J𝑃K def= (J𝑃K𝑃, 𝐼) J𝑇K def= (1, J𝑇KM) J𝐴⊸𝑇𝐵K def= (♭J𝐴K ⇒J𝑇KM ⊗J♯𝐴KM J𝐵K, J𝑇KM) J𝐴⊗𝐵K def= (𝑋× 𝑌,𝑇⊗𝑈) where J𝐴K = (𝑋,𝑇) and J𝐵K = (𝑌,𝑈) J𝑃K𝑃∈Set J1K𝑃 def= 1 JNatK𝑃 def= N J𝑃⊗𝑅K𝑃 def= J𝑃K𝑃× J𝑅K𝑃 J!𝐴K𝑃 def= 1 ⇒𝐼J𝐴K JCirc(𝑇,𝑈)K𝑃 def= M(J𝑇KM, J𝑈KM) J𝑇KM ∈M JQubitKM def= Q JBitKM def= B J𝐼KM def= 𝐼 J𝑇⊗𝑈KM def= J𝑇KM ⊗J𝑈KM Fig.",
Interpretation of Simple Types.,
The interpretation of types is given in Figure 4.,
"Types are interpreted as objects in Set × M, i.e.",
as pairs of a set and an object of M. We write ♯: Set × M →M and ♭: Set × M →Set for the obvious forgetful functors.,
The object 𝑋⇒𝑇𝑌used in the interpretation of the arrow type is the parameterized Kleisli arrow of the closed parameterized Freyd category.,
It is used to model the type of a “code” under the idea that a closure is a pair of a code and an environment.,
The subsctipt 𝑇represents the type of the additional bundled typed arguments corresponding to the free variables.,
"Note that !𝐴is interpreted as (1 ⇒𝐼J𝐴K, 𝐼) representing closures that do not capture any free variables having a bundle type.",
"The objects Q and B are the interpretation of qubits and bits, respectively, that we assumed to exist in M. Lemma 1.",
"Given a type 𝐴, we have J♯𝐴KM = ♯J𝐴K.",
"In our interpretation, circuit types and function types for bundle types are isomorphic, and this supports our design choice for the typing rule box.",
Proposition 1.,
"We have an isomorphism box : ♭J𝑇⊸𝑈K  JCirc(𝑇,𝑈)K𝑃in Set.",
"Therefore, J𝑇⊸𝑈K  JCirc(𝑇,𝑈)K in SetTM.",
"By unrolling the definition, we have the following obvious isomorphisms for sets ♭J𝑇⊸𝑈K = 1 ⇒J𝑇KM (1, J𝑈KM)  1 ⇒Set 1 × M(J𝑇KM, J𝑈KM)  M(J𝑇KM, J𝑈KM) = JCirc(𝑇,𝑈)K𝑃.",
"The isomorphism between J𝑇⊸𝑈K  JCirc(𝑇,𝑈)K is given by 𝐽(box, id𝐼).",
"□ 12 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago JΓ ⊢𝑣𝑉: 𝐴K JΦ ⊢𝑣∗: 1K def= (!JΦK, id𝐼) JΦ, ℓ: 𝑤⊢𝑣ℓ: 𝑤K def= (!JΦK𝑃, idJ𝑤KM) JΦ,𝑥: 𝐴⊢𝑣𝑥: 𝐴K def= (𝜋𝑥, id𝐼) JΓ ⊢𝑣𝜆𝑥.𝑀: 𝐴⊸#(Γ) 𝐵K def= (Λ(J𝑀K), idJ#(Γ)KM) JΦ ⊢𝑣lift 𝑀: !𝐴K def= (Λ(JΦ ⊢𝑐𝑀: 𝐴K), id𝐼) JΦ ⊢𝑣( ¯ℓ,𝐶, ¯𝑘) : Circ(𝑇,𝑈)K def= (!JΦK;  J( ¯ℓ,𝐶, ¯𝑘)KM, id𝐼) where J( ¯ℓ,𝐶, ¯𝑘)KM def= J𝑇KM −→J𝑄KM 𝐶−→J𝐿KM −→J𝑈KM ˆ𝐶 def= 1 Λ(𝐽(id1,𝐶)) −−−−−−−−−→1 ⇒J𝑇KM (1, J𝑈KM) box −−−−−−−−−→  M(J𝑇KM, J𝑈KM) JΦ, Γ1, Γ2 ⊢𝑣⟨𝑉,𝑊⟩: 𝐴⊗𝐵K def= JΦK ⊗JΓ1K ⊗JΓ2K (ΔJΦK,id𝐼)⊗id −−−−−−−−−−→JΦK ⊗JΦK ⊗JΓ1K ⊗JΓ2K  −−−−−−−→(JΦK ⊗JΓ1K) ⊗(JΦK ⊗JΓ2K) J𝑉K⊗J𝑊K −−−−−−−→J𝐴⊗𝐵K Fig.",
Interpretation of Values.,
"As briefly mentioned, in Proto-Quipper calculi, the box operator usually coerces a function of type !",
(𝑇⊸𝑈) to a circuit type.,
"This is because, in presheaf models of Proto-Quipper, there is an isomorphism J!",
"(𝑇⊸𝑈)K  JCirc(𝑇,𝑈)K. In our model, adding a bang to the type 𝑇⊸𝑈means to additionally thunk a function that has no free variable having a bundle type.",
"Invoking this thunk may cause some effects, i.e.",
produce a circuit while returning a function that corresponds to a boxed circuit.,
"Hence, we do not have an isomorphism between J!",
"(𝑇⊸𝑈)K and JCirc(𝑇,𝑈)K. While our calculus draws inspiration from those by Colledan and Dal Lago [2024, 2025], they also coerce !",
"(𝑇⊸𝑈) to Circ(𝑇,𝑈).",
"In their works, the issue of the effect is circumvented using the effect system; either by requiring that !",
(𝑇⊸𝑈) has zero effect or by adding an effect annotation to the circuit type.,
◁ Now we define the interpretation of typing judgments.,
The interpretation is defined in Fig- ure 5 and 6.,
A valid typing judgment for values Γ ⊢𝑣𝑉: 𝐴is interpreted as a morphism JΓ ⊢𝑣𝑉: 𝐴K: JΓK →J𝐴K in V(= Set×disc(M)) capturing the fact that values only produces trivial circuits.,
"In contrast, a computational judgment Γ ⊢𝑐𝑀: 𝐴is interpreted as JΓ ⊢𝑐𝑀: 𝐴K: JΓK →J𝐴K in SetTM.",
We sometimes denote these morphisms by J𝑉K and J𝑀K.,
"The morphisms, Δ, 𝜋𝑥and !",
"in Figure 5 are the diagonal map, projection, and the unique map to the terminal object 1, respectively, which exist in Set; the morphism dup𝑋in Figure 6 is defined as 𝐽(Δ𝑋, id𝐼) and it is used for du- plicating a values with a parameter type.",
Some obvious coherence isomorphisms of the cartesian product in Set are omitted for simplicity.,
"The interpretation of values is standard, except for 𝜆-abstractions and boxed circuits.",
A 𝜆- abstraction is interpreted as a closure.,
"The first element of J𝜆𝑥.𝑀K is the semantic counterpart of the function that takes variables of types ♯(Γ) as additional parameters, and the second element idJ♯ΓK is the “environment”.",
The interpretation of lift 𝑀is just the interpretation of a thunk 𝜆().𝑀.,
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 13 JΓ ⊢𝑐𝑀: 𝐴K JΦ, Γ1, Γ2 ⊢𝑐𝑉𝑊: 𝐵K def= JΦK ⊗JΓ1K ⊗JΓ2K dupJΦK⊗JΓ2K⊗JΓ1K −−−−−−−−−−−−−−→JΦK ⊗JΦK ⊗JΓ1K ⊗JΓ2K  −−−−−−−−−→(JΦK ⊗JΓ1K) ⊗(JΦK ⊗JΓ2K) 𝐽(J𝑉K)⊗𝐽(J𝑊K) −−−−−−−−−−−−→J𝐴⊸𝑇𝐵K ⊗J𝐴K ev −→J𝐵K JΦ ⊢𝑐force𝑉: 𝐴K def= 𝐽(JΦ ⊢𝑣𝑉: !𝐴K); ev JΦ, Γ1, Γ2 ⊢𝑐apply(𝑉,𝑊) :𝑈K def= JΦK ⊗JΓ1K ⊗JΓ2K dupJΦK⊗JΓ2K⊗JΓ1K −−−−−−−−−−−−−−→JΦK ⊗JΦK ⊗JΓ1K ⊗JΓ2K  −−−−−−−−−→(JΦK ⊗JΓ1K) ⊗(JΦK ⊗JΓ2K) 𝐽(J𝑉K)⊗𝐽(J𝑊K) −−−−−−−−−−−−→JCirc(𝑇,𝑈)K ⊗J𝑈K apply −−−−→J𝑈K JΦ ⊢𝑐box𝑇𝑉: Circ(𝑇,𝑈)K def= JΦK 𝐽(J𝑉K) −−−−−→J𝑇⊸𝑈K 𝐽(box,id𝐼) −−−−−−−→  JCirc(𝑇,𝑈)K JΓ ⊢𝑐return 𝑉: 𝐴K def= 𝐽(JΓ ⊢𝑣𝑉: 𝐴K) JΦ, Γ2, Γ1 ⊢𝑐let𝑥= 𝑀in 𝑁:𝐵K def= JΦK ⊗JΓ2K ⊗JΓ1K (dupJΦK⊗JΓ1K⊗JΓ2K); −−−−−−−−−−−−−−−−−→(JΦK ⊗JΓ2K) ⊗(JΦK ⊗JΓ1K) (JΦK⊗JΓ2K)⋊J𝑀K −−−−−−−−−−−−−→JΦK ⊗JΓ2K ⊗J𝐴K J𝑁K −−−→J𝐵K JΓ1,𝑎: 𝐴,𝑏: 𝐵, Γ2 ⊢𝑐𝑀: 𝐶K = JΓ1K ⊗J𝐴K ⊗J𝐵K ⊗JΓ2K −→JΓ1K ⊗J𝐵K ⊗J𝐴K ⊗JΓ2K J𝑀K −−−→J𝐶K Fig.",
Interpretation of computational judgments of the simple type system (excerpt).,
"In the interpretation of boxed circuits, we use isomorphisms J𝑇KM  J𝑄KM and J𝐿KM  J𝑈KM that exist thanks to the premises of the typing rule circ such as 𝑄𝜎𝑄′ and 𝑄′ ⊢𝑣¯ℓ: 𝑇.",
"The important part of the interpretation of a boxed circuit 𝐶is the map ˆ𝐶, which is just the global element ˆ𝐶(∗) def= 𝐶.",
The definition using Λ and box emphasizes the idea that boxed circuits can be seen as special functions.,
"As for the interpretation of terms, the interpretation of apply(𝑉,𝑊) and box𝑇𝑉are the most interesting cases.",
"The morphism apply𝑇,𝑈: (M(𝑇,𝑈), 𝐼) ⊗(1,𝑇) →(1,𝑈) is defined by (M(𝑇,𝑈), 𝐼) ⊗(1,𝑇) 𝐽(box−1,id𝐼)⊗id(1,𝑇) −−−−−−−−−−−−−−−→(1 ⇒𝑇(1,𝑈), 𝐼) ⊗(1,𝑇) ev −→(1,𝑈).",
The box operator is interpreted by the post-composition of the isomorphism between function types and circuit types given in Proposition 1.,
The interpretation of the let operator is also worth explaining.,
The premonoidal product (JΦK ⊗JΓ2K) ⋊J𝑀K adds wires of type ♯JΓ2K (and ♯JΦK = 𝐼 which can be ignored) to the circuit produced by 𝑀so that 𝑁can use these wires.,
The interpretation is somewhat unorthodox in that some syntactic constructs do not have a corresponding semantic operator.,
This is because we are doing two things at once.,
"The interpretation can be factorized into (1) a syntactic translation from Proto-Quipper-C to the internal language of parametrized Freyd categories (called the command calculus) [Atkey 2009] and (2) 14 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago interpreting the translated term.",
"The syntactic translation resembles a variant of closure conversion, but we are not sure if a category theoretic explanation can be given to this translation.",
◁ The categorical semantics is correct with respect to the big-step operational semantics.,
To further elaborate on this result we first extend the interpretation to configurations.,
"Intuitively, J(𝐶, 𝑀)K is the morphism obtained by post-composing J𝑀K, together with some parallel wires, to 𝐶.",
Definition 4 (Interpretation of Configurations).,
"Suppose that 𝑄⊢(𝐶,𝑉) : 𝐴;𝑄′ and suppose that 𝐿and 𝐿′ are the label contexts that satisfy 𝐶: 𝑄→𝐿′,𝑄′; 𝐿𝜎𝐿′; and 𝐿⊢𝑣𝑉: 𝐴.",
"Then we defineJ(𝐶,𝑉)K as (id1,𝐶; perm); (J𝐿⊢𝑣𝑉: 𝐴K ⊗idJ𝑄′K), which is a morphism in Set × M. Here, perm is the isomorphism J𝐿′KM ⊗J𝑄′KM −→J𝐿KM ⊗J𝑄′KM.",
"Similarly, for 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′, we define J(𝐶, 𝑀)K as 𝐽(id1,𝐶); (J𝐿⊢𝑐𝑀: 𝐴K ⋉J𝑄′K), which is a morphism in SetTM.",
"Here, 𝐿is the label context that types 𝑀as in the case of 𝑉.",
◁ 4.3 Main Results We are now ready to state the soundness and computational adequacy properties.,
Theorem 5 (Soundness).,
"Suppose that 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′ and (𝐶, 𝑀) ⇓(𝐶′,𝑉).",
"Then J(𝐶, 𝑀)K = 𝐽(J(𝐶′,𝑉)K).",
By induction on the derivation of the big-step evaluation relation.,
See Appendix A for the details.,
□ Theorem 6 (Computational Adequacy).,
"Suppose that ∅⊢(𝐶, 𝑀) : 1; ∅and J(𝐶, 𝑀)K = 𝐽(J(𝐷,𝑉)K).",
"Then (𝐶, 𝑀) ⇓(𝐷,𝑉) (possibly up to renamings of labels).",
"By an argument utilizing logical relations similar to those defined in [Colledan and Dal Lago 2024], which are given in Appendix A.",
□ 5 Effect System We extend the type system defined in Section 3 with effect annotations that estimate the properties (e.g.,
size) of the circuit generated by a program.,
"To put it another way, we introduce the type system underlying (a non-dependent version of) Proto-Quipper-R. Then we give the interpretation of programs in Proto-Quipper-R by using the category graded monad [Orchard et al.",
"This exemplifies that the type-and-effect system of Proto-Quipper-R, although rooted in an operational perspective, also has a natural denotational reading.",
"5.1 Effects for Circuits When designing an effect system, the key question to ask is “What kind of structure should we assume on effects?”.",
"A common choice is to use a preordered monoid [Katsumata 2014], where the monoid multiplication is used to compute the effect of sequential execution and the preorder is used for subtyping.",
"We make the same choice, but use categories instead of monoids because circuits are many-sorted in the sense that circuits have various input and output interfaces.",
"Moreover, since we have postulated that circuits form a premonoidal category, it is natural to require that the algebraic structure representing the effect—dubbed circuit algebra—also be premonoidal.",
Definition 7 (Circuit Algebra).,
A circuit algebra E is a strict symmetric premonoidal category that is preorder enriched.,
"The preorder enrichment means that: • each homset E(t, u) is a preordered set; • composition of morphisms preservers the order: if 𝑒1 ≲𝑑1 and 𝑒2 ≲𝑑2, then 𝑒1;𝑒2 ≲𝑑1;𝑑2; On Circuit Description Languages, Indexed Monads, and Resource Analysis 15 • t ⋉−(resp.",
"−⋊t) preserves the order for any t: if 𝑒≲𝑑, then t ⋉𝑒≲t ⋉𝑑.",
"We call morphisms of E, ranged over by 𝑒,𝑑, .",
"., effect annotations.",
"The identity over the monoidal unit i of E is denoted by 𝜀, and we call it the null effect.",
Objects of E will often be written in lowercase script letters so that they are distinguishable from objects of M. ◁ Effect annotations are meant to abstract the actual effect.,
We propose to consider this abstraction as a functor.,
Definition 8 (Abstraction).,
An abstraction 𝛼from the category of circuits M to a circuit algebra E is a strict symmetric premonoidal functor 𝛼: M →E.,
"That is, 𝛼is a functor satisfying 𝛼(𝑇⊗𝑈) = 𝛼(𝑇) ⊗𝛼(𝑈) (equality on the nose), 𝛼(𝑇⋊𝑓) = 𝛼(𝑇) ⋊𝛼(𝑓), 𝛼(𝑔⋉𝑈) = 𝛼(𝑔) ⋉𝛼(𝑈) and preserves the symmetry.",
"◁ In case M is the syntactic category of circuits, defining an abstraction is no different from giving a functorial semantics to circuits.",
"We believe that allowing arbitrary interpretations of circuits as effect annotations is not only conceptually clean but also helps us conceive of a wide variety of examples—though in practice, we should seek efficiently implementable effects.",
We defined abstractions as strict premonoidal functors because it is known that a non-strict premonoidal functor is tricky to define [Román and Sobociński 2025; Staton and Levy 2013].,
"A way to circumvent this issue is to use effectful categories [Román and Sobociński 2025], which are premonoidal categories endowed with a chosen family of central morphisms, as the definition of circuits.",
"However, using premonoidal categories and strict premonoidal functors are enough to deal with examples of circuit algebras we show below, which contains examples for resource estimations that have been considered in the literature.",
◁ Examples of Circuit Algebras.,
Here we give some examples of circuit algebras that capture some notions of circuit metrics.,
"Since the concept of circuit metrics is intrinsically intensional, namely the way gates are placed is important, we consider the syntactic category of circuits for the category M in the following examples.",
"A signature is a tuple Σ = (Σ0, Σ1) where Σ0 is the set of object variables, Σ1 is the set of generators, which are typed constants of the form 𝑓: 𝜎→𝜏with 𝜎,𝜏∈Σ∗ 0.",
"We write MΣ for the free strict symmetric premonoidal category (with trivial center) generated by the signature Σ, which can be defined by designing an appropriate term calculus (as in [Joyal and Street 1991]) or by considering string diagrams.",
"To facilitate understanding, we shall informally deal with string diagrams by depicting the graph and considering them as “monoidal string diagrams without interchange law” (see e.g.",
[Román and Sobociński 2025] for a more mathematically formal definition).,
"Note that a functor 𝛼: MΣ →E is determined if we define how object variables and generators are mapped to objects and morphisms of E, respectively.",
"As a prototypical example, we shall consider string diagrams over the signature ΣQC for quantum circuits.",
"The set ΣQC 0 is defined as {Qubit, Bit} and the generators are finite sequences over the set of gates Σ𝐺.",
"We assume that the set Σ𝐺, which is also a set of generators, contains usual quan- tum gates such as the Hadamard gate H : Qubit →Qubit, CNOT gate CNOT : (Qubit, Qubit) → (Qubit, Qubit) and measurement meas: Qubit →Bit.",
"A sequence ˜𝑔= 𝑔1 · · · ,𝑔𝑛∈ΣQC 1 has the type 𝜎1 · · · 𝜎𝑛→𝜏1 · · ·𝜏𝑛provided that 𝑔𝑖: 𝜎𝑖→𝜏𝑖, and intuitively corresponds to applying the gates 𝑔𝑖in parallel.",
"In a sense, this means that it is users responsibility to explicitly state which gates to be placed in parallel, and users cannot expect the gates to automatically slide and be parallelized.",
Example 1 (Gate count and naive depth).,
The simplest notion of circuit metric we consider is gate count.,
"The number of gates can be captured by the monoid (N, +), which can be seen as a single object circuit algebra (where we denote the only object as ★) whose morphisms are natural 16 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago numbers 𝑛: ★→★.",
Sequential composition is defined as addition and the identity morphism is given by 0.,
The functor ★⋉−is simply given as the identity functor: ★⋉★ def= ★and ★⋉𝑛 def= 𝑛.,
"Obviously, this category is order enriched by considering the standard ordering for the natural numbers.",
The abstraction functor 𝛼𝐺: MΣQC →N is defined by mapping ˜𝑔∈ΣQC 1 to the number of gates in ˜𝑔.,
A very rough estimation of the circuit depth can be given by the same circuit algebra.,
We define 𝛼𝐷: MΣQC →N by 𝛼𝐷( ˜𝑔) = 1 for every 𝑔∈ΣQC 1 .,
"For example, let us consider the following circuits.",
"𝑞1 𝐻 𝑋 𝑞2 𝐻 (4) 𝑞1 𝐻 𝑋 𝑞2 𝐻 (5) Here, in (5), the X and Hadamard gate are placed parallelly.",
The number of gates is estimated as 3 both in (4) and (5).,
"On the other hand, the depth is estimated as 3 in (4) since the three gates are sequentially composed, but 2 in (5).",
"While this way of counting depth has its own benefit of being easy to compute, it is not satisfactory because typically the depth of (4) is also defined as 2; below we shall see a better way to count depth.",
◁ Example 2 (Width [Colledan and Dal Lago 2025]).,
We explain a circuit algebra W that is used to estimate (upper bounds on) the width of a circuit and an abstraction 𝛼W : MΣQC →W.,
Recall that the width of a circuit is just a natural number that is defined as the maximum number of wires active at any point in the circuit.,
"Therefore, morphisms in W should be natural numbers.",
"For example, a quantum circuit 𝐶depicted as |0⟩ 𝑞1 𝐻 𝑞2 (6) has with width 3, i.e.",
Note that a wire of type Qubit is counted as a circuit of width 1.,
This leads us to define 𝛼(idQubit) : 𝛼(Qubit) →𝛼(Qubit) as the natural number 1.,
Now how should we define 𝛼(Qubit)?,
"Since this object should contain enough information to define the width of wires of type Qubit, a natural choice is to define this as the natural number 1.",
"Hence, we also define objects of W as natural numbers.",
"The sequential composition of morphisms 𝑘1 𝑚 −→𝑘2 and 𝑘2 𝑛−→𝑘3 in W is defined as 𝑘1 max(𝑚,𝑛) −−−−−−−→𝑘3 reflecting the definition of the width of a circuit.",
The functor 𝑘⋊−(resp.,
𝑘⋊−) represents parallelly adding 𝑘wires.,
"Hence, we define 𝑘⋊𝑚 def= 𝑘+ 𝑚.",
"To summarize, W is given by the following data: • Obj(W) def= N, • W(𝑘1,𝑘2) def= (N, ≤N), and the composition is defined by max, with the identity morphism over an object 𝑘being the morphism 𝑘itself.",
• functors 𝑘⋊−(resp.,
−⋉𝑘) such that the action on objects and morphisms are both defined as 𝑘+ (−).,
◁ Example 3 (Depth).,
"We give a better circuit algebra for depth, which may be seen as a way to count the naive depth of a circuit after optimizing it by sliding gates.",
"The idea is to track the depth using matrices over max-plus tropical semiring (N ∪{−∞}, max, +), where the (𝑖, 𝑗) component On Circuit Description Languages, Indexed Monads, and Resource Analysis 17 of the matrix describes the cost for traversing from the 𝑖-th input to the 𝑗-th output.",
"We define a circuit algebra D as follows: • Obj(D) def= N, • D(𝑘1,𝑘2) def= 𝑀𝑘1,𝑘2 (N) × 𝑀1,𝑘1(N) × 𝑀𝑘2,1(N), where 𝑀𝑘1,𝑘2 (N) is the set of 𝑘1 × 𝑘2 ma- trices over the tropical semiring.",
"The composition (A1, v1, w1) ◦(A2, v2, w2) is given as (A1A2, max(v1A2, v2), max(w1, A1v2); here max acts on vectors component-wise.",
"The iden- tity morphism over 𝑘is given as (I𝑘, 01×𝑘, 0𝑘×1), where I𝑘is the 𝑘× 𝑘identity matrices and 0𝑛×𝑚is the 𝑛×𝑚zero matrix over the tropical semiring.",
"These should not be confused with the standard identity and zero matrices, say over Q.",
"For example, 01×𝑘is (−∞, .",
"The ordering on morphisms is given by ordering over matrices where A ≤B if 𝑎𝑖,𝑗≤𝑏𝑖,𝑗 for every (𝑖, 𝑗).",
• functors 𝑘⋊−(resp.,
−⋉𝑘) are defined by the “direct sum” of matrices.,
"Concretely, for (A, v, w) ∈D(𝑘1,𝑘2), we define (A, v, w)⋊𝑘as (  A 0𝑘1×𝑘 0𝑘×𝑘2 I𝑘  , (v, 01×𝑘),  w 0𝑘×1 ).",
"Here again, the zero and identity matrices are those over the tropical semiring.",
"As mentioned, components of (A, v, w) represents the cost of the paths in a circuit as illustrated in Figure 7.",
"The role of the vectors v and w is to track the depth of wires that have “dead ends”, for instance, created by qubit creation or annihilation.",
The 𝑖-th component of v describes the maximum cost for traversing the circuit from the 𝑖-th input until it reaches an end.,
"Conversely, the 𝑖-th component of the vector w tracks the maximum depth of a path starting from an “end” and ending at the 𝑖-th output.",
1 𝑘1 ... 1 𝑘2 ... 𝑤1 𝑎11 𝑎1𝑘2 𝑎𝑘11 𝑎𝑘1𝑘2 𝑣𝑘1 Fig.,
"Components of (A, v, w).",
"The depth of the circuit (4) of Example 1, is given as ( 1 −∞ −∞ 2  , (−∞, −∞), ( −∞ −∞)).",
"Now we can correctly conclude that the depth is 2, not 3, as the maximum number in the tuple is 2.",
"As an example containing a “dead end”, we show the depth of the circuit (6) of Example 2.",
"It is given as ( 2 2 −∞ −∞−∞ 0  , (−∞, −∞),  1 1 −∞  ).",
◁ It is natural to ask whether any matrix circuit algebra can capture optimized circuit width taking into account the so-called qubit recycling.,
"Given well-known results on the NP-hardness of qubit recycling [Jiang 2024], this seems difficult.",
"On the other hand, it is possible to capture the so-called qubit dependency graph, which serves as input to a (heuristic) solver that outputs a qubit recycling strategy [Jiang 2024].",
The graph can be calculated as an adjacent matrix (over the boolean semiring) by employing an approach similar to that used for capturing circuit depth.,
Example 4 (Assertion-Based Optimization).,
We consider the size of quantum circuits modulo an optimization that removes operations that act trivially on states that are known to satisfy certain conditions.,
"For example, if we know that the input of a CNOT gate is in a state |𝜓⟩= 18 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago 𝛼00 |00⟩+ 𝛼01 |01⟩, then we know that this CNOT operation is trivial in the sense that it behaves as the identity operation because |0𝑏⟩ CNOT ↦−−−−→|0𝑏⟩for 𝑏∈{0, 1}.",
"This is actually the case for the circuit (6) of Example 2, meaning that we may remove the CNOT gate since it is redundant.",
"In (6), the fact that the first qubit is zero is evident because it was created right before the CNOT gate, but we may also assert such properties against inputs of the circuit and remove gates based on these assertions.",
This is the core idea of an automated optimization methodology proposed by Häner et al.,
[2020].8 Our aim here is not to give an effect annotation that works as an optimizer that transforms a given circuit but to capture the size of the circuit after the optimization as an effect annotation.,
"For the size, we consider gate counts for simplicity, but the other circuit metrics can be used as well.",
"Since the size of the circuit depends on the precondition, we consider a function of the type P𝑋→P𝑌× N, essentially a forward predicate transformer combined with a “cost monad ” (i.e.",
"a writer monad) (−) × N. The reason for returning not just the size but also the postcondition in P𝑌, is simply to make the effect annotations compose.",
The choice of the set 𝑋and 𝑌is the key to obtaining a tractable notion of effect annotation.,
"Here, we follow Häner et al.",
"[2020] and take 𝑋 def= {0, 1}𝑚and 𝑌 def= {1, 0}𝑛, where 𝑚and 𝑛are the number of input and output qubits, respectively.",
"A bitstring 𝑏∈{0, 1}𝑛represents the 𝑏-th (written in the binary format) computational base state, and 𝐿⊆{0, 1}𝑛can be considered as the set of possible outcomes of the quantum state.",
"Formally, we say that a (pure) state |𝜓⟩∈C2𝑛satisfies the predicate 𝐿if |𝜓⟩= Í 𝑏∈{0,1}𝑛𝛼𝑏|𝑏⟩ and |𝛼𝑏| > 0 implies 𝑏∈𝐿.9 For CNOT, we define a function 𝑒: {0, 1}2 →{0, 1}2 × N, which, for instance, associates {00, 01} to ({00, 01}, 0) and {00, 10} to ({00, 11}, 1).",
"The reason why we assign the cost 0 to {00, 01} is because CNOT acts as an identity for |𝜓⟩satisfying {00, 01}, meaning that this CNOT gate can be removed by optimization.",
"To capture the linearity of the operation, we require that the effect P({0, 1}𝑚) →P({0, 1}𝑛) × N to be join preserving.10 The circuit algebra we consider, denoted as Asrt is defined as follows: • Objects are natural numbers • A morphism 𝑒∈Asrt(𝑚,𝑛) is a function from P({0, 1}𝑚) to P({0, 1}𝑛) × N that is join preserving.",
"That is, 𝑒(𝐿1 ∪𝐿2) = (𝐿′ 1 ∪𝐿′ 2, max(𝑐1,𝑐2)) where 𝑒(𝐿𝑖) = (𝐿′ 𝑖,𝑐𝑖).",
"Sequential composition is defined as that of the writer monad, and the identity morphism is simply 𝐿↦→ (𝐿, 0).",
"The ordering between morphisms is defined as 𝑒≲𝑑if, for every 𝐿, 𝑒(𝐿) ≲P({0,1}𝑘)×N 𝑑(𝐿), where ≲P({0,1}𝑘)×N is the product order of (P({0, 1}𝑘), ⊆) and (N, ≤N).",
"• the functor 𝑘⋊−that acts on 𝑒: 𝑚→𝑛as (𝑘⋊𝑒)(𝐿) def= Ð (𝑏1,𝑏2)∈𝐿𝑒({𝑏1}) × {𝑏2}.",
"On objects, it just acts as 𝑘+ (−) as the previous examples.",
"If we consider a subsignature of ΣQC that only has Qubit as object variable and unitary gates as generators and the free premonoidal category generated by it, then we can define the abstraction function to Asrt by giving the interpretation to these unitary operators as we did for CNOT.",
"We can also handle Bit and non unitary operation such as meas : Qubit →Bit by considering the set of predicates P({0, 1}𝑚× {0, 1}𝑛) for a state with 𝑚qubits and 𝑛classical bits.",
◁ The circuit algebra examples we have discussed encompass all those previously considered by Colledan and Dal Lago [2025].,
"In contrast, Example 4 represents a novel contribution: to the best of the authors’ knowledge, no existing resource analysis techniques for quantum programs in the literature account for optimizations, i.e., no such techniques is capable of deriving bounds that reflect the improvement in size induced by optimizations.",
It is also worth noting that the notions of 8This optimization method is implemented as a transpiler pass in Qiskit [Qiskit API reference 2025].,
9It is easy to extend this satisfaction relation to mixed states.,
10It is well-known that there is a bijection between join preserving functions from P𝑋to P𝑌and Kleisli morphisms 𝑓: 𝑋→P𝑌.,
"Similarly, we may think that we are working with morphisms 𝑋→P𝑌× N. On Circuit Description Languages, Indexed Monads, and Resource Analysis 19 width and depth used by Colledan and Dal Lago differ in nature: the former is global, assigning a single numerical value to each circuit, while the latter is local, assigning a value to each individual qubit.",
These two types of metrics are captured in different ways in op.,
Circuit algebras provide a unified formalism that can accommodate both global and local metrics within the same framework.,
"As we will see, the proof of soundness of the resulting type system will be done just once.",
5.2 Type-and-Effect System Now we add effect annotations to the types.,
The type system is parameterized by an abstraction to a circuit algebra 𝛼: M →E.,
"As usual, arrow types have annotations that estimate the scope of effect caused by invoking the function.",
We also annotate !,
"and Circ(𝑇,𝑈) because thunks and boxed circuits can be thought of as special functions.",
"The grammar of types and parameter types now become as follows: Types 𝐴, 𝐵F 𝑃| 𝑇| 𝐴⊗𝐵| 𝐴 𝑒: t→u ⊸𝑇 𝐵 Parameter types 𝑃, 𝑅F 1 | Nat | 𝑃⊗𝑅| !𝑒: t→u 𝐴| Circ𝑒: u→s(𝑇,𝑈) The definition of bundle types remains the same.",
The typing judgment for computations now takes the form Γ ⊢𝑐𝑀: 𝐴;𝑒: t →u.,
The annotation 𝑒: t →u gives the information about the circuit generated by 𝑀; the type of the effect annotation t →u are sometimes omitted for readability.,
The shape of typing judgment for values is the same as before.,
Typing rules are given in Figure 8.,
"Rules for lambda abstraction, application, thunking and forcing are the standard rules.",
The rule for boxed circuit adds the effect 𝑒calculated by abstracting the circuit 𝐶as the annotation.,
"In practice, the boxed circuits are the primitive constant circuits for which the abstraction is predefined.",
The rules of greatest interest may be the rules for return and let.,
"Usually, the rule for return in a type-and-effect system adds the null effect since values are effectless.",
"In our calculus, we cannot give such a uniform treatment to values.",
"Instead, we add the annotation idt, which represents the effect of the identity circuit of type ♯J𝐴K.",
"This is different from the null effect; for example, if we are interested in width, the width of identity circuits cannot be treated as zero.",
"For the same reason that we cannot ignore identity circuits, the rule for let is annotated by (id ⋊𝑒1);𝑒2 rather than 𝑒1;𝑒2.",
"We also have a new subsumption rule, which allows us to relax the effect annotation.11 We note that the domain (resp.",
codomain) of the effect is an abstraction of the bundle type of the type environment (resp.,
return type).,
Suppose that Γ ⊢𝑐𝑀: 𝐴;𝑒: t →u.,
Then we have 𝛼(♯JΓK) = t and 𝛼(♯J𝐴K) = u.,
□ We also adjust the typing for configurations.,
"We write 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′;𝑒: t →u if 𝐶: 𝑄→𝐿,𝑄′ and 𝐿⊢𝑐𝑀: 𝐴;𝑒: t →u for some label context 𝐿disjoint from 𝑄.",
Type preservation (Theorem 2) holds even after this modification.,
Example 5 (A program with information about assertion-based optimization).,
We provide a simple example of how Proto-Quipper-R can be used to verify the resource usage of a circuit at the language level.,
"We use a program that generates an inefficient circuit for a linear nearest-neighbor (LNN) architecture, taken from Figure 3 of [Häner et al.",
"In a LNN architecture gates can be applied only to adjacent quibits, and because of this restriction, a programmer (or a compiler) might write an inefficient code following a certain idiom.",
"The program12in Figure 9 is an example of such a program, which generates the left circuit given below.",
"This circuit can be optimized into the one in the right by optimizing the trivial CNOT gates, which are gray in the left circuit.",
11It is also possible to define a subtyping relation based on the 𝑒≲𝑒′ in a standard way.,
"12For readability, we did not write the program using the syntax of Section 3, but a variant that uses a ML-like syntax.",
"20 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago abs Γ,𝑥: 𝐴⊢𝑐𝑀: 𝐵;𝑒: t →u Γ ⊢𝑣𝜆𝑥𝐴.𝑀: 𝐴 𝑒: t→u ⊸#(Γ) 𝐵 app Φ, Γ1 ⊢𝑣𝑉: 𝐴 𝑒: s→u ⊸𝑇 𝐵 Φ, Γ2 ⊢𝑣𝑊: 𝐴 Φ, Γ1, Γ2 ⊢𝑐𝑉𝑊: 𝐵;𝑒: s →u lift Φ ⊢𝑐𝑀: 𝐴;𝑒: t →u Φ ⊢𝑣lift 𝑀: !𝑒𝐴 force Φ ⊢𝑣𝑉: !𝑒𝐴 Φ ⊢𝑐force𝑉: 𝐴;𝑒: t →u circ 𝐶: 𝑄→𝐿 𝑄𝜎𝑄′ 𝐿𝜎𝐿′ 𝑄′ ⊢𝑣¯ℓ: 𝑇𝐿′ ⊢𝑣¯𝑘: 𝑈𝛼(J( ¯ℓ,𝐶, ¯𝑘)KM) = 𝑒 Φ ⊢𝑣( ¯ℓ,𝐶, ¯𝑘) : Circ𝑒(𝑇,𝑈) box Φ ⊢𝑣𝑉: 𝑇 𝑒: t→u ⊸𝐼 𝑈 Φ ⊢𝑐box𝑇𝑉: Circ𝑒: t→u(𝑇,𝑈);𝜀 apply Φ, Γ1 ⊢𝑣𝑉: Circ𝑒: t→u(𝑇,𝑈) Φ, Γ2 ⊢𝑣𝑊: 𝑇 Φ, Γ1, Γ2 ⊢𝑐apply(𝑉,𝑊) : 𝑈;𝑒: t →u dest Φ, Γ1 ⊢𝑣𝑉: 𝐴⊗𝐵 Φ, Γ2,𝑥: 𝐴,𝑦: 𝐵⊢𝑐𝑀: 𝐶;𝑒 Φ, Γ2, Γ1 ⊢𝑐let ⟨𝑥,𝑦⟩= 𝑉in 𝑀: 𝐶;𝑒 ifz Φ ⊢𝑣𝑉: Nat Φ, Γ ⊢𝑐𝑀: 𝐴;𝑒 Φ, Γ ⊢𝑐𝑁: 𝐴;𝑒 Φ, Γ ⊢𝑐ifz 𝑉then 𝑀else 𝑁: 𝐴;𝑒 return Γ ⊢𝑣𝑉: 𝐴 𝛼(♯J𝐴K) = t Γ ⊢𝑐return 𝑉: 𝐴; idt : t →t let Φ, Γ1 ⊢𝑐𝑀: 𝐴;𝑒1 : t1 →t′ 1 Φ, Γ2,𝑥: 𝐴⊢𝑐𝑁: 𝐵;𝑒2 : t2 →t′ 2 𝛼(J♯Γ𝑖K) = u𝑖 𝑒= (idu2 ⋊𝑒1);𝑒2 Φ, Γ2, Γ1 ⊢𝑐let 𝑥= 𝑀in 𝑁: 𝐵;𝑒 sub Γ ⊢𝑐𝑀: 𝐴;𝑒1 : t →u 𝑒1 ≲𝑒2 Γ ⊢𝑐𝑀: 𝐴;𝑒2 : t →u Fig.",
Typing Rules for the Effect System of Proto-Quipper-R (excerpt).,
|0⟩ |0⟩ |0⟩ |0⟩ 𝐻 |0⟩ |0⟩ |0⟩ |0⟩ 𝐻 These circuits are circuits that entangle four qubits as is clear from the right circuit; the left circuit is also a somewhat natural implementation of such a circuit that a compiler may emit.,
"The three consecutive CNOT gates acting on the top two qubits of the left circuit, as well as those acting on the second and third qubits, are implementations of swap gates using CNOT gates.",
These swaps are often inserted to naively implement an operation that acts at a distance.,
The left circuit above is just an implementation of the following circuit (followed by a simple optimization that removes two consecutive applications of the same CNOT gate) that entangles the qubits by repeatedly applying CNOT to the first qubit and the other qubits.,
"|0⟩ |0⟩ |0⟩ |0⟩ 𝐻 = |0⟩ |0⟩ |0⟩ |0⟩ 𝐻 On Circuit Description Languages, Indexed Monads, and Resource Analysis 21 1 −−init4 : Circ( I , qubit ⊗qubit ⊗qubit ⊗qubit) [e1] 2 let (q1, q2, q3, q4) = apply(init4 , ∗) in 3 −−hadamard : Circ(qubit , qubit) [e2] 4 let q1 = apply(hadamard,q1) in 5 −−cnot12 : Circ(qubit ⊗qubit , qubit ⊗qubit) [e3] 6 let (q1, q2) = apply(cnot12, (q1, q2)) in 7 −−cnot21cnot12 : Circ(qubit ⊗qubit , qubit ⊗qubit) [e4] 8 −−The sequential composition of cnot21 and cnot12 9 let (q1, q2) = apply(cnot21cnot12, (q1, q2)) in 10 let (q2, q3) = apply(cnot12, (q2, q3)) in 11 let (q2, q3) = apply(cnot21cnot21) in 12 ... 13 return (q1, q2, q3, q4) Fig.",
A program that generates a chain of redundant CNOTs for a LNN architecture.,
"We show how, by using the circuit algebra given in Exampe 4, the effect system can capture that the number of gates of the produced circuit after being optimized.",
"In Figure 9, the types of circuits that are being applied are given as comments, where Circ(𝑇,𝑆)𝑒is written as Circ(T, S) [e].",
Here we explain how each 𝑒𝑖is defined.,
"The effect annotation 𝑒1 for the four qubit initialization is defined by 𝑒1(𝑆) = ({0000}, 0).",
(We are not counting the initializations as gates.),
"For the Hadamard gate, the effect annotation 𝑒2 is defined by 𝑒2(𝑆) = ({1, 0}, 1); the first element is {1, 0} because the result after applying the Hadamard gate is a superposition of base states, and the second element 1 is the count.",
"The effect annotation 𝑒3 for the CNOT gate is the one that we explained in Example 4, which, in particular, satisfies 𝑒3({00, 10}) = ({00, 11}, 1).",
Note that this means we have 𝑞1 = 𝑞2 after line 6.,
"We can define the effect annotation 𝑒4 for cnot21cnot12 to satisfy 𝑒4({00, 11}) = ({00, 11}, 0) because (CNOT 12 ◦CNOT 21)(|00⟩) = |00⟩and (CNOT 12 ◦CNOT 21)(|11⟩) = |11⟩.",
"That is, cnot21cnot12 acts as identity and can be removed if 𝑞1 = 𝑞2.",
"Hence, the effect annotation 𝑒(restricted to the first two qubits) for the program from line 1 to line 9, satisfies 𝑒({𝜀}) = ({00, 11}, 3) meaning that 𝑞1 = 𝑞2 and we only need three gates, as opposed to five, after the optimization.",
◁ 5.3 Categorical Semantics 5.3.1 Circuit Monad with Effect Annotation.,
We refine the circuit monad from Section 3 by annotating it with effects.,
An established approach to giving a semantics of a type-and-effect system is to use graded monads [Katsumata 2014; Mellies 2012].,
We follow this approach and refine the circuit monad as a category-graded monad [Orchard et al.,
"2020], which can be thought of as a many-sorted generalization of graded monads.",
Definition 9 (Cat-graded Monads [Orchard et al.,
"A (preorder enriched) category-graded monad (or an A-graded monad) on Set consists of a family of endofunctors T 𝑓: Set →Set indexed by morphisms 𝑓in A and families of natural transformations • 𝜂𝑎: IdSet →T id𝑎for 𝑎∈Obj(A), • 𝜇𝑓,𝑔: T 𝑓T 𝑔→T 𝑓;𝑔for 𝑓: 𝑎→𝑏and 𝑔: 𝑏→𝑐, • T 𝑓≲𝑓′ : T 𝑓→T 𝑓′ for 𝑓, 𝑓′ : 𝑎→𝑏such that 𝑓≲𝑓′, 22 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago satisfying the following unital and associativity laws.",
"T𝑓𝜂𝑏 𝜂𝑎T𝑓 𝜇id𝑎,𝑓 𝜇𝑓,id𝑏 T 𝑓 T id𝑎T 𝑓 T 𝑓T id𝑏 T 𝑓 𝜇𝑓,𝑔Tℎ T𝑓𝜇𝑔,ℎ 𝜇𝑓,𝑔;ℎ 𝜇𝑓;𝑔,ℎ T 𝑓T 𝑔T ℎ T 𝑓T𝑔;ℎ T 𝑓;𝑔T ℎ T 𝑓;𝑔;ℎ Moreover, we have the following commutativity concerning the preordering T𝑓≲𝑓 idT𝑓 T 𝑓 T 𝑓 T 𝑓 T𝑓≲𝑔 T𝑓≲ℎ T𝑔≲ℎ T 𝑓 T 𝑔 T ℎ T𝑓≲𝑓′ T𝑔≲𝑔′ 𝜇𝑓,𝑔 𝜇𝑓′,𝑔′ T𝑓;𝑔≲𝑓′;𝑔′ T 𝑓T𝑔 T 𝑓′T𝑔′ T 𝑓;𝑔 T 𝑓′;𝑔′ ◁ We are interested in category-graded monads of a specific kind, namely those constructed from abstractions to circuit algebras.",
"Given 𝛼: M →E, we define an endofunctor on Set, parameterized by 𝑒, by T 𝑒 M(𝑋) def= 𝑋× M≲𝑒(𝑇,𝑈) where the set M≲𝑒(𝑇,𝑈) ⊆M(𝑇,𝑈) is defined as {𝐶∈ M(𝑇,𝑈) | 𝛼(𝐶) ≲𝑒}.",
"This construction is reminiscent of graded monads arising from effect observations [Katsumata 2014].13 There is a caveat to this definition: this is not exactly an E-graded monad, as the grading is defined with respect to a slight modification of E. The annotation 𝑒: t →u does not tell us the type of circuits, but only the type of circuits after the abstraction.",
"To remedy the problem, for the category of grades, we use ˜E whose objects are those of M and whose homset ˜E(𝑇,𝑈) is defined as E(𝛼(𝑇), 𝛼(𝑈)).",
We spell out the definition of the ˜E-graded monad constructed from abstraction to circuit algebra.,
"As mentioned, the endofunctor T 𝑒: 𝑇→𝑈 M acts on an object 𝑋as 𝑋× M≲𝑒(𝑇,𝑈).",
The unit and multiplication are defined exactly the same way as in the (ordinary) circuit monad.,
"That is, we have 𝜂𝑇,𝑋(𝑥) def= (𝑥, id𝑇), 𝜇𝑒1,𝑒2,𝑋((𝑥,𝐶), 𝐷) def= (𝑥,𝐶; 𝐷).",
"Note that the multiplication is well-defined because if 𝛼(𝐶) ≲𝑒1 and 𝛼(𝐷) ≲𝑒2, we have 𝛼(𝐶; 𝐷) = 𝛼(𝐶);𝛼(𝐷) ≲𝑒1;𝑒2.",
"Each component of the natural transformation TM 𝑒1≲𝑒2 𝑋 is the inclusion from 𝑋× M≲𝑒1(𝑇,𝑈) to 𝑋× M≲𝑒2(𝑇,𝑈).",
"Moreover, the category-graded monad constructed this way, has premonoidal lifting as in the case for the ordinary circuit monad.",
"For example, there is a natural transformation (𝑇⋊−)† 𝑒: 𝑇→𝑈,𝑋: T 𝑒 M𝑋→T𝑇⋊𝑒 M 𝑋that respects unit and the multiplication of the monad, which is defined as (𝑥,𝐶) ↦→(𝑥,𝑇⋊𝐶) 5.3.2 Interpretation.",
"The interpretation of types can be obtained by replacing M(𝑇,𝑈) with M≲𝑒(𝑇,𝑈).",
"For example, we define JCirc𝑒: t→u(𝑇,𝑈)K𝑃 def= M≲𝑒(J𝑇KM, J𝑈KM) J𝐴 𝑒: u→s ⊸𝑇 𝐵K def= (♭J𝐴K ⇒Set ♭J𝐵K × M≲𝑒(♯J𝐴K ⊗J𝑇KM, ♯J𝐵K), J𝑇KM).",
The first element of J𝐴 𝑒: u→s ⊸𝑇 𝐵K can also be written as ♭J𝐴K ⇒Set T 𝑒: ♯J𝐴K⊗J𝑇KM→♯J𝐵K M (♭J𝐵K).,
The interpretation of type !𝑒𝐴is similar to that of the function type.,
"Note that we have an isomorphism 13Our recipe is different from Katsumata’s in that (1) we deal with indexed monads rather than ordinary monads, and (2) it is tailored for a specific monad, i.e.",
category-action monads.,
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 23 ♭J𝑇 𝑒⊸𝑈K  JCirc𝑒(𝑇,𝑈)K𝑃as before.14 The rest of the types are interpreted as in the case of the simple type system.",
We now discuss how the judgments are interpreted.,
Computational judgments of the shape Γ ⊢𝑐𝑀: 𝐴;𝑒are interpreted as morphisms J𝑀K: ♭JΓK →T 𝑒: ♯JΓK→♯J𝐴K M ♭J𝐴K in Set.,
Note that this is almost identical to the interpretation in the simple type system when the interpreted term is regarded as a morphism in Set.,
"Although the interpretation of the simply typed terms were given as morphisms in the Kleisli category, we define the interpretation of terms typed in the type-and-effect system in Set.",
This is because the notion of Kleisli category for category-graded monads is tricky to define (see Remark 4 for a further discussion).,
The value judgments remain to be interpreted as morphisms in Set × disc(Obj(M)).,
Let us look at the interpretation of return and let since they highlight the use of the monad.,
"As usual, return is interpreted as the interpretation of a value post-composed with the unit.",
"t Γ ⊢𝑣𝑉: 𝐴 𝛼(♯J𝐴K) = t Γ ⊢𝑐return 𝑉: 𝐴; idt : t →t | def= ♭JΓ ⊢𝑣𝑉: 𝐴K;𝜂♯J𝐴K The interpretation of let is essentially the same as how composition is defined in the Kleisli category of a (standard) monad: u v Φ, Γ1 ⊢𝑐𝑀: 𝐴;𝑒1 : t1 →t′ 1 Φ, Γ2,𝑥: 𝐴⊢𝑐𝑁: 𝐵;𝑒2 : t2 →t′ 2 J♯Γ𝑖K ▷u𝑖 𝑒= (idu2 ⋊𝑒1);𝑒2 Φ, Γ2, Γ1 ⊢𝑐let 𝑥= 𝑀in 𝑁: 𝐵;𝑒 } ~ is given as JΦK × ♭JΓ2K × ♭JΓ1K Δ×id; −−−−−→JΦK × ♭JΓ2K × JΦK × ♭JΓ1K id×𝜂u2 ×J𝑀K −−−−−−−−−→JΦK × T idu2 M ♭JΓ2K × T 𝑒1 M ♭J𝐴K id×<⃝;𝜏 −−−−−→T id⋊𝑒1 M (JΦK × ♭JΓ2K × ♭J𝐴K) Tid⋊𝑒1 M J𝑁K −−−−−−−−→T id⋊𝑒1 M (T 𝑒2 M (♭J𝐵K)) 𝜇id⋊𝑒1,𝑒2 −−−−−−→T (id⋊𝑒1);𝑒2 M (♭J𝐵K).",
"The morphism 𝜏is the strength (which can be defined in a straightforward way), and <⃝𝑇1,𝑈1,𝑇2,𝑈2 is the morphism15 <⃝: T 𝑒1 : 𝑇1→𝑈1 M 𝑋× T 𝑒2 : 𝑇2→𝑈2 M 𝑌→T (id⋊𝑒1);(𝑒2⋉id) M (𝑋× 𝑌) ((𝑥,𝐶), (𝑦, 𝐷)) ↦→((𝑥,𝑦), (𝑇2 ⋊𝐶); (𝐷⋉𝑈1)).",
"Intuitively, <⃝pairs 𝑥,𝑦and, at the same time, “parallelly composes” 𝐶and 𝐷in the order 𝐶first followed by 𝐷.",
"In the interpretation of let, it is simply used to parallelly augment wires of the type ♯JΓ2K to the circuit generated by 𝑀.",
The interpretation for the other constructs is also essentially unchanged from the interpretation in Figure 6.,
"A minor difference is that instead of using the functor 𝐽to map value morphisms into 14By abuse of notation, we also denote this isomorphism as box.",
"15Aside from this elementary definition, <⃝can be defined using liftings of id ⋊−and −⋉id, and the strength and multiplication of the cat-graded monad.",
"24 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago the category of computations, we use 𝜂as we did for the interpretation of return 𝑉.",
"For example, we have JΦ ⊢𝑐box𝑇𝑉: Circ𝑒: t→u(𝑇,𝑈);𝜀K def= ♭JΦ ⊢𝑣𝑉: 𝑇 𝑒: t→u ⊸𝐼 𝑈K; box;𝜂𝐼,Circ𝑒(𝑇,𝑈).",
The subsumption rule is interpreted by postcomposing the component of the natural transformation T 𝑒1≲𝑒2 M at J𝐴K.,
(See Appendix B for the interpretaion of the other constructs.),
"While we used (parameterized) Freyd category in Section 3, in this section, we had to define the interpretation explicitly using the structures of monads.",
Freyd categories have a better match with the fine-grained call-by-value calculus that Proto-Quipper-R is based on.,
The difficulty lies in defining a suitable notion of “locally graded category” [Levy 2019; Wood 1978] for category graded monads.,
A locally graded category is a category-like structure whose homsets are indexed by grades (i.e.,
"elements of a preordered monoid), and this is central to the definition of graded Freyd categories [Gaboardi et al.",
"Naively, we may define a category like structure in which homsets are indexed by morphisms, but determining the laws such a structure should satisfy seems non trivial and is left for future work.",
"◁ 5.4 Correctness Analogous to the case of simple types, the semantics is sound and adequate.",
"Interpretation of configurations are defined as in Section 3 by considering 𝐽as a map that associates (𝑓,𝐶) ∈ (Set × M)((𝑋,𝑇), (𝑌,𝑈)) to a morphism from 𝑋to 𝑌× M≲𝛼(𝐶) (𝑇,𝑈) in Set.",
Theorem 10 (Soundness).,
"Suppose that 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′;𝑒: t →u and (𝐶, 𝑀) ⇓(𝐶′,𝑉).",
"Then J(𝐶, 𝑀)K = 𝐽(J(𝐶′,𝑉)K).",
□ Theorem 11 (Computational adequacy).,
"Suppose that ∅⊢(𝐶, 𝑀) : 1; ∅;𝑒: t →i and J(𝐶, 𝑀)K = 𝐽(J(𝐷,𝑉)K).",
"Then (𝐶, 𝑀) ⇓(𝐷,𝑉) (possibly up to renaming of labels).",
"Moreover, there must exist a circuit 𝐸such that 𝛼(𝐸) ≲𝑒and 𝐶; 𝐸= 𝐷.",
"□ 6 Discussion: Dependent Types One feature that some of the languages of the Proto-Quipper family have is a form of dependent types [Colledan and Dal Lago 2024, 2025; Fu et al.",
"2020, 2022a].",
"Dependent types are useful in the context of quantum circuit programming because one can express, at the level of types, the number of qubits a function takes as input.",
"For example, a function implementing the so-called Quantum Fourier Transform has the type qft : Î(𝑛: Nat), Vec Qubit 𝑛⊸Vec Qubit 𝑛.",
We briefly discuss the possibility of adding dependent types to the languages and models from the previous sections.,
The denotational model given in Section 3 can be straightforwardly extended to support some dependent types.,
We can apply the families construction as shown in Figure 10.,
"Since ordinary adjunctions lift to fibered adjunctions over Set by a pointwise definition, this construction gives an instance of a parameterized version of the fibered adjunction models [Ahman et al.",
Fibered adjunction models are models for a language with dependent types and computational effects.,
"On the syntax side, this means allowing dependent types with “value restriction”: types can only depend on values with parameter type, which correspond to morphisms in Set.",
Although such a type system is less expressive compared to the type system of Proto-Quipper-D [Fu et al.,
"2022a] that allows types to depend on (the shape of) terms with quantum data types, it is expressive enough to express the type for qft we described above.",
What is more challenging is to model effect annotations that are dependent on terms.,
"Colledan and Dal Lago [2024, 2025] considered a type system in which the function qft has the type qft : Î(𝑛: Nat), Vec Qubit 𝑛 𝑛: 𝑛→𝑛 ⊸ Vec Qubit 𝑛, where the 𝑛over the arrow is the effect annotation On Circuit Description Languages, Indexed Monads, and Resource Analysis 25 𝐽 𝑝 {−} 1 ⊣ ⊣ Fam(Set × M) Fam(SetT) Fam(Set) Set Fig.",
Model for the dependently typed Proto-Quipper-R.,
"The functors 1 and {−} are the terminal object functor and the comprehension functor, respectively.",
expressing the estimated width of the circuit qft generates.16 Their syntax forces annotations to be arithmetic terms depending on arithmetic variables so that type inference based on SMT-solving can semi-automatically infer the effect information.,
"If we are only interested in annotations that are arithmetic expressions, then it seems possible to write out the interpretation just by indexing the interpretation we described in Section 5 with (tuples of) natural numbers.",
"However, it is not clear (a) to what extent the effect annotations can be generalized and (b) whether there is a categorical explanation (e.g.",
an explanation in the form of some generalization of fibered monads) that captures the nature of the interpretation.,
Further investigation is left for future work.,
7 Related Work Giving a denotational semantics to quantum programming languages has generally been more challenging than the corresponding problem for classical languages.,
"Even in the case of imperative QRAM languages, this task is not so simple, given that quantum data can be interpreted by semantic domains which are different from the usual ones, see [Ying 2016] for an overview.",
"When, in addition to a quantum store, the underlying language is also endowed with higher- order functions, the task becomes even more complicated.",
"A few years after the quantum 𝜆- calculus [Selinger and Valiron 2005] was introduced, a fully abstract model for the linear fragment of the quantum 𝜆-calculus was given by using the category CPM of completely positive maps [Selinger and Valiron 2008].",
"However, designing a model of (variants of) the full quantum 𝜆-calculus remained a challenge.",
One difficulty is the tension between the finite and the infinite.,
"The category CPM or its subcategory Q of trace-preserving completely positive maps are inherently finite since their definition relies on finite Hilbert spaces, whereas quantum 𝜆-calculus comes with infinite features such as the !",
modality or term level recursion.,
"To overcome this difficulty, various approaches have been studied.",
These include (but are not limited to) studies based on presheaves [Malherbe et al.,
"2013], (Σ-monoid) enriched presheaves [Tsukada and Asada 2024], quantitative semantics of linear logic [Pagani et al.",
"2014], operator algbra [Cho and Westerbaan 2016], geometry of interaction [Hasuo and Hoshino 2017; Yoshimizu et al.",
2014] and game semantics [Clairambault and de Visme 2020; Clairambault et al.,
The first four approaches can be considered as taking a certain “completion” of CPM or Q to support higher-order and infinite types.,
"The latter two approaches are more operational, since they are based on interactive semantics.",
We note that some of these [Clairambault and de Visme 2020; Pagani et al.,
2014; Tsukada and Asada 2024] are fully abstract models of the full quantum 𝜆-calculus.,
"In CDLs, the types of problems encountered in giving a denotational semantics are different.",
"On the one hand, a semantics of CDL needs to cope with the distinction between circuit generation time and circuit execution time that does not exist in languages such as the quantum 𝜆-calculus.",
"Moreover, 16The syntax we are using here is not exactly the syntax used in [Colledan and Dal Lago 2024, 2025] 26 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago circuits can, like in Quipper, support specific operators which do not exist for other data structures and which allow programs to be interpreted as circuits e.g., Quipper’s box operator.",
"On the other hand, giving denotational semantics to CDLs is somehow easier because the characteristics of quantum circuits, say, compared to Boolean circuits, are abstracted away.",
"This simplicity, however, no longer holds if the CDL allows an interplay between the host and the circuit level language, known as dynamic lifting.",
"As already mentioned, since the introduction of the Proto-Quipper family with Proto-Quipper-S and Proto-Quipper-M, a presheaf-based denotational semantics has been known to be adequate [Lin- denhovius et al.",
2018; Rios and Selinger 2017].,
"Later, extensions of the language with dependent types [Fu et al.",
2022a] and dynamic lifting [Fu et al.,
"2022b, 2023] have been considered.",
The model for dependent types uses the families construction as discussed in Section 6.,
"At a superficial level, the model for dynamic lifting also shares an idea with the semantics we introduced in Section 5.",
"Both models hold a morphism representing a quantum circuit and a morphism representing its interpretation, either as a quantum operation or effect annotation.",
"However, the foundations of these models remain fundamentally unaltered in that they use presheaves, and further comparison with our models is left for future work.",
"It should be noted that the members of the Proto-Quipper family intended for the analysis of circuit size, like Proto-Quipper-R [Colledan and Dal Lago 2024], lack a denotational account, although being solidly grounded from an operational point of view.",
We believe this to be a result of the intrinsic difficulty of reflecting intensional properties of the underlying circuit in the aforementioned presheaf-based semantics.,
"Another commonly used type of CDLs, aside from the Proto-Quipper family, consists of those with a clear stratification between classical and quantum layers.",
QWire [Paykin et al.,
2017] and EWire [Rennela and Staton 2020] are languages that have a dedicated language for circuits with a linear type system that can be embedded into a non-linear host language.,
"On the semantic side, this embedding has been nicely captured using enriched category theory [Rennela and Staton 2020].",
VQPL [Jia et al.,
"2022] is another quantum programming language with two subcalculi, one for classical programs and the other for quantum programs, where the quantum programs are more high-level than mere circuits.",
"They support rich features such as classical recursive types, inductive quantum types and dynamic lifting.",
"Furthermore, VQPL has an adequate denotational model that unites domain-theoretic models of classical programming and von Neumann algebras for quantum interpretation.",
"Overall, these languages cannot have data structures with both quantum data and classical data, unlike languages in the Proto-Quipper family.",
"While our semantics also has a clear separation between Set and the category of circuits, our languages allow mixing classical types and quantum types.",
Our key observation here is that the introduction of the closure types allows us to decompose these mixed types into classical and quantum parts.,
"Since the pioneering works of Moggi [1989, 1991] it can certainly be said that the concept of a monad is one of the most powerful mathematical constructions in giving meaning to compu- tational effects.",
"Various generalizations on plain monads, such as indexed (aka parameterized) monads [Atkey 2009], graded monads [Katsumata 2014; Mellies 2012] or category-graded mon- ads [Orchard et al.",
2020] have been proposed in the literature.,
"The circuit monads in Section 4 and Section 5 can be seen as an instance of an indexed monad and category-graded monads, re- spectively.",
Defining effect annotations as abstractions of computational effects is a general idea that could be applied to other things besides circuits.,
Identifying the essences of the construction in Section 5 and deriving a recipe to construct category-graded monad might be of independent interest.,
"Additionally, as discussed in Remark 4, a “Freyd category” for category-graded monads appears to be absent from the literature and is worth investigating further.",
"The literature, by the way, offers some examples of circuit monads [Elliott 2013; Valiron 2016], none of which has been applied to languages in the Proto-Quipper family.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis 27 8 Conclusion In this work, we introduce a monadic denotational semantic model for circuit description languages in which the role of the monad is played by a circuit construction.",
"This way, the structure of the produced circuit can be observed as an effect, making the model potentially capable of reflecting intensional features of circuits produced by terms of any type.",
"This allows us to give semantics to members of the Proto-Quipper family for which a denotational account is not, to the authors’ knowledge, known.",
"Remarkably, by considering an abstract notion of circuit algebra, different notions of circuit metrics are treated uniformly, including simple forms of circuit optimization; we believe that this abstraction may help reveal further concrete circuit metrics.",
"Among the future developments of this work, we must certainly mention the study of denotational semantic models for Proto-Quipper-RA, a recently introduced member of the Proto-Quipper family able to support the analysis of a wide range of circuit metrics via types.",
"Some of its features, such as effect and dependent typing, have already been treated separately (see Sections 5 and 6), but the study of their combination in the same semantic framework is left to future work.",
"Although our work primarily focuses on denotational semantics, it is natural to consider implementing the type system described in Section 5, e.g., by adapting or extending the QuRA tool [Colledan and Dal Lago 2025].",
"While supporting the circuit algebra examples discussed in this paper should not pose major technical challenges, aspects such as efficient automatic type inference merit further investigation.",
We plan to explore these directions in future work.,
We would also like to extend the language and the model with features of Quipper that we did not cover in this paper such as term and type level recursion and dynamic lifting.,
Acknowledgments The research leading to these results has received funding from the MUR grant PRIN 2022 PNRR No.,
"P2022HXNSC - “Resource Awareness in Programming” and European Union - NextGenera- tionEU through the Italian Ministry of University and Research under PNRR - M4C2 - I1.4 Project CN00000013 “National Centre for HPC, Big Data and Quantum Computing”.",
"References Danel Ahman, Neil Ghani, and Gordon D. Plotkin.",
Dependent Types and Fibred Computational Effects.,
"In Foundations of Software Science and Computation Structures - 19th International Conference, FOSSACS 2016, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2016, Eindhoven, The Netherlands, April 2-8, 2016, Proceedings (Lecture Notes in Computer Science, Vol.",
"9634), Bart Jacobs and Christof Löding (Eds.).",
"Springer, 36–54.",
doi:10.1007/978-3-662-49630-5_3 Matthew Amy.,
Sized Types for Low-Level Quantum Metaprogramming.,
"In Reversible Computation - 11th International Conference, RC 2019, Lausanne, Switzerland, June 24-25, 2019, Proceedings (Lecture Notes in Computer Science, Vol.",
"11497), Michael Kirkedal Thomsen and Mathias Soeken (Eds.).",
"Springer, 87–107.",
doi:10.1007/978-3-030-21500-2_6 Robert Atkey.,
Parameterised notions of computation.,
"19, 3-4 (2009), 335–376.",
doi:10.1017/ S095679680900728X P. N. Benton.,
"A Mixed Linear and Non-Linear Logic: Proofs, Terms and Models (Extended Abstract).",
"In Computer Science Logic, 8th International Workshop, CSL ’94, Kazimierz, Poland, September 25-30, 1994, Selected Papers (Lecture Notes in Computer Science, Vol.",
"933), Leszek Pacholski and Jerzy Tiuryn (Eds.).",
"Springer, 121–135.",
"doi:10.1007/BFB0022251 S. Bettelli, T. Calarco, and L. Serafini.",
Toward an architecture for quantum programming.,
"The European Physical Journal D - Atomic, Molecular and Optical Physics 25, 2 (Aug. 2003), 181–200.",
doi:10.1140/epjd/e2003-00242-2 Kenta Cho and Abraham Westerbaan.,
Von Neumann Algebras form a Model for the Quantum Lambda Calculus.,
CoRR abs/1603.02133 (2016).,
arXiv:1603.02133 http://arxiv.org/abs/1603.02133 Pierre Clairambault and Marc de Visme.,
Full abstraction for the quantum lambda-calculus.,
ACM Program.,
"4, POPL (2020), 63:1–63:28. doi:10.1145/3371131 Pierre Clairambault, Marc de Visme, and Glynn Winskel.",
Game semantics for quantum programming.,
ACM Program.,
"3, POPL (2019), 32:1–32:29. doi:10.1145/3290345 28 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago Andrea Colledan and Ugo Dal Lago.",
On Dynamic Lifting and Effect Typing in Circuit Description Languages.,
"In 28th International Conference on Types for Proofs and Programs (TYPES 2022) (Leibniz International Proceedings in Informatics (LIPIcs), Vol.",
"269), Delia Kesner and Pierre-Marie Pédrot (Eds.).",
"Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl, Germany, 3:1–3:21. doi:10.4230/LIPIcs.TYPES.2022.3 Andrea Colledan and Ugo Dal Lago.",
Circuit Width Estimation via Effect Typing and Linear Dependency.,
"In Programming Languages and Systems - 33rd European Symposium on Programming, ESOP 2024, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2024, Luxembourg City, Luxembourg, April 6-11, 2024, Proceedings, Part II (Lecture Notes in Computer Science, Vol.",
"14577), Stephanie Weirich (Ed.).",
"Springer, 3–30.",
doi:10.1007/978-3-031- 57267-8_1 Andrea Colledan and Ugo Dal Lago.,
Flexible Type-Based Resource Estimation in Quantum Circuit Description Languages.,
ACM Program.,
"9, POPL, Article 47 (Jan. 2025), 31 pages.",
doi:10.1145/3704883 Cirq Developers.,
doi:10.5281/zenodo.15191735 Conal Elliott.,
Circuits as a bicartesian closed category.,
http://conal.net/blog/posts/circuits-as-a-bicartesian-closed- category.,
"Blog post, Accessed: 2025-01-23.",
"Peng Fu, Kohei Kishida, Neil J. Ross, and Peter Selinger.",
A Tutorial Introduction to Quantum Circuit Programming in Dependently Typed Proto-Quipper.,
"In Reversible Computation - 12th International Conference, RC 2020, Oslo, Norway, July 9-10, 2020, Proceedings (Lecture Notes in Computer Science, Vol.",
"12227), Ivan Lanese and Mariusz Rawski (Eds.).",
"Springer, 153–168.",
"doi:10.1007/978-3-030-52482-1_9 Peng Fu, Kohei Kishida, Neil J. Ross, and Peter Selinger.",
A biset-enriched categorical model for Proto-Quipper with dynamic lifting.,
"In Proceedings 19th International Conference on Quantum Physics and Logic, QPL 2022, Wolfson College, Oxford, UK, 27 June - 1 July 2022 (EPTCS, Vol.",
"394), Stefano Gogioso and Matty Hoban (Eds.).",
doi:10.4204/EPTCS.,
"394.16 Peng Fu, Kohei Kishida, Neil J. Ross, and Peter Selinger.",
Proto-Quipper with Dynamic Lifting.,
ACM Program.,
"7, POPL (2023), 309–334.",
"doi:10.1145/3571204 Peng Fu, Kohei Kishida, and Peter Selinger.",
Linear Dependent Type Theory for Quantum Programming Languages.,
Methods Comput.,
"18, 3 (2022).",
"doi:10.46298/LMCS-18(3:28)2022 Marco Gaboardi, Shin-ya Katsumata, Dominic Orchard, and Tetsuya Sato.",
Graded Hoare Logic and its Categorical Semantics.,
"In Programming Languages and Systems - 30th European Symposium on Programming, ESOP 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27 - April 1, 2021, Proceedings (Lecture Notes in Computer Science, Vol.",
"12648), Nobuko Yoshida (Ed.).",
"Springer, 234–263.",
"doi:10.1007/978-3-030-72019-3_9 Alexander S. Green, Peter LeFanu Lumsdaine, Neil J. Ross, Peter Selinger, and Benoît Valiron.",
Quipper: a scalable quantum programming language.,
"In ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’13, Seattle, WA, USA, June 16-19, 2013, Hans-Juergen Boehm and Cormac Flanagan (Eds.).",
"ACM, 333–342.",
"1145/2491956.2462177 Thomas Häner, Torsten Hoefler, and Matthias Troyer.",
Assertion-based optimization of Quantum programs.,
ACM Program.,
"4, OOPSLA (2020), 133:1–133:20. doi:10.1145/3428201 Ichiro Hasuo and Naohiko Hoshino.",
Semantics of higher-order quantum computation via geometry of interaction.,
"168, 2 (2017), 404–469.",
"doi:10.1016/J.APAL.2016.10.010 Ali Javadi-Abhari, Matthew Treinish, Kevin Krsulich, Christopher J.",
"Wood, Jake Lishman, Julien Gacon, Simon Martiel, Paul D. Nation, Lev S. Bishop, Andrew W. Cross, Blake R. Johnson, and Jay M. Gambetta.",
Quantum computing with Qiskit.,
"arXiv:2405.08810 [quant-ph] https://arxiv.org/abs/2405.08810 Xiaodong Jia, Andre Kornell, Bert Lindenhovius, Michael W. Mislove, and Vladimir Zamdzhiev.",
Semantics for variational Quantum programming.,
ACM Program.,
"6, POPL (2022), 1–31.",
doi:10.1145/3498687 Hanru Jiang.,
Qubit Recycling Revisited.,
ACM Program.,
"8, PLDI (2024), 1264–1287.",
doi:10.1145/3656428 André Joyal and Ross Street.,
"The geometry of tensor calculus, I.",
"Advances in Mathematics 88, 1 (1991), 55–112.",
doi:10.1016/0001-8708(91)90003-P Shin-ya Katsumata.,
Parametric effect monads and semantics of effect systems.,
"In The 41st Annual ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages, POPL ’14, San Diego, CA, USA, January 20-21, 2014, Suresh Jagannathan and Peter Sewell (Eds.).",
"ACM, 633–646.",
doi:10.1145/2535838.2535846 E. Knill.,
Conventions for Quantum Pseudocode.,
arXiv:2211.02559 [quant-ph] https://arxiv.org/abs/2211.02559 [A version of LANL report LAUR-96-2724 (1996) with a modern formatting].,
Paul Blain Levy.,
Locally graded categories.,
https://pblevy.github.io/papers/locgrade.pdf.,
Talk slides.,
"Paul Blain Levy, John Power, and Hayo Thielecke.",
Modelling environments in call-by-value programming languages.,
"185, 2 (2003), 182–210.",
"doi:10.1016/S0890-5401(03)00088-9 Bert Lindenhovius, Michael W. Mislove, and Vladimir Zamdzhiev.",
Enriching a Linear/Non-linear Lambda Calculus: A Programming Language for String Diagrams.,
"In Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic On Circuit Description Languages, Indexed Monads, and Resource Analysis 29 in Computer Science, LICS 2018, Oxford, UK, July 09-12, 2018, Anuj Dawar and Erich Grädel (Eds.).",
"ACM, 659–668.",
"doi:10.1145/3209108.3209196 Junyi Liu, Bohua Zhan, Shuling Wang, Shenggang Ying, Tao Liu, Yangjia Li, Mingsheng Ying, and Naijun Zhan.",
Quantum Hoare Logic.,
Formal Proofs 2019 (2019).,
"https://www.isa-afp.org/entries/QHLProver.html Octavio Malherbe, Philip J. Scott, and Peter Selinger.",
Presheaf Models of Quantum Computation: An Outline.,
"In Computation, Logic, Games, and Quantum Foundations.",
"The Many Facets of Samson Abramsky - Essays Dedicated to Samson Abramsky on the Occasion of His 60th Birthday (Lecture Notes in Computer Science, Vol.",
"7860), Bob Coecke, Luke Ong, and Prakash Panangaden (Eds.).",
"Springer, 178–194.",
doi:10.1007/978-3-642-38164-5_13 Paul-André Mellies.,
Parametric monads and enriched adjunctions.,
https://www.irif.fr/~mellies/tensorial-logic/8- parametric-monads-and-enriched-adjunctions.pdf.,
Eugenio Moggi.,
Computational Lambda-Calculus and Monads.,
"In Proceedings of the Fourth Annual Symposium on Logic in Computer Science (LICS ’89), Pacific Grove, California, USA, June 5-8, 1989.",
"IEEE Computer Society, 14–23.",
doi:10.1109/LICS.1989.39155 Eugenio Moggi.,
Notions of Computation and Monads.,
"93, 1 (1991), 55–92.",
"doi:10.1016/0890-5401(91)90052- 4 Dominic Orchard, Philip Wadler, and Harley Eades III.",
Unifying graded and parameterised monads.,
"In Proceedings Eighth Workshop on Mathematically Structured Functional Programming, MSFP@ETAPS 2020, Dublin, Ireland, 25th April 2020 (EPTCS, Vol.",
"317), Max S. New and Sam Lindley (Eds.).",
"doi:10.4204/EPTCS.317.2 Michele Pagani, Peter Selinger, and Benoît Valiron.",
Applying quantitative semantics to higher-order quantum computing.,
"In The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’14, San Diego, CA, USA, January 20-21, 2014, Suresh Jagannathan and Peter Sewell (Eds.).",
"ACM, 647–658.",
doi:10.1145/2535838.,
"2535879 Jennifer Paykin, Robert Rand, and Steve Zdancewic.",
QWIRE: a core language for quantum circuits.,
"In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, Paris, France, January 18-20, 2017, Giuseppe Castagna and Andrew D. Gordon (Eds.).",
"ACM, 846–858.",
doi:10.1145/3009837.3009894 Simon Perdrix.,
Quantum Entanglement Analysis Based on Abstract Interpretation.,
"In Static Analysis, 15th International Symposium, SAS 2008, Valencia, Spain, July 16-18, 2008.",
"Proceedings (Lecture Notes in Computer Science, Vol.",
"5079), María Alpuente and Germán Vidal (Eds.).",
"Springer, 270–282.",
doi:10.1007/978-3-540-69166-2_18 John Power and Edmund Robinson.,
Premonoidal Categories and Notions of Computation.,
"7, 5 (1997), 453–468.",
doi:10.1017/S0960129597002375 Qiskit API reference.,
HoareOptimizer.,
https://quantum.cloud.ibm.com/docs/en/api/qiskit/qiskit.transpiler.passes.,
HoareOptimizer.,
Accessed: 2025-07-10.,
Mathys Rennela and Sam Staton.,
"Classical Control, Quantum Circuits and Linear Logic in Enriched Category Theory.",
Methods Comput.,
"16, 1 (2020).",
doi:10.23638/LMCS-16(1:30)2020 Francisco Rios and Peter Selinger.,
A categorical model for a quantum circuit description language.,
"In Proceedings 14th International Conference on Quantum Physics and Logic, QPL 2017, Nijmegen, The Netherlands, 3-7 July 2017 (EPTCS, Vol.",
"266), Bob Coecke and Aleks Kissinger (Eds.).",
doi:10.4204/EPTCS.266.11 Mario Román and Paweł Sobociński.,
String Diagrams for Premonoidal Categories.,
"Logical Methods in Computer Science Volume 21, Issue 2, Article 9 (Apr 2025).",
doi:10.46298/lmcs-21(2:9)2025 J. W. Sanders and P. Zuliani.,
Quantum Programming.,
"In Mathematics of Program Construction, Roland Backhouse and José Nuno Oliveira (Eds.).",
"Springer Berlin Heidelberg, Berlin, Heidelberg, 80–99.",
Peter Selinger.,
Towards a quantum programming language.,
Mathematical.,
Structures in Comp.,
"14, 4 (Aug. 2004), 527–586.",
doi:10.1017/S0960129504004256 Peter Selinger and Benoît Valiron.,
A Lambda Calculus for Quantum Computation with Classical Control.,
"In Typed Lambda Calculi and Applications, Paweł Urzyczyn (Ed.).",
"Springer Berlin Heidelberg, Berlin, Heidelberg, 354–368.",
Peter Selinger and Benoît Valiron.,
On a Fully Abstract Model for a Quantum Linear Functional Language: (Extended Abstract).,
"In Proceedings of the 4th International Workshop on Quantum Programming Languages, QPL 2006, Oxford, UK, July 17-19, 2006 (Electronic Notes in Theoretical Computer Science, Vol.",
"210), Peter Selinger (Ed.).",
"Elsevier, 123–137.",
doi:10.1016/J.ENTCS.2008.04.022 Peter W. Shor.,
Algorithms for Quantum Computation: Discrete Logarithms and Factoring.,
"In 35th Annual Symposium on Foundations of Computer Science, Santa Fe, New Mexico, USA, 20-22 November 1994.",
"IEEE Computer Society, 124–134.",
doi:10.1109/SFCS.1994.365700 Sam Staton and Paul Blain Levy.,
Universal properties of impure programming languages.,
"In The 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’13, Rome, Italy - January 23 - 25, 2013, Roberto Giacobazzi and Radhia Cousot (Eds.).",
"ACM, 179–192.",
"doi:10.1145/2429069.2429091 Krysta Svore, Alan Geller, Matthias Troyer, John Azariah, Christopher Granade, Bettina Heim, Vadym Kliuchnikov, Mariia Mykhailova, Andres Paz, and Martin Roetteler.",
"Q#: Enabling Scalable Quantum Computing and Development 30 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago with a High-level DSL.",
In Proceedings of the Real World Domain Specific Languages Workshop 2018 (RWDSL2018).,
doi:10.1145/3183895.3183901 Takeshi Tsukada and Kazuyuki Asada.,
Enriched Presheaf Model of Quantum FPC.,
ACM Program.,
"8, POPL (2024), 362–392.",
doi:10.1145/3632855 Benoît Valiron.,
Generating Reversible Circuits from Higher-Order Functional Programs.,
"In Reversible Computation - 8th International Conference, RC 2016, Bologna, Italy, July 7-8, 2016, Proceedings (Lecture Notes in Computer Science, Vol.",
"9720), Simon J. Devitt and Ivan Lanese (Eds.).",
"Springer, 289–306.",
doi:10.1007/978-3-319-40578-0_21 R. J.,
V-indexed categories.,
In Indexed Categories and Their Applications.,
"Springer Berlin Heidelberg, Berlin, Heidelberg, 126–140.",
Mingsheng Ying.,
Foundations of Quantum Programming (1st ed.).,
"Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.",
"Mingsheng Ying, Shenggang Ying, and Xiaodi Wu.",
Invariants of quantum programs: characterisations and generation.,
"In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, Paris, France, January 18-20, 2017, Giuseppe Castagna and Andrew D. Gordon (Eds.).",
"ACM, 818–832.",
"doi:10.1145/3009837.3009840 Akira Yoshimizu, Ichiro Hasuo, Claudia Faggian, and Ugo Dal Lago.",
Measurements in Proof Nets as Higher-Order Quantum Circuits.,
"In Programming Languages and Systems - 23rd European Symposium on Programming, ESOP 2014, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2014, Grenoble, France, April 5-13, 2014, Proceedings (Lecture Notes in Computer Science, Vol.",
"8410), Zhong Shao (Ed.).",
"Springer, 371–391.",
doi:10.1007/978-3-642- 54833-8_20 Nengkun Yu and Jens Palsberg.,
Quantum abstract interpretation.,
"In PLDI ’21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021, Stephen N. Freund and Eran Yahav (Eds.).",
"ACM, 542–558.",
"doi:10.1145/3453483.3454061 Li Zhou, Nengkun Yu, and Mingsheng Ying.",
An applied quantum Hoare logic.,
"In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, Kathryn S. McKinley and Kathleen Fisher (Eds.).",
"ACM, 1149–1162.",
"doi:10.1145/3314221.3314584 On Circuit Description Languages, Indexed Monads, and Resource Analysis 31 A Soundness & Adequacy We prove the soundness and the computational adequacy of the interpretation given in Section 3.",
The soundness and the adequacy for the interpretation given in Section 5 can be shown by almost the same argument (and thus we do not repeat the argument).,
"A.1 Soundness As usual, the soundness is proved by induction on the derivation of the evaluation relation.",
The proof also uses the following standard substitution lemma.,
Lemma 3 (Substitution).,
"Suppose that Φ, Γ1 ⊢𝑣𝑉: 𝐴.",
"(1) If Φ, Γ2,𝑥: 𝐴, Γ′ 2 ⊢𝑣𝑊: 𝐵, then JΦ, Γ2, Γ1, Γ′ 2 ⊢𝑣𝑊[𝑉/𝑥] : 𝐵K = JΦK ⊗JΓ2K ⊗JΓ1K ⊗JΓ′ 2K (Δ,id)⊗id −−−−−−−→ JΦK ⊗JΦK ⊗JΓ2K ⊗JΓ1K ⊗JΓ′ 2K −→ JΦK ⊗JΓ2K ⊗JΦK ⊗JΓ1K ⊗JΓ′ 2K id⊗J𝑉K⊗id −−−−−−−−→ JΦK ⊗JΓ2K ⊗J𝐴K ⊗JΓ′ 2K J𝑊K −−−→J𝐵K.",
"(2) If Φ, Γ,𝑥: 𝐴⊢𝑐𝑀: 𝐵, then JΦ, Γ, Γ1 ⊢𝑣𝑀[𝑉/𝑥] : 𝐵K = JΦK ⊗JΓK ⊗JΓ1K dup⊗id −−−−−→ JΦK ⊗JΦK ⊗JΓK ⊗JΓ1K −→ JΦK ⊗JΓK ⊗JΦK ⊗JΓ1K id⊗𝐽(J𝑉K) −−−−−−−−→ JΦK ⊗JΓK ⊗J𝐴K J𝑀K −−−→J𝐵K.",
By a simultaneous induction on the structure of the type derivation.,
□ Theorem 5 (Soundness).,
"Suppose that 𝑄⊢(𝐶, 𝑀) : 𝐴;𝑄′ and (𝐶, 𝑀) ⇓(𝐶′,𝑉).",
"Then J(𝐶, 𝑀)K = 𝐽(J(𝐶′,𝑉)K).",
"By induction on the derivation of (𝐶, 𝑀) ⇓(𝐶′,𝑉).",
Throughout the proof we write 𝑄𝑀 (resp.,
𝑄𝑉) for the label context that types the term 𝑀(resp.,
"𝑉); this means that we have 𝑄= 𝑄𝑀,𝑄′ and similarly for 𝑉.",
"Case app: We have 𝑀= (𝜆𝑥.𝑁) 𝑊with (𝐶, 𝑁[𝑊/𝑥]) ⇓(𝐷,𝑉).",
It suffices to prove that J(𝜆𝑥.𝑁)𝑊K = J𝑁[𝑊/𝑥]K because then we can close this case by the induction hypothesis.,
"By inversion on the typing rule, we must have 𝑄𝑁,𝑥: 𝐵⊢𝑐𝑁: 𝐴 and 𝑄𝑊⊢𝑣𝑊: 𝐵 with 𝑄= 𝑄𝑁,𝑄𝑊and ♯J𝑄𝑊K = ♯J𝐵K.",
"J(𝜆𝑥.𝑁)𝑊K = (𝐽((Λ(J𝑄𝑁,𝑥: 𝐵⊢𝑐𝑁: 𝐴K), idJ♯(𝑄𝑁,𝑥:𝐵)KM)) ⊗𝐽(J𝑄𝑊⊢𝑣𝑊: 𝐵K)); ev (by def.)",
"= (𝐽(Λ(J𝑄𝑁,𝑥: 𝐵⊢𝑐𝑁: 𝐴K), J♯(𝑄𝑁,𝑥: 𝐵)KM) ⊗𝐽(J𝑄𝑊⊢𝑣𝑊: 𝐵K)); ev (by def.",
"of the functor 𝐽(−,𝑇)) = (𝐽(id𝑄𝑁⊗J𝑄𝑊⊢𝑣𝑊: 𝐵K)); J𝑄𝑁,𝑥: 𝐵⊢𝑐𝑁: 𝐴K (by the universaity of ev) = J𝑁[𝑊/𝑥]K (by substituion lemma (Lemma 3)) 32 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago Case dest: Similar to let.",
"Case force: Similar to the case app, it suffices to show that Jforce(lift 𝑁)K = J𝑁K, for some 𝑁such that 𝑀= force(lift 𝑁).",
Since lift is a special case of abstraction (i.e.,
"thunking) and force is a special case for application, this can be proved as in the case of app.",
"Case apply: In this case, we have 𝑀= apply(( ¯ℓ, 𝐸, ¯𝑘), ¯𝑡) for some circuit 𝐸and wire bundles ¯ℓ, ¯𝑘and ¯𝑡.",
"Moreover, we have 𝐶; (𝐸⋉J𝑄′KM) = 𝐶′.",
"By inversion on the typing rules, we must have ∅⊢𝑣( ¯ℓ, 𝐸, ¯𝑘) : Circ(𝑇,𝑈) and ¯𝑡: 𝑇⊢𝑣¯𝑡: 𝑇 with 𝑄𝑀⊢𝑣¯𝑡: 𝑇and 𝐴= 𝑈for some 𝑇and 𝑈.",
"We first show that Japply(( ¯ℓ, 𝐸, ¯𝑘), ¯𝑡)K = 𝐽(id1, 𝐸).",
"By definition, we have Japply(( ¯ℓ, 𝐸, ¯𝑘), ¯𝑡)K = (J( ¯ℓ, 𝐸, ¯𝑘)K ⊗id𝑄𝑀); apply = ((𝐽(Λ(id1, J( ¯ℓ, 𝐸, ¯𝑘)KM)); 𝐽(box, id𝐼)) ⊗id𝑄𝑀); (𝐽(box−1, id𝐼) ⊗id𝑄𝑀); ev = ((𝐽(Λ(id1, J( ¯ℓ, 𝐸, ¯𝑘)KM)) ⊗id𝑄𝑀); ev = 𝐽(id1, J( ¯ℓ, 𝐸, ¯𝑘)KM).",
"Therefore, J(𝐶, 𝑀)K = 𝐽(id1,𝐶); (Japply(( ¯ℓ, 𝐸, ¯𝑘), ¯𝑡)K ⊗id𝑄′) = 𝐽(id1,𝐶); (𝐽(id1, J( ¯ℓ, 𝐸, ¯𝑘)KM) ⊗𝐽(id1, id𝑄′)) = 𝐽(id1,𝐶); 𝐽(id1, J( ¯ℓ, 𝐸, ¯𝑘)KM ⋉J𝑄′K) = 𝐽(id1,𝐶′; perm) = J(𝐶′, ¯𝑘)K as desired.",
"Here, perm is the isomorphism induced by the codomain of 𝐶′ and type of ¯𝑘.",
"Case box: It must be the case that 𝑀= box𝑇𝑊and 𝑉= ( ¯ℓ, 𝐷, ¯𝑘) for some value𝑊, circuit 𝐷, and wire bundles ¯ℓand ¯𝑘, and moreover, (id𝑄,𝑊¯ℓ) ⇓(𝐷, ¯𝑘).",
"By inversion on the typing rules, we have ∅⊢𝑣𝑊: 𝑇⊸𝑈 and ∅⊢𝑣( ¯ℓ, 𝐷, ¯𝑘) : Circ(𝑇,𝑈) where 𝐴= Circ(𝑇,𝑈).",
"Since the underlying circuit does not change during the reduc- tion, it suffices to show that Jbox𝑇𝑊K = 𝐽(J( ¯ℓ, 𝐷, ¯𝑘)K).",
"Moreover, since Jbox𝑇𝑊K = 𝐽(J𝑊K); 𝐽(box, id𝐼) and 𝐽(J( ¯ℓ, 𝐷, ¯𝑘)K) = 𝐽(  J( ¯ℓ, 𝐷, ¯𝑘)KM, id𝐼); 𝐽(box, id𝐼) by the definitions, we only need to show that J𝑊K = (  J( ¯ℓ, 𝐷, ¯𝑘)KM, id𝐼).",
"By the induction hypothesis, we have J(id𝑄,𝑊¯ℓ)K = J𝑊¯ℓK = 𝐽(id1, J( ¯ℓ, 𝐷, ¯𝑘)KM) = J(𝐷, ¯𝑘)K. Thus, we have J𝑊K = (Λ(J𝑊¯ℓK), id𝐼) = (Λ(𝐽(id1, J( ¯ℓ, 𝐷, ¯𝑘)KM)), id𝐼) = (  J( ¯ℓ, 𝐷, ¯𝑘)KM, id𝐼) as desired.",
"Case return: In this case, we have 𝑀= return 𝑉and 𝐷= 𝐶.",
"Since 𝐷= 𝐶, it suffices to show that J𝑀K = 𝐽((id1, perm); J𝑉K) for some suitable permutation isomorphism perm, and this is obvious by the definition of Jreturn 𝑉K.",
"Case let: In this case, we must have (𝐶, 𝑁1) ⇓(𝐷,𝑊) (𝐸, 𝑁2[𝑊/𝑥]) ⇓(𝐷,𝑉) (𝐶, let 𝑥= 𝑁1 in 𝑁2) ⇓(𝐶′,𝑉) On Circuit Description Languages, Indexed Monads, and Resource Analysis 33 with 𝑀= let 𝑥= 𝑁1 in 𝑁2 for some terms 𝑁1, 𝑁2, value 𝑊and circuit 𝐸.",
"By the inversion on the typing rule, we must also have 𝑄𝑁1 ⊢𝑐𝑁1 : 𝐵 and 𝑄𝑁2,𝑥: 𝐵⊢𝑐𝑁2 : 𝐴 for some 𝑄𝑁1 and 𝑄𝑁2 such that 𝑄𝑀= 𝑄𝑁2,𝑄𝑁1.",
"J(𝐶, 𝑀)K = 𝐽(id1,𝐶); (Jlet 𝑥= 𝑁1 in 𝑁2K ⋉J𝑄′K) = 𝐽(id1,𝐶);  (J𝑄𝑁2K ⋊J𝑁1K); J𝑁2K ⋉J𝑄′K) = 𝐽(id1,𝐶); (J𝑄𝑁2K ⋊J𝑁1K ⋉J𝑄′K); (J𝑁2K ⋉J𝑄′K) = 𝐽(id1, 𝐷); (idJ𝑄𝑁2K ⊗𝐽(J𝑊K) ⋉J𝑄′K); (J𝑁2K ⋉J𝑄′K) (by I.H.)",
"= 𝐽(id1, 𝐷);  (idJ𝑄𝑁2K ⊗𝐽(J𝑊K)); J𝑄𝑁2,𝑥: 𝐵⊢𝑐𝑁2 : 𝐴K  ⋉J𝑄′K  = 𝐽(id1, 𝐷);  J𝑁2[𝑊/𝑥]K ⋉J𝑄′K (by substitution lemma (Lemma 3)) = 𝐽(id1,𝐶′); (𝐽(J𝑉K) ⋉J𝑄′K) (by I.H.)",
"= J(𝐶′,𝑉)K □ A.2 Computational Adequacy The proof of computational adequacy also follows a standard strategy: we define a logical relation between semantics and syntax of Proto-Quipper-M.",
"Similar logical relations have been considered by Colledan and Dal Lago [Colledan and Dal Lago 2024, 2025] to prove the correctness of the type system of Proto-Quipper-R. (These are defined by purely operational means.)",
We say that a value (resp.,
term) is closed if the value (resp.,
term) does not have any free variables (but it may have some labels).,
The set of closed values of type (resp.,
term) 𝐴that is well- typed under the labeled context 𝑄is denoted by CVal𝑄⊢𝐴(resp.,
"We define relations (indexed by judgment of the form 𝑄⊢𝐴) R𝑄⊢𝐴⊆(♭J𝐴K × M(J𝑄KM, ♯J𝐴K)) × CTerm𝑄⊢𝐴and V𝑄⊢𝐴⊆♭J𝐴K × CVal𝑄⊢𝐴as the smallest relations satisfying the following conditions.",
"• (𝑣,𝐶; perm) R𝑄⊢𝐴𝑀if and only if (idJ𝑄KM, 𝑀) ⇓(𝐶,𝑉), 𝐶: 𝑄→𝐿′ and 𝑣V𝐿⊢𝐴𝑉for some label contexts 𝐿′ and 𝐿such that J𝐿′KM perm −−−−→  J𝐿KM where perm is a permutation isomorphism.",
"• ∗V∅⊢1 ∗ • 𝑛V∅⊢Nat 𝑛 • ∗Vℓ:𝑤⊢𝑤ℓ • 𝑓V𝑄⊢𝐴⊸𝑇𝐵𝑉if and only if for all 𝑊such that 𝑤V𝐿⊢𝐴𝑊, we have 𝑓𝑤R𝑄,𝐿⊢𝐵𝑉𝑊.",
"• 𝑓V∅⊢!𝐴𝑉if and only if 𝑓∗R∅⊢𝐵force𝑉 • J( ¯ℓ,𝐶, ¯𝑘)K V∅⊢Circ(𝑇,𝑈) ( ¯ℓ,𝐶, ¯𝑘) • (𝑣,𝑤) V𝑄1,𝑄2⊢𝐴⊗𝐵⟨𝑉,𝑊⟩if and only if 𝑣V𝑄1⊢𝑉and 𝑤V𝑄2⊢𝑊.",
"We extend the relation to a relation between pairs of a value and a substitution and typing contexts by (𝑣,𝛾) V𝑄(𝑎1 : 𝐴1, .",
",𝑎𝑛: 𝐴𝑛) ⇐⇒𝑣V𝑄⊢𝐴1⊗···⊗𝐴𝑛⟨𝛾(𝑎1), .",
",𝛾(𝑎𝑛)⟩ where • 𝑣is an element of ♭J𝐴1K × · · · × ♭J𝐴𝑛K and • 𝛾is a map from dom(Γ) to closed values such that, for each 𝑎𝑖, the only labels appearing in 𝛾(𝑎𝑖) are those in 𝑄.",
"34 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago The definition of R𝑄⊢𝐴evaluates 𝑀with the identity circuit, but this does not loose generality because if the initial configuration has a circuit that is not the identity we can just concatenate the initial circuit to the circuit obtained by evaluating 𝑀with the identity circuit.",
"Suppose that 𝐶: 𝑄0 →𝑄1, 𝐿,𝑄2 and 𝐿⊢𝑀: 𝐴.",
"If (idJ𝐿KM, 𝑀) ⇓(𝐷,𝑉) and (𝐶, 𝑀) ⇓ (𝐸,𝑊), then 𝑉=𝑊(up to renaming of labels) and 𝐸= 𝐶; (J𝑄1KM ⋊(𝐷⋉J𝑄2KM)).",
"By induction on the derivation of (𝐶, 𝑀) ⇓(𝐸,𝑊).",
□ The fundamental property of the logical relations hold.,
"And, as usual, the theorem is a direct consequence of the fundamental property.",
Lemma 5 (Fundamental Property).,
"Let Γ be a type environment, 𝑄be a label context such that J𝑄KM = ♯JΓK.",
"Suppose that (𝑣,𝛾) V𝑄Γ.",
Then the following holds.,
(1) Γ ⊢𝑣𝑉: 𝐴implies ♭J𝑉K(𝑣) V𝑄⊢𝐴𝛾(𝑉).,
"(2) Γ ⊢𝑐𝑀: 𝐴implies, J𝑀K(𝑣) R𝑄⊢𝐴𝛾(𝑀) Proof.",
By simultaneous induction on the type derivation.,
We show two of the most interesting cases: the case for box and let.,
The other cases can be proved in a similar manner.,
"Case box: In this case, we must have 𝑀= box 𝑉, Γ = Φ and 𝐴= Circ(𝑇,𝑈) for some value 𝑉, parameter context Φ and bundle types 𝑇and 𝑈.",
"Moreover, we have Φ ⊢𝑣𝑉: 𝑇⊸𝑈.",
"Suppose that (𝑣,𝛾) V∅Φ.",
"Our goal is to show J𝑀K(𝑣) R∅⊢Circ(𝑇,𝑈) 𝛾(𝑀).",
"By induction hypothesis, we have ♭J𝑉K(𝑣) V∅⊢𝑇⊸𝑈𝛾(𝑉).",
"Hence, we have ♭J𝑉K(𝑣)(∗) R ¯ℓ:𝑇⊢𝑈𝛾(𝑉) ¯ℓ.",
"By the definition of R ¯ℓ:𝑇⊢𝑈, we have (idJ𝑇KM,𝛾(𝑉) ¯ℓ) ⇓(𝐶, ¯𝑘) (7) ♭J𝑉K(𝑣)(∗) = (∗,𝐶; perm) (8) ∗V𝐿⊢𝑈¯𝑘 (9) perm: J𝐿′KM −→J𝐿KM (10) where 𝐶: J𝑇KM →J𝐿′KM By applying the box rule to (7), we have (id𝐼,𝛾(box 𝑉)) ⇓ (id𝐼, ( ¯ℓ,𝐶, ¯𝑘)).",
"It remains to show that J𝑀K(𝑣) = (J( ¯ℓ,𝐶, ¯𝑘)KM, id𝐼), and this is an immediate consequence of (8) and the interpretation of box .",
"Case let: It must be the case that 𝑀= (let 𝑥= 𝑁in 𝑃) Γ = Φ, Γ2, Γ1 Φ, Γ1 ⊢𝑐𝑁: 𝐵 Φ, Γ2,𝑥: 𝐵⊢𝑐𝑃: 𝐴 for some terms 𝑁and 𝑀, some type 𝐵and contexts Φ, Γ1 and Γ2.",
Let 𝑄1 and 𝑄2 be label contexts such that J𝑄1KM = ♯JΓ1K and J𝑄2KM = ♯JΓ2K.,
"Suppose that (𝑣,𝛾) V𝑄2,𝑄1 Γ.",
"We need to show that J𝑀K(𝑣) R⊢𝑄2,𝑄1 𝛾(𝑀).",
"Let us write 𝑣0 , 𝑣1 and 𝑣2 for the Φ, Γ1 and Γ2 part of 𝑣, respectively, and let 𝛾1 def= 𝛾↾dom(Φ,Γ1) and 𝛾2 def= 𝛾↾dom(Φ,Γ2).",
"By the induction hypothesis, we have J𝑁K(𝑣0, 𝑣1) R𝑄1⊢𝐵𝛾1(𝑁) (11) J𝑃K(𝑣0, 𝑣2,𝑤) R𝑄2,𝑄1⊢𝐴[𝑊/𝑥]𝛾2(𝑃) (12) where J𝑁K(𝑣0, 𝑣1) = (𝑤,𝐶) (13) (idJ𝑄1KM,𝛾1(𝑁)) ⇓(𝐶′,𝑊) (14) On Circuit Description Languages, Indexed Monads, and Resource Analysis 35 By (11), we must have 𝑤V𝑄1⊢𝐵𝑊and 𝐶= 𝐶′ (up to permutation which we shall ignore for simplicity).",
"By applying Lemma 4 to (14), we also have (idJ𝑄2,𝑄1KM,𝛾1(𝑁)) ⇓(J𝑄2KM ⋊𝐶′,𝑊).",
"(15) Now suppose that J𝑃K(𝑣0, 𝑣2,𝑤) = (𝑣ret, 𝐷) (16) (idJ𝑄1KM,𝛾1(𝑁)) ⇓(𝐷′,𝑉) (17) By the definition of (12), it must be the case that 𝑣ret V𝑄2,𝑄1⊢𝐵𝑉and 𝐷= 𝐷′ (again, up to permutation which we ignore).",
"Then, once again, by Lemma 4, we have (J𝑄2KM ⋊𝐶′, [𝑊/𝑥]𝛾2(𝑃)) ⇓((J𝑄2KM ⋊𝐶′); 𝐷′,𝑉) (18) It follows that (idJ𝑄2,𝑄1KM,𝛾(𝑀)) ⇓((J𝑄2KM ⋊𝐶′); 𝐷,𝑉) by applying the rule let to (15) and (18).",
"From the definition of J𝑀K, (13), and (16) together with 𝑣ret V𝑄2,𝑄1⊢𝐵𝑉we have ♭(J𝑀K(𝑣0, 𝑣2, 𝑣1)) V𝑄2,𝑄1⊢𝐴𝑉.",
The equality (up to permutation) on the circuit part also holds.,
"We, therefore, have J𝑀K(𝑣) R𝑄2,𝑄1⊢𝐴𝛾(𝑀).",
□ Theorem 6 (Computational Adequacy).,
"Suppose that ∅⊢(𝐶, 𝑀) : 1; ∅and J(𝐶, 𝑀)K = 𝐽(J(𝐷,𝑉)K).",
"Then (𝐶, 𝑀) ⇓(𝐷,𝑉) (possibly up to renamings of labels).",
"The assumption ∅⊢(𝐶, 𝑀) : 1; ∅means that there is a label context 𝑄such that 𝑄⊢𝑐𝑀: 1.",
"Since 𝑀is closed, using the previous lemma, we obtain J𝑀K(∗) R𝑄⊢1 𝑀.",
"By the definition of R𝑄⊢1, we have (idJ𝑄KM, 𝑀) ⇓(𝐸, ∗) where J𝑀K(∗) = (∗, 𝐸).",
"By J(𝐶, 𝑀)K = 𝐽(J(𝐷,𝑉)K), it follows that 𝐷= 𝐶; 𝐸.",
"From this and Lemma 4, we have (𝐶, 𝑀) ⇓(𝐷,𝑉) as desired.",
□ Remark 5.,
"To prove the adequacy for terms typed in the type-and-effect system, we just need to add effect annotations to the logical relations.",
"That is, the logical relation for computations, now becomes R𝑒 𝑄⊢𝐴⊆(♭J𝐴K × M≲𝑒(J𝑄KM, ♯J𝐴K)) × CTerm𝑄⊢𝐴;𝑒where CTerm𝑄⊢𝐴;𝑒is the set of terms 𝑀such that 𝑄⊢𝑐𝑀: 𝐴;𝑒.",
"Note that the set of circuits are now restricted to M≲𝑒(J𝑄KM, ♯J𝐴K)).",
"Relations V𝑄⊢𝐴for arrow type, thunk type, and circuit type are changed accordingly.",
"◁ 36 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago B Omitted Definitions from Section 5 This section shows the typing rules and the interpretation that was omitted from Section 5.",
B.1 Full Definition of the Typing Rules Here we list the full list of typing rules for reference.,
"The rules that were omitted from Section 5 are mostly identical to the corresponding rules of Proto-Quipper-C. unit Φ ⊢𝑣∗: 1 nat Φ ⊢𝑣𝑛: Nat lab Φ, ℓ: 𝑤⊢𝑣ℓ: 𝑤 var Φ,𝑥: 𝐴⊢𝑣𝑥: 𝐴 abs Γ,𝑥: 𝐴⊢𝑐𝑀: 𝐵;𝑒: t →u Γ ⊢𝑣𝜆𝑥𝐴.𝑀: 𝐴 𝑒: t→u ⊸#(Γ) 𝐵 app Φ, Γ1 ⊢𝑣𝑉: 𝐴 𝑒: s→u ⊸𝑇 𝐵 Φ, Γ2 ⊢𝑣𝑊: 𝐴 Φ, Γ1, Γ2 ⊢𝑐𝑉𝑊: 𝐵;𝑒: s →u lift Φ ⊢𝑐𝑀: 𝐴;𝑒: t →u Φ ⊢𝑣lift 𝑀: !𝑒𝐴 force Φ ⊢𝑣𝑉: !𝑒𝐴 Φ ⊢𝑐force𝑉: 𝐴;𝑒: t →u circ 𝐶: 𝑄→𝐿 𝑄𝜎𝑄′ 𝐿𝜎𝐿′ 𝑄′ ⊢𝑣¯ℓ: 𝑇𝐿′ ⊢𝑣¯𝑘: 𝑈𝛼(J( ¯ℓ,𝐶, ¯𝑘)K) = 𝑒 Φ ⊢𝑣( ¯ℓ,𝐶, ¯𝑘) : Circ𝑒(𝑇,𝑈) box Φ ⊢𝑣𝑉: 𝑇 𝑒: t→u ⊸𝐼 𝑈 Φ ⊢𝑐box𝑇𝑉: Circ𝑒: t→u(𝑇,𝑈);𝜀 apply Φ, Γ1 ⊢𝑣𝑉: Circ𝑒: t→u(𝑇,𝑈) Φ, Γ2 ⊢𝑣𝑊: 𝑇 Φ, Γ1, Γ2 ⊢𝑐apply(𝑉,𝑊) : 𝑈;𝑒: t →u dest Φ, Γ1 ⊢𝑣𝑉: 𝐴⊗𝐵 Φ, Γ2,𝑥: 𝐴,𝑦: 𝐵⊢𝑐𝑀: 𝐶;𝑒 Φ, Γ2, Γ1 ⊢𝑐let ⟨𝑥,𝑦⟩= 𝑉in 𝑀: 𝐶;𝑒 ifz Φ ⊢𝑣𝑉: Nat Φ, Γ ⊢𝑐𝑀: 𝐴;𝑒 Φ, Γ ⊢𝑐𝑁: 𝐴;𝑒 Φ, Γ ⊢𝑐ifz 𝑉then 𝑀else 𝑁: 𝐴;𝑒 pair Φ, Γ1 ⊢𝑣𝑉: 𝐴Φ, Γ2 ⊢𝑣𝑊: 𝐵 Φ, Γ1, Γ2 ⊢𝑣⟨𝑉,𝑊⟩: 𝐴⊗𝐵 return Γ ⊢𝑣𝑉: 𝐴 𝛼(♯J𝐴K) = t Γ ⊢𝑐return 𝑉: 𝐴; idt : t →t let Φ, Γ1 ⊢𝑐𝑀: 𝐴;𝑒1 : t1 →t′ 1 Φ, Γ2,𝑥: 𝐴⊢𝑐𝑁: 𝐵;𝑒2 : t2 →t′ 2 𝛼(J♯Γ𝑖K) = u𝑖 𝑒= (idu2 ⋊𝑒1);𝑒2 Φ, Γ2, Γ1 ⊢𝑐let 𝑥= 𝑀in 𝑁: 𝐵;𝑒 sub Γ ⊢𝑐𝑀: 𝐴;𝑒1 : t →u 𝑒1 ≲𝑒2 Γ ⊢𝑐𝑀: 𝐴;𝑒2 : t →u ex perm : JΓ1K ⊗J𝐴K ⊗J𝐵K ⊗JΓ2K −→JΓ1K ⊗J𝐵K ⊗J𝐴K ⊗JΓ2K Γ1,𝑎: 𝐴,𝑏: 𝐵, Γ2 ⊢𝑐𝑀: 𝐶;𝑒: t →u Γ1,𝑏: 𝐵,𝑎: 𝐴, Γ2 ⊢𝑐𝑀: 𝐶; (𝛼(perm);𝑒) Fig.",
Typing rules for the effect system of Proto-Quipper-R (full definition).,
B.2 Interpretation of Value Judgments We first show how value judgments are interpreted.,
"As explained, value judgments are interpreted in V(= Set × disc(M)) and the interpretation is essentially the same as that of the simply-typed On Circuit Description Languages, Indexed Monads, and Resource Analysis 37 system.",
Here we only show the interpretation of values with types that have effect annotations.,
"JΓ ⊢𝑣𝜆𝑥𝐴.𝑀: 𝐴 𝑒: t→u ⊸#(Γ) 𝐵K def= (Λ𝑒(JΓ,𝑥: 𝐴⊢𝑐𝑀: 𝐵;𝑒K), idJ#(Γ)KM)) JΦ ⊢𝑣lift 𝑀: !𝑒𝐴K def= (Λ𝑒(JΦ ⊢𝑐𝑀: 𝐴K), id𝐼) JΦ ⊢𝑣( ¯ℓ,𝐶, ¯𝑘) : Circ𝑒(𝑇,𝑈)K def= (!JΦK;  J( ¯ℓ,𝐶, ¯𝑘)K, id𝐼) The above interpretations are almost identical to those of Figure 5.",
"The only difference is the natural bijection Λ𝑒, which is now indexed with 𝑒: Set(𝑋× 𝑌,𝑍× M≲𝑒(𝑇,𝑈)) Λ𝑒,𝑋,𝑌,𝑍 −−−−−−→  Set(𝑋,𝑌⇒Set 𝑍× M≲𝑒(𝑇,𝑈)).",
"The morphism ˆ𝐶: 1 →M≲𝑒(J𝑇KM, J𝑈KM) is the global element ˆ𝐶(∗) = 𝐶(which in turn can be defined using Λ𝑒).",
B.3 Interpretation of Computational Judgments Recall that Γ ⊢𝑐𝑀: 𝐴;𝑒is interpreted as a morphism in Set from ♭JΓK to T 𝑒: ♯JΓK→♯J𝐴K(♭J𝐴K).,
Application and forcing.,
"t Φ, Γ1 ⊢𝑣𝑉: 𝐴 𝑒: s→u ⊸𝑇 𝐵 Φ, Γ2 ⊢𝑣𝑊: 𝐴 Φ, Γ1, Γ2 ⊢𝑐𝑉𝑊: 𝐵;𝑒: s →u | def= JΦK𝑃× ♭JΓ1K × ♭JΓ2K ΔJΦK𝑃×id −−−−−−−→JΦK𝑃× JΦK × ♭JΓ1K𝑃× ♭JΓ2K  −−−−−−−−−→(JΦK𝑃× ♭JΓ1K) × (JΦK𝑃× ♭JΓ2K) ♭J𝑉K×♭J𝑊K −−−−−−−−−→♭J𝐴⊸𝑇𝐵K × ♭J𝐴K ev −−−−−−−−−→T 𝑒(♭J𝐵K) t Φ ⊢𝑣𝑉: !𝑒𝐴 Φ ⊢𝑐force𝑉: 𝐴;𝑒: t →u | def= JΦK𝑃 ♭J𝑉K −−−→♭J!𝑒𝐴K × 1 ev −−−−−→T 𝑒(♭J𝐴K) Here, unlike in Section 4, ev is the evaluation morphism for the exponential objects in Set.",
"38 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago Circuit operations.",
"t Φ ⊢𝑣𝑉: 𝑇 𝑒: t→u ⊸𝐼 𝑈 Φ ⊢𝑐box𝑇𝑉: Circ𝑒: t→u(𝑇,𝑈);𝜀 | def= JΦK𝑃 J𝑉K −−−−−→♭J𝑇 𝑒⊸𝑈K box −−−−−→M≲𝑒(J𝑇KM, J𝑈KM) 𝜂𝐼 −−−−−→T 𝜀(M≲𝑒(J𝑇KM, J𝑈KM)) t Φ, Γ1 ⊢𝑣𝑉: Circ𝑒: t→u(𝑇,𝑈) Φ, Γ2 ⊢𝑣𝑊: 𝑇 Φ, Γ1, Γ2 ⊢𝑐apply(𝑉,𝑊) : 𝑈;𝑒: t →u | def= JΦK𝑃× ♭JΓ1K × ♭JΓ2K ΔJΦK𝑃×id −−−−−−−→JΦK𝑃× JΦK𝑃× ♭JΓ1K × ♭JΓ2K  −−−−−−−→(JΦK𝑃× ♭JΓ1K) × (JΦK𝑃× ♭JΓ2K) ♭J𝑉K×♭J𝑊K −−−−−−−−−→M≲𝑒(J𝑇KM, J𝑈KM) 𝜂J𝑇KM −−−−−−−→T idJ𝑇K(M≲𝑒(J𝑇KM, J𝑈KM))) comp −−−−−−−→T 𝑒(1).",
"Here comp𝑇1,𝑇2,𝑇3 : M(𝑇1,𝑇2) × M(𝑇2,𝑇3) →M(𝑇1,𝑇3) is the composition morphism.",
Return and let.,
"t Γ ⊢𝑣𝑉: 𝐴 𝛼(♯J𝐴K) = t Γ ⊢𝑐return 𝑉: 𝐴; idt : t →t | def= ♭JΓK ♭J𝑉K −−−→♭J𝐴K 𝜂♯J𝐴K −−−−−→T id♯J𝐴K(♭J𝐴K) u v Φ, Γ1 ⊢𝑐𝑀: 𝐴;𝑒1 : t1 →t′ 1 Φ, Γ2,𝑥: 𝐴⊢𝑐𝑁: 𝐵;𝑒2 : t2 →t′ 2 J♯Γ𝑖K ▷u𝑖 𝑒= (idu2 ⋊𝑒1);𝑒2 Φ, Γ2, Γ1 ⊢𝑐let 𝑥= 𝑀in 𝑁: 𝐵;𝑒 } ~ def= JΦK × ♭JΓ2K × ♭JΓ1K (Δ×id); −−−−−−−→JΦK × ♭JΓ2K × JΦK × ♭JΓ1K id×𝜂u2 ×J𝑀K −−−−−−−−−→JΦK × T idu2 M ♭JΓ2K × T 𝑒1 M ♭J𝐴K id×<⃝;𝜏 −−−−−→T id⋊𝑒1 M (JΦK × ♭JΓ2K × ♭J𝐴K) Tid⋊𝑒1 M J𝑁K −−−−−−−−→T id⋊𝑒1 M (T 𝑒2 M (♭J𝐵K)) 𝜇id⋊𝑒1,𝑒2 −−−−−−→T (id⋊𝑒1);𝑒2 M (♭J𝐵K).",
Subsumption.,
"t Γ ⊢𝑐𝑀: 𝐴;𝑒1 : t →u 𝑒1 ≲𝑒2 Γ ⊢𝑐𝑀: 𝐴;𝑒2 : t →u | def= ♭JΓK J𝑀K −−−→T 𝑒1 : ♯JΓK→♯J𝐴K(♭J𝐴K) T𝑒1≲𝑒2 ♭J𝐴K −−−−−→T 𝑒2(♭J𝐴K) On Circuit Description Languages, Indexed Monads, and Resource Analysis 39 C Supplementary Materials on Indexed Monads and Parameterized Freyd Categories As supplementary material, we include a brief review of indexed monads and the parameterized Freyd category to make the paper self-contained.",
Most of the definitions are drawn from [Atkey 2009].,
"We present only the minimal definitions required to understand this paper, with explanations tailored to our purposes.",
C.1 Parameterized Freyd category We give the precise definition of parameterized Freyd category that was omitted from the body of the paper.,
Definition 12 ([Atkey 2009]).,
"A parameterized Freyd category consists of three functors 𝐽: C×S → K, ⋊⃝: C × K →K and ⋉⃝: K × C →K such that • 𝐽is identity on objects, • the cartesian product of C is respected: 𝑋⋊⃝𝐽(𝑌,𝑆) = 𝐽(𝑋,𝑆) ⋉⃝𝑌= 𝐽(𝑋× 𝑌,𝑆), • for each 𝑆∈S, the transformations given by the associativity 𝐽(𝛼,𝑆), the left unitor 𝐽(𝜆,𝑆), the right unitor 𝐽(𝜌,𝑆) and the symmetry 𝐽(𝑠,𝑆) of the symmetric monoidal structure arising from the finite products of C must be natural in the variables in all combinations of ×, ⋊⃝and ⋉⃝that make up their domain and codomain.",
"As is clear from the above conditions, we are assuming that C has finite products.",
The pair of functors ⋊⃝and ⋉⃝is often called the premonoidal structure of the parameterized Freyd category (with respect to C).,
◁ The most important example (and the only example appearing in the paper) of a parameterized Freyd category is the one given by the Kleisli construction of an indexed monad.,
"As already explained, the Kleisli category of T : Sop × S × C →C has pairs of objects of C and S as its objects and the homset CT((𝑋,𝑆), (𝑌,𝑇) is defined as C(𝑋, T (𝑆,𝑇,𝑌)).",
"Compositions of morphisms 𝑓: (𝑋,𝑆) →(𝑌,𝑇) and 𝑔: (𝑌,𝑇) →(𝑍,𝑈) are defined by the Kleisli composition: 𝑋 𝑓−→T (𝑆,𝑇,𝑌) T(𝑆,𝑇,𝑔) −−−−−−−→T (𝑆,𝑇, T (𝑇,𝑈,𝑍)) 𝜇𝑆,𝑇,𝑈,𝑍 −−−−−−→T (𝑆,𝑈,𝑍).",
"The (morphism part of the) functor ⋊⃝is defined by 𝑓⋊⃝𝑐 def= 𝑋×𝑊 𝑓×𝑐 −−−→𝑌× T (𝑆,𝑇,𝑍) 𝜏𝑌,𝑆,𝑇,𝑍 −−−−−→T (𝑆,𝑇,𝑌× 𝑍).",
"(19) for 𝑓: 𝑋→𝑌, a morphism in C, and 𝑐: (𝑊,𝑆) →(𝑍,𝑇).",
"C.2 Lifting of the Premonoidal Structure In the main part of the paper, we claimed that the premonoidal structure is lifted to the indexed monad, parameterized Freyd category, and the category-graded monad.",
Here we make precise what we mean by that.,
"This is just a review of an existing, but perhaps not so known, notion [Atkey 2009], and we believe that having this supplementary section would help readers understand the technical details.",
"However, readers not interested in the technical definitions may safely ignore this section.",
"To understand the interpretation, it suffices to understand that there is an operation, called the lifting of −⋉𝑇, that acts on computations by modifying the underlying circuit 𝐶to 𝐶⋉𝑇.",
"It may be worth mentioning that our interpretation, as well as the soundness result, does not use any specific property of the circuit monad except for circuit related operations such as box and lift.",
The interpretation of the other constructs only relies on the parameterized Freyd structure and the existence of the premonoidal lifting.,
We start by reviewing the notion of lifting for indexed monads.,
"40 Ken Sakayori, Andrea Colledan, and Ugo Dal Lago Definition 13 (Lifting for indexed monads [Atkey 2009]).",
Let T be a S-indexed monad over a cartesian category C and 𝐹: S →S be an endofunctor.,
"A lifting of 𝐹to T is a natural transformation 𝐹† 𝑆,𝑇,𝑋: T (𝑆,𝑇,𝑋) →T (𝐹𝑆, 𝐹𝑇,𝑋) that commutes with the unit, multiplication (and strength of the monad): 𝜂𝐹𝑆,𝑋 𝜂𝑆,𝑋 𝐹† 𝑆,𝑆,𝑋 𝑋 T (𝑆,𝑆,𝑋) T (𝐹𝑆, 𝐹𝑆,𝑋) 𝜇𝑆,𝑇,𝑈,𝑋 𝐹† 𝑆,𝑇,T(𝑇,𝑈,𝑋) T(𝐹𝑆,𝐹𝑇,𝐹†) 𝐹† 𝑆,𝑈,𝑋 𝜇𝐹𝑆,𝐹𝑇,𝐹𝑈,𝑋 T (𝑆,𝑇, T (𝑇,𝑈,𝑋)) T (𝐹𝑆, 𝐹𝑇, T (𝑇,𝑈,𝑋)) T (𝑆,𝑈,𝑋) T (𝐹𝑆,𝑆𝑈,𝑋) T (𝐹𝑆, 𝐹𝑇, T (𝐹𝑇, 𝐹𝑈,𝑋)) A natural transformation 𝜃: 𝐹→𝐺is natural for liftings 𝐹† and 𝐺† if the following diagram commutes.",
"𝐺† 𝑆,𝑇,𝑋 𝐹† 𝑆,𝑇,𝑋 T(𝐹𝑆,𝜃𝑇,𝑋) T(𝜃𝑆,𝐺𝑇,𝑋) T (𝑆,𝑇,𝑋) T (𝐹𝑆, 𝐹𝑇,𝑋) T (𝐺𝑆,𝐺𝑇,𝑋) T (𝐹𝑆,𝐺𝑇,𝑋) ◁ Definition 14.",
"We say that an S-indexed monad T has premonoidal lifting if there are liftings for the functors −⋉𝑆and 𝑆⋊−, written (−⋉𝑆)† and (𝑆⋊−)†, for every 𝑆∈S, such that all the associativity and left and right unitors are natural for them.",
"Furthermore, the indexed monad is said to have a symmetric premonoidal lifting if the symmetry natural transformations are also natural.",
"◁ As we briefly explained, the circuit monad TM has premonoidal lifting.",
Next we review the notion of liftings for the parameterized Freyd category.,
The liftings for an indexed monad and the liftings for the parameterized Freyd category obtained as the Kleisli category of that indexed monad are mutually related.,
Definition 15 (Lifting for parameterized Freyd categories [Atkey 2009]).,
"Let 𝐹: S →S be an endofunctor and (𝐽, ⋊⃝, ⋉⃝) be a parametric Freyd category where 𝐽: C×S →K.",
"The parameterized Freyd category has a lifting of 𝐹if it has a functor 𝐹★: K →K such that • 𝐹★(𝐽(𝑋,𝑆)) = 𝐽(𝑋, 𝐹𝑆) and 𝐹★(𝐽(𝑓,𝑠)) = 𝐽(𝑓, 𝐹𝑠) for 𝑓∈C(𝑋,𝑌) and 𝑠∈S(𝑆,𝑇) • 𝐹★respects the premonoidal structure: 𝐹★(𝑋⋊⃝(𝑌,𝑆)) = 𝐹★((𝑋,𝑆) ⋉⃝𝑌) = (𝑋× 𝑌, 𝐹𝑆) and 𝐹★(𝑓⋊⃝𝑐) = 𝑓⋊𝐹★𝑐(and similarly for ⋉⃝).",
"A natural transformation 𝜃: 𝐹→𝐺is natural for liftings 𝐹★and 𝐺★if the following diagram commutes for all 𝑐: (𝑋,𝑆) →(𝑌,𝑇).",
"𝐹†𝑓 𝐽(𝑋,𝜃𝑆) 𝐺†𝑓 𝐽(𝑌,𝜃𝑇) 𝐽(𝑋, 𝐹𝑆) 𝐽(𝑋,𝐺𝑆) 𝐽(𝑌, 𝐹𝑇) 𝐽(𝑍,𝐺𝑇) ◁ On Circuit Description Languages, Indexed Monads, and Resource Analysis 41 Proposition 2 ([Atkey 2009, Theorem 3]).",
"Let T be an indexed monad over a cartesian category C. If 𝐹† is a lifting of an endofunctor 𝐹: S →S, then we can construct a lifting 𝐹★on the parameterized Freyd category CT and vice versa.",
These operations are inverse.,
"If a natural transformation from 𝐹to 𝐺is natural for liftings 𝐹† and 𝐺†, then it is also natural for liftings 𝐹★and 𝐺★, and vice versa.",
"The premonoidal lifting for parameterized Freyd category is defined as in Definition 14, and it is easy to check that if T has a premonoidal lifting so does its Kleisli category.",
"We note that a parametrized Freyd category with a premonoidal lifting, in a sense, has two premonoidal structures one respect to C and the other respect to S. This allows us to define a binoidal functor (𝑋,𝑆) ⋊ −: K →K, for each (𝑋,𝑆) ∈K, defined as (𝑋⋊⃝−); (𝑆⋊−)★.",
"The functor −⋉(𝑋,𝑆) : K →K is defined analogously.",
It is not hard to see that this is makes K a premonoidal category.,
"In particular, for SetTM, we have ((𝑋,𝑆) ⋊𝑓)(𝑥,𝑦) = ((𝑥,𝑧),𝑆⋊𝐶) provided that 𝑓: 𝑌→TM(𝑇,𝑈,𝑍) maps 𝑦to (𝑧,𝐶).",
"In other words, (𝑋,𝑆) ⋊𝑓just performs the computation against the second element and, at the same time, augments wires to the underlying circuit of the computation.",
It is this premonoidal structure that is used in the interpretation of the computational judgments given in Section 4.,
"We note that 𝐽(𝑓,𝑠) is a central morphism if 𝑠is with respect to the premonoidal structure of S. Therefore, the values are interpreted as central morphisms in the interpretation.",
The notion of lifting of functors can be naturally extended to category-graded monads.,
Definition 16 (Lifting for category-graded monad).,
Let T be a A-graded monad over a carte- sian category C and 𝐹: A →A be an endofunctor.,
"A lifting of 𝐹to T is a family of natural transformation 𝐹† 𝑓,𝑋: T 𝑓𝑋→T 𝐹𝑓𝑋indexed by morphisms 𝑓in A that commutes with the unit, multiplication (and strength of the monad): 𝜂𝐹𝑎,𝑋 𝜂𝑎,𝑋 𝐹† id𝑎,𝑋 𝑋 T id𝑎 T id𝐹𝑎 𝜇𝑓,𝑔𝑋 𝐹† 𝑓,T𝑔𝑋 T𝐹𝑓(𝐹†) 𝐹† 𝑓;𝑔,𝑋 𝜇𝐹𝑓,𝐹𝑔,𝑋 T 𝑓(T 𝑔𝑋) T 𝐹𝑓(T 𝑔𝑋) T 𝑓;𝑔𝑋 T 𝐹𝑓;𝐹𝑔𝑋 T 𝐹𝑓(T 𝐹𝑔𝑋) A natural transformation 𝜃: 𝐹→𝐺is natural for liftings 𝐹† and 𝐺† if there is a generalized unit [Orchard et al.",
"2020] 𝜂𝜃𝑎,𝑋: 𝑋→T 𝜃𝑎𝑋for each component 𝜃𝑎such that 𝐺† 𝑓,𝑋 𝐹† 𝑓,𝑋 T𝐹𝑓𝜂𝜃𝑏,𝑋 𝜇 𝜂𝑎,T𝐺𝑓𝑋 𝜇𝜃𝑎;𝐺𝑓 T 𝑓𝑋 T 𝐹𝑓 T 𝐹𝑓T 𝜃𝑏𝑋 T 𝐹𝑓;𝜃𝑏 T𝐺𝑓𝑋 T 𝜃𝑎T 𝐺𝑓𝑋 T 𝜃𝑎;𝐺𝑓𝑋 commutes for each 𝑓: 𝑎→𝑏.",
◁ For the T 𝑒 M the lift of the premonoidal product 𝑇⋊−is once again just the operation that maps the circuit part of a computation 𝐶to 𝑇⋊𝐶.,
"The only natural transformation we are interested in is the symmetry (as we are working in a strict premonoidal category), and the generalized unit for the symmetry 𝑠is just the map 𝑥↦→(𝑥,𝑠).",
"TypeDis: A Type System for Disentanglement Extended Version ALEXANDRE MOINE, New York University, USA STEPHANIE BALZER, Carnegie Mellon University, USA ALEX XU, Carnegie Mellon University, USA SAM WESTRICK, New York University, USA Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other’s allocations.",
"As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks.",
"However, as a low-level property, disentanglement can be difficult to reason about for programmers.",
"The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant.",
"DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement.",
Yet its employment requires significant expertise and per-program proof effort.,
"This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer.",
"It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it.",
TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps.,
"Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming.",
The paper illustrates TypeDis and its features on a range of examples.,
"The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2.",
CCS Concepts: • Software and its engineering →Parallel programming languages; • Theory of computation →Type theory; Separation logic.,
"Additional Key Words and Phrases: disentanglement, parallelism, type system, separation logic ACM Reference Format: Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick.",
TypeDis: A Type System for Dis- entanglement: Extended Version.,
ACM Program.,
"10, POPL, Article 13 (January 2026), 34 pages.",
https://doi.org/10.1145/3776655 1 Introduction A recent line of work has identified a key memory property of parallel programs called disen- tanglement [Acar et al.,
2015; Arora et al.,
"2024, 2021, 2023; Guatto et al.",
2018; Moine et al.,
2024; Raghunathan et al.,
2016; Westrick et al.,
"2022, 2020].",
"Roughly speaking, disentanglement is the prop- erty that concurrent tasks remain oblivious to each other’s memory allocations.",
As demonstrated by the MaPLe compiler [Acar et al.,
"2020], this property makes it possible to perform task-local memory management (allocations and garbage collection) independently, in parallel, without any synchronization between concurrent tasks.",
"MaPLe in particular features a provably efficient Authors’ Contact Information: Alexandre Moine, alexandre.moine@nyu.edu, New York University, New York, USA; Stephanie Balzer, balzers@cs.cmu.edu, Carnegie Mellon University, Pittsburgh, USA; Alex Xu, alexxu@andrew.cmu.edu, Carnegie Mellon University, Pittsburgh, USA; Sam Westrick, shw8119@nyu.edu, New York University, New York, USA.",
This work is licensed under a Creative Commons Attribution 4.0 International License.,
© 2026 Copyright held by the owner/author(s).,
ACM 2475-1421/2026/1-ART13 https://doi.org/10.1145/3776655 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
"Publication date: January 2026. arXiv:2511.23358v2 [cs.PL] 2 Dec 2025 13:2 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick let r = ref """" let rec write_max x = let current = !r in if x <= current || compare_and_swap r current x then () else write_max x let entangled = par(fun () -> write_max (Int.to_string 1234), fun () -> write_max (Int.to_string 5678)) r “” “1234” let current = !r in … “5678” task-local heap heap object pointer Fig.",
Entanglement example memory management system for a dialect of Parallel ML—a parallel functional programming language—and offers competitive performance in practice relative to low-level parallel code written in languages such as C/C++ [Arora et al.,
This line of work aims to gain control over the synchronization costs of parallel garbage collection by taking advantage of structured fork-join parallelism.,
"The idea is to synchronize the garbage collector only at application-level forks and joins, thereby making these synchronization costs predictable at the source level, and avoiding the need for any global synchronization of the garbage collector.",
"At each fork and join, the runtime system performs 𝑂(1) work to maintain a dynamic tree of heaps which mirrors the parent/child relationships between tasks.",
"Each task thus has its own task- local heap, in which it allocates memory objects and may perform garbage collection independently, in parallel.",
"The independence of these task-local garbage collections hinges upon disentanglement, which can be defined as a “no cross-pointers” invariant.",
"Specifically, disentanglement allows for up- pointers from descendant heaps to ancestors, as well as down-pointers from ancestors to descendants, but disallows cross-pointers between concurrent tasks (siblings, cousins, etc.).",
The existence of cross-pointers is called entanglement.,
"When two tasks become entangled with a cross-pointer, neither task can perform garbage collection without synchronizing with the other.",
These additional synchronizations lead to significant performance degradations [Arora et al.,
"2023], and in this sense, entanglement is a performance hazard.",
"Entanglement arises from a particular communication pattern, where one task allocates a local heap object and then another task (executing concurrently, relative to the first task) acquires a pointer to the object.",
An example is shown in Figure 1.,
"The example uses the fork-join primitive par(f1,f2) to execute two functions in parallel; these two function calls correspond to two child tasks.",
"In the example, the two child tasks perform write_max concurrently, both attempting to update r to point to a locally allocated string.",
"After one task finishes, the other task reads the updated r and acquires a cross-pointer, which constitutes entanglement.",
"As shown below, this example could be rewritten to be disentangled by moving the allocations of the strings “up” into the parent task (making all pointers involved up-pointers).",
"... (* same definitions of r and write_max, as in Figure 1 *) ... let disentangled = let a = Int.to_string 1234 in let b = Int.to_string 5678 in par(fun () -> write_max a, fun () -> write_max b) Preventing entanglement.",
One way to rule out entanglement is to disallow side effects entirely.,
"Indeed, the original study of disentanglement emerged out of an interest in improving the perfor- mance of parallel functional programming techniques, which naturally have a high rate of allocation and whose scalability and efficiency is largely determined by the performance of automatic memory management.",
"In this setting, disentanglement is guaranteed by construction due to a lack of side effects.",
"But the full power of disentanglement lies in its expressivity beyond purely functional programming—in particular, disentanglement allows for judicious utilization of side effects such as in-place updates and irregular and/or data-dependent access patterns in shared memory.",
These side Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:3 effects are crucial for efficiency in state-of-the-art implementations of parallel algorithms, such as those in the PBBS Benchmark Suite [Abdi et al.",
2024; Anderson et al.,
2022; Shun et al.,
"2012], which have been found to be naturally disentangled [Westrick et al.",
"In these more general settings, where there are numerous opportunities to efficiently utilize side effects, it is easy for a programmer to accidentally entangle concurrent tasks.",
"Ideally, it would be evident at the source level where entanglement may or may not occur.",
"However, in practice, this is not the case.",
"To reason about entanglement, the programmer effectively has to know the memory allocation and access patterns of the entire program.",
"This makes it especially difficult to reason about higher-order functions, because the memory effects of a function taken as argument are unknown.",
"Other high-level programming features also complicate the matter, such as parametric polymorphism which allows for code to be specialized for both “boxed” (heap-allocated) and “unboxed” types, potentially resulting in entanglement in one case but not the other.",
These details can be formally considered using the program logic DisLog [Moine et al.,
"2024], but verifying disentanglement using DisLog requires significant expertise and effort, even in small examples.",
An interesting question therefore is whether it is possible to guarantee disentanglement statically through a type system.,
"This would have the advantage of being mostly automatic, requiring (ideally) only a modest amount of type annotation.",
"Most importantly, a type system would raise the level of abstraction at which the programmer can reason about disentanglement, clarifying how the property interacts with high-level abstractions such as parametric polymorphism, higher-order functions, algebraic datatypes, and other desirable features.",
A type system for disentanglement.,
"In this paper, we present TypeDis, the first static type sys- tem for disentanglement.",
"We intend for TypeDis to be the type system for a high-level ML-like language with structured fork-join parallelism, in-place atomic operations on shared memory, and disentangled parallel garbage collection.",
"The language features a single parallel construct, written par(𝑓1, 𝑓2), which calls 𝑓1() and 𝑓2() in parallel, waits for both to complete, and returns their results as a pair.",
"Here, we think of the execution of the two function calls as two child tasks, which themselves might execute par(...) recursively, creating a dynamic tree (parent-child) relationship between tasks.",
"TypeDis identifies tasks with timestamp variables 𝛿, and annotates every value computed during execution with the timestamp of the task that allocated that value.",
This is tracked explicitly in the type of the value.,
"For example, 𝑠: string@𝛿indicates that the value 𝑠is a string that was allocated by a task 𝛿.",
"The type system implicitly maintains a partial order over timestamps, written 𝛿′ ≼𝛿, intuitively corresponding to the tree relationship between tasks.",
"Crucially, TypeDis guarantees an invariant that we call the up-pointer invariant: for every task running at timestamp 𝛿, every value accessed by this task must have a timestamp 𝛿′ ≼𝛿, i.e., the value must have been allocated “before” the current timestamp.",
"In other words, the key insight in this paper is to restrict all memory references to point backwards in time, which is checked statically.",
This restriction is a deep invariant over values: every data structure will only contain values allocated at the same timestamp or a preceding timestamp.,
"As a result, all loads in the language are guaranteed to be safe for disentanglement.",
The up-pointer invariant statically rules out one feature of disentanglement: down-pointers.,
"This restriction is mild, however, because down-pointers are fairly rare.",
"Quantitatively, there has been at least one relevant study: in their work on entanglement detection, Westrick et al.",
"[2022] observe in multiple benchmarks that down-pointers do not arise at all, and more broadly they measure that the number of objects containing down-pointers is small.",
"The creation of a down-pointer requires a combination of dynamic allocation and pointer indirection, each of which is typically avoided in parallel performance-sensitive code to reduce memory pressure and improve cache efficiency.",
"In this paper, we have found the up-pointer invariant to be sufficiently expressive to encode a number Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:4 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick of interesting examples (§6), fully typed within TypeDis, and therefore guaranteed disentangled.",
"The up-pointer invariant is especially well-suited for immutable data (which naturally adheres to the invariant), as well as parallel batch processing of pre-allocated data.",
"The up-pointer invariant also allows for structure sharing, even in the presence of mutable state.",
Maintaining the up-pointer invariant.,
"To maintain the up-pointer invariant in the presence of mutable state, TypeDis places a restriction on writes (in-place updates), requiring that the timestamp of the written value precedes the timestamp of the reference pointing to it.",
"This restriction is implemented in the type system with a form of subtyping, dubbed subtiming, which affects only the timestamps of values within their types.",
The idea is to allow for any value to be (conservatively) restamped with a newer timestamp.,
Subtiming makes it possible to express the restriction on writes as a simple unification over the type of the contents of a mutable reference or array.,
"Restamping with an older timestamp would be unsound in TypeDis, as it would allow for a child’s (heap-allocated) value to be written into a parent’s container, potentially making that value accessible to a concurrent sibling.",
"This is prevented throughout the type system, except in one place: at the join point of par.",
"At this point, the two sub-tasks have completed and their parent inherits the values they allocated.",
"To allow the parent task to access these values, TypeDis restamps the result of par with the timestamp of the parent.",
We dub this operation backtiming.,
"TypeDis features first-class function types (𝛼→𝛿𝛽), annotated by a timestamp variable 𝛿, indicating which task the function may be called by.",
"Timestamp variables can be universally quantified, effectively allowing for timestamp polymorphism.",
"For example, pure functions that have no side-effects are type-able as (∀𝛿.",
"𝛼→𝛿𝛽), indicating that the function may be safely called by any task.",
TypeDis also allows for constrained timestamp polymorphism.,
"For example, a function of type (∀𝛿′ ≼𝛿.",
string@𝛿′ →𝛿()) only accepts as argument strings timestamped at some 𝛿′ that precede the timestamp 𝛿of the calling task.,
"Typically, such constraints arise from the use of closures, especially those that close over mutable state.",
The soundness of TypeDis is verified in the Rocq prover (the new name of the Coq proof assistant) on top of the Iris higher-order concurrent separation logic framework [Jung et al.,
We use the approach of semantic typing [Constable et al.,
1986; Martin-Löf 1982; Timany et al.,
"2024], and define a logical relation targeting a variation of DisLog [Moine et al.",
"2024], from which we reuse the technical parts.",
As illustrated by RustBelt [Jung et al.,
"2018a], semantic typing facilitates manual verification of programs that are correct (e.g.",
"disentangled), but ill-typed, by carrying out a logical relation inhabitation proof using the program logic—overcoming incompleteness inherent to any type system.",
"For example, in the case of TypeDis, this allows the user to verify part of the code that use down-pointers.",
"We note that, similar to many other type systems (such as those in OCaml and Haskell), TypeDis relies on dynamic checks in the operational semantics to enforce memory safety for out-of-bounds (OOB) array accesses.",
"The formal statement of soundness (§5.1) therefore explicitly distinguishes between three kinds of program states: those that have terminated, those that can step, and those that are stuck due to OOB.",
The soundness theorem states that all executions of programs typed within TypeDis always remain disentangled throughout execution.,
Contributions.,
"Our contributions include: • TypeDis, the first static type system for disentanglement.",
It includes the notion of a timestamp to track which object is accessible by which task.,
TypeDis offers (iso-)recursive types as well as polymorphism over types and over timestamps.,
"Moreover, TypeDis supports polymorphic recursion over timestamps, and offers a relaxation of the value restriction.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:5 • Two mechanisms to update a timestamp annotation: via subtiming, a form of subtyping, and specifically at join points via the new operation of backtiming.",
• A new model for disentanglement with cyclic computation graphs.,
We prove this model equivalent to the standard one and explain why it is more amenable to verification.,
• A soundness proof of TypeDis mechanized in the Rocq prover using the Iris framework.,
We use semantic typing [Timany et al.,
"2024] and DisLog2, an improved version of DisLog.",
"• A range of case studies, including building and iterating over an immutable tree in parallel, as well the challenging example of deduplication via concurrent hashing.",
"2 Key Ideas In this section, we cover the key ideas of our work.",
We start by recalling the definition of disen- tanglement (§2.1).,
"We then present the main idea of TypeDis: adding task identifiers, specifically timestamp variables, to types (§2.2).",
"Based on examples, we then illustrate two core principles of TypeDis, allowing for updating timestamps within types: backtiming (§2.3) and subtiming (§2.4).",
2.1 Preliminaries Nested fork-join parallelism and task trees.,
"We consider programs written in terms of a single parallel primitive: par(𝑓1, 𝑓2), which creates two new child tasks 𝑓1() and 𝑓2() to execute in parallel, waits for both of the child tasks to complete, and then returns the results of the two calls as an immutable pair.",
"Creating the two child tasks is called a fork, and waiting for the two children to complete is called a join.",
The behavior of the par primitive guarantees that every fork has a corresponding join.,
"Any task may (recursively) fork and join, facilitating nested parallelism and giving rise to a dynamic tree during execution called the task tree.",
"The nodes of the task tree correspond to (parent) tasks that are waiting for their children to join, and the leaves of the task tree correspond to tasks which may actively take a step.",
"Whenever two sibling tasks join, the children are removed from the tree and the parent resumes as a leaf task.",
The task tree therefore dynamically grows and shrinks as tasks fork and join.,
"In this paper, we will use the letter 𝑡to denote tasks (leaves of the task tree), and will equivalently refer to these as timestamps.",
Computation graphs.,
"The evolution of the task tree over time can be recorded as a computation graph, where vertices correspond to tasks and edges correspond to scheduling dependencies.",
"The computation graph records not just the current task tree, but also the history of tasks that have joined.",
"When a task 𝑡forks into two children 𝑡1 and 𝑡2, two edges (𝑡,𝑡1) and (𝑡,𝑡2) are added to the graph; later when 𝑡1 and 𝑡2 join, two edges (𝑡1,𝑡) and (𝑡2,𝑡) are added to the graph.",
"We say that 𝑡precedes 𝑡′ in graph 𝐺and write 𝐺⊢𝑡≼𝑡′, when there exists a sequence of edges from 𝑡to 𝑡′.",
Note that ≼is reflexive.,
Two tasks are concurrent when neither precedes the other.,
Cyclic versus standard computation graphs.,
"We contribute a new definition of computation graphs, which we call the cyclic approach, that differs slightly from the standard presentation used in prior work [Acar et al.",
2016; Moine et al.,
2024; Westrick et al.,
"The standard approach is to use a fresh task identifier at each join point, effectively renaming the resumed parent task.",
"In the cyclic approach, we instead use the same task identifier after the join point.",
Figure 2 illustrates the difference between the two approaches.,
"It presents two computation graphs representing the same computation: Figure 2a shows the standard approach, and Figure 2b shows the (new) cyclic approach.",
The distinction occurs when two tasks join.,
In Figure 2a tasks 𝑡3 and 𝑡4 join and form a new task 𝑡′ 2 whereas in Figure 2b the two tasks join by going back to task 𝑡2.,
This distinction occurs again when 𝑡′ 2 (resp.,
𝑡2) join to form 𝑡′ 0 (resp.,
"The cyclic approach considerably reduces the need to manipulate timestamps, not only in our proofs (for example the soundness proof of backtiming), but also in the design of the type system Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:6 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick 𝑡0 𝑡1 𝑡2 𝑡3 𝑡4 𝑡′ 2 𝑡′ 0 𝑡5 𝑡6 (a) Standard computation graph 𝑡0 𝑡1 𝑡2 𝑡3 𝑡4 𝑡5 𝑡6 (b) Equivalent “cyclic” approach used in this paper Fig.",
Comparison of two computation graphs equivalent for disentanglement itself as well as in the underlying program logic (§5.3).,
We prove the two approaches equivalent for the purpose of verifying disentanglement [Moine et al.,
"Intuitively, the two approaches are equivalent because we never need to check reachability between two tasks that have both completed.",
"We have formally proven this equivalence with a simulation theorem: every reduction in the semantics with the standard approach implies the existence of a reduction with the same scheduling reaching the same expression in the semantics with the cyclic approach, and vice-versa.",
"Moreover, if one state is disentangled in one semantics, so it is in the other.",
"At any moment, every task has a set of task-local roots which are the memory locations directly mentioned within a subexpression of that task.",
"For example, the expression ‘let𝑥= (ℓ1, ℓ2) in fst(𝑥)’ has roots {ℓ1, ℓ2}, where (formally) ℓ1 and ℓ2 are locations within the memory store.",
"Note that the roots of a task change over time: for example, the above expression eventually steps to ℓ1 at which point it only has one root, {ℓ1}.",
The set of roots can grow due to allocations and loads from memory.,
Disentanglement.,
Disentanglement restricts the set of possible task-local roots.,
A program state is disentangled if each root of a task has been allocated by some preceding task.,
"More precisely, a program state with a computation graph 𝐺is disentangled if, for a root ℓof a task 𝑡, ℓ has been allocated by a task 𝑡′ such that 𝐺⊢𝑡′ ≼𝑡, that is, such that 𝑡′ precedes 𝑡in 𝐺.",
"Following the computation graph definition, preceding tasks include 𝑡itself, parent tasks, but also children tasks that have terminated.",
The formal definition of disentanglement appears in Section 3.3.,
"TypeDis, the type system we present, verifies that a program is disentangled, that is, every reachable program state is disentangled.",
"2.2 TypeDis 101: Timestamps in Types In order to keep track of which task allocated which location, TypeDis incorporates timestamps in types.",
"More precisely, every heap-allocated (“boxed”) type is annotated by a timestamp variable, written 𝛿, which can be understood as the timestamp of the task that allocated the underlying location.",
"For example, a reference allocated by task 𝛿on an (unboxed) integer has type ref(int)@𝛿. Timestamp polymorphism.",
"Functions in TypeDis are annotated by a timestamp variable, re- stricting which task they may run on.",
"Such a variable can be universally quantified, allowing for functions to be run by different tasks.",
"For example, consider the function fun x -> newref(x) which allocates a new mutable reference containing an integer x.",
This function can be given the type ∀𝛿.,
int →𝛿ref(int)@𝛿.,
"The superscript 𝛿on the arrow indicates that the function must run on a task at timestamp 𝛿, and the result type ref(int)@𝛿indicates that the resulting reference will be allocated at the same timestamp 𝛿.",
"By universally quantifying 𝛿, the function is permitted to run on any task, with the type system tracking that the resulting reference will be allocated at the same timestamp as the caller.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:7 newref : ∀𝛼.,
𝛼→𝛿ref(𝛼)@𝛿 get : ∀𝛼.,
ref(𝛼)@𝛿′ →𝛿𝛼 set : ∀𝛼.,
ref(𝛼)@𝛿′ →𝛼→𝛿() Fig.,
"Example: typing reference primitives let r = newref ""hello"" 𝑟: ref(string@𝛿0)@𝛿0 let w = ""world"" 𝑤: string@𝛿0 let f () = set r w 𝑓: (∀𝛿.",
() →𝛿())@𝛿0 let g i = 𝑔: (int →𝛿0 ())@𝛿0 set r (Int.to_string i) Fig.,
Example: typing closures type tree@𝛿= ( int + (tree@𝛿× tree@𝛿)@𝛿)@𝛿 let leaf x = inj1 x leaf : (∀𝛿.,
"int →𝛿tree@𝛿)@𝛿0 let node x y = inj2 (x,y) node : (∀𝛿.",
tree@𝛿→tree@𝛿→𝛿tree@𝛿)@𝛿0 let rec build n x = build : (∀𝛿.,
"int →int →𝛿tree@𝛿)@𝛿0 if n <= 0 then leaf x else let n' = n - 1 in let (l,r) = par (fun () -> build n' x) (fun () -> build n' (x + pow2 n')) in node l r Fig.",
Example: building a tree in parallel Type polymorphism.,
TypeDis allows type variables 𝛼to be universally quantified.,
"Using type poly- morphism, we can now give the function fun x -> newref(x) the more general type ∀𝛼.∀𝛿.",
"𝛼→𝛿 ref(𝛼)@𝛿, indicating that it is polymorphic in the type 𝛼of the contents of the mutable reference.",
"Corresponding get and set primitives for mutable references are then typed as shown in Figure 3, all of which are polymorphic in the type variable 𝛼.",
"For functions with multiple arguments, such as set, we adopt the notational convention to only specify the timestamp variable on the last arrow.",
The up-pointer invariant.,
"In Figure 3, the type of get is given as ∀𝛼.",
ref(𝛼)@𝛿′ →𝛿𝛼.,
Note that this type is parameterized over both a caller time 𝛿as well as a (potentially different) timestamp 𝛿′ associated with the input reference.,
"Intuitively, this type specifies that get is safe to call at any moment, by any task, with any reference given as argument.",
"The design of TypeDis in general guarantees that all loads from memory, both mutable and immutable, are always safe.",
"Specifically, this is guaranteed by enforcing an invariant that we call the up-pointer invariant: all data structures in the language may only contain values allocated at the same timestamp or a preceding timestamp.",
"For example, given two non-equal timestamps 𝛿1 and 𝛿2 where 𝛿1 ≺𝛿2, the type ref(ref(int)@𝛿1)@𝛿2 is valid, but ref(ref(int)@𝛿2)@𝛿1 is not.",
"In TypeDis, functions are first-class values and may be passed as arguments to other functions, or stored in data structures, etc.",
"Function values are implemented as heap-allocated closures [Appel 1992; Landin 1964], and must be given a timestamp indicating when they were allocated.",
"For example, consider the definition of function 𝑓in Figure 4, which closes over a mutable reference 𝑟and an immutable string 𝑤, both allocated at timestamps 𝛿0 which (in this example) is the timestamp of the current task.",
We can give 𝑓the type (∀𝛿.,
"() →𝛿())@𝛿0, indicating that 𝑓 itself was allocated at timestamp 𝛿0.",
"Additionally, the type of 𝑓specifies that it may be freely called at any timestamp; this is safe for disentanglement because 𝑓preserves the up-pointer invariant, regardless of when it will be called.",
"Contrast this with the definition of function 𝑔, which (when called) allocates a new string and writes this string into the reference 𝑟.",
"If 𝑔were called at some timestamp 𝛿1 where 𝛿0 ≺𝛿1, then this would violate the up-pointer invariant for 𝑟.",
"The function 𝑔 does however admit the type (int →𝛿0 ())@𝛿0, indicating that 𝑔may be safely called only by tasks at time 𝛿0 (the same timestamp as the reference 𝑟).",
"2.3 Backtiming the Result of a par As explained earlier (§2.1), we consider in this paper the parallel primitive par(...), which executes two closures in parallel and returns their result as an immutable pair.",
The par primitive can be Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:8 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick let rec selectmap p f t = selectmap : (∀𝛿𝛿𝑝𝛿𝑓𝛿𝑡.",
int →𝛿′ bool)@𝛿𝑝 match t with →(∀𝛿′.,
"int →𝛿′ int)@𝛿𝑓 | inj1 x -> if p x then leaf (f x) else t →tree@𝛿𝑡→𝛿tree@𝛿)@𝛿0 | inj2 (l,r) -> let (nl,nr) = par (fun () -> selectmap p f l) (fun () -> selectmap p f r) in if nl == l && nr == r then t else node nl nr Fig.",
Example: the selectmap function used to build data structures in parallel.,
Consider the code presented in Figure 5.,
The recursive type tree@𝛿= ( int + (tree@𝛿× tree@𝛿)@𝛿)@𝛿describes a binary tree with integer leaves.,
It consists of an immutable sum of either an integer (a leaf) or a product of two subtrees (a node).,
All the parts of a tree are specified in the type to have been allocated at the same timestamp 𝛿.,
"A leaf is built with the first injection, and a node with the second injection.",
"The function build n x builds in parallel a binary tree of depth 𝑛, with leaves labeled from 𝑥to 𝑥+ 2𝑛−1 in left-to-right order.",
TypeDis type-checks build with the type ∀𝛿.,
int →int →𝛿tree@𝛿.,
"The reader may be surprised: we announced that the type tree@𝛿has all of its parts allocated at the same timestamp 𝛿, but we are showing a function that builds a tree in parallel, hence with some parts allocated by different tasks at different timestamps.",
What’s the trick?,
The key observation is that we can pretend that the objects allocated by a completed sub-task were instead allocated by its parent.,
"Indeed, disentanglement prevents sharing of data allocated in parallel, but as soon as the parallel phase has ended, there is no restriction anymore!",
"In TypeDis, the par primitive implements backtiming, meaning that it replaces the timestamp of the child task by the timestamp of the parent task in the return type of the closures executed in parallel.",
"Indeed, the par primitive admits the following, specialized for build, type: ∀𝛿𝛿𝑙𝛿𝑟.",
() →𝛿′ tree@𝛿′)@𝛿𝑙→(∀𝛿′.,
() →𝛿′ tree@𝛿′)@𝛿𝑟→𝛿(tree@𝛿× tree@𝛿)@𝛿 This type for par does exactly what we need: it returns the result of the two closures in a pair as if they were called at time 𝛿. Backtiming is a powerful feature: it reduces parallelism to almost an implementation detail.,
"Indeed, the type of build does not reveal its internal use of parallelism.",
"2.4 Making Something New out of Something Old with Subtiming A common practice (especially in functional programming) is data structural sharing, where components of an old structure are reused inside part of a new structure.",
"In the context of TypeDis, data structural sharing is interesting in that it mixes data of potentially different timestamps within the same structure.",
Here we consider one such example and describe a key feature of TypeDis which enables such “mixing” of timestamps.,
"Figure 6 presents the selectmap p f t function, which selectively applies the function f to the leaves of the tree t, following a predicate p on integers.",
The selectmap function traverses the tree in parallel and crucially preserves sharing as much as possible.,
"Specifically, when none of the leaves of the tree satisfy the predicate, the function returns the original input tree as-is, instead of building another identical tree.",
"To type this function in TypeDis, it may not be immediately clear what the timestamp of the resulting tree should be: selectmap might directly return the argument passed as argument (potentially coming from an older task), or it might return a new tree.",
TypeDis type-checks selectmap with the type ∀𝛿𝛿𝑝𝛿𝑓𝛿𝑡.,
int →𝛿′ bool)@𝛿𝑝→(∀𝛿′.,
"int →𝛿′ int)@𝛿𝑓→tree@𝛿𝑡→𝛿tree@𝛿 This type universally quantifies over 𝛿(the timestamp at which selectmap will run), 𝛿𝑝and 𝛿𝑓(the timestamps of the two closure arguments), and 𝛿𝑡(the timestamp of the tree argument).",
"Crucially, the result is of type tree@𝛿, as if the whole result tree was allocated by 𝛿. What’s the trick?",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:9 Values 𝑣,𝑤::= () | 𝑏∈{true, false} | 𝑖∈Z | ℓ∈L | vfold𝑣 Blocks 𝑟::= ®𝑤| (𝑣, 𝑣) | inj𝜄∈{0,1} 𝑣| ˆ𝜇𝑓.",
𝜆®𝑥.𝑒 Primitives ⊲⊳::= + | −| × | ÷ | mod | == | < | ≤| > | ≥| ∨| ∧ Expressions 𝑒::= 𝑣| 𝑥∈V | let𝑥= 𝑒in𝑒| if𝑒then𝑒else𝑒| 𝑒⊲⊳𝑒 | 𝜇𝑓.,
"𝜆®𝑥.𝑒| 𝑒®𝑒 closures | (𝑒,𝑒) | proj𝜄∈{1,2} 𝑒 pairs | inj𝑖∈{1,2} 𝑒| match𝑒with inj1 𝑥⇒𝑒| inj2 𝑥⇒𝑒end sums | alloc 𝑒𝑒| 𝑒.",
"[𝑒] ←𝑒| length𝑒 arrays | fold𝑒| unfold𝑒 iso-recursive types | par(𝑒,𝑒) | 𝑒∥𝑒| CAS𝑒𝑒𝑒𝑒 parallelism and concurrency Contexts 𝐾::= let𝑥= □in𝑒 | if □then𝑒else𝑒 | alloc □𝑒 | alloc 𝑣□ | length □ | □.",
[𝑒] ←𝑒 | 𝑣.,
[□] ←𝑒 | 𝑣.,
"[𝑣] ←□ | □⊲⊳𝑒 | 𝑣⊲⊳□ | □®𝑒 | 𝑣(®𝑣++ □++ ®𝑒) | fold □ | unfold □ | (□,𝑒) | (𝑣, □) | proj𝜄□ | inj𝑖□ | match □with inj1 𝑥⇒𝑒| inj2 𝑥⇒𝑒end | par(□,𝑒) | par(𝑣, □) | CAS □𝑒𝑒𝑒 | CAS𝑣□𝑒𝑒 | CAS𝑣𝑣□𝑒 | CAS𝑣𝑣𝑣□ Fig.",
Syntax of DisLang2.,
Constructs in blue are runtime-level.,
"TypeDis supports subtiming, that is, a way of “advancing” timestamps within a type, following the precedence.",
The rules of subtiming are as follows.,
For a mutable type (e.g.,
"an array or a reference), subtiming is shallow: the outermost timestamp can be updated, but not the inner timestamps; this is due to well-known variance issues [Pierce 2002, §15].",
For an immutable type (e.g.,
"products and sums), subtiming is deep: any timestamp within the type can be advanced, as long as the up-pointer invariant is preserved.",
"For selectmap, we need to use deep subtiming on the recursive immutable type tree@𝛿𝑡in order to update it to tree@𝛿.",
"How can we be sure that 𝛿𝑡, the timestamp of the tree, precedes 𝛿, the timestamp at which we call selectmap?",
"We unveil a key invariant of TypeDis: every timestamp of every memory location in scope precedes the “current” timestamp, that is, the timestamp of the task executing the function.",
In our case the current timestamp is precisely 𝛿.,
"We hence deduce that 𝛿𝑡precedes 𝛿, allowing us to use subtiming to “restamp” the value 𝑡: tree@𝛿𝑡as 𝑡: tree@𝛿.",
"To allow the user to express additional knowledge about the dependencies between timestamps, TypeDis annotates universal timestamp quantification with a set of constraints, which are supposed to hold while typing the function body, and are verified at call sites.",
"For example, the following function let par' f g = ignore (par f g) that executes two closures f and g from unit to unit in parallel and ignores the result can be given the type: ∀𝛿𝛿1 𝛿2.",
() →𝛿′ ())@𝛿1 →(∀𝛿′ 𝛿≼𝛿′.,
"() →𝛿′ ())@𝛿2 →𝛿() This type says that, if par' gets called at timestamp 𝛿with arguments 𝑓and 𝑔, then 𝑓and 𝑔can assume that they will be called at timestamp 𝛿′ such that 𝛿≼𝛿′.",
"These constraints are discussed in Section 4, and the fully general type of par is presented in Section 4.6.",
"3 Syntax and Semantics The formal language we study, dubbed DisLang2, can be understood as an extension of DisLang, the language studied by Moine et al.",
"DisLang2 adds support for immutable pairs and sums, iso-recursive types, and directly offers the par primitive for fork-join parallelism.",
"We present the syntax of DisLang2 (§3.1), its semantics (§3.2), and the formal definition of disentanglement (§3.3).",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:10 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick 3.1 Syntax The syntax of DisLang2 appears in Figure 7.",
The constructs in blue are forbidden in the source program and occur only at runtime.,
"A value 𝑣∈V can be the unit value (), a boolean 𝑏∈{true, false}, an idealized integer 𝑖∈Z, a memory location ℓ∈L, where L is an infinite set of locations, or a folded value vfold𝑣, witnessing our use of iso-recursive types [Pierce 2002, §20].",
"A block describes the contents of a heap cell, amounting to either an array of values, written ®𝑤, an immutable pair (𝑣, 𝑣), the first injection inj1 𝑣or the second injection inj2 𝑣of an immutable sum, or a 𝜆-abstraction ˆ𝜇𝑓.",
"Lambdas can close over free variables, compilers of functional languages usually implement them as closures [Appel 1992; Landin 1964].",
"A closure is a heap-allocated object carrying a code pointer as well as an environment, recording the values of the free variable.",
"Thus, acquiring a closure can create entanglement.",
"Moreover, because functions and tuples are heap allocated, currying and uncurrying—that is, converting a function taking multiple arguments to a function taking a tuple of arguments and vice-versa—does not come for free.",
"Hence, we chose to support a version of the language were every function takes possibly multiple arguments.",
Closure allocation is written 𝜇𝑓.,
"This notation binds a recursive name 𝑓, argument names ®𝑥in the expression 𝑒.",
A function call is written 𝑒®𝑒.,
"In DisLang2, fork-join parallelism is available via the parallel primitive par(𝑒1,𝑒2), which re- duces 𝑒1 and 𝑒2 to closures, calls them in parallel, and returns their result as an immutable pair.",
"This parallel computation is represented by the active parallel pair 𝑒1 ∥𝑒2, appearing only at runtime.",
"DisLang2 supports a compare-and-swap instruction CAS𝑒𝑒𝑒𝑒, which targets an array, and is parameterized by 4 arguments: the location of the array, the index in the array, the old value and the new value.",
"A (sequential) evaluation context 𝐾describes a term with a hole, written □.",
The syntax of evaluation contexts dictates a left-to-right call-by-value evaluation strategy.,
Note that evaluation contexts 𝐾in this presentation are sequential.,
"Specifically, we intentionally excluded active parallel pairs (−∥−) from the grammar of 𝐾.",
"The evaluation strategy for active parallel pairs allows for interleaving of small steps, which is handled separately by a “scheduler reduction” relation in the operational semantics (§3.2).",
3.2 Operational Semantics Head reduction relation.,
"A head configuration 𝜎\𝛼\𝑒is composed of a store 𝜎, an allocation map 𝛼, and an expression 𝑒.",
The store 𝜎represents the heap and consists of a finite map of locations to blocks.,
"The allocation map 𝛼is a finite map of locations to timestamps, recording the timestamps at which locations were allocated.",
"Figure 8 presents parts of the definition of the head reduction relation between two head configurations 𝐺,𝑡⊢𝜎\𝛼\𝑒−→𝜎′ \𝛼′ \𝑒′ occurring at the (local) task of timestamp 𝑡in the (global) computation graph 𝐺.",
"A head configuration consists of the expression 𝑒being evaluated, the store 𝜎, and an allocation map 𝛼.",
Figure 8 omits rules for the length array primitive as well as the atomic compare-and-swap on arrays.,
We write 𝜎(ℓ) to denote the block stored at the location ℓin the store 𝜎.,
We write [ℓ:= 𝑟]𝜎for the insertion of block 𝑟at location ℓin 𝜎.,
"Note that only arrays can be updated; closures, pairs and sums are immutable.",
We write ®𝑤(𝑖) to refer to the index 𝑖of an array ®𝑤.,
"We write [𝑖:= 𝑣] ®𝑤for an update to an array, and we similarly write [ℓ:= 𝑡]𝛼for an insertion in the allocation map.",
"We write 𝑣𝑛for an array of length 𝑛, where each element of the array is initialized with the value 𝑣. HeadAlloc allocates an array, extending the store and the allocation map.",
HeadLoad acquires the value 𝑣from an index of an array.,
"HeadStore, HeadLetVal, HeadIfTrue and HeadIfFalse are standard.",
HeadClosure allocates a closure and HeadCall calls a closure.,
"HeadCallPrim calls a primitive, whose result is computed at the meta-level by the pure −−−→relation.",
HeadPair and Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:11 HeadAlloc 0 < 𝑛 ℓ∉dom(𝜎) ℓ∉dom(𝛼) 𝐺,𝑡⊢𝜎\𝛼\alloc 𝑛𝑣−→[ℓ:= 𝑣𝑛]𝜎\ [ℓ:= 𝑡]𝛼\ℓ HeadLoad 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| ®𝑤(𝑖) = 𝑣 𝐺,𝑡⊢𝜎\𝛼\ℓ.",
"[𝑖] −→𝜎\𝛼\𝑣 HeadStore 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| 𝐺,𝑡⊢𝜎\𝛼\ℓ.",
"[𝑖] ←𝑣−→[ℓ:= [𝑖:= 𝑣] ®𝑤]𝜎\𝛼\ () HeadLetVal 𝐺,𝑡⊢𝜎\𝛼\let𝑥= 𝑣in𝑒−→𝜎\𝛼\ [𝑣/𝑥]𝑒 HeadIfTrue 𝐺,𝑡⊢𝜎\𝛼\if true then𝑒1 else𝑒2 −→𝜎\𝛼\𝑒1 HeadIfFalse 𝐺,𝑡⊢𝜎\𝛼\if false then𝑒1 else𝑒2 −→𝜎\𝛼\𝑒2 HeadClosure ℓ∉dom(𝜎) ℓ∉dom(𝛼) 𝐺,𝑡⊢𝜎\𝛼\ ˆ𝜇𝑓.",
𝜆®𝑥.𝑒−→[ℓ:= ˆ𝜇𝑓.,
𝜆®𝑥.𝑒]𝜎\ [ℓ:= 𝑡]𝛼\ℓ HeadCall 𝜎(ℓ) = ˆ𝜇𝑓.,
"𝜆®𝑥.𝑒 |®𝑥| = | ®𝑤| 𝐺,𝑡⊢𝜎\𝛼\ℓ®𝑤−→𝜎\𝛼\ [ℓ/𝑓][ ®𝑤/®𝑥]𝑒 HeadCallPrim 𝑣1 ⊲⊳𝑣2 pure −−−→𝑣 𝐺,𝑡⊢𝜎\𝛼\𝑣1 ⊲⊳𝑣2 −→𝜎\𝛼\𝑣 HeadPair ℓ∉dom(𝜎) ℓ∉dom(𝛼) 𝐺,𝑡⊢𝜎\𝛼\ (𝑣1, 𝑣2) −→[ℓ:= (𝑣1, 𝑣2)]𝜎\ [ℓ:= 𝑡]𝛼\ℓ HeadProj 𝜎(ℓ) = (𝑣1, 𝑣2) 𝐺,𝑡⊢𝜎\𝛼\proj𝜄ℓ−→𝜎\𝛼\𝑣𝜄 HeadInj ℓ∉dom(𝜎) ℓ∉dom(𝛼) 𝐺,𝑡⊢𝜎\𝛼\inj𝑖𝑣−→[ℓ:= inj𝑖𝑣]𝜎\ [ℓ:= 𝑡]𝛼\ℓ HeadCase 𝜎(ℓ) = inj𝜄𝑣 𝐺,𝑡⊢𝜎\𝛼\ (match ℓwith inj1 𝑥1 ⇒𝑒1 | inj2 𝑥2 ⇒𝑒2 end) −→𝜎\𝛼\ [𝑣/𝑥𝜄]𝑒𝜄 HeadFold 𝐺,𝑡⊢𝜎\𝛼\fold𝑣−→𝜎\𝛼\vfold𝑣 HeadUnfold 𝐺,𝑡⊢𝜎\𝛼\unfold (vfold𝑣) −→𝜎\𝛼\𝑣 Fig.",
"Head reduction (selected rules) HeadProj allocate and project immutable pairs, respectively.",
"HeadInj and HeadCase allocate and case over immutable sums, respectively.",
HeadFold and HeadUnfold handle iso-recursive types in a standard way.,
Scheduler reduction relation.,
"In order to keep track of the timestamp of each task and whether the task is activated or suspended, we follow Westrick et al.",
"[2020] and enrich the semantics with an aux- iliary structure called a task tree, written 𝑇, of the following formal grammar: 𝑇≜𝑡∈T | 𝑇⊗𝑡𝑇.",
A leaf 𝑡indicates an active task denoted by its timestamp.,
"A node 𝑇1 ⊗𝑡𝑇2 represents a suspended task 𝑡that has forked two parallel computations, recursively described by the task trees 𝑇1 and 𝑇2.",
"Figure 9 presents the scheduling reduction relation 𝜎/𝛼/𝐺/𝑇/𝑒 sched −−−−→𝜎′ /𝛼′ /𝐺′ /𝑇′ /𝑒′ as either a head step, a fork, or a join.",
"In this reduction relation, 𝜎is a store, 𝛼an allocation map, 𝐺 a computation graph, 𝑇a task tree, and 𝑒an expression.",
SchedHead reduction describes a head reduction.,
"SchedFork reduction describes a fork: the task tree consists of a leaf 𝑡and the ex- pression par(𝑣1, 𝑣2), where both 𝑣1 and 𝑣2 are closures to be executed in parallel.",
"The reduction generates two fresh timestamps 𝑡1 and 𝑡2, adds the corresponding edges to the computation graph, and updates the task tree to comprise the node with two leaves 𝑡1 ⊗𝑡𝑡2.",
"The reduction then updates the expression to the active parallel pair 𝑣1 [()] ∥𝑣2 [()], reflecting the parallel call of the two closures 𝑣1 and 𝑣2, each one called with a single argument, the unit value ().",
SchedJoin reduction describes a join and differs from prior semantics for disentanglement [Moine et al.,
2024; Westrick et al.,
2022] because it reuses a timestamp (§2.1).,
"The task tree is at a node 𝑡with two leaves 𝑡1 ⊗𝑡𝑡2, and both leaves reached a value.",
"The reduction adds edges (𝑡1,𝑡) and (𝑡2,𝑡) to the computation Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:12 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick SchedHead 𝐺,𝑡⊢𝜎\𝛼\𝑒−→𝜎′ \𝛼′ \𝑒′ 𝜎/𝛼/𝐺/𝑡/𝑒 sched −−−−→𝜎′ /𝛼′ /𝐺/𝑡/𝑒′ SchedFork 𝑡1,𝑡2 ∉vertices(𝐺) 𝐺′ = 𝐺∪{(𝑡,𝑡1), (𝑡,𝑡2)} 𝑒′ = 𝑣1 [()] ∥𝑣2 [()] 𝜎/𝛼/𝐺/𝑡/par(𝑣1, 𝑣2) sched −−−−→𝜎/𝛼/𝐺′ /𝑡1 ⊗𝑡𝑡2 /𝑒′ SchedJoin ℓ∉dom(𝜎) ℓ∉dom(𝛼) 𝐺′ = 𝐺∪{(𝑡1,𝑡), (𝑡2,𝑡)} 𝜎/𝛼/𝐺/𝑡1 ⊗𝑡𝑡2 /𝑣1 ∥𝑣2 sched −−−−→[ℓ:= (𝑣1, 𝑣2)]𝜎/ [ℓ:= 𝑡]𝛼/𝐺′ /𝑡/ℓ StepSched 𝜎/𝛼/𝐺/𝑇/𝑒 sched −−−−→𝜎′ /𝛼′ /𝐺′ /𝑇′ /𝑒′ (𝜎, 𝛼,𝐺) /𝑇/𝑒 step −−−→(𝜎′, 𝛼′,𝐺′) /𝑇′ /𝑒′ StepBind 𝑆/𝑇/𝑒 step −−−→𝑆′ /𝑇′ /𝑒′ 𝑆/𝑇/𝐾[𝑒] step −−−→𝑆′ /𝑇′ /𝐾[𝑒′] StepParL 𝑆/𝑇1 /𝑒1 step −−−→𝑆′ /𝑇′ 1 /𝑒′ 1 𝑆/𝑇1 ⊗𝑡𝑇2 /𝑒1 ∥𝑒2 step −−−→𝑆′ /𝑇′ 1 ⊗𝑡𝑇2 /𝑒′ 1 ∥𝑒2 StepParR 𝑆/𝑇2 /𝑒2 step −−−→𝑆′ /𝑇′ 2 /𝑒′ 2 𝑆/𝑇1 ⊗𝑡𝑇2 /𝑒1 ∥𝑒2 step −−−→𝑆′ /𝑇1 ⊗𝑡𝑇′ 2 /𝑒1 ∥𝑒′ 2 Fig.",
Reduction under a context and parallelism DELeaf ∀ℓ.,
"ℓ∈𝑟𝑜𝑜𝑡𝑠(𝑒) =⇒𝐺⊢𝛼(ℓ) ≼𝑡 Disentangled (_, 𝛼,𝐺) /𝑡/𝑒 DEPar Disentangled 𝑆/𝑇1 /𝑒1 Disentangled 𝑆/𝑇2 /𝑒2 Disentangled 𝑆/𝑇1 ⊗𝑡𝑇2 /𝑒1 ∥𝑒2 DEBind 𝑆= (_, 𝛼,𝐺) Disentangled 𝑆/𝑇1 ⊗𝑡𝑇2 /𝑒 ∀ℓ.",
ℓ∈𝑟𝑜𝑜𝑡𝑠(𝐾) =⇒∀𝑡′.𝑡′ ∈leaves(𝑇1) ∪leaves(𝑇2) =⇒𝐺⊢𝛼(ℓ) ≼𝑡′ Disentangled 𝑆/𝑇1 ⊗𝑡𝑇2 /𝐾[𝑒] Fig.,
"Definition of Disentanglement graph, and allocates a memory cell to store the result of the (active) parallel pair.",
It then updates the task tree to the leaf 𝑡.,
Parallelism and reduction under a context.,
"The lower part of Figure 9 presents the main reduction relation 𝑆/𝑇/𝑒 step −−−→𝑆′ /𝑇′ /𝑒′, which describes a scheduling reduction inside the whole parallel program [Moine et al.",
"A configuration 𝑆/𝑇/𝑒consists of the program state 𝑆, the task tree 𝑇, and an expression 𝑒.",
"This expression 𝑒can consist of multiple tasks, governed by the nesting of active parallel pairs (𝑒1 ∥𝑒2).",
The corresponding timestamps of these tasks are given by the accompanying task tree 𝑇.,
"A state 𝑆consists of the tuple (𝜎, 𝛼,𝐺), denoting a store 𝜎, an allocation map 𝛼, and a computation graph 𝐺. StepSched reduction describes a scheduling step.",
The other reductions describe where the scheduling reduction takes place in the whole parallel program.,
StepBind reduction describes a reduction under an evaluation context.,
"StepParL and StepParR reductions are non-deterministic: if a node of the task tree is encountered facing an active parallel pair, the left side or the right side can reduce.",
"3.3 Definition of Disentanglement The property Disentangled 𝑆/𝑇/𝑒asserts that, given a program state 𝑆and a task tree 𝑇, the expression 𝑒is disentangled—that is, the roots of each task in 𝑒were allocated by preceding tasks.",
Figure 10 gives the inductive definition of Disentangled 𝑆/𝑇/𝑒.,
"If the program state has an allocation map 𝛼and a computation graph 𝐺, and if the task tree is a leaf 𝑡, DELeaf requires Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:13 Timestamp variables 𝛿 Type variables 𝛼 Logical graphs Δ ≜ Δ, 𝛿≼𝛿| ∅ Kinds 𝜅 ≜ ★| ⊲⊳ ⇒𝜅 Unboxed types 𝜏 ≜ () | bool | int Boxed types 𝜎 ≜ array(𝜌) | (𝜌× 𝜌) | (𝜌+ 𝜌) | ∀®𝛿Δ.",
®𝜌→𝛿𝜌 Types 𝜌 ≜ 𝜏| 𝜆𝛿.,
𝜌| 𝜌𝛿| ∀𝛼:: 𝜅. 𝜌| 𝜇𝛼.,
"𝜎@𝛿| 𝛼| 𝜎@𝛿 Environments Γ ≜ 𝑥: 𝜌, Γ | 𝛼:: 𝜅, Γ | ∅ Fig.",
"Syntax of types for every location ℓin 𝑟𝑜𝑜𝑡𝑠(𝑒), that is, the set of locations syntactically occurring in 𝑒, that the location ℓhas been allocated by a task 𝛼(𝑡) preceding 𝑡in 𝐺.",
"If the task tree is a node𝑇1 ⊗𝑡𝑇2, there are two cases.",
"In the first case, if the expression is an active parallel pair, DEPar requires that the two sub-expressions are disentangled.",
"Otherwise, the expression must be of the form 𝐾[𝑒], and then DEBind requires that 𝑒itself is disentangled and that for every location ℓoccurring in the evaluation context 𝐾, the location ℓhas been allocated before every leaf 𝑡′ of 𝑇1 and 𝑇2.",
Difference with Previous Semantics for Disentanglement.,
Inspired by Westrick et al.,
"[2022], we equip DisLang2 with a mostly standard semantics, instrumented with a computation graph and an allocation map.",
"We then distinguish disentangled states using the Disentangled property, resem- bling the “rootsde” invariant proposed by Westrick et al.",
"The novelty of our approach resides in the instrumentation with the, more amenable to verification, cyclic computation graph (§2.1).",
DisLog [Moine et al.,
2024] chooses a slightly different formalization in which the semantics gets stuck if entanglements is detected.,
"Each time a task acquires a location from the heap, the semantics performs a check to verify that the location was allocated by a preceding task.",
"Intuitively, this check ensures by construction that a program’s evaluation reaches only states satisfying the Disentangled property.",
"Conversely, guaranteeing the Disentangled property at every step ensures that a disentanglement check cannot fail.",
"4 Type System In this section, we describe TypeDis in depth.",
"First, we present the formal syntax of types (§4.1) as well as the typing judgment (§4.2).",
"We then comment on typing rules for mutable heap blocks (§4.3), which enforce disentanglement.",
"Next, we present the rules for creating and calling closures (§4.4), which are crucial for understanding our approach for typing the par primitive (§4.5).",
We then focus on advanced features of TypeDis: general recursive types and type polymorphism (§4.6).,
We conclude by presenting subtiming (§4.7).,
"4.1 Syntax of Types To reason statically about the runtime notions of timestamps 𝑡and computation graphs 𝐺(§3.2), we introduce their corresponding static notions: timestamp variables 𝛿and logical graphs Δ, respectively.",
"A logical graph Δ is a set of pairs 𝛿1 ≼𝛿2, asserting that the timestamp 𝛿1 precedes the timestamp 𝛿2, that is, everything allocated by the task at𝛿1 is safe to acquire for the task at𝛿2.",
Figure 11 summarizes these notions together with the syntax of types.,
"A powerful feature of our type system is its support for timestamp polymorphism, facilitated through higher-order types.",
"This higher-order feature is instrumental in typing the par primi- tive (§4.5), and thus supporting the cyclic approach detailed in §2.1.",
"Because our system is higher- order, we introduce kinds, written 𝜅, which capture the number of timestamps a type expects as Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:14 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick arguments.",
"The ground kind, written ★, indicates that the type does not take a timestamp argument.",
"The successor kind, written ⊲⊳ ⇒𝜅, indicates that the type expects 𝜅+ 1 timestamp arguments.",
"A base type 𝜏describes an unboxed value, that is, a value that is not allocated on the heap.",
"Base types include the unit type, Booleans, and integers.",
The syntax of types 𝜌is mutually inductive with the syntax of boxed types 𝜎.,
"A type 𝜌is either a base type 𝜏, a type taking a timestamp argument 𝜆𝛿.",
"𝜌, an application of a type to a timestamp 𝜌𝛿, a universal quantification of a type variable with some kind ∀𝛼:: 𝜅. 𝜌, a recursive type 𝜇𝛼.",
"𝜎@𝛿, a type variable 𝛼, or a boxed type annotated with a timestamp 𝜎@𝛿.",
"When the timestamp 𝛿does not matter, we write 𝜎@_.",
"A boxed type 𝜎is either an array array(𝜌), an immutable pair (𝜌× 𝜌), an immutable sum (𝜌+ 𝜌), or a function ∀®𝛿Δ.",
"Types support 𝛼-equivalence for both type and timestamp variables, as well as 𝛽-reduction.",
"4.2 The Typing Judgment A typing environment Γ is a map from free program variables to types, and from free type variables to kinds.",
"The general form of the typing judgment of TypeDis is: Δ | Γ ⊢𝑒: 𝜌⊲𝛿 where Δ is a logical graph, Γ a typing environment, 𝑒the expression being type-checked at type 𝜌 and at current timestamp 𝛿.",
Selected rules of the type system appear in Figure 12.,
"The rules adopt Barendregt’s conven- tion [Barendregt 1984], assuming bound variables to be distinct from already existing free variables in scope.",
"The reader might notice that several rules (for example, T-Abs or T-TAbs) require the user to manually decide where to apply these rules and with which arguments.",
We leave to future work the design of syntactic features together with a type inference mechanism for simplifying this process.,
"Various rules are standard: T-Var type-checks variables and T-Unit, T-Int, and T-Bool type-check base types.",
"The structural rules T-Let and T-If are also standard, and type-check let bindings and if statements, respectively.",
"In the remainder, we discuss the rules that deserve special attention with regard to disentanglement.",
"4.3 Typing Rules for Heap Blocks Heap blocks must be handled with care to guarantee disentanglement: every time the program acquires a location—that is, the address of a heap block—we must ensure that this location has been allocated by a preceding task.",
"Otherwise, this newly created root would break the disentanglement invariant (§3.3).",
"Because load operations are so common in programming languages, we chose to enforce the following invariant on the typing judgment Δ | Γ ⊢𝑒: 𝜌⊲𝛿: every location that can be acquired from Γ was allocated before the current timestamp 𝛿(§2.4).",
"Hence, load operations (from immutable blocks and from mutable blocks) do not have any timestamp check.",
"Operations on immutable blocks are type-checked by T-Pair and T-Proj, for pairs, and by T-Inj and T-Case, for sums.",
"In particular, T-Pair and T-Inj reflect that pair creation and injection allocate heap blocks, hence, the resulting type is annotated with @𝛿, denoting the allocating timestamp.",
"Operations on mutable blocks are type-checked by T-Array, T-Store, and T-Load.",
4.4 Abstractions and Timestamp Polymorphism A function can be seen as a delayed computation.,
"In our case, this notion of “delay” plays an interesting role: a function can run on a task distinct from the one that allocated it.",
"Hence, functions in TypeDis have three non-standard features related to timestamps, roughly describing the status of the computation graph when the function will run.",
"First, a function takes timestamp parameters, Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:15 T-Var Γ(𝑥) = 𝜌 Δ | Γ ⊢𝑥: 𝜌⊲𝛿 T-Unit Δ | Γ ⊢() : unit ⊲𝛿 T-Int Δ | Γ ⊢𝑖: int ⊲𝛿 T-Bool Δ | Γ ⊢𝑏: bool ⊲𝛿 T-Let Δ | Γ ⊢𝑒1 : 𝜌′ ⊲𝛿 Δ | 𝑥: 𝜌′, Γ ⊢𝑒2 : 𝜌⊲𝛿 Δ | Γ ⊢let𝑥= 𝑒1 in𝑒2 : 𝜌⊲𝛿 T-If Δ | Γ ⊢𝑒1 : bool ⊲𝛿 Δ | Γ ⊢𝑒2 : 𝜌⊲𝛿 Δ | Γ ⊢𝑒3 : 𝜌⊲𝛿 Δ | Γ ⊢if𝑒1 then𝑒2 else𝑒3 : 𝜌⊲𝛿 T-Pair Δ | Γ ⊢𝑒1 : 𝜌1 ⊲𝛿 Δ | Γ ⊢𝑒2 : 𝜌2 ⊲𝛿 Δ | Γ ⊢(𝑒1,𝑒2) : (𝜌1 × 𝜌2)@𝛿⊲𝛿 T-Proj Δ | Γ ⊢𝑒: (𝜌1 × 𝜌2)@_ ⊲𝛿 Δ | Γ ⊢proj𝜄𝑒: 𝜌𝑖⊲𝛿 T-Inj Δ | Γ ⊢𝑒: 𝜌𝑖⊲𝛿 Δ | Γ ⊢inj𝜄𝑒: (𝜌1 + 𝜌2)@𝛿⊲𝛿 T-Case Δ | Γ ⊢𝑒: (𝜌1 + 𝜌2)@_ ⊲𝛿 Δ | 𝑥1 : 𝜌1, Γ ⊢𝑒1 : 𝜌⊲𝛿 Δ | 𝑥2 : 𝜌2, Γ ⊢𝑒2 : 𝜌⊲𝛿 Δ | Γ ⊢match𝑒with inj1 𝑥1 ⇒𝑒1 | inj2 𝑥2 ⇒𝑒2 end : 𝜌⊲𝛿 T-Array Δ | Γ ⊢𝑒1 : int ⊲𝛿 Δ | Γ ⊢𝑒2 : 𝜌⊲𝛿 Δ | Γ ⊢alloc 𝑒1 𝑒2 : array(𝜌)@𝛿⊲𝛿 T-Store Δ | Γ ⊢𝑒1 : array(𝜌)@_ ⊲𝛿 Δ | Γ ⊢𝑒2 : int ⊲𝛿 Δ | Γ ⊢𝑒3 : 𝜌⊲𝛿 Δ | Γ ⊢𝑒1.",
[𝑒2] ←𝑒3 : () ⊲𝛿 T-Load Δ | Γ ⊢𝑒1 : array(𝜌)@_ ⊲𝛿 Δ | Γ ⊢𝑒2 : int ⊲𝛿 Δ | Γ ⊢𝑒1.,
"[𝑒2] : 𝜌⊲𝛿 T-Abs Δ, Δ1,𝛿≼𝛿𝑓| 𝑓: (∀®𝛿1 Δ1.",
"®𝜌1 →𝛿𝑓𝜌2)@𝛿, (®𝑥: ®𝜌1), Γ ⊢𝑒: 𝜌2 ⊲𝛿𝑓 Δ | Γ ⊢𝜇𝑓.",
𝜆®𝑥.𝑒: (∀®𝛿1 Δ1.,
®𝜌1 →𝛿𝑓𝜌2)@𝛿⊲𝛿 T-App 𝛿= [ ®𝛿′ 1/ ®𝛿1]𝛿𝑓 ®𝜌′ 1 = [ ®𝛿′ 1/ ®𝛿1] ®𝜌1 𝜌′ 2 = [ ®𝛿′ 1/ ®𝛿1]𝜌2 Δ′ 1 = [ ®𝛿′ 1/ ®𝛿1]Δ1 Δ | Γ ⊢𝑒: (∀®𝛿1 Δ1.,
®𝜌1 →𝛿𝑓𝜌2)@_ ⊲𝛿 Δ ⊢Δ′ 1 Δ | Γ ⊢®𝑒′ : ®𝜌′ 1 ⊲𝛿 Δ | Γ ⊢𝑒®𝑒′ : 𝜌′ 2 ⊲𝛿 T-Par Γ ⊢𝜑1 :: ⊲⊳ ⇒★ Γ ⊢𝜑2 :: ⊲⊳ ⇒★ Δ | Γ ⊢𝑒1 : (∀𝛿′ 𝛿≼𝛿′.,
() →𝛿′ 𝜑1 𝛿′)@_ ⊲𝛿 Δ | Γ ⊢𝑒2 : (∀𝛿′ 𝛿≼𝛿′.,
"() →𝛿′ 𝜑2 𝛿′)@_ ⊲𝛿 Δ | Γ ⊢par(𝑒1,𝑒2) : (𝜑1 𝛿× 𝜑2 𝛿)@𝛿⊲𝛿 T-Fold Γ ⊢𝜇𝛼.",
𝜎@𝛿:: ★ Δ | Γ ⊢𝑒: ([𝜇𝛼.,
𝜎@𝛿/𝛼]𝜎)@𝛿⊲𝛿 Δ | Γ ⊢fold𝑒: 𝜇𝛼.,
𝜎@𝛿⊲𝛿 T-Unfold Γ ⊢𝜇𝛼.,
𝜎@𝛿:: ★ Δ | Γ ⊢𝑒: 𝜇𝛼.,
𝜎@𝛿⊲𝛿 Δ | Γ ⊢unfold𝑒: ([𝜇𝛼.,
"𝜎@𝛿/𝛼]𝜎)@𝛿⊲𝛿 T-TAbs Δ | 𝛼:: 𝜅, Γ ⊢𝑒: 𝜌⊲𝛿 veryPure𝑒 Δ | Γ ⊢𝑒: ∀𝛼:: 𝜅. 𝜌⊲𝛿 T-TApp Γ ⊢𝜌′ :: 𝜅 Δ | Γ ⊢𝑒: ∀𝛼:: 𝜅. 𝜌⊲𝛿 Δ | Γ ⊢𝑒: [𝜌′/𝛼]𝜌⊲𝛿 T-GetRoot Γ(𝑥) = array(𝜎′)@𝛿′ ∨Γ(𝑥) = 𝜇𝛼.",
"𝜎′@𝛿′ Δ, 𝛿′ ≼𝛿| Γ ⊢𝑒: 𝜌⊲𝛿 Δ | Γ ⊢𝑒: 𝜌⊲𝛿 T-Subtiming Δ | Γ ⊢𝑒: 𝜌⊲𝛿 Δ ⊢𝜌⊆𝛿𝜌′ Δ | Γ ⊢𝑒: 𝜌′ ⊲𝛿 Fig.",
The type system (selected rules) Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:16 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick which are universally quantified.",
"Second, a function takes a constraint over these timestamps, as a logical graph.",
"Third, a function is annotated with a timestamp representing the task it will run on.",
Let us focus on the abstraction rule T-Abs.,
This rule type-checks a function definition of the form 𝜇𝑓.,
"𝜆®𝑥.𝑒, and requires the user to provide timestamp parameters ®𝛿1, logical graph Δ1, and a running timestamp 𝛿𝑓.",
The current timestamp is 𝛿and the type associated to the function is (∀®𝛿1 Δ1.,
®𝜌1 →𝛿𝑓𝜌2)@𝛿.,
"This type asserts that, if (i) there is some instantiation of ®𝛿1 satisfying Δ1, (ii) there are some arguments of type ®𝜌1, and (iii) the timestamp of the calling task is 𝛿𝑓, then the function will produce a result of type 𝜌2.",
"This type also reminds us that a function is a heap-allocated object, and is hence annotated with the task that allocated it, here 𝛿.",
"The premise of T-Abs changes the current timestamp to be 𝛿𝑓, the timestamp of the invoking task, and requires the body 𝑒to be of type 𝜌2.",
T-Abs is in fact the sole rule of the system “changing” the current timestamp while type-checking.,
"The logical graph is augmented with Δ1 plus the knowledge that 𝛿 precedes 𝛿𝑓, conveying the fact that a function can only be called at a subsequent timestamp.",
The environment Γ is extended with the parameters (®𝑥: ®𝜌1) as well as the recursive name 𝑓.,
Note that timestamp parameters ®𝛿1 and logical graph Δ1 are before the arguments ®𝑥.,
"This means that the body 𝑒will be able to recursively call 𝑓with different timestamp arguments (potentially including a different 𝛿𝑓), for example after it forked.",
We already saw such an example for the build and selectmap functions in Sections 2.3 and 2.4.,
"Let us now focus on T-App, type-checking a function application.",
The conclusion type-checks the expression 𝑒®𝑒′ to be of type 𝜌′ 2 at the current timestamp 𝛿.,
The premise of T-App requires 𝑒 to be a function of type ∀®𝛿1 Δ1.,
"®𝜌1 →𝛿𝑓𝜌2, allocated by some irrelevant task.",
The premise then substitutes in all the relevant parts the user-supplied timestamps ®𝛿′ 1 in place of ®𝛿1.,
"Hence, the result type 𝜌′ 2 is equal to [ ®𝛿′ 1/ ®𝛿1]𝜌2.",
"In particular, the premise 𝛿= [ ®𝛿′ 1/ ®𝛿1]𝛿𝑓requires that the running timestamp 𝛿𝑓to be equal to 𝛿, the current timestamp.",
"The premise also requires the logical graph Δ′ 1 to be a subgraph of the logical graph Δ, written Δ ⊢Δ′ 1, meaning that every pair of vertices reachable in Δ′ 1 must also be reachable in Δ.",
This property is formally defined in Appendix A.3.,
"Finally, the premise requires the arguments ®𝑒to be of the correct type ®𝜌′ 1.",
4.5 The Par Rule The typing rule for the par primitive is at the core of TypeDis.,
"T-Par type-checks par(𝑒1,𝑒2) at cur- rent timestamp 𝛿.",
Recall (§3.2) that the results of 𝑒1 and 𝑒2 must be closures; these closures are then called in parallel and their results are returned as an immutable pair.,
"To preserve disentanglement, the two closures must not communicate allocations they make with each other.",
"Hence, the premise of T-Par requires the two expressions 𝑒1 and 𝑒2 to be of type ∀𝛿′ 𝛿≼𝛿′.",
", signaling that they must be closures that are expected to run on a task 𝛿′, universally quantified, and subsequent to 𝛿.",
"Because of this universal quantification over the running timestamp 𝛿′ and because the rules allocating blocks (T-Array, T-Pair, T-Proj and T-Abs) always tag the value they allocate with the running timestamp, the tasks will not be able to communicate allocations they make.",
"After these two closure calls terminate, and their underlying tasks join, the parent task gains access to everything the two children allocated.",
"In fact, from the point of view of disentanglement, we can even pretend that the parent task itself allocated these locations!",
"T-Par does more than pretending and backtimes the return types of the two closures, by substituting the running timestamp of the children 𝛿′ by the running timestamp of the parent 𝛿.",
"Indeed, the return types of the closures, 𝜑1 𝛿′ for 𝑒1 and 𝜑2 𝛿′ for 𝑒2, signal that these two closures will return some type, parametrized by the running timestamp 𝛿′.",
"This formulation allows the rule to type-check the original par(𝑒1,𝑒2) as (𝜑1 𝛿× 𝜑2 𝛿)@𝛿, that is, a pair of the two types returned by the closures, but where the running timestamp of the child 𝛿′ was replaced by the running timestamp of the parent 𝛿. Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:17 4.6 Recursive Types and Type Polymorphism Recursive types.,
"TypeDis supports iso-recursive types [Pierce 2002, §20.2].",
"In TypeDis, a recursive type takes the form 𝜇𝛼.",
"𝜎@𝛿, binding the recursive name 𝛼in the boxed type 𝜎which must have been allocated at 𝛿.",
"This syntax ensures that types are well-formed, and forbids meaningless types 𝜇𝛼.",
𝛼as well as useless types 𝜇𝛼.,
𝜌. T-Fold and T-Unfold allow for going from 𝜇𝛼.,
𝜎@𝛿to ([𝜇𝛼.,
𝜎@𝛿/𝛼]𝜎)@𝛿and vice-versa.,
Note that this approach requires that the recursive occurrences of 𝛼are all allocated at the same timestamp; all the nodes of the recursive data structures must have been allocated at the same timestamp.,
"This may seem restrictive, but subtiming will relax this requirement (§4.7).",
Let us give an example.,
The type of lists allocated at timestamp 𝛿containing integers is: 𝜇𝛼.,
"(() + (int × 𝛼)@𝛿)@𝛿 This type describes that a list of integers is either the unit value (describing the nil case), or the pair of an integer and a list of integers (describing the cons case).",
Type polymorphism.,
"TypeDis supports type polymorphism, through type abstraction T-TAbs and type application T-TApp.",
"Whereas the former is standard, the latter has an unusual premise veryPure𝑒, our variant of the value restriction.",
The value restriction [Wright 1995] is a simple syntactic restriction guaranteeing soundness of polymorphism in the presence of mutable state—a combination that is well known to be unsound if unrestricted.,
"In particular, the value restriction permits only values to be polymorphic.",
"However, DisLang2 has an unusual aspect: functions are not values, they are allocated on the heap (§3.1).",
"Hence, the value restriction is not applicable as-is, yet it is crucial to allow universal type quantification in front of functions.",
"We contribute a variant of the value restriction, that allows type quantification in front of any pure expression that does not call a function, project a pair, case over a sum, or fork new tasks.",
"This includes function allocation, pair allocation, sums injection, as well as other control-flow constructs.",
This syntactic check is ensured by the predicate veryPure𝑒that appears as a premise of the type abstraction rule T-TAbs.,
The predicate veryPure𝑒is defined in Appendix A.2.,
"It can be seen as an alternative to the solution proposed by de Vilhena [2022], in which every arrow has a purity attribute, indicating if the function interacts with the store.",
"Contrary to de Vilhena’s solution, we support some benign interactions with the store: the allocation of immutable data structures.",
TypeDis supports higher-kind type polymorphism.,
"For example, reminding of the typing rule T-Par, one could present par as a higher-order function of the following type par : ∀(𝜑1 :: ⊲⊳ ⇒★) (𝜑2 :: ⊲⊳ ⇒★).",
() →𝛿′ 𝜑1 𝛿′)@𝛿1 →(∀𝛿′ 𝛿≼𝛿′.,
() →𝛿′ 𝜑2 𝛿′)@𝛿2 →𝛿(𝜑1 𝛿× 𝜑2 𝛿)@𝛿 Taking 𝜑1 = 𝜑2 = 𝜆𝛿.,
tree@𝛿and doing 𝛽-reduction matches the type presented in Section 2.3.,
"4.7 Subtiming As presented so far, backtiming—that is, substituting the timestamp of a child task by the one of its parent task at the join point—is the only way of changing a timestamp inside a type (§4.5).",
We propose here another mechanism that we dub subtiming.,
"As the name suggests, subtiming is a form of subtyping [Pierce 2002, §15] for timestamps.",
"At a high-level, subtiming allows for “advancing” a timestamp within a type, as long as this update makes sense.",
"This notion of “advancing” relates to the notion of precedence, describing reachability between two timestamps.",
We write Δ ⊢𝛿1 ≼𝛿2 to describe that 𝛿1 can reach 𝛿2 in Δ (Appendix A.3).,
"Equipped with this reachability predicate, we make a first attempt at capturing Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:18 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick S-Refl Δ ⊢𝜌⊆𝛿𝜌 S-ReflAt Δ ⊢𝜎⊆𝛿𝜎 S-TAbs Δ ⊢𝜌1 ⊆𝛿𝜌2 Δ ⊢∀𝛼:: 𝜅. 𝜌1 ⊆𝛿∀𝛼:: 𝜅. 𝜌2 S-Pair Δ ⊢𝜌𝑙1 ⊆𝛿𝜌𝑙2 Δ ⊢𝜌𝑟1 ⊆𝛿𝜌𝑟2 Δ ⊢(𝜌𝑙1 × 𝜌𝑟1) ⊆𝛿(𝜌𝑙2 × 𝜌𝑟2) S-Sum Δ ⊢𝜌𝑙1 ⊆𝛿𝜌𝑙2 Δ ⊢𝜌𝑟1 ⊆𝛿𝜌𝑟2 Δ ⊢(𝜌𝑙1 + 𝜌𝑟1) ⊆𝛿(𝜌𝑙2 + 𝜌𝑟2) S-At Δ ⊢𝛿1 ≼𝛿2 (𝛿1 ≠𝛿2 =⇒Δ ⊢𝛿2 ≼𝛿) Δ ⊢𝜎1 ⊆𝛿2 𝜎2 Δ ⊢𝜎1@𝛿1 ⊆𝛿𝜎2@𝛿2 S-Rec Δ ⊢𝛿1 ≼𝛿2 (𝛿1 ≠𝛿2 =⇒Δ ⊢𝛿2 ≼𝛿) Δ ⊢𝜎1 ⊆𝛿2 𝜎2 Δ | 𝛼↦→𝛿2 ⊢𝛿2 𝜎2 Δ ⊢𝜇𝛼.",
𝜎1@𝛿1 ⊆𝛿𝜇𝛼.,
𝜎2@𝛿2 S-Abs Δ′ = Δ ∪Δ2 Δ′ ⊢Δ1 Δ′ ⊢® 𝜌𝑠2 ⊆𝛿𝑓 ® 𝜌𝑠1 Δ′ ⊢𝜌1 ⊆𝛿𝑓𝜌2 Δ ⊢∀®𝛿𝑠Δ1.,
® 𝜌𝑠1 →𝛿𝑓𝜌1 ⊆𝛿∀®𝛿𝑠Δ2.,
® 𝜌𝑠2 →𝛿𝑓𝜌2 S-Inst 𝛿2 = [𝛿𝑦/𝛿𝑥]𝛿1 ® 𝜌𝑠2 = [𝛿𝑦/𝛿𝑥] ® 𝜌𝑠1 Δ2 = [𝛿𝑦/𝛿𝑥]Δ1 Δ ⊢∀( ® 𝛿𝑠𝑙++[𝛿𝑥] ++ ® 𝛿𝑠𝑟) Δ1.,
® 𝜌𝑠1 →𝛿1 𝜌1 ⊆𝛿∀( ® 𝛿𝑠𝑙++ ® 𝛿𝑠𝑟) Δ2.,
® 𝜌𝑠2 →𝛿2 𝜌2 Fig.,
"The subtiming judgment the idea of subtiming as follows: Specialized-Subtiming Δ | Γ ⊢𝑒: 𝜎@𝛿1 ⊲𝛿 Δ ⊢𝛿1 ≼𝛿2 Δ ⊢𝛿2 ≼𝛿 Δ | Γ ⊢𝑒: 𝜎@𝛿2 ⊲𝛿 Specialized-Subtiming asserts that an expression of type 𝜎@𝛿1 can be viewed as an expression of type 𝜎@𝛿2 as long as 𝛿1 precedes 𝛿2 and 𝛿2 is not ahead of time, that is 𝛿2 precedes the current timestamp 𝛿.",
"Indeed, TypeDis enforces that, if Δ | Γ ⊢𝑒: 𝜎@𝛿′ ⊲𝛿holds, then 𝛿′ precedes 𝛿.",
"While Specialized-Subtiming is admissible in TypeDis, it is not general enough, as it only considers the timestamp at the root of a type.",
"This motivates rule T-Subtiming in Figure 12, which relies on the subtiming judgment Δ ⊢𝜌⊆𝛿𝜌′, given in Figure 13, and acts as a subsumption rule.",
"Intuitively, the judgment Δ ⊢𝜌⊆𝛿𝜌′ captures the fact the timestamps in 𝜌precede the timestamps in 𝜌′ under logical graph Δ, knowing that every timestamp occurring in 𝜌′ must precede 𝛿.",
The definition of the judgment now allows changing the timestamps inside immutable types.,
"Because of variance issues (see [Pierce 2002, §15.5]), however, subtiming for mutable types is only shallow: a timestamp can be changed only at the root of an array type.",
The subtiming judgment Δ ⊢𝜌⊆𝛿𝜌′ assumes that types are in 𝛽-normal form.,
S-Refl and S-ReflAt assert that the subtiming judgment is reflexive.,
"S-TAbs asserts that subtiming goes below type quantifiers (which are irrelevant here, the subtiming judgment tolerates open terms).",
S-Pair and S-Sum reflect that subtiming for immutable types is deep.,
S-At illustrates the case presented in Specialized-Subtiming.,
"This rule asserts that, with logical graph Δ and maximum allowed timestamp 𝛿, the boxed type 𝜎1@𝛿1 is a subtype of 𝜎2@𝛿2 if three conditions are met.",
"First, 𝛿1 must precede 𝛿2.",
"Second, if subtiming is applied here, that is, if 𝛿1 ≠𝛿2, then 𝛿2 must precede 𝛿, the maximum timestamp allowed.",
"Third, 𝜎1 must recursively be a subtype of 𝜎2, with maximum timestamp allowed 𝛿2.",
"Indeed, recall that TypeDis allows only for up-pointers: every timestamp in 𝜎2 must precede 𝛿2.",
S-Rec allows subtiming for recursive types 𝜇𝛼.,
𝜎1@𝛿1 and 𝜇𝛼.,
"The first three premises (in the left-to-right, top-to-bottom order) are the same as for S-At.",
The fourth premise Δ | 𝛼↦→𝛿2 ⊢𝛿2 𝜎2 requires explanations.,
"This predicate, dubbed the “valid variable” judgment and formally defined in Appendix A.4, ensures two properties.",
"First, that 𝛼does not appear in an array type (because subtiming is not allowed at this position) or in an arrow or another recursive type (for simplicity).",
"Second, that if 𝛼appears under a timestamp 𝛿, then 𝛿2 must precede 𝛿. Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:19 OOB-Alloc 𝑖< 0 OOB𝜎(alloc 𝑖𝑣) OOB-Load 𝜎(ℓ) = ®𝑣 𝑖< 0 ∨𝑖≥|®𝑣| OOB𝜎(ℓ.,
[𝑖]) OOB-Store 𝜎(ℓ) = ®𝑣 𝑖< 0 ∨𝑖≥|®𝑣| OOB𝜎(ℓ.,
"[𝑖] ←𝑤) OOB-CAS 𝜎(ℓ) = ®𝑣 𝑖< 0 ∨𝑖≥|®𝑣| OOB𝜎(CAS ℓ𝑖𝑤1 𝑤2) Red-Sched 𝑆/𝑇/𝑒 sched −−−−→𝑆′ /𝑇′ /𝑒′ AllRedOrOOB 𝑆/𝑇/𝑒 Red-OOB OOB𝜎𝑒 AllRedOrOOB (𝜎, 𝛼,𝐺) /𝑡/𝑒 Red-Ctx AllRedOrOOB 𝑆/𝑇/𝑒 AllRedOrOOB 𝑆/𝑇/𝐾[𝑒] Red-Par (𝑒1 ∉V ∨𝑒2 ∉V) (𝑒1 ∉V =⇒AllRedOrOOB 𝑆/𝑇1 /𝑒1) (𝑒2 ∉V =⇒AllRedOrOOB 𝑆/𝑇2 /𝑒2) AllRedOrOOB 𝑆/𝑇1 ⊗𝑡𝑇2 /𝑒1 ∥𝑒2 Safe-Final Safe 𝑆/𝑡/𝑣 Safe-NonFinal AllRedOrOOB 𝑆/𝑇/𝑒 Safe 𝑆/𝑇/𝑒 Fig.",
"The OOB, AllRedOrOOB and Safe predicates S-Abs allows subtiming for function types ∀®𝛿𝑠Δ1.",
® 𝜌𝑠1 →𝛿𝑓𝜌1 and ∀®𝛿𝑠Δ2.,
® 𝜌𝑠2 →𝛿𝑓𝜌2.,
The quantified timestamps ®𝛿𝑠and the calling timestamp 𝛿𝑓must be the same.,
"The extended logical graph Δ′, equal to Δ ∪Δ2, must subsume Δ1.",
"Moreover, the arguments ® 𝜌𝑠2 must subtime ® 𝜌𝑠1 (note the polarity inversion).",
The return type 𝜌1 must subtime 𝜌2.,
S-Inst allows for specializing a universally-quantified timestamp and has a more standard subtyping flavor.,
"In this rule, the quantified timestamp 𝛿𝑥is being instantiated with 𝛿𝑦(similarly to the instantiation occurring in T-App).",
"Before using subtiming, information about precedence may be needed.",
TypeDis guarantees a strong invariant: every timestamp occurring in the typing environment comes before the current timestamp.,
"Such an invariant is illustrated by T-GetRoot, which allows adding to the logical graph Δ an edge (𝛿′,𝛿), where 𝛿′ is a timestamp in the environment and 𝛿the current timestamp.",
"5 Soundness In this section, we state the soundness of TypeDis and give an intuition for its proof, which takes the form of a logical relation in Iris and is mechanized in Rocq [Moine et al.",
We first enunciate the soundness theorem (§ 5.1).,
"We then recall the concepts of Iris we need (§ 5.2) and present DisLog2 (§5.3), the verification logic we use.",
"We then devote our attention to the formal proof, by presenting the high-level ideas of the logical relation (§5.4) we developed and its fundamental theorem (§5.5).",
We conclude by assembling all the building blocks we presented and sketching the soundness proof of TypeDis (§5.6).,
5.1 Soundness Statement of TypeDis Our soundness statement adapts Milner [1978]’ slogan “well-typed programs cannot go wrong” by proving that the reduction of a well-typed program reaches only configurations that are safe and disentangled.,
We already formally defined the concept of disentanglement (§3.3).,
What about safety?,
"Intuitively, a configuration is safe if all tasks can take a step or, conversely, no task is stuck.",
"However, this property is too strong for our type system due to reasons unrelated to disentanglement.",
"Being purposefully designed for disentanglement, our type system is not capable of verifying arbitrary functional correctness conditions.",
"In particular, while the semantics of DisLang2 ensures that accesses to arrays by load and store operations are within bounds and thus cannot cause a task to get stuck, our type system does not enforce that.",
This restriction comes at the advantage of freeing Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:20 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick programmers from carrying out correctness proofs themselves, which are carried out by the type- checker instead.",
"Intuitively, we say that a configuration is safe if it is final, or each task can either take a step or encounters a load or a store operation out-of-bounds.",
We formalize these properties in Figure 14.,
"The property OOB𝜎𝑒asserts that the expression 𝑒faces an out-of-bounds operation: that is, an allocation, a load, a store, or a CAS outside the bounds.",
"The property AllRedOrOOB 𝑆/𝑇/𝑒 asserts that, within the configuration of the program state 𝑆, the task tree 𝑇and the expression 𝑒, every task of the task tree can either take a step or faces an out-of-bounds operation.",
"Red-Sched asserts that the configuration can take a scheduling step (that is, either a head step, a fork, or a join).",
Red-OOB asserts that the configuration is at a leaf and faces an out-of-bounds operation.,
Red-Ctx asserts that an expression under evaluation is reducible if this very expression is reducible.,
Red-Par asserts that an active parallel pair 𝑒1 ∥𝑒2 is reducible if at least one of its components 𝑒𝑖is not a value and any 𝑒𝑖that is not a value is reducible.,
"(If both expressions are values, a join is possible).",
"The property Safe 𝑆/𝑇/𝑒asserts that the configuration 𝑆/𝑇/𝑒is either final (Safe-Final), that is, the task tree is at a leaf and the expression is a value, or that every task of the task tree can either take a step or faces an out-of-bounds operation (Safe-NonFinal).",
"An expression 𝑒is always safe and disentangled if (∅, ∅, {(𝑡0,𝑡0)})/𝑡/𝑒 step −−−→∗𝑆′ /𝑇′ /𝑒′ im- plies that Safe 𝑆′ /𝑇′ /𝑒′ and Disentangled 𝑆′ /𝑇′ /𝑒′ hold, for some initial timestamp 𝑡0.",
Theorem 5.1 (Soundness of TypeDis).,
If ∅| ∅⊢𝑒: 𝜌⊲𝛿then 𝑒is always safe and disentangled.,
We prove this theorem using a logical relation [Timany et al.,
"2024], which makes use of DisLog2, a variation of DisLog [Moine et al.",
We present the proof sketch in Section 5.6.,
□ 5.2 Iris Primer We set up our proofs in Iris [Jung et al.,
"2018b], and recall here the base notations.",
Iris’ assertions are of type iProp.,
"We write Φ for an assertion, ⌜𝑃⌝for an assertion of the meta-logic (that is, Rocq), Φ1 ∗Φ2 for a separating conjunction, and Φ1 −∗Φ2 for a separating implication.",
"We write a postcondition—that is, a predicate over values—using Ψ.",
One of the most important features of Iris are invariants.,
"An invariant assertion Φ, written Φ , holds true in-between every computation step.",
"(Formally, invariants are annotated with so- called masks [Jung et al.",
"2018b, §2.2], we omit them for brevity.)",
"Invariants, as well as other logical resources in Iris, are implemented using ghost state.",
"We write Φ1 ⇛Φ2 to denote a ghost update—that is, an update of the ghost state between Φ1 and Φ2.",
Iris features a variety of modalities.,
In this work we use two of them extensively.,
"First, the persistence modality, written Φ, asserts that the assertion Φ is persistent, meaning in particular that Φ is duplicable.",
"Second, the later modality, written ⊲Φ, asserts that Φ holds “one step of computation later”.",
We write ℓ↦→®𝑣to denote that ℓpoints-to an array with contents ®𝑣.,
"We write ℓ↦→ 𝑟, with a discarded fraction [Vindum and Birkedal 2021], to denote that ℓpoints-to an immutable block 𝑟(that is, either a closure, an immutable pair, or an immutable sum).",
This latter assertion is persistent.,
5.3 Taking Advantage of the Cyclic Approach with DisLog2 Moine et al.,
"[2024] contributed DisLog, the first program logic for verifying disentanglement.",
"DisLog depends on the very definition of disentanglement, and uses the standard approach presented in Section 2.1: when two tasks join, they form a new task with a fresh timestamp.",
"This choice impacts the logic: the weakest precondition (WP) modality of DisLog takes the form wp ⟨𝑡, 𝑒⟩{𝜆𝑡′ 𝑣. Φ} and asserts that the expression 𝑒running on current timestamp 𝑡is disentangled, and if the evaluation of 𝑒terminates, it does so on the end timestamp 𝑡′, with final value 𝑣and satisfying the assertion Φ.",
"In particular, 𝑡and 𝑡′ may not be the same, for example if 𝑒contains a call to par.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:21 D-Load ⌜0 ≤𝑖< | ®𝑤| ∧®𝑤(𝑖) = 𝑣⌝ ℓ↦→𝑝®𝑤 𝑣 𝑡 wp ⟨𝑡, ℓ.[𝑖]⟩{𝜆𝑣′.",
"⌜𝑣′ = 𝑣⌝∗ℓ↦→𝑝®𝑤} D-LoadOOB ⌜𝑖< 0 ∨𝑖≥| ®𝑤|⌝ ℓ↦→𝑝®𝑤 wp ⟨𝑡, ℓ.[𝑖]⟩{𝜆_.",
⊥} D-Par ∀𝑡1 𝑡2.,
𝑡≼𝑡1 ∗𝑡≼𝑡2 ⇛∃Ψ1 Ψ2.,
"wp ⟨𝑡1, ℓ1 [()]⟩{Ψ1} ∗wp ⟨𝑡2, ℓ2 [()]⟩{Ψ2} ∗  ∀𝑣1 𝑣2 ℓ. Ψ1 𝑣1 ∗Ψ2 𝑣2 ∗𝑡1 ≼𝑡∗𝑡2 ≼𝑡∗ℓ↦→(𝑣1, 𝑣2) −∗Ψ ℓ wp ⟨𝑡, par(ℓ1, ℓ2)⟩{Ψ} D-ClockMono 𝑣 𝑡1 𝑡1 ≼𝑡2 𝑣 𝑡2 Fig.",
"Selected rules of DisLog2 To take advantage of the cyclic approach for disentanglement (§2.1), we had to develop a new version of DisLog, yielding the logic DisLog2.",
DisLog2 allows reusing the timestamp of the forking task for the child tasks upon join.,
"As a result, the current timestamp and end timestamp of an expression always coincide, allowing us to simplify the WP of DisLog by simply removing the end timestamp parameter of the postcondition.",
"Formally, the WP of DisLog2 then takes the form wp ⟨𝑡, 𝑒⟩{𝜆𝑣.",
"Φ} and asserts that the expression 𝑒running on timestamp 𝑡is disentangled, and if the evaluation of 𝑒terminates, it does so with final value 𝑣and satisfying the assertion Φ.",
"In contrast to DisLog, DisLog2 tolerate out-of-bounds accesses to cater to the TypeDis type system which only enforces disentanglement.",
"(In practice, DisLog2 is parameterized by a boolean flag which can be used to enable or disable inboundedness proof obligations; when such obligations are enabled, DisLog2 has the same expressive power as DisLog.)",
"Otherwise, DisLog2 adapts all the ideas of DisLog.",
"In particular, the logic features two persistent assertions related to timestamps.",
"First, the clock assertion ℓ 𝑡asserts that location ℓwas allocated by a task that precedes 𝑡.",
"Similarly, 𝑣 𝑡has the same meaning, if 𝑣is a location ℓ, or otherwise denotes ⌜𝑇𝑟𝑢𝑒⌝.",
"Second, the precedence assertion 𝑡1 ≼𝑡2 asserts that task 𝑡1 precedes task 𝑡2 in the underlying computation graph.",
The precedence assertion forms a pre-order: it is reflexive and transitive.,
"Crucially, the clock assertion is monotonic with respect to the precedence pre-order [Moine et al.",
"In the remainder of the paper, we write 𝑡1 ≈𝑡2 to denote that 𝑡1 and 𝑡2 are equivalent, that is, both 𝑡1 ≼𝑡2 and 𝑡2 ≼𝑡1 hold.",
Selected rules of DisLog2.,
Figure 15 presents four key rules of DisLog2.,
The premise of these rules are implicitly separated by a separating conjunction ∗.,
"D-Load, targeting a load operation on the array ℓat offset 𝑖on task 𝑡, ensures disentanglement.",
"Indeed, the rule requires that ℓpoints-to the array ®𝑤and that the offset 𝑖in ®𝑤corresponds to the value 𝑣.",
"It also requires the assertion 𝑣 𝑡, witnessing that if 𝑣is a location, then this location must have been allocated by a preceding task.",
"D-LoadOOB is unusual for a program logic and reflects that we purposefully allow for OOB accesses in verified programs, because our type system does.",
"Because an OOB access results in a crash, the postcondition of the WP is ⌜𝐹𝑎𝑙𝑠𝑒⌝, allowing the user to conclude anything.",
D-Par is at the heart of DisLog2 and allows verifying a parallel call to two closures ℓ1 and ℓ2 at timestamp 𝑡.,
"The premise universally quantifies over 𝑡1 and 𝑡2, the two timestamps of the forked tasks, that are both preceded by 𝑡.",
"Then, the user must provide two postconditions, Ψ1 and Ψ2 for the two tasks, and verify that the closure call ℓ1 [()] (resp.",
ℓ2 [()]) is safe at timestamp 𝑡1 (resp.,
𝑡2) with postcondition Ψ1 (resp.,
"The second line of the premise requires the user to prove that, after the two tasks terminated and joined, the initial postcondition Ψ ℓmust hold, for some location ℓpointing to the pair (𝑣1, 𝑣2) where 𝑣1 is the final result of 𝑡1 and𝑣2 of 𝑡2.",
D-ClockMono formalizes monotonicity of the clock assertion.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:22 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick The adequacy theorem of DisLog2.",
"The adequacy theorem of DisLog2 asserts that if 𝑒can be verified using the program logic, then 𝑒is always safe and disentangled.",
Theorem 5.2 (Adeqacy of DisLog2).,
"If wp ⟨𝑡, 𝑒⟩{Ψ} holds then 𝑒is always safe and dis- entangled.",
Similar to the adequacy proof of DisLog; see our mechanization [Moine et al.,
"□ 5.4 A Logical Relation The very heart of the soundness proof of TypeDis is a logical relation, set up in Iris using DisLog2.",
"Logical relations [Girard 1972; Pitts and Stark 1998; Plotkin 1973; Statman 1985; Tait 1967] are a technique that allows one to prescribe properties of valid programs in terms of their behavior, as opposed to solely their static properties.",
We adopt the semantic approach [Constable et al.,
1986; Martin-Löf 1982; Timany et al.,
"2024], which admits terms that are not necessarily (syntactically) well-typed to be an inhabitant of the logical relation and has been successfully deployed in the RustBelt project [Jung et al.",
"2018a], for example.",
Our logical relation is presented in Appendix A.6; we comment next on the high-level ideas.,
"As usual, our (unary) logical relation gives the interpretation of a type 𝜌with kind ★as a predicate on values.",
Values satisfying the predicate are said to inhabit the relation.,
"Because our types have higher kinds, our logical relation includes predicates on timestamps.",
"In particular, the interpretation of a type 𝜌with kind 𝜅is a function taking 𝜅timestamp arguments (where ★indicates zero timestamps and ⊲⊳ ⇒𝜅indicates 𝜅+ 1 timestamps) and producing a predicate over values.",
The presented relations involve two sorts of closing substitutions for variables occurring in types.,
"First, a timestamp substitution, written ℎ, which is a finite map from timestamp variables 𝛿 to concrete timestamps 𝑡.",
"Second, a type substitution, written 𝑚, which is a finite map from type variables to tuples of a kind 𝜅and a tuple of two functions depending on 𝜅.",
The first function takes 𝜅timestamps and produces a predicate over values; it represents the semantic interpretation of the type by which the variable will be instantiated.,
The second function takes 𝜅timestamps and produces a timestamp; its result corresponds to the root timestamp of the type by which the variable will be instantiated.,
"The interpretation of a type guarantees that the type only contains up-pointers (§2.2), that is, the interpretation of 𝜎@𝛿ensures that if 𝛿′ appears in 𝜎, then 𝛿′ precedes 𝛿.",
"To enforce this invariant, our approach makes use of transitivity: the interpretation of 𝜎@𝛿 ensures that, for each outermost 𝜌encountered in 𝜎, the root timestamp of 𝜌—conceptually, the outermost timestamp in 𝜌—precedes 𝛿.",
"Because this invariant is enforced at each stage of the type interpretation, and because precedence is transitive, we guarantee that there are only up-pointers.",
"Appendix A.5 presents a function that computes the root timestamp of a type and defines the assertion root 𝜌≼ℎ 𝑚𝛿, asserting that the root timestamp of 𝜌comes before 𝛿with the mappings ℎ and 𝑚.",
The main relation is the type relation J𝜌Kℎ 𝑚𝜅.,
"It produces a predicate waiting for 𝜅timestamps, a value 𝑣, and captures that 𝑣is of type 𝜌, given the timestamp mapping ℎand type mapping 𝑚.",
"Apart from timestamps, the seasoned reader of logical relations in Iris will not be surprised by our approach, as it follows the standard recipe [Timany et al.",
"2024]: a recursive type is interpreted using a guarded fixpoint, universal type quantification is interpreted as a universal quantification in the logic, an array is interpreted using an invariant, and an arrow using WP.",
"Moreover, every predicate is designed such that it is persistent.",
"5.5 Interpretation of Typing Judgments We now focus on the interpretation of the TypeDis typing judgment, paving our way to state the fundamental theorem of the logical relation.",
"Figure 16 gives its interpretation, appealing to the WP of Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:23 J Δ | Γ ⊢𝑒: 𝜌⊲𝛿K ≜ ∀ℎ𝑚𝑢.,
⌜dom Γ = dom𝑢⌝−∗ ⌜∀𝛼𝜅Ψ𝑟.,
"𝑚(𝛼) = (𝜅, (Ψ,𝑟)) =⇒proper𝜅Ψ ∧regular𝜅𝑟⌝−∗ ∗(𝑡1,𝑡2)∈Δ ℎ(𝑡1) ≼ℎ(𝑡2) −∗ ∗(𝑥,𝜌)∈Γ, (𝑥,𝑣)∈𝑢(root 𝜌≼ℎ 𝑚𝛿∗J𝜌Kℎ 𝑚★𝑣) −∗ ∀𝑡.",
"𝑡≈ℎ(𝛿) −∗wp ⟨𝑡, [𝑢/]𝑒⟩{𝜆𝑣.",
root 𝜌≼ℎ 𝑚𝛿∗J𝜌Kℎ 𝑚★𝑣} Fig.,
The interpretation of typing judgments DisLog2.,
"For the judgment with logical graph Δ, type environment Γ, and expression 𝑒with type 𝜌at timestamp 𝛿, the interpretation starts by quantifying over three closing substitutions: the timestamp mappingℎ, the type mapping𝑚, as well as a variable mapping 𝑢, a map from term variables to values.",
The variable mapping must have the same domain as the environment Γ.,
The type mapping 𝑚 is restricted such that type variables are given only a proper interpretation (via the proper𝜅Ψ property) and a regular root function (via the regular𝜅𝑟property).,
The property proper𝜅Ψ captures that any timestamp parameter of Ψ can be replaced by an equivalent one.,
The property regular𝜅𝑟 captures that the function 𝑟either ignores all its arguments or returns one of them.,
These two properties are needed in order to prove the correctness of T-Par.,
"Then, the interpretation requires that Δ is a valid logical graph, that is, each edge between two timestamp variables in Δ corresponds to an edge between their mapping.",
"The interpretation also requires that, for every variable 𝑥that has type 𝜌in Γ and is associated to value 𝑣in 𝑢, the root timestamp of 𝑣precedes the interpretation of 𝛿and 𝑣inhabits the interpretation of 𝜌.",
"Next, the definition quantifies over a timestamp 𝑡, equivalent to the interpretation of 𝛿, and asserts WP at timestamp 𝑡of the expression 𝑒in which variables are substituted by values following the variable mapping 𝑢.",
The postcondition asserts that the root timestamp of 𝑣precedes the interpretation of 𝛿 and that the returned value 𝑣inhabits the interpretation of type 𝜌.,
"Having the interpretation of typing judgment defined, we can state the fundamental theorem, ensuring that syntactically well-typed terms (§4) inhabit the logical relation.",
Theorem 5.3 (Fundamental).,
"If Δ | Γ ⊢𝑒: 𝜌⊲𝛿holds, then J Δ | Γ ⊢𝑒: 𝜌⊲𝛿K holds too.",
By induction over the typing derivation; see our mechanization [Moine et al.,
"□ 5.6 Putting Pieces Together: The Soundness Proof of TypeDis We can finally unveil the proof of the soundness Theorem 5.1 of TypeDis, which we formally establish in Rocq.",
Let us suppose that ∅| ∅⊢𝑒: 𝜌⊲𝛿holds.,
"Making use of the fundamental Theorem 5.3, we deduce that J ∅| ∅⊢𝑒: 𝜌⊲𝛿K holds too.",
"Unfolding the definition (Fig- ure 16), instantiating the timestamp mapping ℎwith the singleton map [𝛿:= 𝑡0]—for some initial timestamp 𝑡0—and the type mapping 𝑚and the variable mapping 𝑢with empty maps, and simplifying trivial premises concerning these mappings, we are left with the statement ∀𝑡.",
"𝑡≈𝑡0 −∗ wp ⟨𝑡, 𝑒⟩{𝜆𝑣.",
root 𝜌≼ℎ 𝑚𝛿∗J𝜌K[𝛿:=𝑡0] ∅ ★𝑣}.,
"Instantiating 𝑡with 𝑡0, we deduce that wp ⟨𝑡0, 𝑒⟩{𝜆𝑣.",
root 𝜌≼ℎ 𝑚𝛿∗J𝜌K[𝛿:=𝑡0] ∅ ★𝑣} holds.,
We finally use the adequacy Theorem 5.2 of DisLog2 and deduce that 𝑒is always safe and disentangled.,
6 Case Studies We evaluate the usefulness of TypeDis by type-checking several case studies in Rocq using the rules presented in Section 4.,
We verify the examples presented in the “Key Ideas” Section 2.,
"These examples illustrate: simple mechanics of the type system (§2.2), backtiming (§2.3), and subtiming (§2.4).",
"In particular, the last two examples, build and selectmap, illustrate the use of TypeDis with higher-order functions and Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:24 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick parfor ≜𝜇𝑓.",
"if 𝑏≤𝑎then () else if (𝑏−𝑎) ==1 then 𝑘[𝑎] else let 𝑚𝑖𝑑= 𝑎+ ((𝑏−𝑎)/2) in par(𝑓[𝑎;𝑚𝑖𝑑;𝑘], 𝑓[𝑚𝑖𝑑;𝑏;𝑘]) parfor : ∀𝛿𝛿𝑘.",
int →int →(∀𝛿′ 𝛿≼𝛿′.,
int →𝛿′ ())@𝛿𝑓→𝛿() Fig.,
Implementation and type of the parfor primitive a recursive immutable type (a binary tree with integer leaves).,
"For more details than the intuitions we already gave for these examples, we refer the reader to our formalization [Moine et al.",
Our largest case study consists of the typing of a parallel deduplication algorithm via concurrent hashing.,
This example is a case study of DisLog [Moine et al.,
"2024, §6.3].",
"Deduplication consists of removing duplicates from an array—something that can be done efficiently in a parallel, disentangled setting [Westrick 2022].",
"The algorithm relies on a folklore [VerifyThis 2022] concurrent, lock-free, fixed-capacity hash set using open addressing and linear probing to handle collisions [Knuth 1998].",
"The main deduplication function allocates a new hash set, inserts in parallel every element into the hash set using a parallel for loop, and finally returns the elements of the set.",
We first comment on the parallel for loop (§6.1) and then on the main deduplication algorithm (§6.2).,
6.1 The Parallel For Loop Our implementation of the parallel for loop appears in the upper part of Figure 17 and is a direct translation of MaPLe standard library’s implementation [Acar et al.,
"2020], The function parfor takes three arguments: a lower bound 𝑎, a higher bound 𝑏, and a closure 𝑘to execute at each index between these bounds.",
"The function parfor is defined recursively: it returns immediately if 𝑏≤𝑎, executes the closure 𝑘[𝑎] if 𝑏−𝑎= 1, and otherwise calls itself recursively in parallel, splitting the range in two.",
"The type we give to parfor appears in the lower part of Figure 17, and is as one could expect.",
"Indeed, the type quantifies over two timestamps 𝛿, at which parfor will be called, and 𝛿𝑘, the (irrelevant) timestamp of the closure.",
"The function then requires two integers, and a closure that will be called at some timestamp 𝛿′ that succeeds 𝛿.",
The type-checking of parfor is non-trivial because it involves polymorphic recursion.,
"Indeed, parfor’s type universally quantifies over the calling timestamp, but calls itself recursively after a par— that is, at another (subsequent) timestamp.",
TypeDis supports natively such a pattern thanks to T-Abs.,
"Interestingly, polymorphic recursion introduce a need for subtiming.",
"Indeed, while type-checking parfor’s body at current timestamp 𝛿, the closure 𝑘has type (∀𝛿′ 𝛿≼𝛿′.",
int →𝛿′ ()).,
"However, the recursive call happens after a par, hence at a new current timestamp 𝛿1 such that 𝛿≼𝛿1.",
"But in order to type-check the recursive call, the user has to give to 𝑘the type (∀𝛿′ 𝛿1 ≼𝛿′.",
int →𝛿′ ())—notice the difference between the precedence information on 𝛿′.,
"This is a typical use of subtiming, and because 𝛿≼𝛿1, we conclude using S-Abs.",
"6.2 Internals of the Deduplication Case Study Let us now focus on the code for our deduplication algorithm, which appears in the upper part of Figure 18.",
This code assumes a maximum size 𝐶for the underlying hash set.,
"The function dedup takes three arguments: a hashing function ℎ, a dummy element 𝑑in order to populate the result array, and the array to deduplicate ℓ.",
The function first allocates the hash set 𝑎and then calls in parallel the add function for every index in ℓ.,
"The function add consists of a CAS loop, that tries to insert the element in the first available slot.",
"Finally, dedup filters the remaining dummy elements 𝑑 using an omitted function filter_compact.",
"Because it involves no fork or join, the function add admits a simple polymorphic type, shown in the lower part of Figure 18, quantifying over the Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:25 add ≜ 𝜆[ℎ;𝑎;𝑑;𝑥].,
let 𝑝𝑢𝑡= 𝜇𝑓.𝜆[𝑖].,
if (CAS𝑎𝑖𝑑𝑥∨𝑎.,
[𝑖] ==𝑥) then () else 𝑓[(𝑖+ 1) mod 𝐶] in 𝑝𝑢𝑡[ℎ[𝑥] mod 𝐶] dedup ≜ 𝜆[ℎ;𝑑; ℓ].,
let 𝑎= alloc 𝐶𝑑in let 𝑘= 𝜆[𝑖].,
add [ℎ;𝑎;𝑑; ℓ.,
[𝑖]] in parfor [0; length ℓ;𝑘] ; filter_compact [𝑎;𝑑] add : ∀𝛼:: ★.,
(𝛼→𝛿int)@𝛿1 →array(𝛼)@𝛿2 →𝛼→𝛼→𝛿() dedup : ∀𝛼:: ★.,
𝛼→𝛿′ int)@𝛿1 →𝛼→array(𝛼)@𝛿2 →𝛿array(𝛼)@𝛿 Fig.,
"Case study: deduplication of an array by concurrent hashing type 𝛼of the elements of the array to deduplicate, the calling timestamp 𝛿and two timestamps 𝛿1 and 𝛿2.",
The first argument is a closure of a hashing function on 𝛼that will be called at timestamp 𝛿.,
"The second argument is the hash set, a array(𝛼).",
"The third and fourth arguments are of type 𝛼and correspond to the dummy element and the element to insert, respectively.",
"Using this type for add, we are able to type-check dedup with the type shown in the lower part of Figure 18.",
"This type quantifies again over the type 𝛼of the elements of the array to deduplicate, and then quantifies over 𝛿, the calling timestamp, and 𝛿1 and 𝛿2, the (irrelevant) allocation timestamps of the first and third argument, respectively.",
"The first argument is a closure of a hashing function on 𝛼, that will be called at subsequent tasks 𝛿′.",
The second argument is a dummy element.,
The third argument is the array to deduplicate.,
"Again, type-checking dedup requires subtiming: the add function expects a hashing function at its calling timestamp 𝛿, whereas the supplied ℎis more general, because it is polymorphic with respect to its calling timestamp.",
We use subtiming (S-Abs and S-Inst) to convert the latter into the former.,
7 Related Work Disentanglement.,
The specific property we consider in this paper is based on the definition by Westrick et al.,
[2020] which was later formalized by Moine et al.,
"Most of the existing work on disentanglement considers structured fork-join parallel code, as we do in this paper.",
"More recently, Arora et al.",
"[2024] showed that disentanglement is applicable in a more general setting involving parallel futures, and specifically prove deadlock-freedom in this setting.",
We plan to investigate whether TypeDis could be extended to support futures.,
Verification of Disentanglement.,
Two approaches to check for and/or verify disentanglement have been proposed prior to TypeDis.,
"First, as currently implemented in the MaPLe compiler, the programmer can rely on a runtime entanglement detector [Westrick et al.",
This approach is similar in principle to dynamic race detection [Flanagan and Freund 2009].,
"In the case of entanglement, dynamic detection has been shown to have low overhead, making it suitable for automatic run-time management of entanglement [Arora et al.",
"However, run-time detection cannot guarantee disentanglement due to the inherent non-determinism of entanglement, which typically arises due to race conditions and may or may not occur in individual executions.",
"The second approach, as developed by Moine et al.",
"[2024], is full-blown static verification of disentanglement using a separation logic called DisLog, proven sound in Rocq.",
"This approach can be used to statically verify disentanglement for a wide variety of programs—for example, even for non- deterministic programs that utilize intricate lock-free data structures in shared memory.",
"However, static verification with DisLog is difficult, requiring significant effort even to verify small examples.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:26 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick Region-based Systems.",
TypeDis associates timestamp variables with values in their types.,
"Im- mediately, we note similarities with region-based type and effect systems [Grossman et al.",
2002; Tofte et al.,
2004; Tofte and Talpin 1997] which have also recently received attention in supporting parallelism [Elsman and Henriksen 2023].,
"The timestamps in our setting are somewhat analogous to regions, with parent-child relationships between timestamps and the up-pointer invariant of TypeDis bearing resemblance to the stack discipline of region-based memory management systems.",
"However, there are a number of key differences.",
"In region-based systems, allocations may occur within any region, and all values within a region are all deallocated at the same moment; one chal- lenge in such systems is statically predicting or conservatively bounding the lifetime of every value.",
"In contrast, in TypeDis, allocations only ever occur at the “current” timestamp, and timestamps tell you nothing about deallocation—every value in our approach is dynamically garbage collected.",
"Each timestamp in TypeDis is associated with a task within a nested fork-join task structure, and values with the same timestamp are all allocated by the same task (or one of its subtasks).",
Possible Worlds Type Systems.,
Our type system falls into what can broadly be categorized as a possible worlds type system.,
These type systems augment the typing judgment with world modalities (in our case timestamp variables 𝛿) that occur as syntactic objects in propositions (a.k.a.,
"types), and typing is then carried out relative to an accessibility relation (in our case the logical graph Δ).",
"While our work is the first to contribute a possible worlds type system for disentanglement, world modalities have been successfully used for other purposes.",
"In the context of fork-join parallelism, Muller et al.",
"[2017] employed world modalities to track priorities of tasks and guarantee absence of priority inversions, ensuring responsiveness and interactivity.",
While Muller et al.,
"[2017] also require their priorities to be partially ordered, as we require timestamps to be partially ordered, their priorities are fixed, whereas ours are not.",
"In the context of message-passing concurrency, world modalities have been employed to verify deadlock-freedom [Balzer et al.",
"2019], domain accessibility [Caires et al.",
"2019], and information flow control [Derakhshan et al.",
"2021, 2024].",
"This line of work not only differs in underlying computation model, considering a process calculus, but also adopts linear typing to control data races and non-determinism.",
"While disentanglement does not forbid races, adopting some form of linear typing may be an interesting avenue for future work, to admit even more disentangled programs as well-typed, e.g.",
those with down-pointers.,
Information Flow Control Type Systems.,
Information flow type systems [Sabelfeld and Myers 2003; Smith and Volpano 1998; Volpano et al.,
"1996] can also be viewed as representatives of possible worlds type systems, where modalities capture confidentiality (or integrity) and pc labels, and the accessibility relation is a lattice.",
"Typically, modalities can change by typing.",
"For example, when type-checking the branches of an if statement the pc label is increased to the join of its current value and the confidentiality label of the branching condition.",
"A similar phenomenon happens in TypeDis upon type-checking a fork, where the sibling threads are type-checked at a later timestamp.",
"Besides these similarities in techniques employed, the fundamental invariants preserved by type-checking are different.",
"In our setting it is the “no cross-pointers invariant”, whereas it is noninterference for IFC type systems.",
"As a result, the metatheory employed also differs: whereas we use a unary logical relation, noninterference demands a binary logical relation.",
"Such a binary logical relation for termination-insensitive noninterference in the context of a sequential, higher-order language with higher-order store, for example, has been contributed by Gregersen et al.",
"The authors develop an IFC type system, in the spirit of Flow Caml [Pottier and Simonet 2003; Simonet 2003], with label polymorphism, akin to our timestamp polymorphism.",
"Like our work, the authors use the semantic typing approach supported by the Iris separation logic framework.",
"Similarly, the authors support subtyping on labels, allowing a label to be raised in accordance with the lattice, akin to our subtiming, in accordance with the precedence relation.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:27 Type Systems for Parallelism and Concurrency.,
"There has been significant work on developing static techniques, especially type systems, to guarantee correctness and safety properties (such as race-freedom, deadlock-freedom, determinism, etc.)",
for parallel and concurrent programs.,
"For example, the idea of ownership [Clarke et al.",
1998; Dietl and Müller 2005; Müller 2002; Noble et al.,
1998] has been exploited to rule out races and deadlocks among threads [Boyapati et al.,
"2002, 2003; Boyapati and Rinard 2001].",
"Ownership is also enforced by linear type systems [Wadler 1990], which rule out races by construction and have been successfully employed in message-passing concurrency [Caires et al.",
2019; Wadler 2012].,
"The approach has then been popularized by Rust [Klabnik and Nichols 2023], in particular, focusing on statically restricting aliasing and mutability [Jung et al.",
"2018a], which in Rust takes the form of ownership and borrowing as well as reference-counted mutexes for maximal flexibility.",
"Recently flexible mode-based systems have been explored, as present in the work on DRFCaml [Georges et al.",
"2025], which exploits modes (extending Lorenzen et al.",
[2024]) to distinguish values that can and cannot be safely shared between threads.,
"Other systems leverage region-based techniques to restrict concurrent threads, ensuring safe disjoint access to the heap with minimal annotations [Milano et al.",
"2022], or leveraging explicit annotations to limit the set of permissible effects on shared parts of the heap [Bocchino Jr. et al.",
"Much of these related works focus on the hazards of concurrency: data races, race conditions, non-determinism, and similar issues.",
"Disentanglement (and by extension, TypeDis) focuses on an equally important but different issue, namely, the performance of parallel programs.",
"TypeDis in particular is designed to allow for unrestricted sharing of immutable data (as demonstrated in Section 2.4) mixed with disentangled sharing of mutable data (for example, in Section 6).",
"This support for data sharing is motivated by the implementation of efficient parallel algorithms, many of which rely upon access to shared memory with irregular and/or data-dependent access patterns, which are difficult to statically analyze for safety.",
"For example, Abdi et al.",
"[2024] find that many standard implementations of parallel algorithms are rejected by the Rust type system, yet these same implementations have been shown to be disentangled [Westrick et al.",
We consider one such implementation as a case study in Section 6 and confirm that it is typeable under TypeDis.,
"8 Conclusion and Future Work Disentanglement is an important property of parallel programs, which can in particular serve for improving performance.",
"This paper introduces TypeDis, a static type system that proves disentanglement.",
"TypeDis annotates types with timestamps, recording for each object the task that allocated it.",
"Moreover, TypeDis supports iso-recursive types, as well as type and timestamp polymorphism.",
TypeDis allows restamping the timestamps in types using a particular form of subtyping we dub subtiming.,
"This paper focuses on type-checking, that is, given a program annotated with types, checking if these types are valid.",
"We are currently working on a prototype type-checker, written in OCaml.",
"An immediate direction for future work is type inference, that is, generating a valid type for a program.",
"For future work, we plan to use the framework of Odersky et al.",
"[1999], which adapts Hindley-Milner to a system with constrained universal quantification.",
We believe subtiming and backtiming will be inferrable.,
"One challenging case will be mixing polymorphic recursion with par, which might require annotations in order to remain decidable (this is a known problem in region-based type systems [Tofte and Birkedal 1998]).",
Acknowledgments We thank Umut A. Acar for sharing his insights during early design discussions and helping us shape the context for this work.,
"We also thank Kashish Raimalani for reviewing an initial draft, and we thank the anonymous reviewers for their helpful comments.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:28 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick A Appendix A.1 The Kinding Judgment K-Var 𝑥:: 𝜅∈Γ Γ ⊢𝑥:: 𝜅 K-Unboxed Γ ⊢𝜏:: ★ K-At Γ ⊢𝜎:: ★ Γ ⊢𝜎@𝛿:: ★ K-Lam Γ ⊢𝜌:: 𝜅 Γ ⊢𝜆𝛿.",
"𝜌:: ⊲⊳ ⇒𝜅 K-App Γ ⊢𝜌:: ⊲⊳ ⇒𝜅 Γ ⊢𝜌𝛿:: 𝜅 K-TAbs 𝛼:: 𝜅, Γ ⊢𝜌:: ★ Γ ⊢∀𝛼:: 𝜅. 𝜌:: ★ K-Rec 𝛼:: ★, Γ ⊢𝜎:: ★ Γ ⊢𝜇𝛼.",
𝜎@𝛿:: ★ K-Array Γ ⊢𝜌:: ★ Γ ⊢array(𝜌) :: ★ K-Pair Γ ⊢𝜌1 :: ★ Γ ⊢𝜌2 :: ★ Γ ⊢(𝜌1 × 𝜌2) :: ★ K-Sum Γ ⊢𝜌1 :: ★ Γ ⊢𝜌2 :: ★ Γ ⊢(𝜌1 + 𝜌2) :: ★ K-Arrow (∀𝜌.,
𝜌∈®𝜌1 =⇒Γ ⊢𝜌:: ★) Γ ⊢𝜌2 :: ★ Γ ⊢∀®𝛿1 Δ.,
®𝜌1 →𝛿2 𝜌2 :: ★ Fig.,
"Kinding judgment Figure 19 presents the kinding judgment Γ ⊢𝜌:: 𝜅, asserting that type 𝜌has kind 𝜅considering the environment Γ. A.2 The veryPure Predicate VP-Val veryPure𝑣 VP-Abs veryPure (𝜇𝑓.",
"𝜆®𝑥.𝑒) VP-Var veryPure𝑥 VP-Prim veryPure𝑒1 veryPure𝑒2 𝑒1 ⊲⊳𝑒2 VP-Let veryPure𝑒1 veryPure𝑒2 let𝑥= 𝑒1 in𝑒2 VP-Pair veryPure𝑒1 veryPure𝑒2 (𝑒1,𝑒2) VP-Fold veryPure𝑒 veryPure fold𝑒 VP-Fold veryPure𝑒 veryPure unfold𝑒 VP-If veryPure𝑒1 veryPure𝑒2 veryPure𝑒3 if𝑒1 then𝑒2 else𝑒3 Fig.",
The veryPure predicate Figure 20 presents the veryPure predicate over an expression.,
"This predicate ensures that the expression does not contain any array allocation, load, store, par, projection, case, or function call.",
"A.3 Reachability Predicates R-Refl Δ ⊢𝛿≼𝛿 R-Cons (𝛿1,𝛿2) ∈Δ Δ ⊢𝛿2 ≼𝛿3 Δ ⊢𝛿1 ≼𝛿3 R-Logical ∀𝛿1 𝛿2.",
"(𝛿1,𝛿2) ∈Δ′ =⇒Δ ⊢𝛿1 ≼𝛿2 Δ ⊢Δ′ Fig.",
The reachability predicates Figure 21 presents the reachability predicates that appear in T-App and in Figure 13.,
R-Refl asserts that a timestamp can always reach itself.,
"R-Cons asserts that if there is an edge between 𝛿1 and 𝛿2 and if 𝛿2 can reach 𝛿3, then 𝛿1 can reach 𝛿3.",
R-Logical asserts that a logical graph Δ subsumes a logical graph Δ′ if every edge between 𝛿1 and 𝛿2 in Δ′ can be simulated in Δ. Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:29 A.4 The “Valid Variable” Judgment VA-Var 𝛼= 𝛼′ =⇒Δ ⊢𝛿1 ≼𝛿2 Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝛼′ VA-Base Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜏 VA-At Δ | 𝛼↦→𝛿1 ⊢𝛿𝜎 Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜎@𝛿 VA-TAbs 𝛼≠𝛼′ =⇒Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜌 Δ | 𝛼↦→𝛿1 ⊢𝛿2 ∀𝛼′ :: 𝜅. 𝜌 VA-Pair Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜌1 Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜌2 Δ | 𝛼↦→𝛿1 ⊢𝛿2 (𝜌1 × 𝜌2) VA-Sum Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜌1 Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜌2 Δ | 𝛼↦→𝛿1 ⊢𝛿2 (𝜌1 + 𝜌2) VA-TRec 𝛼∉fv(𝜌) \ {𝛼′} Δ | 𝛼↦→𝛿1 ⊢𝛿2 𝜇𝛼′.,
𝜎@𝛿 VA-Array 𝛼∉fv(𝜌) Δ | 𝛼↦→𝛿1 ⊢𝛿2 array(𝜌) VA-Abs 𝛼∉fv( ®𝜌′) ∪fv(𝜌′′) Δ | 𝛼↦→𝛿1 ⊢𝛿2 ∀®𝛿′ Δ′.,
®𝜌′ →𝛿𝑓𝜌′′ Fig.,
The “valid variable” judgment Figure 22 presents the “valid variable” judgment that is used for subtiming recursive types (S-Rec).,
"A.5 The Root Functions and Assertions answer ≜Timestamp𝛿| Unboxed | Nonsense rootfℎ𝑚𝜅𝜌 : fkind𝜅answer rootfℎ𝑚𝜅𝛼≜if𝑚(𝛼) = (𝜅, (_,𝑟)) then𝑟else Nonsense𝜅 rootfℎ𝑚★𝜏≜Unboxed rootfℎ𝑚★(𝜎@𝛿) ≜Timestamp𝛿 rootfℎ𝑚( ⊲⊳ ⇒𝜅) (𝜆𝛿.",
rootf ([𝛿:= 𝑡]ℎ) 𝑚𝜅𝜌 rootfℎ𝑚𝜅(𝜌𝛿) ≜(rootfℎ𝑚( ⊲⊳ ⇒𝜅) 𝜌) (ℎ(𝛿)) rootfℎ𝑚★(∀𝛼:: 𝜅. 𝜌) ≜rootfℎ([𝛼:= Nonsense𝜅]𝑚) ★𝜌 rootfℎ𝑚★(𝜇𝛼.,
𝜎@𝛿) ≜Timestamp𝛿 root 𝜌≼ℎ 𝑚𝛿≜match (rootfℎ𝑚★𝜌) with | Unboxed ⇒⌜𝑇𝑟𝑢𝑒⌝ | Nonsense ⇒⌜𝐹𝑎𝑙𝑠𝑒⌝ | Timestamp𝛿′ ⇒ℎ(𝛿′) ≼ℎ(𝛿) Fig.,
"Root-related functions and assertions Figure 23 presents the rootf function, expecting a timestamp mapping ℎ, a type mapping 𝑚, a kind 𝜅and a type 𝜌, and produces a function expecting 𝜅timestamps and returning an “answer”, representing the root timestamp of 𝜌.",
"An answer is either a timestamp, Unboxed to indicate an unboxed type, of Nonsense if the type has no sensible root timestamp (for example, ∀𝛼:: 𝜅. 𝛼).",
"In this definition, we write Nonsense𝜅the function expecting 𝜅arguments and returning Nonsense.",
"The assertion root 𝜌≼ℎ 𝑚𝛿, also presented in Figure 23 matches root timestamp of 𝜌.",
"If it is unboxed, the assertion is true, if it is nonsensical, the assertion is false, and if it is a regular timestamp 𝛿′, then ℎ(𝛿′) must precede ℎ(𝛿).",
A.6 Definition of our Logical Relations Figure 24 presents the logical relations we define.,
"Formally, we define the (meta-type-level) function fkind𝜅𝐴producing a function waiting for 𝜅timestamps and returning something of type 𝐴.",
This function is defined by induction over the kind 𝜅. Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:30 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick fkind ★𝑎≜𝐴 fkind ( ⊲⊳ ⇒𝜅) 𝐴≜T →fkind𝜅𝐴 J𝜌Kℎ 𝑚𝜅 : fkind𝜅(V →iProp) J𝛼Kℎ 𝑚𝜅≜ ( Ψ ∗𝜅 𝜅𝑟 if 𝑚(𝛼) = (𝜅, (Ψ,𝑟)) ⊥𝜅 else J𝜏Kℎ 𝑚★≜𝜆𝑣.",
"⌜𝜏= () ∧𝑣= ()⌝∨⌜𝜏= bool ∧𝑣∈{true, false}⌝∨⌜𝜏= int ∧𝑣∈Z⌝ J𝜎@𝛿Kℎ 𝑚★≜𝜆𝑣.",
𝑣 ℎ(𝛿) ∗JJ𝜎KKℎ 𝑚𝛿𝑣 J𝜇𝛼.,
𝜎@𝛿Kℎ 𝑚★≜𝜇(Ψ : V →iProp).,
"⌜𝑣= vfold𝑤⌝∗𝑤 ℎ(𝛿) ∗⊲JJ𝜎KKℎ [𝛼:=(★,(Ψ,ℎ(𝛿)))]𝑚𝛿𝑣 J∀𝛼:: 𝜅. 𝜌Kℎ 𝑚★≜𝜆𝑣.",
∀(Ψ : fkind𝜅(V →iProp)) (𝑟: fkind𝜅T).,
"⌜proper𝜅Ψ ∧regular𝜅𝑟⌝−∗J𝜌Kℎ [𝛼:=(𝜅,(Ψ,𝑟))]𝑚★𝑣 J𝜆𝛿.",
𝜌Kℎ 𝑚( ⊲⊳ ⇒𝜅) ≜𝜆𝑡.,
J𝜌K[𝛿:=𝑡]ℎ 𝑚 𝜅 J𝜌𝛿Kℎ 𝑚𝜅≜J𝜌Kℎ 𝑚( ⊲⊳ ⇒𝜅) ℎ(𝛿) L𝜌Mℎ 𝑚≜𝜆𝛿𝑣.,
root 𝜌≼ℎ 𝑚𝛿∗J𝜌Kℎ 𝑚★𝑣 JJ𝜎KKℎ 𝑚 : T →V →iProp JJarray(𝜌)KKℎ 𝑚≜𝜆𝛿𝑣.,
⌜𝑣= ℓ⌝∗∃®𝑤.,
ℓ↦→®𝑤∗∗𝑣′∈®𝑤(L𝜌Mℎ 𝑚𝛿𝑣′) JJ(𝜌1 × 𝜌2)KKℎ 𝑚≜𝜆𝛿𝑣.,
"⌜𝑣= ℓ⌝∗ℓ↦→ (𝑣1, 𝑣2) ∗L𝜌1Mℎ 𝑚𝛿𝑣1 ∗L𝜌2Mℎ 𝑚𝛿𝑣2 JJ(𝜌1 + 𝜌2)KKℎ 𝑚≜𝜆𝛿𝑣.",
⌜𝑣= ℓ⌝∗ (ℓ↦→ inj1 𝑣′ ∗L𝜌1Mℎ 𝑚𝛿𝑣′) ∨(ℓ↦→ inj2 𝑣′ ∗L𝜌2Mℎ 𝑚𝛿𝑣′) JJ∀®𝛿1 Δ1.,
®𝜌1 →𝛿𝑓𝜌2KKℎ 𝑚≜𝜆𝛿𝑣.,
"⌜| ®𝛿1| = |®𝑡| ∧| ®𝜌1| = | ®𝑤|⌝−∗ letℎ′ = [ ®𝛿1 := ®𝑡]ℎin ℎ(𝛿) ≼ℎ′(𝛿𝑓) −∗JΔ1Kℎ′ −∗∗𝜌∈®𝜌1, 𝑣′∈®𝑤(L𝜌Mℎ′ 𝑚𝑣′) −∗ ∀𝑡′.",
"𝑡′ ≈ℎ′(𝛿𝑓) −∗wp ⟨𝑡′, 𝑣®𝑤⟩{𝜆𝑣′.",
L𝜌2Mℎ′ 𝑚𝑣′} Fig.,
"The interpretation of types More precisely, Figure 24 presents the type relation and the boxed type relation.",
"The type relation J𝜌Kℎ 𝑚𝜅produces a function waiting for 𝜅timestamps, a value 𝑣, and captures that 𝑣is of type 𝜌, within the timestamp mapping ℎand type mapping 𝑚.",
"The boxed type relation JJ𝜎KKℎ 𝑚 produces a predicate over a timestamp 𝛿and a value 𝑣, capturing that 𝑣is of type 𝜎allocated at 𝛿, within the timestamp mapping ℎand type mapping 𝑚.",
The type relation and the boxed type relation are defined by mutual induction over their type argument.,
"The omitted cases are all sent to ⊥𝜅, the always false predicate ignoring its 𝜅arguments.",
Interpretation of types.,
Let us first present the relation J𝜌Kℎ 𝑚𝜅.,
"If 𝜌is a variable 𝛼, then 𝛼must have kind 𝜅in the type mapping 𝑚, linked with predicate Ψ and timestamp function 𝑟.",
The relation returns the predicate Ψ ∗𝜅 𝜅𝑟.,
The operator ∗𝜅lifts the separating conjunction to predicated in fkind𝜅(V →iProp) by distributing 𝜅timestamp arguments and a value to Ψ and 𝜅𝑟.,
"The predicate 𝜅𝑟is of type fkind𝜅(V →iProp); it feeds 𝜅 timestamps to 𝑟, and asserts that the value argument was allocated before the result of 𝑟.",
"For example, in the particular case of 𝜅= ⊲⊳ ⇒★, we have that Ψ ∗𝜅 𝜅𝑟= 𝜆𝛿𝑣.",
Ψ𝛿𝑣∗𝑣 (𝑟𝛿).,
"If 𝜌is a base type 𝜏, the kind must be the base kind ★, and the relation binds a value which must correspond to the particular base type under consideration.",
"If 𝜌is a boxed type 𝜎@𝜌, the kind must be ★, and the relation binds a value 𝑣which must have been allocated before ℎ(𝛿), the timestamp associated to 𝛿in ℎ.",
The relation also ensures that 𝑣 recursively satisfies the interpretation of 𝜎. Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
TypeDis: A Type System for Disentanglement 13:31 If 𝜌is a recursive type 𝜇𝛼.,
"𝜎@𝛿, the kind must be ★, and the relation is expressed as a guarded fixed-point.",
"Intuitively, the predicate Ψ, from a value to iProp, captures the interpretation of the recursive type itself.",
The interpretation binds a value 𝑣and asserts that it is of the form vfold𝑤.,
"The interpretation is then similar to a boxed type: 𝑤must have been allocated before ℎ(𝛿) and be in relation with the interpretation of 𝜎, with a type environment updated to bind 𝛼to Ψ as well as the root timestamp ℎ(𝛿).",
"If 𝜌is a type abstraction ∀𝛼:: 𝜅. 𝜌, the kind must be ★, and the relation binds a value 𝑣.",
"The relation universally quantifies over the predicate Ψ and the timestamp function 𝑟, which will be instantiated during the T-TApp rule.",
Both Ψ and 𝑟are constrained.,
The property proper𝜅Ψ captures that any timestamp parameter of Ψ can be replaced by an equivalent one.,
The property regular𝜅𝑟 captures that the function 𝑟either ignores all its arguments or returns one of them.,
These two properties are needed in order to prove that T-Par is sound.,
"The relation then calls itself recursively on 𝜌, augmenting the type mapping by associating 𝛼to its kind 𝜅and the pair of Ψ and 𝑟.",
If 𝜌is a timestamp abstraction 𝜆𝛿.,
"𝜌, the kind must be of the form ⊲⊳ ⇒𝜅, and the relation expands to a function waiting for a timestamp 𝛿and adding it to the timestamp mapping ℎ.",
"If 𝜌is a timestamp application 𝜌𝛿at some kind 𝜅, then the relation applies the timestamp ℎ(𝛿) to the interpretation of 𝜌at kind ⊲⊳ ⇒𝜅.",
Interpretation of boxed types.,
"The enriched type interpretation L𝜌Mℎ 𝑚, defined next in Figure 24, is a predicate over a timestamp 𝛿and a value 𝑣.",
It asserts that the root timestamp of 𝜌comes before 𝛿 and that 𝑣is in relation with the interpretation of 𝜌.,
"This wrapper in used for the interpretation of boxed types, which we present next.",
The interpretation of boxed types is written JJ𝜎KKℎ 𝑚and is a predicate over a timestamp variable 𝛿and a value 𝑣.,
"If 𝜎is an array array(𝜌), then 𝑣must be a location ℓ, such that ℓpoints-to an array ®𝑤and that for each value 𝑣′ in ®𝑤is in relation with the enriched interpretation of 𝜌.",
"The points-to assertion and the relation on the values of the array appears inside an invariant, ensuring their persistence.",
"If 𝜎is a pair (𝜌1 × 𝜌2), then 𝑣must be a location ℓpointing to a pair of values (𝑣1, 𝑣2) such that 𝑣1 (resp.",
𝑣2) is in relation with the enriched interpretation of 𝜌1 (resp.,
The sum case is similar.,
If 𝜎is an arrow ∀®𝛿1 Δ1.,
"®𝜌1 →𝛿𝑓𝜌2, then the interpretation quantifies over the list of timestamp arguments ®𝑡and the list of arguments of the function ®𝑤, which must both have the correct length.",
"The relation then defines ℎ′, the new timestamp environment, being ℎwhere ®𝛿1 are instantiated with ®𝑡.",
"The relation next requires that the allocation timestamp ℎ(𝛿) precedes the timestamp of the caller ℎ′(𝛿𝑓), and that every value in ®𝑤is of the correct type.",
"Last, the relation requires that for any timestamp equivalent to ℎ′(𝛿𝑓), the WP of the function call holds, and that the returned value is in relation with the enriched interpretation of the return type 𝜌2.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:32 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick References Javad Abdi, Gilead Posluns, Guozheng Zhang, Boxuan Wang, and Mark C. Jeffrey.",
When Is Parallelism Fearless and Zero-Cost with Rust?.,
"In Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA 2024, Nantes, France, June 17-21, 2024, Kunal Agrawal and Erez Petrank (Eds.).",
"ACM, 27–40.",
"doi:10.1145/3626183.3659966 Umut A. Acar, Jatin Arora, Matthew Fluet, Ram Raghunathan, Sam Westrick, and Rohan Yadav.",
MPL: A high- performance compiler for Parallel ML.,
"https://github.com/MPLLang/mpl Umut A. Acar, Guy E. Blelloch, Matthew Fluet, Stefan K. Muller, and Ram Raghunathan.",
Coupling Memory and Computation for Locality Management.,
"In 1st Summit on Advances in Programming Languages, SNAPL 2015, May 3-6, 2015, Asilomar, California, USA (LIPIcs, Vol.",
"32), Thomas Ball, Rastislav Bodík, Shriram Krishnamurthi, Benjamin S. Lerner, and Greg Morrisett (Eds.).",
"Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 1–14.",
"doi:10.4230/LIPICS.SNAPL.2015.1 Umut A. Acar, Arthur Charguéraud, Mike Rainey, and Filip Sieczkowski.",
Dag-calculus: a calculus for parallel computation.,
In International Conference on Functional Programming (ICFP).,
https://doi.org/10.1145/2951913.,
"2951946 Daniel Anderson, Guy E. Blelloch, Laxman Dhulipala, Magdalen Dobson, and Yihan Sun.",
"The problem-based benchmark suite (PBBS), V2.",
"In PPoPP ’22: 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Seoul, Republic of Korea, April 2 - 6, 2022, Jaejin Lee, Kunal Agrawal, and Michael F. Spear (Eds.).",
"ACM, 445–447.",
doi:10.1145/3503221.3508422 Andrew W. Appel.,
Compiling with Continuations.,
Cambridge University Press.,
"http://www.cambridge.org/ 9780521033114 Jatin Arora, Stefan K. Muller, and Umut A. Acar.",
"Disentanglement with Futures, State, and Interaction.",
ACM Program.,
"8, POPL (2024), 1569–1599.",
"doi:10.1145/3632895 Jatin Arora, Sam Westrick, and Umut A. Acar.",
Provably space-efficient parallel functional programming.,
ACM Program.,
"5, POPL (2021), 1–33.",
"doi:10.1145/3434299 Jatin Arora, Sam Westrick, and Umut A. Acar.",
Efficient Parallel Functional Programming with Effects.,
ACM Program.,
"7, PLDI (2023), 1558–1583.",
"doi:10.1145/3591284 Stephanie Balzer, Bernardo Toninho, and Frank Pfenning.",
Manifest Deadlock-Freedom for Shared Session Types.,
"In 28th European Symposium on Programming (ESOP) (Lecture Notes in Computer Science, Vol.",
"Springer, 611–639.",
doi:10.1007/978-3-030-17184-1_22 Henk P. Barendregt.,
"The Lambda Calculus, Its Syntax and Semantics.",
"http://www.elsevier.com/wps/find/ bookdescription.cws_home/501727/description Robert L. Bocchino Jr., Vikram S. Adve, Danny Dig, Sarita V. Adve, Stephen Heumann, Rakesh Komuravelli, Jeffrey Overbey, Patrick Simmons, Hyojin Sung, and Mohsen Vakilian.",
A type and effect system for deterministic parallel Java.,
"In Proceedings of the 24th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2009, October 25-29, 2009, Orlando, Florida, USA, Shail Arora and Gary T. Leavens (Eds.).",
"ACM, 97–116.",
"doi:10.1145/1640089.1640097 Chandrasekhar Boyapati, Robert Lee, and Martin C. Rinard.",
Ownership types for safe programming: preventing data races and deadlocks.,
"In ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA).",
"ACM, 211–230.",
"doi:10.1145/582419.582440 Chandrasekhar Boyapati, Barbara Liskov, and Liuba Shrira.",
Ownership types for object encapsulation.,
In 30th SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL).,
"ACM, 213–223.",
doi:10.1145/604131.604156 Chandrasekhar Boyapati and Martin C. Rinard.,
A Parameterized Type System for Race-Free Java Programs.,
"In ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA).",
"ACM, 56–69.",
"doi:10.1145/504282.504287 Luís Caires, Jorge A. Pérez, Frank Pfenning, and Bernardo Toninho.",
Domain-Aware Session Types.,
"In 30th International Conference on Concurrency Theory (CONCUR) (LIPIcs, Vol.",
"Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 39:1–39:17. doi:10.4230/LIPICS.CONCUR.2019.39 David G. Clarke, John Potter, and James Noble.",
Ownership Types for Flexible Alias Protection.,
"In ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA).",
"ACM, 48–64.",
"doi:10.1145/ 286936.286947 Robert L. Constable, Stuart F. Allen, Mark Bromley, Rance Cleaveland, J. F. Cremer, Robert Harper, Douglas J. Howe, Todd B. Knoblock, Nax Paul Mendler, Prakash Panangaden, James T. Sasaki, and Scott F. Smith.",
Implementing Mathematics with the Nuprl Proof Development System.,
Prentice Hall.,
http://dl.acm.org/citation.cfm?id=10510 Paulo Emílio de Vilhena.,
Proof of Programs with Effect Handlers.,
Université Paris Cité.,
https://inria.hal.,
"science/tel-03891381 Farzaneh Derakhshan, Stephanie Balzer, and Limin Jia.",
Session Logical Relations for Noninterference.,
In 36th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS).,
"IEEE Computer Society, 1–14.",
doi:10.1109/LICS52264.2021.,
9470654 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"TypeDis: A Type System for Disentanglement 13:33 Farzaneh Derakhshan, Stephanie Balzer, and Yue Yao.",
Regrading Policies for Flexible Information Flow Control in Session-Typed Concurrency.,
"In 38th European Conference on Object-Oriented Programming (ECOOP) (LIPIcs, Vol.",
"Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 11:1–11:29. doi:10.4230/LIPICS.ECOOP.2024.11 Werner Dietl and Peter Müller.",
Universes: Lightweight Ownership for JML.,
"Journal of Object Technology 4, 8 (2005), 5–32.",
doi:10.5381/JOT.2005.4.8.A1 Martin Elsman and Troels Henriksen.,
Parallelism in a Region Inference Context.,
ACM Program.,
"7, PLDI (2023), 884–906.",
doi:10.1145/3591256 Cormac Flanagan and Stephen N. Freund.,
FastTrack: efficient and precise dynamic race detection.,
In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI).,
"ACM, 121–133.",
"doi:10.1145/1542476.1542490 Aïna Linn Georges, Benjamin Peters, Laila Elbeheiry, Leo White, Stephen Dolan, Richard A. Eisenberg, Chris Casinghino, François Pottier, and Derek Dreyer.",
Data Race Freedom à la Mode.,
ACM Program.,
"9, POPL (2025), 656–686.",
doi:10.1145/3704859 Jean-Yves Girard.,
Interprétation fonctionnelle et élimination des coupures de l’arithmétique d’ordre supérieur.,
Thèse d’État.,
"Université Paris 7. https://girard.perso.math.cnrs.fr/These.pdf Simon Oddershede Gregersen, Johan Bay, Amin Timany, and Lars Birkedal.",
Mechanized logical relations for termination-insensitive noninterference.,
ACM Program.,
"5, POPL (2021), 1–29.",
"doi:10.1145/3434291 Dan Grossman, J. Gregory Morrisett, Trevor Jim, Michael W. Hicks, Yanling Wang, and James Cheney.",
Region-Based Memory Management in Cyclone.,
"In Proceedings of the 2002 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), Berlin, Germany, June 17-19, 2002, Jens Knoop and Laurie J. Hendren (Eds.).",
"ACM, 282–293.",
"doi:10.1145/512529.512563 Adrien Guatto, Sam Westrick, Ram Raghunathan, Umut A. Acar, and Matthew Fluet.",
Hierarchical memory management for mutable state.,
"In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 2018, Vienna, Austria, February 24-28, 2018, Andreas Krall and Thomas R. Gross (Eds.).",
"ACM, 81–93.",
"doi:10.1145/ 3178487.3178494 Ralf Jung, Jacques-Henri Jourdan, Robbert Krebbers, and Derek Dreyer.",
RustBelt: Securing the Foundations of the Rust Programming Language.,
"Proceedings of the ACM on Programming Languages 2, POPL (2018), 66:1–66:34. https://people.mpi-sws.org/~dreyer/papers/rustbelt/paper.pdf Ralf Jung, Robbert Krebbers, Jacques-Henri Jourdan, Aleš Bizjak, Lars Birkedal, and Derek Dreyer.",
Iris from the ground up: A modular foundation for higher-order concurrent separation logic.,
"Journal of Functional Programming 28 (2018), e20.",
https://people.mpi-sws.org/~dreyer/papers/iris-ground-up/paper.pdf Steve Klabnik and Carol Nichols.,
The Rust programming language.,
No Starch Press.,
Donald E. Knuth.,
"The Art of Computer Programming, Volume 3: (2nd Ed.)",
Sorting and Searching.,
"Addison Wesley Longman Publishing Co., Inc., USA.",
Peter J. Landin.,
The Mechanical Evaluation of Expressions.,
"Computer Journal 6, 4 (Jan. 1964), 308–320.",
"Anton Lorenzen, Leo White, Stephen Dolan, Richard A. Eisenberg, and Sam Lindley.",
Oxidizing OCaml with Modal Memory Management.,
ACM Program.,
"8, ICFP (2024), 485–514.",
doi:10.1145/3674642 Per Martin-Löf.,
Constructive Mathematics and Computer Programming.,
"In Logic, Methodology and Philosophy of Science VI.",
"Studies in Logic and the Foundations of Mathematics, Vol.",
"Elsevier, 153–175.",
"doi:10.1016/S0049-237X(09)70189-2 Mae Milano, Joshua Turcotti, and Andrew C. Myers.",
A flexible type system for fearless concurrency.,
"In PLDI ’22: 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation, San Diego, CA, USA, June 13 - 17, 2022, Ranjit Jhala and Isil Dillig (Eds.).",
"ACM, 458–473.",
doi:10.1145/3519939.3523443 Robin Milner.,
A Theory of Type Polymorphism in Programming.,
System Sci.,
"17, 3 (Dec. 1978), 348–375.",
"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.5276 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick.",
TypeDis: A Type System for Disentanglement (Artifact).,
"doi:10.5281/zenodo.17336385 Alexandre Moine, Sam Westrick, and Stephanie Balzer.",
DisLog: A Separation Logic for Disentanglement.,
ACM Program.,
"8, POPL, Article 11 (Jan. 2024), 30 pages.",
doi:10.1145/3632853 Peter Müller.,
Modular Specification and Verification of Object-Oriented Programs.,
"Lecture Notes in Computer Science, Vol.",
"doi:10.1007/3-540-45651-1 Stefan K. Muller, Umut A. Acar, and Robert Harper.",
Responsive parallel computation: bridging competitive and cooperative threading.,
In 38th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI).,
"ACM, 677–692.",
"doi:10.1145/3062341.3062370 James Noble, Jan Vitek, and John Potter.",
Flexible Alias Protection.,
"In 12th European Conference on Object-Oriented Programming (ECOOP) (Lecture Notes in Computer Science, Vol.",
"Springer, 158–185.",
"doi:10.1007/BFB0054091 Martin Odersky, Martin Sulzmann, and Martin Wehr.",
Type Inference with Constrained Types.,
"Theory and Practice of Object Systems 5, 1 (1999), 35–55.",
https://doi.org/10.1002/(SICI)1096-9942(199901/03)5:1%3C35::AID-TAPO4%3E3.0.CO;2- 4 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"13:34 Alexandre Moine, Stephanie Balzer, Alex Xu, and Sam Westrick Benjamin C. Pierce.",
Types and Programming Languages.,
Andrew M. Pitts and Ian Stark.,
Operational Reasoning for Functions with Local State.,
"Higher Order Operational Techniques in Semantics (HOOTS) (1998), 227–273.",
Gordon D. Plotkin.,
Lambda-definability and logical relations.,
Technical Report.,
University of Edinburgh.,
François Pottier and Vincent Simonet.,
Information flow inference for ML.,
"ACM Transactions on Programming Languages and Systems (TOPLAS) 25, 1 (2003), 117–158.",
"doi:10.1145/596980.596983 Ram Raghunathan, Stefan K. Muller, Umut A. Acar, and Guy E. Blelloch.",
Hierarchical memory management for parallel programs.,
"In Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming, ICFP 2016, Nara, Japan, September 18-22, 2016, Jacques Garrigue, Gabriele Keller, and Eijiro Sumii (Eds.).",
"ACM, 392–406.",
doi:10.1145/2951913.2951935 Andrei Sabelfeld and Andrew C. Myers.,
Language-Based Information-Flow Security.,
IEEE J. Sel.,
Areas Commun.,
"21, 1 (2003), 5–19.",
"Julian Shun, Guy E. Blelloch, Jeremy T. Fineman, Phillip B. Gibbons, Aapo Kyrola, Harsha Vardhan Simhadri, and Kanat Tangwongsan.",
Brief announcement: the problem based benchmark suite.,
"In 24th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA ’12, Pittsburgh, PA, USA, June 25-27, 2012, Guy E. Blelloch and Maurice Herlihy (Eds.).",
"ACM, 68–70.",
doi:10.1145/2312005.2312018 Vincent Simonet.,
Flow Caml in a Nutshell.,
"In 1st APPSEM-II Workshop, Graham Hutton (Ed.).",
Geoffrey Smith and Dennis M. Volpano.,
Secure Information Flow in a Multi-Threaded Imperative Language.,
"ACM, 355–364.",
Richard Statman.,
Logical Relations and the Typed 𝜆-calculus.,
"Information and Control 65, 2/3 (1985), 85–97.",
1016/S0019-9958(85)80001-2 William W. Tait.,
Intensional Interpretations of Functionals of Finite Type I.,
"The Journal of Symbolic Logic 32, 2 (1967), 198–212.",
"http://www.jstor.org/stable/2271658 Amin Timany, Robbert Krebbers, Derek Dreyer, and Lars Birkedal.",
A Logical Approach to Type Soundness.,
"J. ACM 71, 6, Article 40 (Nov. 2024), 75 pages.",
doi:10.1145/3676954 Mads Tofte and Lars Birkedal.,
A Region Inference Algorithm.,
"20, 4 (1998), 724–767.",
"doi:10.1145/291891.291894 Mads Tofte, Lars Birkedal, Martin Elsman, and Niels Hallenberg.",
A Retrospective on Region-Based Memory Management.,
"Higher-Order and Symbolic Computation 17, 3 (Sept. 2004), 245–265.",
https://doi.org/10.1023/B: LISP.0000029446.78563.a4 Mads Tofte and Jean-Pierre Talpin.,
Region-based memory management.,
"Information and Computation 132, 2 (1997), 109–176.",
http://www.irisa.fr/prive/talpin/papers/ic97.pdf VerifyThis.,
Challenge 3 - The World’s Simplest Lock-Free Hash Set.,
https://ethz.ch/content/dam/ethz/special- interest/infk/chair-program-method/pm/documents/Verify%20This/Challenges2022/verifyThis2022-challenge3.pdf Simon Friis Vindum and Lars Birkedal.,
Contextual refinement of the Michael-Scott queue.,
In Certified Programs and Proofs (CPP).,
"https://cs.au.dk/~birke/papers/2021-ms-queue-final.pdf Dennis M. Volpano, Cynthia E. Irvine, and Geoffrey Smith.",
A Sound Type System for Secure Flow Analysis.,
"4, 2/3 (1996), 167–188.",
Philip Wadler.,
Linear Types Can Change the World!.,
"In IFIP Working Group 2.2, 2.3 on Programming Concepts and Methods.",
"North-Holland, 561.",
Philip Wadler.,
Propositions as Sessions.,
In ACM SIGPLAN International Conference on Functional Programming (ICFP).,
"ACM, 273–286.",
doi:10.1145/2364527.2364568 Sam Westrick.,
Efficient and Scalable Parallel Functional Programming through Disentanglement.,
D. Dissertation.,
"Department of Computer Science, Carnegie Mellon University.",
"Sam Westrick, Jatin Arora, and Umut A. Acar.",
Entanglement Detection with Near-Zero Cost.,
ACM Program.,
"6, ICFP, Article 115 (aug 2022), 32 pages.",
"doi:10.1145/3547646 Sam Westrick, Rohan Yadav, Matthew Fluet, and Umut A. Acar.",
Disentanglement in Nested-Parallel Programs.,
ACM Program.,
"4, POPL, Article 47 (jan 2020), 32 pages.",
doi:10.1145/3371115 Andrew K. Wright.,
Simple Imperative Polymorphism.,
"Lisp and Symbolic Computation 8, 4 (Dec. 1995), 343–356.",
http://www.cs.rice.edu/CS/PLT/Publications/Scheme/lasc95-w.ps.gz Received 2025-07-10; accepted 2025-11-06 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 13.",
Publication date: January 2026.,
"Quantum Circuit Equivalence Checking: A Tractable Bridge From Unitary to Hybrid Circuits Jérome Ricciardi1,2[0009−0001−7433−8384], Sébastien Bardin1[0000−0002−6509−3506], Christophe Chareton1[0000−0001−7113−563X], and Benoît Valiron2[0000−0002−1008−5605] 1 Université Paris-Saclay, CEA, List, F-91120, Palaiseau, France, first.name@cea.fr 2 Université Paris-Saclay, CNRS, CentraleSupélec, ENS Paris-Saclay, Inria, Laboratoire Méthodes Formelles, 91190, Gif-sur-Yvette, France, benoit.valiron@lmf.cnrs.fr Abstract.",
"Equivalence checking of hybrid quantum circuits is of pri- mary importance, given that quantum circuit transformations are om- nipresent along the quantum compiler chain.",
"While some approaches exist for automating this task, most focus on the simple case of uni- tary circuits.",
"At the same time, real quantum computing requires hybrid circuits equipped with measurement operators.",
"Moreover, the few ap- proaches targeting the hybrid case are limited to a restricted class of problems.",
We propose tackling the Quantum Hybrid Circuit Equiva- lence Checking problem through lifting unitary circuit verification using a transformation known as deferred measurement.,
"We show that this approach alone significantly outperforms prior work, and that, with the addition of specific unitary-level techniques we call separation and pro- jection, it can handle much larger classes of hybrid circuit equivalence problems.",
"We have implemented and evaluated our method over standard circuit transformations such as teleportation, one-way measurement, or the IBM Qiskit compiler, demonstrating its promises.",
"As a side finding, we have identified and reported several unexpected behaviours with the Qiskit compiler.",
"Keywords: Hybrid classical/quantum circuits, Automated verification, Formal certification.",
1 Introduction Quantum computation is a realm where information is stored on the state of ob- jects governed by the laws of quantum physics [33].,
"This model of computation is believed to provide important speedup for many applications, ranging from high-performance computing to optimisation.",
"In recent years, quantum comput- ers have become a near-term physical, industrial, and economic reality.",
"Compared to classical information, quantum data is very peculiar: it cannot be duplicated, and reading is a probabilistic operation done through measure- ment, changing the global state of the memory.",
"Unlike classical data, whose arXiv:2511.22523v1 [quant-ph] 27 Nov 2025 2 J. Ricciardi, et al.",
"typical semantics is based on discrete data structures, quantum information is modelled with vectorial structures in Hilbert spaces.",
"Because they manipulate data structures radically different from the classical case, quantum computers require specific developments at every development stage (user languages, veri- fication, optimisation, compilation, etc.)",
"The typical execution flow for quantum programs relies on the notion of quantum coprocessor: the quantum memory is stored in an external device, seen as a coprocessor to a CPU, similar to what happens for a GPU, for instance.",
"The quantum coprocessor keeps the memory alive, while the main computa- tion occurs on the classical CPU.",
"A quantum process, therefore, consists of a (classical) interaction with the coprocessor by sending a series of instructions to initialise, update (with quantum, unitary gates), and measure the quantum memory to retrieve classical information.",
A measurement’s result is a classical piece of information that might be stored for later use or discarded.,
Such a series of instructions is represented by a quantum circuit.,
"A circuit can generally mix qubit initialisations, unitaries, measurements, and discards.",
We call unitary (or purely quantum) a circuit that consists only of quantum gates.,
"A circuit with unitary gates, initialisation, measurements, discards, and classically controlled instruction is called a hybrid circuit.",
"Known as the deferred measurement principle, a result from folklore states that measurements and discards can be postponed to the end of the computa- tion.",
This principle is partly why circuits found in most quantum algorithms are given uniquely in terms of unitary gates: measurement can always be thought of happening at the end of the computation.,
"However, from an implementation point of view, it makes sense to consider the hybrid case as many quantum pro- cesses, such as repeat-until-success, measurement-based quantum computation, or optimisation techniques, rely on it.",
Quantum circuit transformations turn a circuit into another (equivalent) quantum circuit.,
"Such techniques are key components of the quantum software stack, whether for optimisation, adaptation to hardware capabilities in terms of qubits and connectivity, error correction, circuit robustification, one-way mea- surement.",
Compilers like Qiskit do propose several transformation passes.,
"As these passes are omnipresent between the programmer and the quantum hard- ware, checking their correctness is paramount.",
"While we could envision quantum compilers fully certified in interactive theorem provers, another approach involves designing dedicated automated circuit equivalence checkers [30].",
This paper focuses on the automated equivalence verification problem arising from quantum circuit transformation: how to verify that two quantum circuits are functionally equivalent.,
"In particular, we focus on verifying circuits involv- ing initialisation, measurements, discards, and classically controlled instructions: hybrid circuits.",
Challenge with hybrid circuits equivalence verification.,
"Current methodologies for automatic equivalence checking predominantly focus on purely quantum, unitary circuits [1,2,29,36].",
This restriction is becoming increasingly unrealistic as physical chips progress.,
"Indeed, the current trend in the design of quantum Quantum Circuit Equivalence Checking 3 compilation toolchains tends to split the purely quantum processes into smaller components, to make them more robust to noise.",
"Moreover, even pure quantum primitives require hybridisation and classical control for error detection and cor- rection.",
"Compared to purely quantum circuits, the formalisation of hybrid computa- tions brings several extra difficulties.",
"A first difficulty is that hybrid quantum computation is, by nature, non- deterministic: the functional behaviour of a quantum program is a branching probabilistic structure instead of a linear trace.",
Quantum computation can then be regarded as a strict superset of probabilistic computation—a field where the formal analysis of programs is still a research question [6].,
A second difficulty is that a quantum program manipulates both quantum and classical registers.,
"Some of these registers might contain “garbage”, i.e.",
data that should not be considered as output: a relevant equivalence notion should not consider them.,
"We talk of partial equivalence when the state equivalence is evaluated only up to non-discarded registers, an essential feature to model a program’s state and its observable behaviour.",
"However, discarding quantum information is not innocuous, as it corresponds to tracing it out: discarding corresponds to a measurement and is then a probabilistic process.",
"Therefore, the question of partial equivalence of quantum programs is not trivial to define.",
"Finally, the measurement operation induces a deep conceptual shift from a (quantum) deterministic process purely modelled in linear algebra to a stochastic process acting on a mix of quantum states and usual data structures (Boolean values, integers, etc).",
"Designing a unified, tractable mechanism for the equivalence of hybrid quantum circuits is therefore a major challenge for formal methods.",
Contributions.,
"To address this challenge, this paper presents an approach based on two main ingredients: (1) a classification of hybrid circuit equivalence problem instances, and (2) the lifting of verification methods designed for the unitary case to verify hybrid circuits, based on the deferred measurement principle mentioned earlier.",
"In doing so, we advocate for the versatility of the formalism of path-sums in the context of hybrid quantum circuit equivalence checking.",
"More precisely, we bring the following contributions: 1.",
We clarify the current landscape of hybrid circuit equivalence checking (Sec- tion 3).,
"In particular, we draw the separation between circuits with and without discards and the induced partitioning of equivalence cases.",
We propose a generic method based on deferred measurement that extends unitary circuit equivalence checkers to support hybrid circuits (Section 3).,
"We introduce a verification tool, SQbricks, dedicated to hybrid circuit equiv- alence checking (Section 4), included when discard is at stake.",
We system- atically evaluate this implementation.,
"Overall, while our technique already allows for significant clarification and pushes forward the state-of-the-art of hybrid circuit equivalence checking, we also believe 4 J. Ricciardi, et al.",
that our findings and benchmark establish a solid baseline for future research in the field.,
"To this end, SQbricks and our experimental setup will be made available as open source.",
State of the Art.,
Equivalence checking of purely unitary circuits is preva- lent among the state-of-the-art.,
"One can cite AutoQ [17,16,1], Feynman [2,3,4], SliQEC [41,15], and PyZX [29].",
"These tools leverage distinct methodologies, re- spectively: Tree Automata, Path-Sum, Decision Diagrams, and ZX-calculus.",
"On the other hand, to the best of our knowledge, only QCEC [10] and VeriQC [24] address the equivalence checking of hybrid quantum circuits.",
"How- ever, concrete implementations are limited to the case without discard (charac- terized as the DisFree case in Section 4.5 of [24]) .",
"QCEC relies on a combination of ZX-calculus, Decision Diagrams, Simula- tion, and deferred measurement for hybrid circuit equivalence.",
"Yet, the approach is limited [10, Section.",
"4, p. 4] to the narrow case of hybrid circuits with no dis- cards, preventing the analysis of typical transformations such as teleportation, one-way measurement, error correction, optimisations with ancillas, etc., and any form of non-reversible computation.",
VeriQC [24] implements measurement and classical control in quantum cir- cuits using Tensor Decision Diagrams.,
"The authors have already identified one of the main challenges in hybrid circuit equivalence verification: managing dis- cards, and they have formally defined an equivalence relation that accounts for it.",
"Still, their tool does not tackle it and does not adress discard.",
Table 1 provides an overview of the current state-of-the-art and our results.,
"It details the tools in terms of the technology they employ, the types of equivalence verification they support, and whether they rely on projection and separation techniques (last column).",
"2 Background This section is devoted to the presentation of the model of quantum computation, in particular, its main programming model: quantum circuits.",
We then introduce the two main aspects of the state-of-the-art that underpin our proposal: the deferred measurement principle and the path-sums semantics.,
Quantum Circuits.,
"The interaction with the quantum coprocessor consists of emitting a sequence of initialisation, unitary gates, measurements, and discards, potentially classically controlled by the result of previous measurements.",
"As an example of a hybrid circuit, Figure 1a illustrates the teleporta- tion protocol [7].",
This protocol operates over mixed quantum and classical data and describes the transmission of a quantum state from Alice (input register InpVar) to Bob (observable register Obs).,
"The protocol highlights the general structure of quantum circuits: initialisation (Init), unitary evolution, measure- ment, discards, and classically controlled operations.",
The circuit consists of three quantum wires (single lines) and two classical wires (double lines).,
It employs Quantum Circuit Equivalence Checking 5 Table 1: Automatic Quantum Circuit Equivalence Tools.,
"SQV: SQbricks-Verif, SQL: SQbricks-Lifting, (*): Any unitary equivalence checker considered here, **: The approach is unclear on these points (VeriQC), †: Meet our prerequisites (featuring separation tests and projections) to be able to verify lifted problems from Mix and Dis, ✓: Valid, ×: Invalid.",
Equivalence Checking Abilities Prerequisites† Name Techno.,
"Unitary Hybrid DisFree Mix Dis AutoQ [17,1] Tree Automata ✓ × × × × AutoQ-2 [16] Tree Automata ✓ × × × × Feynman [2,3,4] Path-Sum ✓ × × × × PyZX [29] ZX Calculus ✓ × × × × SliQEC [41,15] Decision Diagram (DD) ✓ × × × × QCEC [36,34,10] ZX, DD, Simulation ✓ ✓ × × × VeriQC [25,24] Tensor DD ✓ ✓ ** ** × SQV Path-Sum ✓ × × × ✓ SQV + SQL Path-Sum ✓ ✓ ✓ ✓ ✓ (*) + SQL ✓ ✓ × × × p p |x⟩InpVar H Init H X X X Rz(1) |x⟩Obs (a) Standard presentation p p |x⟩InpVar H Init H X X X Rz(1) |x⟩Obs (b) With deferred measurement Fig.",
"1: The one-qubit teleportation algorithm three types of gates: the Hadamard gate (H), the (controlled) NOT gate (X) and the phase gate (Rz(1)).",
"Measurement is a non-deterministic, described by the so-called Born’s rule, with results stored in classical wires .",
Classical wires are assumed to be initialised at 0.,
"Here, their values are not outputs of the circuit: they are discarded with .",
"Historically, mathematical semantics for quantum circuits have relied on the underlying mathematical representation of unitary gates: linear, unitary maps.",
One problem is that the matrix size corre- sponds to the number of basis elements in the corresponding vector space: it is exponentially costly to compute.,
"Another issue is that quantum circuits gener- ally contain measurements, typically handled with more involved representations such as density matrices and superoperators.",
"By sake of space, we cannot further introduce these quantum circuit component and their interpretation.",
We refer the desirous reader to [33] (Ch.4) for the standard introductory material.,
"6 J. Ricciardi, et al.",
"The deferred measurement is a circuit transformation that postpones all mea- surements to the end by replacing classical controls with quantum controls, leav- ing the inner circuit purely unitary.",
"This transformation preserves the semantics, resulting in equivalent circuits.",
Figure 1b shows the result of applying the pro- cess to the teleportation algorithm of Figure 1a.,
"The output circuit consists of a round of initialisations p , a unitary block, and a round of measurements, possibly immediately followed by discards (represented with ).",
We refer to the resulting pattern as the IUM circuits.,
"Note that in our figures, the alignment of the wires in the drawing is not meaningful (they might, for instance, be shuf- fled inside the circuit).",
A formal definition of our implementation of the deferred measurement transformation is provided in Appendix B.1.,
"An alternative to matrix representation has recently been proposed: path-sums [2,3,40,39,21,12].",
"This symbolic representation gives a more compact representation for purely quantum circuits, and has been at the core of an equiv- alence checking tool for unitary circuits: Feynman [2,3].",
"The name refers to Feynman’s paths, where the quantum evolution of a system is represented as a weighted sum of possible “paths”.",
"In the path-sums formalism, these weights are parameterised by the input basis states.",
"The path-sums formalism supports symbolic sequential ([2, Definition 2.6]) and parallel composition of operators.",
"Nevertheless, path-sums are over-expressive: two circuits corresponding to the same linear operation might have distinct path- sum representations.",
"To reconcile such equivalence representations, path-sums come with an equational theory, made of (in addition to Boolean and dyadic ring theories) a set of rewriting rules simplifying path-sum while preserving V equivalence.",
3 Partial Equivalences of Quantum Circuits The core of this paper focuses on the equivalence checking of quantum circuits.,
This section is dedicated to presenting our proposition’s context and main struc- turing elements.,
"For the sake of readability, in the following we assume the par- ticular case where all input wires are quantum (the general case, with possibly classical inputs, is formally treated in Appendix B).",
The prevalent approach to equivalence checking within the field predominantly addresses circuits with no discards.,
This paper elaborates on our methodological approach to adapt and extend equivalence checking to encompass circuits with discards.,
Shapes of Hybrid Circuits.,
We claim the presence or the absence of discards to be the main difficulty in the equivalence checking of hybrid circuits.,
It straightfor- wardly generates a three classes typology of hybrid circuit equivalence instances: – DisFree concerns circuits without discards.,
"The primary applications of Dis- Free involve dynamising unitary circuits: basically, the transformation inverse of the deferred measurement, introducing measurement instructions in order to turn quantumly controlled commands into classically controlled ones.",
"A Quantum Circuit Equivalence Checking 7 key usage is robustification, with examples including the Quantum Fourier Transform [11] and Quantum Phase Estimation [18].",
"The state-of-the-art methods for hybrid circuits currently address confines to this case [10,24].",
"– Dis, where both circuits feature discard.",
"Dis is the most generic equivalence task over hybrid circuits teleportation, one-way measurement, error correc- tion, optimisations with ancillas, etc.",
"In fact, most of the non-reversible quantum processes require the use of ancillas and discards.",
"– Mix is the mixed case, where one circuit is with discards, and the second is without.",
"With Mix, we can verify the equivalence between unitary and hybrid circuits resulting from transformations such as one-way measurement or teleportation.",
Taking advantage of the Deferred Measurement Transformation.,
"To open the field to the equivalence checking of classes Mix and Dis, we propose a unified approach by capitalising on the deferred measurement principle discussed in Section 2.",
"Given a circuit C, the deferred measurement transformation isolates the ini- tialisation, unitary, and measurement components, obtaining a IUM circuit.",
"A central result of this paper consists in characterising the relationship between the equivalence of two circuits C1 and C2 possibly with measures and discards, and the equivalence of their deferred measurement versions I1U1M1 and I2U2M2.",
"Indeed, it can be shown that if (i) I1 (resp.",
M1) and I2 (resp.,
"M2) are equal and (ii) the unitary blocks U1 and U2 behaves equivalently over non-discarded qubits and after initialisation (unitary circuit partial equivalence), one can directly in- fer the hybrid equivalence between I1U1M1 and I2U2M2, and thus address the original problem.",
"Hence, while our method pre-processes hybrid circuits along the deferred measurement transformation, it does not properly apply the principle.",
"Instead, it builds a logical over-approximation of the equivalence relation that takes ad- vantage of the deferred measurement transformation (the IUM circuit normal form) while eventually ignoring the final measurement.",
"4 Implementation We have implemented our method in a prototype named SQbricks, made of about 3,500 lines of OCaml code.",
The code will be made open source3.,
"The implementation comprises two parts: (1) the SQbricks-Verif component imple- ments a path-sum calculus to perform unitary verification and partial equiva- lence checking; (2) the SQbricks-Lifting component focuses on our generic lifting method, based on the deferred measurement transformation.",
"SQbricks-Verif (SQV) is a unitary circuit verification tool based on path-sums, with a core rewriting system.",
"Circuits are built from a pseudo-universal set of 3 https://github.com/Qbricks/qbricks.github.io/tree/main/Artifacts/ SQbricks 8 J. Ricciardi, et al.",
"gates, comprising H, X, U1 (Z rotation up to global phase) and combined through sequence and quantum control.",
"Furthermore, the user can explicitly indicate the address of qubits that are discarded.",
SQbricks-Lifting (SQL) implements deferred measurement to extend unitary equivalence verification tools to hybrid circuits.,
"This approach maintains con- sistency between hybrid circuit classes (DisFree, Mix, and Dis) while ensuring broad interoperability with existing verification tools through the OpenQASM 2 standard.",
"5 Experimental evaluation To assess our circuit equivalence checking technique, we conducted a series of benchmarks, all led on a laptop using Linux Mint 21.2, equipped with an Intel Core i9-9880H CPU (2.3GHz x 8), 31 GiB of RAM, and a 500 GB SSD.",
5.1 Experimental setup Considered tools.,
"We evaluate SQbricks-Lifting with the state-of-the-art unitary circuit verification tools AutoQ (Tree Automata), Feynman [2,22] (Path-Sums), PyZX [29,35] (ZX) and QCEC [8,9,32] in unitary mode (ZX, Decision Diagram, Simulation, labelled lifting, L in the following tables) in addition to SQbricks- Verif (Path-Sums).",
"We compare our overall performance for hybrid equivalence checking against QCEC [10] (in full hybrid mode, labelled standalone, S)and VeriQC [24] (figures from Table 1 in [24], since we were not able to use the tool directly (Appendix, Section A.2).",
Circuit collection.,
"We selected two libraries of quantum circuits in OpenQASM 2 [19] commonly used in the literature—VeriQBench [14] and the Feynman li- brary [2], plus a hybrid implementation of the Shor algorithm [37] (Hybrid Cir- cuit) inspired by the version provided in QASMBench [31].",
"For each library, we only retained the circuits that SQbricks could parse (some seem to have issues or need refinement, see Appendix A.1).",
"Observe that multiple samples represent the same algorithms with varying parameter settings, typically the size.",
This results in a compilation of 420 unitary circuits and 204 hybrid circuits.,
Circuit equivalence challenges.,
We consider the following circuit transforma- tions.,
(1) Qiskittr: the generate_preset_path_manager of IBM Qiskit [28] with an optimisation level of 3 (the highest level) to maximise circuit trans- formations.,
"(2) OWM: One-Way Measurement [20], a circuit transformation aiming at enhancing the practical application of quantum computers by min- imising the time a qubit remains coherent.",
"(3) Tele: Teleportation, described in Figure 1a.",
"In the end, our equivalence challenge categories are: – DisFree, with two classes of challenges: 169 pairs of unitary-hybrid circuits provided by VeriQBench and implementing the same algorithms (U-DC), either as a unitary or as a hybrid circuit; and 88 pairs consisting of a hybrid circuit and its transformed version using Qiskittr (DC-Q(DC)).",
"Quantum Circuit Equivalence Checking 9 – Mix, with three classes of challenges: 347 pairs made of a unitary circuit together with its OWM version (O(U)-U); 406 pairs made of a unitary circuit and its Tele version (T(U)-U); 88 pairs corresponding to a hybrid circuit transformed by both Qiskittr and OWM (O(DC)-Q(DC)).",
– Dis: 347 pairs (O(U)-T(U)) obtained from unitary circuits transformed with OWM and Tele.,
5.2 Experimental observations We address the three following research questions.,
– RQ1: Can we lift unitary verification for checking DisFree equivalence?,
– RQ2: How does it compare to prior works on DisFree equivalence?,
– RQ3: Can we lift unitary verification for checking Dis and Mix equivalence?,
"We also performed a sanity check to evaluate the correctness of the considered tools (Section 5.3), and report some additional findings about unexpected be- haviours in Qiskit we found along our experiments (Section 5.3).",
RQ1: Can we lift unitary verification for checking DisFree?,
"For DisFree challenges, we could draw performance comparisons between SQV and our selected set of unitary verification tools.",
"To do so, we performed a pre-processing deferred measurement transformation over DisFree circuits.",
Re- sults are summarized in Table 2: path-sum based methods (Feynman and SQV) achieved perfect success rates.,
"In contrast, PyZX demonstrated limitations, fail- ing to verify equivalence between two CSWAP variants (Appendix, Example 2) and showing scalability issues.",
"For instance, PyZX took 534s to verify a DC- Q(DC) QPE instance of size 35, whereas Feynman verified an instance of size 42 in just 3s.",
"AutoQ successfully verified all circuit families for small instances but faced scalability challenges, such as being limited to DC-Q(DC) to QPE of size 6.",
"Conclusion: Our approach successfully lifts unitary verification tools to han- dle hybrid DisFree cases, with path-sums showing superior performance com- pared to ZX-calculus and automata-based methods.",
RQ2: How does it compare to prior works on DisFree?,
"We compare QCEC [10] and VeriQC [24] for DisFree hybrid equivalence, evaluating QCEC directly and VeriQC via published results.",
"QCEC (standalone), lifted Feynman, and lifted SQV show similar performance on 88 DC-Q(DC) challenges, but QCEC fails all 169 U-DC challenges, and incorrectly reports 27 non-equivalence proofs, indicat- ing correctness issues (see Appendix, Example 3 for a minimal example).",
VeriQC [24] performs hybrid equivalence checking on DisFree.,
"4 Unfortunately, despite contacting VeriQC [24] authors, we couldn’t use this tool for our experiments.",
"We compared our methods against its published [24] experimental performance 4 The paper claims results over Dis [24, Definition 2].",
"However, [24, Section 4.5] ex- plains how to check dynamic circuit equivalence with TDDs, but is limited to circuits without discard (our DisFree).",
"10 J. Ricciardi, et al.",
Table 2: Evaluation results of the lifting application of deferred measurement.,
"S: Standalone, L: Lifted, DC: Dynamic Circuit, subclass of Hybrid Circuit with- out Discard, O: OWM, T: Tele, Q: Qiskittr, TO: Time Out (10 min) Wrong: equivalence check returns not equivalent for equivalent circuits, NA: Not Appli- cable, NW: Not Working.",
Success DisFree Mix Dis Tool Lift U-DC DC-Q(DC) O(U)-U T(U)-U O(DC)-Q(DC) O(U)-T(U) Total TO QCEC S 0 87 NA NA NA NA 87 143 Wrong: 27 Wrong: 27 VeriQC (Cf.,
"Table 3) S NW NW NA NA NA NA NA NA AutoQ-2.0 L 10 15 NA NA NA NA 25 23 Feyn-24 L 169 88 NA NA NA NA 257 0 PyZX L 73 66 NA NA NA NA 139 112 QCEC L 151 69 NA NA NA NA 220 19 SQV L 169 88 180 343 50 137 967 426 #challenges 169 88 347 406 88 347 1445 Table 3: Comparing against VeriQC from published results on their available benchmark (selection of the most significant results, time in seconds, ✓: Success) SQbricks-Lifting + Task VeriQC [24] AutoQ Feyn-24 PyZX QCEC SQV QFT _11 0.86 ✓ Err 0,01 ✓ 0,39 ✓0,01 ✓0,01 ✓ QFT _16 23.38 ✓ Err 0,02 ✓ 3,64 ✓0,02 ✓0,03 ✓ pe_9 1.62 ✓ Err 0,01 ✓ 0,29 ✓0,01 ✓0,01 ✓ phaseflip 0.18 ✓ 2,50 ✓ 0,00 ✓ 0,01 ✓0,00 ✓0,00 ✓ teleportation 0.01 ✓ 0,00 ✓ 0,00 ✓ 0,00 ✓0,00 ✓0,00 ✓ state_inj_T 0.01 ✓ 0,00 ✓ 0,00 ✓ 0,00 ✓0,00 ✓0,00 ✓ data.",
Results are shown in Table 3.,
"With our lifting approach, all tools except AutoQ verify the entire VeriQC benchmark faster than VeriQC itself.",
Conclusion: Our lifting method is highly effective on DisFree challenges.,
"While our method matches QCEC’s performance on its best subcategory, it outperforms both QCEC and VeriQC in all other cases.",
RQ3: Can we lift unitary verification for checking Dis and Mix?,
"This problem considers challenges from Dis and Mix, out of the scope of prior work.",
"Therefore, no comparative analysis was possible here.",
"Our method effectively addresses a substantial portion of the Mix and Dis hybrid equivalence challenges, handling 39.5% of Dis and 67.7% of Mix.",
"Conclusion: Our approach can indeed address hybrid equivalence checking out of the scope of the current state-of-the-art tools, with a reasonable success rate, establishing an acceptable first solution for these problems.",
Quantum Circuit Equivalence Checking 11 5.3 Additional Findings Sanity check.,
"We also performed a sanity check consisting of 73 equivalence tasks from VeriQBench and QASMBench, with deliberately modified quantum circuits (mutants).",
These modifications ensure non-equivalence by design.,
All versions of Feynman and SQbricks successfully passed the sanity check with no false positives.,
"Other tools exhibited two types of failures, primarily related to rotation gates: (1) PyZX and QCEC failed to handle very small angles (≤π/226 and ≤π/227 respectively).",
(2) AutoQ failed to distinguish between controlled and uncontrolled Z axis rotation gates (CRz(k) and Rz(k) gates).,
Unexpected behaviors with Qiskittr.,
"During our experiments, we uncovered two bugs in the Qiskit compiler: (1) Angle approximation: Qiskit version 1.1.0 approximated small angles to 0, leading to incorrect circuit simplifications in e.g., QFT, QPE, Shor algorithm, etc, when quantum registers are over 42 qbits5.",
This issue was fixed in version 1.4.0.,
"(2) Introduction of Floats: In version 1.4.0, transformations introduced floating-point values (e.g., converting 3π/32 to 0.2945243112740431) instead of rational numbers, causing equivalence loss in some circuits.",
We reported this to the Qiskit team.,
These findings highlight the practical utility of our method in identifying and mitigating approximation- related issues in quantum circuit transformations.,
Acknowledgments.,
"This work has been partially funded by the French National Research Agency (ANR): projects TaQC ANR-22-CE47-0012 and within the framework of “Plan France 2030”, under the research projects EPIQ ANR-22- PETQ-0007, OQULUS ANR-23-PETQ-0013, HQI-Acquisition ANR-22-PNCQ- 0001 and HQI-R&D ANR-22-PNCQ-0002.",
References 1.,
"Abdulla, P.A.",
: Verifying quantum circuits with level-synchronized tree au- tomata.,
POPL 9 (2025).,
"Amy, M.: Towards Large-scale Functional Verification of Universal Quantum Cir- cuits.",
EPTCS 287 (2019).,
"Amy, M.: Complete equational theories for the sum-over-paths with unbalanced amplitudes.",
EPTCS 384 (2023).,
"Amy, M., Lunderville, J.: Linear and non-linear relational analyses for quantum program optimization.",
POPL 9 (2025).,
"AutoQ Git, https://github.com/fmlab-iis/AutoQ 6.",
"Barthe, G., Katoen, J.P., Silva, A.",
): Foundations of Probabilistic Program- ming.,
Cambridge University Press (2020) 7.,
"Bennett, C.H.",
: Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels.,
"70, 1895–1899 (1993).",
"Burgholzer, L., Wille, R.: Improved DD-based equivalence checking of quantum circuits.",
ASP-DAC (2020).,
"5 This simplification imposed the induced restrictions over our experiments 12 J. Ricciardi, et al.",
"Burgholzer, L., Wille, R.: The power of simulation for equivalence checking in quantum computing.",
DAC (2020).,
"Burgholzer, L., Wille, R.: Handling Non-Unitaries in Quantum Circuit Equivalence Checking.",
DAC (2022).,
"Bäumer, E. et al.",
: Quantum Fourier transform using dynamic circuits (2024).,
"Chareton, C. et al.",
: An automated deductive verification framework for circuit- building quantum programs.,
ESOP (2021).,
"Chareton, C. et al.",
: Formal methods for quantum algorithms.,
In: Handbook of Formal Analysis and Verification in Cryptography.,
CRC Press (2023).,
"Chen, K. et al.",
: Veriqbench: A benchmark for multiple types of quantum circuits.,
arXiv:2206.10880 (2022).,
: Partial equivalence checking of quantum circuits.,
: AutoQ 2.0: From verification of quantum circuits to verification of quantum programs.,
arXiv:2411.09121 (2024).,
: AutoQ: An automata-based quantum circuit verifier.,
"Córcoles, A.D. et al.",
: Exploiting dynamic quantum circuits in a quantum algorithm with superconducting qubits.,
"127, 100501 (2021).",
"Cross, A.W., Bishop, L.S., Smolin, J.A., Gambetta, J.M.",
: Open quantum assembly language.,
arXiv:1707.03429 (2017).,
"Danos, V., Kashefi, E., Panangaden, P.: The Measurement Calculus.",
"Deng, H., Tao, R., Peng, Y., Wu, X.: A case for synthesis of recursive quantum unitary programs.",
POPL (2024).,
"Feynman Git, https://github.com/meamy/feynman 23.",
"Devitt, S. J. and Munro, W. J. and Nemoto, K.: Quantum error correction for beginners.",
"(2013), 24.",
"Hong, X., Feng, Y., Li, S., Ying, M.: Equivalence checking of dynamic quantum circuits.",
ICCAD (2022) 25.,
"Hong, X. et al.",
: A tensor network based decision diagram for representation of quantum circuits.,
27(6) (Jun 2022).,
Ying M. and Zhou L. and Barthe G.: Laws of Quantum Programming.,
"Ioannou, L. M.: Computational complexity of the quantum separability problem (2006).",
"Javadi, A. et al.",
: Quantum computing with Qiskit.,
arXiv:2405.08810 (2024).,
"Kissinger, A., van de Wetering, J.: PyZX: Large Scale Automated Diagrammatic Reasoning.",
QPL (2020).,
"Lewis, M., Soudjani, S., Zuliani, P.: Formal verification of quantum programs: Theory, tools, and challenges.",
ACM TQC 5(1) (2023).,
"Li, A., Stein, S., Krishnamoorthy, S., Ang, J.: Qasmbench: A low-level qasm bench- mark suite for NISQ evaluation and simulation.",
arXiv:2005.13018 (2022).,
"MQT-QCEC, https://mqt.readthedocs.io/projects/qcec/en/latest/ 33.",
"Nielsen, M.A., Chuang, I., Grover, L.K.",
: Quantum computation and quantum in- formation.,
"70(5), 558–559 (2002).",
"Peham, T., Burgholzer, L., Wille, R.: Equivalence checking of quantum circuits with the ZX-calculus.",
"IEEE JESTCS 12(3), 662–675 (2022).",
https://pyzx.readthedocs.io/en/latest/gettingstarted.html 36.,
"Sander, A., Burgholzer, L., Wille, R.: Equivalence checking of quantum circuits via intermediary matrix product operator.",
arXiv:2410.10946 (2024).,
: Polynomial-time algorithms for prime factorization and discrete loga- rithms on a quantum computer.,
SIAM J. Comp.,
"26(5), 1484–1509 (1997).",
"VeriQC dynamic quantum circuits benchmarks, https://github.com/Veriqc/ EC-for-Dynamic-Quantum-Circuits/tree/main/Benchmarks2 Quantum Circuit Equivalence Checking 13 39.",
"Vilmart, R.: The structure of sum-over-paths, its consequences, and completeness for Clifford.",
arXiv:2003.05678 (2020).,
"Vilmart, R.: Rewriting and completeness of sum-over-paths in dyadic fragments of quantum computing.",
LMCS 20(1) (2024).,
: Accurate BDD-based unitary operator manipulation for scalable and robust quantum circuit verification.,
In: DAC (2022).,
"A Experimental evaluation details This appendix provides details about our experimental evaluation, including specifics about the collection of circuits we used (Appendix A.1) and information about the tools we compared ourselves to (Appendix A.2).",
"A.1 Details on Circuit Collection Our circuit collection comprises two categories: – Unitary Circuits: 420 circuits, including 43/44 circuits from the Feynman library 6. and 377/782 circuits from the VeriQBench library combinational subset, and without the sequential and variational subsets due to parsing issues.",
"Parsing issues arise if a circuit is ill-formed or contains elements not accounted for in our syntax, such as macros.",
"– Hybrid Circuits: 204 circuits, including 198/205 circuits from the VeriQBench library (QPE and QFT, bit flip and phase flip correction, state injection and teleportation) and an implementation of Shor [37] over 5 qubits inspired from the QASMBench [31] library.",
"A.2 Tool Limitations and Failure Cases This appendix provides information on the benchmark for VeriQC (Table 3), and presents concrete examples demonstrating specific limitations of state-of-the-art quantum circuit verification tools, including cases where tools return incorrect or inconclusive results despite the functional equivalence of the circuits being compared.",
"After encountering difficulties in reproducing their experiments with DisFree, we began email correspondence with the authors.",
"However, we faced on- going challenges in replicating their results.",
"As a result, we utilised their findings: [24, Table 1].",
Example 2 (PyZX minimal inconclusive result).,
The following pair of circuits implements the controlled-swap operation (CSWAP) in two different ways.,
This operation swaps two qubits conditionally on the state of a third control qubit.,
"For basis states |x0, x1, x2⟩, the operation should: 6 Except cycle_17_3.qasm which has an implementation issue 14 J. Ricciardi, et al.",
"– Leave x0 unchanged (control qubit) – Swap x1 and x2 when x0 = 1 – Leave x1 and x2 unchanged when x0 = 0 While these implementations are functionally equivalent (both implement controlled- swap), PyZX [29] (version 0.9.0) fails to verify this equivalence, demonstrating a limitation in handling certain control structures.",
2: CSWAP first implementation |x0⟩ |x0⟩ |x1⟩ X |x0x2 ⊕(1 ⊕x0)x1⟩ |x2⟩ X X |x0x1 ⊕(1 ⊕x0)x2⟩ Fig.,
3: CSWAP alternative implementa- tion |x0⟩ |x0⟩ |x1⟩ X |x0x2 ⊕(1 ⊕x0)x1⟩ |x2⟩ X X |x0x1 ⊕(1 ⊕x0)x2⟩ Example 3 (QCEC minimal incorrect result).,
"The circuits below illustrate a min- imal case where QCEC [10] (version 2.8.1), when using its deferred measurement option, incorrectly determines that the circuits C1 and C2 are not equivalent.",
4: C1 Rz(1) H Fig.,
5: C2 Rz(1) H B Technical Details B.1 Lifting unitary verification tools for hybrid circuits This section provides the main formal ingredients for our deferred measurement- based lifting of unitary circuits’ equivalence to the hybrid case.,
Definition 1 (Hybrid circuits).,
"A hybrid circuit is a sequence of instructions generated by the following syntax G := Ph(k) | Rz(k) | X | H GA := Apply(G, [qb], [qb]) Ins := GA | if cb then GA | Meas(qb, cb) | Init(qb) | Not(cb) C := [Ins] where [t] denotes a standard list construction of type t, built either as the nill list [l] := nill or as a t type object o appended to another t list [l] := o −[l′].",
"By abuse of notation, we use the same l −l′ for l, l′ being either a list or a type t object (assimilated to a list of one single element).",
"Apply(G, [qb1], [qb2]) intuitively commands the parallel application of gate G on qubits in [qb2], controlled by the conjunction of qubits in [qb1]; Meas(qb, cb) commands the measure of qubit qb and the storage of the obtained data in clas- sical bit cbA circuit is well-formed if it respects the following syntactical con- straints: that (i) a classical wire should receive at most one measurement result in a given circuit and (ii) a quantum wire is not further addressed when having been measured.",
"Quantum Circuit Equivalence Checking 15 The deferred measurement principle is introduced in [33] as a rather obvious property of circuits: that measurement can always be moved from an intermedi- ate stage of the computation to the end of the circuit, while classically controlled instructions are replaced by quantum conditionals.",
"Surprisingly, to the best of our knowledge, the first formal and generic proof of the principle was established as late as 2022 [23], and a computer-assisted proof was even recently given in [26].",
"For a circuit C, we write lInit(C) the sum of, for each initialisation command Init, the number of non-initialisation commands that precede it in C. Symmet- rically, eMeas(C) denotes the sum of, for each measure command Meas in C, the number of non-measure commands succeeding it in C. We also informally introduce the function mC.",
It maps a classical wire cb to a quantum wire qb in instructions.,
"In our deferred measurement circuit transformation, as the mea- surement Meas(qb, cb) of a qubit is postponed to the end of the execution, the intermediary classical control commands over cb should be turned into quan- tum control commands over qb.",
"This is achieved by applying the instruction transformation mC(qb, cb) to the instructions Meas(qb, cb) is permuted with.",
"In addition, it turns a classical bit-flip instruction Not(cb) into its quantum coun- terpart Apply(X, nill, qb).",
"Formally, for any instruction Ins, we have that: Ins[mC(cb, qb)] = if Ins := if cb then Apply(G, [co], [ta]) then Apply(G, [co ∪{qb}], [ta]) else if Ins := Not(cb) then Apply(X, nill, qb) else Ins Definition 2 (Deferred measurement circuit transformation).",
"The de- ferred measurement transformation is built as a double inductive rewriting pass, where initialisation (resp.",
measurement) commands systematically commute with non-init (non-measure) commands whenever they occur in non-initial (resp.,
non- final) position.,
"pI(C) := if lInit(C) = 0 then C else if C := Init(qb) −C′ then Init(qb) −pI(C′) else if C := Ins −Init(qb) −C′ then Init(qb) −pI(Ins −C′) else if C := Ins −C′ then pI(Ins −pI(C′)) lM(C) := if lM(C) = 0 then C else if C := C′ −Meas(qb, cb) then lM(C′) −Meas(qb) else if C := C′ −Meas(qb, cb) −Ins then lM(C′ −Ins[mC(qb, qb)]) −Meas(qb, cb) else if C := C′ −Ins then lM(lM(C′) −Ins) DM(C) := lM(pI(C)) This transformation provides a circuit equivalent to C and sequentially struc- tured as three successive blocks [Init]C −[U]C −[Meas]C of (i) initialisation (ii) unitary gate application, and (iii) measure commands.",
"Formally, we have the following theorem: Theorem 1 (Deferred measurement).",
"Let C be a hybrid circuit, then: (i) DM(C) ≡H C, (ii) lInit(DM(C)) = 0, (iii) eMeas(DM(C)) = 0 16 J. Ricciardi, et al.",
Proof (Sketch).,
"By structural induction over C. Transformation pI preserves the semantical equivalence and ensures condition 2., transformation lM preserves both and ensures condition 3 in addition.",
"As an illustration of Definition 2, Figure 1b draws it application to the teleportation case discussed in Example 1: measurement instructions are delayed to the end of the execution, and intermediary classical instruction – orig- inally controlling over measurement results – are turned into quantum controlled instruction – controlling over the corresponding not yet measured quantum wires.",
"The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods ETHAN FRIESEN, SQM Research Lab, Computer Science, University of Manitoba, Canada SASHA MORTON-SALMON, SQM Research Lab, Computer Science, University of Manitoba, Canada MD NAHIDUL ISLAM OPU, SQM Research Lab, Computer Science, University of Manitoba, Canada SHAHIDUL ISLAM, SQM Research Lab, Computer Science, University of Manitoba, Canada SHAIFUL CHOWDHURY, SQM Research Lab, Computer Science, University of Manitoba, Canada Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort.",
"We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability.",
"Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs.",
"At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods.",
"However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation.",
"To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables).",
"These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.",
CCS Concepts: • Software and its engineering →Maintaining software.,
"Additional Key Words and Phrases: ExtremelyBuggy, software bug, bug-proneness, bug prediction, software maintenance, code metrics.",
1 Introduction Software maintenance is one of the most expensive phases of the development lifecycle [5].,
"A major contributor to this cost is the effort required to identify and correct software bugs, which alone can account for 50-70% of the total development costs [13, 73].",
"To no surprise, building models for accurate bug prediction is referred to as the prince of empirical software engineering research [37] with researchers developing bug prediction models in an attempt to take early actions [13, 20, 50, 56, 66, 73, 74].",
"The assumption being that if bug-prone code components can be identified early in the development lifecycle or prevented altogether, future maintenance costs will be significantly reduced [7].",
"Prior bug prediction models predominantly focused on the class or file levels [2, 21, 75].",
"Unfortunately, these models remain underutilized in practice [72], at least partly because practitioners find it challenging to locate bugs at such coarse levels of granularity [13, 24, 37, 43, 50, 64].",
Practical use would require developers to sift through entire classes or files without an indication of what characteristics the model considered bug-prone.,
"Consequently, a significant amount of recent research has focused on bug prediction at the method level granularity [13, 19, 20, 41, 50, 65].",
"However, these Authors’ Contact Information: Ethan Friesen, SQM Research Lab, Computer Science, and University of Manitoba, Winnipeg, Canada, fries432@ myumanitoba.ca; Sasha Morton-Salmon, SQM Research Lab, Computer Science, and University of Manitoba, Winnipeg, Canada, mortonss@myumanitoba.",
"ca; Md Nahidul Islam Opu, SQM Research Lab, Computer Science, and University of Manitoba, Winnipeg, Canada, opumni@myumanitoba.ca; Shahidul Islam, SQM Research Lab, Computer Science, and University of Manitoba, Winnipeg, Canada, islams32@myumanitoba.ca; Shaiful Chowdhury, SQM Research Lab, Computer Science, and University of Manitoba, Winnipeg, Canada, shaiful.chowdhury@umanitoba.ca.",
"1 arXiv:2511.22726v1 [cs.SE] 27 Nov 2025 2 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury models treat all buggy methods equally, without distinguishing between methods that were fixed once and those that required multiple bug fixes.",
"In contrast to previous research, we focus on ExtremelyBuggy methods—methods that have required bug fixes more than once—because identifying them early could prevent a disproportionate number of future failures and reduce the need for repeated maintenance on the same code fragments.",
Such methods may indicate architectural weak points or persistently complex areas that tend to accumulate defects over time.,
"By identifying and targeting these methods, we aim to help practitioners prioritize their efforts where it matters most, enabling them to address a large number of bugs by focusing on a relatively small subset of problematic methods.",
"To support this goal, we analyze the characteristics of ExtremelyBuggy methods in an effort to predict them at their inception—that is, as soon as they are pushed to a software project.",
"To do so, we collected the change history of 1.25 million methods from 98 widely used open-source Java projects.",
"To the best of our knowledge, this represents the largest and most comprehensive dataset to date focused on method-level bug prediction.",
"Using this dataset, we address the following four research questions: RQ1: What proportion of methods are ExtremelyBuggy?",
Contribution 1: We found that 0.04-6.63% of methods are ExtremelyBuggy.,
"However, this small proportion of methods can often account for a large number of bug fixes in a project.",
This implies that identifying this small fraction of methods early will significantly reduce future maintenance burden.,
RQ2: Do ExtremelyBuggy methods exhibit signifantly different code quality than others?,
"Contribution 2: We found that, at their inception, ExtremelyBuggy methods are significantly larger, less readable, and have lower maintainability scores compared to both Buggy and NotBuggy methods.",
"This is encouraging, as it suggests that these methods exhibit distinguishable code quality that could be leveraged by machine learning models for early prediction.",
That led us to our next research question.,
RQ3: Can code quality-based machine learning models predict these ExtremelyBuggy methods at their inception?,
"Contribution 3: Unfortunately, our results indicate that machine learning algorithms perform poorly in distinguish- ing ExtremelyBuggy methods from other methods.",
"This is partly due to a significant class imbalance, as ExtremelyBuggy methods represent only a small fraction of the overall dataset.",
Common techniques such as oversampling and under- sampling did not lead to meaningful improvements in prediction performance.,
RQ4: What are the observable characteristics of ExtremelyBuggy methods?,
Contribution 4: The futility in predicting with machine learning models led us to manually investigate the characteristics of the ExtremelyBuggy methods so that we can provide actionable guidance for the practitioners.,
"As such, we conducted a thematic analysis on a curated dataset of 287 ExtremelyBuggy methods.",
"In general, we focused on three different aspects while applying thematic analysis: visual issues, context of a method, and bug-fix type.",
Our analysis reveals some common themes that exist within these 287 methods.,
"For example, we found that methods that deal with the core logic and algorithms tend to become ExtremelyBuggy.",
"To help replication and extension in future method-level bug prediction research, we share this dataset publicly.",
"1 2 Related Work In this section, we discuss previous studies involving bug prediction and code metrics to show how they have motivated this paper.",
1https://github.com/SQMLab/ExtremelyBuggyPublicData/tree/main/Dataset The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 3 2.1 Software Maintenance and Bug Prediction Software maintenance is expensive [5] and the bug-proneness of code components has been identified as one of the largest indicators of future maintenance burdens [9].,
"As such, there has been an enormous effort by the research community over the last forty years [38] to predict bug-prone code components so that early actions can be taken [9, 16].",
"Despite the great amount of research dedicated to predicting bugs, bug prediction models have yet to see widespread industry use [37, 72].",
"This is due, at least in part, to the fact that prior studies have predominantly focused on bug prediction at the file or class level [2, 21, 75], a granularity that practitioners often consider too coarse for practical use [13, 24, 43, 50].",
"These concerns have led to a recent shift in bug prediction research towards the method level [19, 20, 25] granularity.",
"However, multiple recent studies [13, 50] have shown that the high prediction accuracy2 reported by these studies is often inflated due to unrealistic evaluation scenarios, and, when tested on realistic scenarios, the accuracy of these models drops significantly.",
"Additionally, prior studies have not differentiated between methods associated with a single bug fix and those that are repeatedly involved in bug-fixing.",
"This distinction is critical, as methods that frequently require maintenance—often referred to as maintenance hotspots—should be prioritized and allocated additional resources.",
"Motivated by this important gap in prior research, this paper focuses exclusively on source code methods that have been associated with bugs more than once.",
We investigate whether these methods exhibit distinct characteristics and whether they can be identified early in their lifecycle.,
"2.2 Code Metrics For bug prediction models to work, they need some quantifiable metrics that characterize the source code to use as inputs to the models.",
"These metrics often come in the form of product metrics [11, 19, 41, 50] and process metrics [22, 41, 55].",
"Process metrics are typically derived from the change history of a code component and have been shown to be effective predictors of bug-prone components [20, 55].",
"However, such metrics are not available immediately after a piece of code is written and therefore cannot support the early identification of maintenance-prone components.",
"Waiting for sufficient change history to accumulate may allow these methods to cause significant issues, leading to high maintenance costs and potentially eroding stakeholder confidence.",
"Product metrics, on the other hand, are often readily available and easy to compute at the inception of a code component.",
"As a result, building maintenance prediction models using only product metrics has been a long-standing goal in software engineering research [9, 46, 50, 61].",
"Despite their extensive usage in software engineering research, some studies have found that product metrics, apart from size, are not helpful for building models that predict future maintenance burdens [17, 21, 67].",
"However, these studies with negative results were conducted at the class/file level, whereas studies that focused on method-level granularity have found product metrics as useful predictors of maintenance [11–13, 36].",
"Motivated by the need for early prediction and the practicality of method-level source code metrics, we focus exclusively on these metrics to characterize and predict ExtremelyBuggy methods.",
"3 Methodology In this section, we outline the overall methodology, including project selection, change history tracking, and the identification of bug-proneness indicators.",
"We also detail the data preprocessing steps and the selection of code 2Unless otherwise stated, accuracy in this paper refers to precision, recall, and F1-score.",
"4 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury metrics.",
"To improve readability, methodologies specific to individual research questions are presented alongside each corresponding research question.",
3.1 Dataset A substantial portion of our dataset originates from the work of Chowdhury et al.,
"[13], which comprises 774,051 Java methods drawn from 49 open-source projects.",
"Each method in this dataset is accompanied by its complete change history and is enriched with metadata such as commit messages, timestamps, authorship information, and change types.",
"The dataset additionally includes code diffs for all historical modifications, binary labels indicating whether each change corresponds to a bug fix, and the number of methods modified within the associated commit.",
"These 49 projects have been widely utilized, either fully or partially, in prior method-level software engineering studies [9, 12, 13, 21, 24, 49, 58, 69].",
"Building upon this foundation, we extend the dataset by collecting additional open-source projects and extracting their method-level change histories.",
3.1.1 Project Selection.,
We employed the GitHub REST API3 and the PyGitHub library4 to collect open-source projects.,
"To ensure the quality and reliability of our dataset, we followed recommended best practices for mining GitHub data [35].",
We began by retrieving the 1000 most-starred GitHub repositories that satisfied several additional criteria.,
"First, to reflect contemporary industry practices, we restricted our search to repositories created within the past 15 years5 and excluded repositories that had been inactive for more than one year6.",
"Second, to ensure representativeness of real-world Java projects, we excluded templates, archived repositories, and forks by specifying appropriate parameters in our GitHub API queries.",
"Third, we required repositories to contain at least 2000 commits to guarantee substantial change history.",
"We further filtered out repositories whose codebase consisted of less than 95% Java, thereby ensuring the availability of a sufficiently large number of Java methods for extraction.",
"Following the automated filtering process, we manually reviewed all remaining repositories to verify adherence to the above criteria and to confirm that their descriptions were written in English.",
Ensuring English descriptions facilitates the identification of bug-fixing commits using English-language keywords and supports subsequent manual qualitative analysis of commit messages.,
"Based on this review, we selected 49 additional projects.",
Combined with the 49 projects from Chowdhury et al.,
"[13], our final dataset comprises 98 projects in total.",
The complete list of filtered projects is included in our repository 7.,
3.1.2 Extracting Method-level History.,
"To quantify the bug-proneness of a method, it is necessary to determine how frequently that method has been involved in bug-fixing activities, which in turn requires capturing its complete change history.",
"To obtain this historical information, we employed the CodeShovel tool [24].",
"CodeShovel has demonstrated high accuracy in identifying method-level change histories and is robust to complex code transformations, including method relocations across files [24].",
"Its reliability has also been independently evaluated by industry practitioners, who reported similarly strong accuracy [24].",
"Although a more recent tool, CodeTracker [33], has been shown to achieve higher accuracy than CodeShovel, its substantially slower performance renders it impractical for tracing the histories of large numbers of methods.",
"Moreover, CodeTracker has not undergone independent validation by industry professionals.",
"3https://docs.github.com/en/rest 4https://github.com/PyGithub/PyGithub 5Created after January 1st, 2010 6No commits since May 13, 2024 7https://github.com/SQMLab/ExtremelyBuggyPublicData/tree/main/AdditionalFigures/RepositoryList.md The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 5 To support large-scale and reliable analysis, we used CodeShovel to generate structured JSON outputs for approxi- mately 1.25 million methods across 98 projects, capturing their complete method-level evolution across commits.",
These outputs provide a robust foundation from which we extract both code metrics and change metrics for every method revision included in our dataset.,
3.2 Labeling Bug-fix Commits The bug-proneness of a method is historically estimated by extracting bug-related keywords from commit messages.,
"A commonly used set of such keywords (and their variations) includes: error, bug, fix, issue, mistake, incorrect, fault, defect, and flaw [42, 58].",
"However, this keyword set often produces false positives, primarily due to the inclusion of the keyword issue [13].",
"To reduce these false positives, Chowdhury et al.",
[13] proposed removing the keyword issue and focusing on commit messages that contain both bug-related and fix-related keywords.,
"Although this method significantly improves the accuracy of identifying bug-fix commits, it does not eliminate noise caused by tangled commits—commits that include changes to both bug-related and unrelated methods—leading to incorrectly labeled methods.",
"While handling tangled code changes remains an active research area [13, 27, 48], we conduct our analysis using three distinct datasets to mitigate the impact of tangled commits.",
"Our underlying hypothesis is that if similar results are observed across all three datasets, the findings are more robust and less likely to be impacted by tangled changes.",
(1) HighRecall Dataset.,
"Here, we used all the above-mentioned keywords and their variations (include issue) and did not consider the problem of tangled changes.",
"While this approach obtains high recall since any change remotely related to a bug-fix is labeled as a bug-fix, it results in low precision as many changes not related to a bug-fix are labeled as such.",
(2) HighPrecision Dataset.,
In this dataset we used the set of keywords introduced by Chowdhury et al.,
"[13], which aims at increasing the precision of bug-fix labeling.",
"To remove the effects of tangled changes, we only labeled a change as a bug-fix if at most 1 method was changed in that commit—if there is only one modified method in a bug-fix commit, that method was certainly a bug-prone method [13].",
"While this approach achieves a high level of precision, it ultimately reduces recall, as many bug fixes may be ignored.",
(3) Balanced Dataset.,
"We used the same set of keywords as the HighPrecision Dataset; however, we allowed up to 5 methods changed in a commit for it to be labeled as a bug fix.",
This approach aims at achieving a balance between precision and recall.,
3.3 Age Normalization The age of the methods in our dataset varies significantly.,
"This means that comparing newer methods to older methods may introduce bias, as older methods have more time to undergo changes and hence, undergo bug-fix related changes.",
"This assumption is confirmed in figure 1a, where we can see that for many projects, there is a positive correlation when comparing age against both revisions and bugs.",
"Specifically, for approximately 60% of projects, the correlation between a method’s age and its number of bugs is ≥0.2—while weak, such correlations still should not be ignored to reduce confounding in our analysis.",
"Due to this correlation, we removed all methods that are under 5 years old from our dataset.",
"However, we still cannot directly compare methods that are 5 years old with those that are more than 5 years old for the aforementioned reasons.",
"Thus, we also remove all changes made after the first 5 years from the remaining methods.",
"This, unfortunately, resulted 6 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury in us losing 626,529 methods and all methods from the following 6 projects from our dataset: ’junit5’, ’SmartTube’, ’RxJava’, ’Essentials’, ’titan’, ’xpipe’.",
"Since we are interested in the Extremely Bug-Prone methods, we care more about retaining change history—allowing methods to exhibit their true bug-proneness.",
"Thus, the reasoning behind a 5-year threshold can be summarized in Figure 1b.",
Here we see that 5 years allows us to retain ~90% of changes and ~85% of bugs while still saving ~50% of the methods in our dataset.,
"If we decrease the threshold, we will lose more bug-proneness information in our dataset.",
"The end result of this age-normalization is a dataset of 691,610 methods from 92 projects, where each method has exactly 5 years of change history.",
"All the projects in our dataset, after age normalization, as well as the number of ExtremelyBuggy methods in each project for the Balanced dataset can be viewed in our public repository 8.",
"0.0 0.2 0.4 0.6 Correlation 0.0 0.2 0.4 0.6 0.8 1.0 CDF Revisions Bugs (a) Correlation between age and both revisions and bug-proneness for all 98 projects 0.0 2.5 5.0 7.5 10.0 Year 0 20 40 60 80 100 Percent #Methods #Revisions #Bugs (b) Percent of total methods, revisions and bugs re- tained at different age-normalization thresholds.",
Both figures use the HighRecall Dataset to measure bugs.,
"To enhance readability, we only use 11 markers when presenting each distribution, however, all data points are represented.",
"3.4 Labeling Bug Proneness For all three datasets, we assign each method to one of three categories based on the number of bug-fix commits in which it was involved: • NotBuggy: Methods that were never involved in a bug-fix commit.",
• Buggy: Methods that were involved in exactly one bug-fix commit.,
• ExtremelyBuggy: Methods that were involved in more than one bug-fix commit.,
"3.5 Code Metrics Code metrics have traditionally served as indicators of quality and maintainability of code [13, 21, 50].",
"Encouraged by previous research in method-level code metrics [12, 41, 50], we used 14 code metrics to determine if the code quality indicators of the ExtremelyBuggy methods are different than others.",
The selected metrics are described as follows.,
8https://github.com/SQMLab/ExtremelyBuggyPublicData/tree/main/AdditionalFigures/RepositoryList.md The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 7 Size.,
"Size is one of the simplest metrics, yet it is considered one of the most important with regard to software quality and maintenance [14, 17, 21].",
"We used the number of source lines of code without comments and blank lines as our size measurement (referred to as SLOCStandard) [12, 13, 36, 57].",
Readability.,
The ability to read and understand existing code is an important part of software maintenance [62].,
"Code readability can be broadly defined by how easy it is for a developer to understand the structure and flow of source code [4, 5, 34, 52].",
Developers can more easily understand and therefore modify readable code with reduced risk of issues.,
We used two distinct readability metrics in this paper: Buse et al.,
[4] (Readability) and Posnett et al.,
[52] (SimpleReadability).,
"Both provide a value for each method in the range [0, 1].",
Complexity and Testability.,
We used the McCabe metric [39] to analyze the complexity of methods.,
"This metric represents the number of independent paths in a method; the greater the number of paths, the more challenging it becomes to test the method thoroughly.",
"In addition, we used the number of control variables (NVAR), the number of comparisons (NCOMP), and the McClure metric [40], which is calculated as the sum of NVAR and NCOMP.",
These metrics offer additional insights into method complexity that are not captured by the McCabe metric.,
Hindle et al.,
"[29] proposed the proxy indentation metric (IndentSTD), which performs similarly to the McCabe metric in capturing code complexity but, unlike McCabe, does not require a language-specific parser.",
We included IndentSTD in our metric set as well.,
"Finally, we used the MaximumBlockDepth metric, which measures the maximum nested block depth, as deep nesting within a method can further complicate testing.",
Dependency.,
"When methods are highly coupled, a bug within one method is more likely to affect another.",
"To measure this dependency between methods, we used totalFanOut, which is the total number of methods called by a given method.",
Maintainability.,
"Maintainability Index (MI) [47] is a composite software quality metric that integrates HalsteadVol- ume, which quantifies the amount of information a reader must process to comprehend the code’s functionality, along with McCabe complexity and Size.",
"The MI is computed using the following formula: 𝑀𝐼= 171 −5.2 × ln(𝐻𝑎𝑙𝑠𝑡𝑒𝑎𝑑𝑉𝑜𝑙𝑢𝑚𝑒) −0.23 × (𝑀𝑐𝐶𝑎𝑏𝑒) −16.2 × ln(𝑆𝑖𝑧𝑒) (1) It produces a single score (−∞, 171], designed to reflect the maintainability of a given method, where a higher score reflects better maintainability.",
"Industry standard tools such as Visual Studio9 use this metric, making it a worthy inclusion for our purposes.",
"Halstead Length (Length), which measures the total number of operators and operands, the number of parameters (Parameters) and the number of local variables (LocalVariables), are also included alongside the other metrics.",
"We also intended to include the FanIn metric in our analysis, but were unable to do so due to the complexity involved.",
"For example, consider the Hadoop project, which contains approximately 70,000 methods.",
"Since our goal is to make predictions at method inception, we need to capture code metrics at the time each method was first introduced.",
Measuring the FanIn metric would require constructing a separate call graph for the whole codebase at each method’s introduction commit.,
"Unfortunately, building such call graphs is extremely time-consuming, as it requires preprocessing the entire codebase.",
Given that we work with 98 projects—each potentially requiring thousands of unique call graphs corresponding to thousands of distinct commits—this approach was computationally infeasible within the scope of our study.,
"9https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-maintainability-index-range-and-meaning?view=vs-2022 8 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury 3.6 Statistical Tests After evaluating our data with the Anderson-Darling normality test [59], we found that most of the distributions in our data (e.g., the code metrics) do not follow a normal distribution.",
"Therefore, we adopted the non-parametric Wilcoxon rank sum test to assess if there is a statistical difference between two given distributions.",
"To understand the size of those difference, we employed the non-parametric Cliff’s d and categorized the effect sizes following Hess et al.",
"[28]: Negligible (N) (< 0.147), Small (S (0.147 ≤𝛿< 0.33), Medium (M) (0.33 ≤𝛿< 0.474), and Large (L) (𝛿≥0.474).",
"Both of these statistical tests do not assume normally distributed distributions and have been widely used in software engineering research [1, 8, 10, 26, 51].",
"In addition, we used the non-parametric Kendall’s 𝜏correlation coefficient instead of Pearson’s r [10, 21, 31].",
"Unless otherwise stated, we use a significance threshold of 𝑝≤0.05 when evaluating the statistical significance of correlation coefficients.",
"4 Approach, Analysis and Results In this section, we discuss the approach of each RQ along with the findings.",
"4.1 RQ1: Proportion of ExtremelyBuggy methods and their impact Past research has shown that a high proportion of changes come from a small segment of code [9, 23].",
"Motivated by this observation, we investigate what proportion of methods are ExtremelyBuggy and how many bugs the ExtremelyBuggy methods account for in a project.",
Number of Buggy and ExtremelyBuggy methods in all 3 datasets.,
Dataset #Buggy (%) #ExtremelyBuggy (%) HighRecall 79715 (11.53) 45860 (6.63) HighPrecision 2704 (0.39) 287 (0.04) Balanced 8024 (1.16) 1195 (0.17) Table 1 shows the number of Buggy and ExtremelyBuggy methods for all three different datasets when we consider all 92 projects aggregated together.,
"Although the high recall dataset contains a large number of ExtremelyBuggy methods (6.63%), these numbers decline significantly when we observe the HighPrecision and Balanced datasets.",
"Specifically, only 0.04% (or 287 of the ~700000) methods are ExtremelyBuggy in the HighPrecision dataset.",
Another observation is that the ratio of Buggy to ExtremelyBuggy methods is smaller in the HighPrecision and balanced datasets.,
"In the HighRecall dataset, approximately 37% of all methods with at least one bug-fix (Buggy and ExtremelyBuggy) are ExtremelyBuggy whereas in the HighPrecision and Balanced datasets, this percentage drops to approximately 10% and 13% respectively.",
"By viewing the results in figure 2, we can see the percent of methods that are Buggy and ExtremelyBuggy for individual projects across all datasets.",
Each plot shows the CDF of the percent of methods that are Buggy and ExtremelyBuggy on a log scale where each data point is an individual project.,
"For example, in the HighPrecision dataset, in ~80% of projects, ≤1% of methods are Buggy.",
"One interesting finding is that for the HighPrecision dataset, ~35% of projects have no ExtremelyBuggy methods.",
This will make it very hard to do a project-wise analysis on the ExtremelyBuggy methods using this dataset since the results will be skewed by the large number of projects with zero ExtremelyBuggy methods.,
We now investigate what percent of bugs are captured by the ExtremelyBuggy methods alone.,
We counted the number of bugs in the ExtremelyBuggy methods in each project and divided it by the total number of bugs in the project The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 9 100 101 % Methods(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF ExtremelyBuggy Buggy (a) HighRecall Dataset 10 2 10 1 100 % Methods(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF ExtremelyBuggy Buggy (b) HighPrecision Dataset 10 2 10 1 100 101 % Methods(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF ExtremelyBuggy Buggy (c) Balanced Dataset Fig.,
CDFs of the percent of methods in that are Buggy and ExtremelyBuggy for each of the 3 datasets for all projects.,
"To enhance readability, we only use 11 markers when presenting each distribution, however, all 92 projects are represented.",
0 25 50 75 100 Coverage 0.0 0.2 0.4 0.6 0.8 1.0 CDF HighRecall HighPrecision Balanced Fig.,
"CDFs of the percent of bugs covered by the ExtremelyBuggy methods for each of the three datasets for all 92 projects, excluding projects with no ExtremelyBuggy methods.",
"To enhance readability, we only use 11 markers when presenting each distribution.",
for each of the 92 projects.,
"To avoid overcounting the number of bugs in each project, we only count a bug once for each unique commit.",
The results for each dataset are presented in figure 3.,
"For the HighRecall Dataset, we can see that the ExtremelyBuggy methods account for a high proportion of bugs, as for ~80% of projects the ExtremelyBuggy methods accounted for ≥60% of bugs and ~50% of projects the ExtremelyBuggy methods covered ≥75% of bugs.",
"Not surprisingly, the results are much different for the HighPrecision and Balanced datasets.",
"For these two datasets, we see much less bug coverage of the top percent of methods with bugs.",
"However, this is expected as in the HighPrecision and Balanced datasets, many of the ExtremelyBuggy methods were excluded for maintaining higher precision.",
"RQ1 Summary: Across all datasets, only a small fraction of methods are ExtremelyBuggy, yet they account for a disproportionately large share of bugs.",
"In the HighRecall dataset, these rare methods account for a large share of bugs, often more than 60% for 80% of the projects.",
"This underscores the importance of early identification and remediation of these methods, as proactive action is crucial for minimizing future maintenance costs.",
"10 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury 4.2 RQ2: Code Quality of ExtremelyBuggy Methods To answer this research question, we analyze how the code quality indicators of ExtremelyBuggy methods differ from those of both Buggy and NotBuggy methods.",
"Since our goal is to identify ExtremelyBuggy methods at the time of their introduction, we compare the code metrics of all three categories—NotBuggy, Buggy, and ExtremelyBuggy—at the point when the methods are first added to the codebase.",
100 101 102 103 SLOCStandard(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy 100 101 102 McCabe(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy 100 101 102 103 totalFanOut(log) 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy 0.00 0.25 0.50 0.75 1.00 Readability 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy 0.00 0.25 0.50 0.75 1.00 SimpleReadability 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy 0 50 100 150 MaintainabilityIndex 0.0 0.2 0.4 0.6 0.8 1.0 CDF NotBuggy Buggy ExtremelyBuggy Fig.,
"CDFs showing the distributions of different code metrics at the first commit between NotBuggy, Buggy, and ExtremelyBuggy methods for the Balanced dataset.",
"To enhance readability, we only use 11 markers when presenting each distribution; however, all methods are represented.",
Note that 34 of the ~700000 methods had a MaintainabilityIndex below zero.,
"To improve readability, we excluded these methods when constructing the MaintainabilityIndex graph.",
Figure 4 compares the three different method classes for six of the selected code metrics using the Balanced dataset; the results are similar for all code metrics across all three datasets.,
"Clearly, ExtremelyBuggy methods are larger, less readable, and more complex than both the Buggy and NotBuggy methods.",
"To confirm these visual findings, we also performed a statistical analysis to compare ExtremelyBuggy methods with both Buggy and NotBuggy methods.",
In Table 2 we present the results when comparing the ExtremelyBuggy methods to Buggy and NotBuggy methods grouped together for the Balanced dataset.,
The comparisons were done on all 92 projects’ distributions aggregated together.,
"We can see that all the P-values are significant (𝑃≤0.05) and that all the effect sizes are large, except for Parameters, which is medium.",
This indicates that there is a substantial difference The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 11 between ExtremelyBuggy methods and all other methods at inception.,
"Additionally, the sign of the effect size is negative (−) for Readability, SimpleReadability, and MaintainabilityIndex indicating that these methods are less readable and maintainable.",
For other code metrics the sign is positive (+) indicating that the ExtremelyBuggy methods are larger and more complex.,
"The results for the HighRecall dataset are the same; however, the effect sizes for the HighRecall dataset are mostly medium.",
"This is because the HighRecall dataset has a higher number of false positives when identifying the ExtremelyBuggy methods, resulting in more noise and less distinction of the ExtremelyBuggy methods.",
The results of statistical significance test (Wilcoxon rank sum test) comparing ExtremelyBuggy methods with the combined group of Buggy and NotBuggy methods for the Balanced dataset dataset.,
The method groups used in the comparison are formed by aggregating methods from all projects.,
"Metric P-value Sign(+/-) Effect Size SLOCStandard 0.00 + large Readability 0.00 - large SimpleReadability 0.00 - large NVAR 0.00 + large NCOMP 0.00 + large Mcclure 0.00 + large McCabe 0.00 + large IndentSTD 0.00 + large MaximumBlockDepth 0.00 + large totalFanOut 0.00 + large Length 0.00 + large MaintainabilityIndex 0.00 - large Parameters 0.00 + medium LocalVariables 0.00 + large In addition to the aggregated analysis, we also perform individual project analysis to obtain generalizable observations.",
Projects that has no ExtremelyBuggy methods were excluded from this analysis.,
Table 3 presents the results for the ExtremelyBuggy compared to the Buggy and NotBuggy methods grouped together for the Balanced dataset.,
"For example, for 78.21% of projects, the difference in SLOCStandard between ExtremelyBuggy and other methods is statistically significant (𝑃≤0.05).",
"For these projects, the difference was large (L) for 98.36% of the projects and for the remaining 1.64% it was medium (M).",
"For all code metrics, some projects (usually approximately 20-30%) did not show statistical differences between ExtremelyBuggy and other methods.",
"This is because some projects in the Balanced dataset have a very low number of ExtremelyBuggy methods, making it difficult to discern differences in distributions.",
"However, for the projects that did have a statistical difference, that difference was almost always large.",
"Out of the 14 code metrics, only one, Parameter, exhibited distinct characteristics.",
"While more than 62% of projects consistently demonstrated statistical significance for all other metrics, Parameter differed for only 20% of projects, indicating that the bug-proneness of methods is not significantly influenced by parameter count.",
"For the HighPrecision dataset, approximately 40% of projects exhibited statistical differences between ExtremelyBuggy and other methods for all metrics, with the effect size remaining almost always large.",
"For the HighRecall dataset, approximately 90-95% of projects demonstrated statistical differences for all metrics; however, the effect size ranged from small (S) to large (L) (typically ~30% in each category).",
This is expected due to the high number of ExtremelyBuggy methods but lower precision in identifying ExtremelyBuggy methods present in the HighRecall dataset.,
"12 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury Table 3.",
"The results of statistical significance test (Wilcoxon rank sum test) and effect size (Cliff’s delta) comparing ExtremelyBuggy methods with the combined group of Buggy and NotBuggy methods for the Balanced dataset dataset, evaluated per project.",
Each value denotes the percentage of projects exhibiting the corresponding behavior.,
"For instance, for the totalFanOut metric, 80.77% of projects show a statistically significant difference, and in 98.41% of those cases, the effect size is classified as large (L).",
"Metric 𝑃≤0.05 N S M L SLOCStandard 78.21 0.00 0.00 1.64 98.36 Readability 65.38 0.00 3.92 5.88 90.20 SimpleReadability 74.36 0.00 0.00 1.72 98.28 NVAR 66.67 0.00 0.00 7.69 92.31 NCOMP 69.23 0.00 0.00 3.70 96.30 Mcclure 67.95 0.00 0.00 3.77 96.23 McCabe 69.23 0.00 0.00 3.70 96.30 IndentSTD 62.82 0.00 4.08 12.24 83.67 MaximumBlockDepth 69.23 0.00 0.00 5.56 94.44 totalFanOut 80.77 0.00 0.00 3.17 96.83 Length 79.49 0.00 0.00 1.61 98.39 MaintainabilityIndex 80.77 0.00 0.00 1.59 98.41 Parameters 20.51 0.00 25.00 50.00 25.00 LocalVariables 65.38 0.00 1.96 7.84 90.20 To further understand how ExtremelyBuggy methods compare to Buggy and NotBuggy ones, we also did these comparisons without grouping the Buggy and NotBuggy methods together.",
We found that the effect size differences between NotBuggy and Buggy methods were generally small to medium and the same was found when comparing the Buggy to ExtremelyBuggy methods.,
"However, the difference between NotBuggy and ExtremelyBuggy methods was mostly large for all code metrics and datasets.",
This finding confirms the visual findings from figure 4 about the differences between the three bug-proneness types.,
"RQ2 Summary: At their inception, ExtremelyBuggy methods are significantly larger, less readable, and more com- plex than Buggy and NotBuggy methods.",
This is encouraging because it suggests these methods are distinguishable at the very beginning of their lifetime.,
"4.3 RQ3: Predicting the ExtremelyBuggy Methods In this section, we investigate whether ExtremelyBuggy methods can be predicted at the time of their creation.",
"Inspired by prior research [9, 41, 50], we employ five machine learning models of increasing complexity: Logistic Regression [30], Decision Tree [68], Random Forest [60], AdaBoost [63], and a Multi-Layer Perceptron (MLP) model [44].",
"Our goal is to systematically evaluate the predictive power of both simple and complex models, assessing whether more sophisticated algorithms offer a significant advantage in identifying buggy code early in the development lifecycle.",
All models were implemented using the default parameters provided by the scikit-learn library.,
"This choice was motivated by two factors: (i) hyperparameter tuning is computationally expensive under the leave-one-out validation setting—the evaluation approach used in this paper—as it requires training a separate model for each test instance, and (ii) prior similar studies have found that default settings often yield competitive performance [9, 50].",
"Moreover, using default parameters enhances the reproducibility of our study and facilitates future replication efforts.",
"The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 13 To build and train these models, we first merged the Buggy and NotBuggy methods into a single category: not ExtremelyBuggy.",
"This transformation enables a binary classification task, allowing the models to specifically focus on distinguishing ExtremelyBuggy methods from all others.",
The models were trained using the 14 code metrics computed for each method at the time of its introducing commit.,
Making predictions by relying solely on metrics available immediately after a method is created allows us to assess the feasibility of detecting ExtremelyBuggy methods as early as possible in the software development process.,
"Due to the highly imbalanced nature of our dataset, we do not use metrics such as accuracy to evaluate the performance of the machine learning models, as they could overstate the performance of our models.",
"Instead, we use precision and recall, F-measure (F1), Area Under the Curve (AUC) score, and the Mathews Correlation Coefficient (MCC).",
"AUC ranges from 0 to 1, with a score of 0.5 being no better than random classification and 1 being perfect.",
"MCC ranges from -1 to 1, with 0 being about random classification and a score of 1 being perfect prediction.",
We assess prediction performance using a leave-one-out approach at the project level.,
"In each iteration, all methods from one project are used as the test set, while methods from all other projects serve as the training data.",
"This process is repeated for every project, ensuring that each serves as the test set exactly once.",
"This evaluation strategy is motivated by two key factors: (i) it prevents data leakage between training and test sets [13, 50], and (ii) it allows us to assess how well the models generalize across different projects.",
"To address the data imbalance problem—specifically, the significantly smaller number of ExtremelyBuggy methods—we apply both undersampling and oversampling techniques using modules provided by the scikit-learn library.",
"While the overall prediction performance remains similar across these strategies, undersampling yields slightly better results.",
Figure 5 presents various metrics for two selected algorithms using the undersampling approach.,
"The results indicate that recall is substantially higher than precision, suggesting that while the machine learning models are effective at identifying most ExtremelyBuggy methods, they tend to generate a high number of false positives.",
"Most project-level evaluations fall within a similar range, but a few notable outliers stand out.",
"For example, at least one project achieved perfect precision (1.00), despite approximately 95% of projects having precision below 0.50.",
"Likewise, although around 90% of projects achieved recall scores above 0.5, there was at least one project with a recall below 0.25.",
RQ3 Summary: Predicting ExtremelyBuggy methods at inception using simple code metrics and standard machine learning algorithms remains highly challenging.,
"While prediction performance varies across projects, the overall results are not yet reliable enough for practical use.",
4.4 RQ4: Observable Characteristics of ExtremelyBuggy Methods The futility of accurate prediction led us to manually characterize the ExtremelyBuggy methods to provide insights for both practitioners and the research community.,
We employed the thematic analysis [18] approach to find themes of the ExtremelyBuggy methods.,
"Thematic analysis involves identifying general patterns (or ""themes"") within the data.",
These themes are created by labeling the data with codes that capture and characterize individual instances of the data and later identifying themes within the codes.,
Our thematic analysis approach followed the guidelines of Cruzes et.,
"al [15], which provides a set of steps for thematic analysis in software engineering research.",
The approach is described as follows.,
"10https://github.com/SQMLab/ExtremelyBuggyPublicData/tree/main/AdditionalFigures/LeaveOneOutTraining 14 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury 0.00 0.25 0.50 0.75 1.00 Score 0.0 0.2 0.4 0.6 0.8 1.0 CDF Precision Recall F1 AUC MCC (a) Logistic Regression 0.00 0.25 0.50 0.75 1.00 Score 0.0 0.2 0.4 0.6 0.8 1.0 CDF Precision Recall F1 AUC MCC (b) Decision Tree Fig.",
Results for two of the select machine learning algorithms for the High Recall dataset.,
"Results are very similar with other algorithms, and the results from the other algorithms can be viewed in our publicly shared repository 10.",
"To enhance readability, we use less number of markers than the original number of projects.",
"Data extraction.For this analysis, we’ve chosen the ExtremelyBuggy methods from only the HighPrecision dataset.",
"The reasons behind this are: (i) it contains only 287 unique methods, allowing for thorough manual analysis, and (ii) it has the potential of minimal false positives compared to the other datasets.",
"However, during our review, we still encountered some falsely labeled bugs—mainly due to the inclusion of the keyword flaw in commit messages (e.g., “fixing quality flaws”), which sometimes referred to non-functional changes.",
"As a result, we excluded 22 such methods, leaving 265 ExtremelyBuggy methods and their evolution history for analysis.",
Generating initial codes from the methods.,
"A code is a label that represents some kind of specific, meaningful concept about the data.",
"To help remove internal bias caused by preconceived ideas about the data, we used an open coding [70] approach to generate codes.",
"This approach allowed us not to start with any code and create code intuitively, only creating new code when no existing codes fit the method.",
"To ensure consistency and correctness in the coding process, the first two authors independently coded 10 selected methods analyzing their history.",
"Following this, the third and fourth authors—who have 2 and 10 years of industry experience, respectively—facilitated a discussion with the first two authors to review the initial codes.",
"We realized that when manually analyzing a data sample, there are multiple dimensions one can take to characterize and assign ""codes"" to it, as was also discussed in other studies (e.g., [32]).",
This makes it difficult to generate very similar codes by two independent annotators.,
"For example, in our case, one may choose to look at how a method is written, noting it is very large and hard to read, while another may look at the context it is written in, noting the method is used to parse a piece of text.",
"As such, after the discussion session, we agreed to perform our manual analysis in accordance with the following three distinct dimensions: (1) Visual codes.",
These codes correspond to how a method looks.,
"When generating these codes, we asked: What stands out visually when looking at this method (e.g., is it less readable)?",
(2) Context codes.,
These codes represent the context a method is used.,
"When generating these codes, we asked: What is the main purpose of this method (e.g., data handling)?",
The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 15 (3) Bug codes.,
These codes are generated to represent the reasons for bugs in a method.,
"When generating these codes, we asked for each bug in a method: Why did this bug occur (e.g., missing conditionals)?",
The first two authors independently coded an additional set of 10 shared methods and produced highly similar codes.,
"With growing confidence in their consistency, the remaining methods were divided between them for individual coding.",
"While it is difficult to calculate agreement scores in open coding [6], the authors conducted a validity check after analyzing all 265 methods.",
"To assess consistency, each author randomly reviewed 30 methods originally coded by the other, and high agreement was observed—differences appeared in only 12 out of 60 cases.",
"Importantly, these were not cases of conflicting codes; rather, one author had identified additional codes that the other had missed.",
"For instance, one author noted the presence of self-admitted technical debt (SATD) [12, 53], which the other did not.",
We do not consider these additional codes a limitation.,
"As discussed later, all identified codes, including the extra ones, were collaboratively reviewed by the two authors and synthesized into broader themes, ensuring the robustness and comprehensiveness of the analysis.",
Translating our codes into more general themes.,
"Once all our methods had been assigned codes, the first two authors collaborated to identify codes with very similar ideas.",
"They then combined these specific codes into a single, more general theme that accurately encompassed all the codes.",
It is important to note that a method can belong to multiple themes even within the same theme dimension due to its diverse characteristics.,
Creating a model of higher-order themes.,
"Once the more general themes were created, the two authors then worked together to merge themes into higher-order themes that encompass a variety of different codes and themes.",
"These themes were created with the aim of being generic enough to be present in a significant proportion of the identified methods and accurately quantify what the authors observed in the ExtremelyBuggy methods, but not too generic that they lose usefulness in characterizing the ExtremelyBuggy methods.",
"Some codes and themes had a very small number of methods that contained them, and did not fit into any of the identified higher-order themes.",
These codes and themes were placed into a higher-order theme labeled as Other.,
4.4.1 Results.,
"In figure 6, we present an example of the complete thematic analysis process for one of our higher-order themes in the bug codes category and the percentages of methods that contain each of the codes and themes.",
The initial codes that the methods were labeled with are in the right column.,
"In the middle column, we show the initial, more general themes that were identified after looking at the codes.",
"For example, the codes logging and avoiding too many logs are combined into the more general theme of logging practices.",
"Finally, the left column displays the higher-order theme, exception/error handling and logging that was identified using the two themes exception and error handling and logging practices.",
"We combined logging and exception handling into a single broader category because both are commonly used to detect, respond to, and monitor erroneous behavior in program code.",
"Together, they serve as essential tools for maintaining robustness, facilitating debugging, and ensuring system observability during failure scenarios [45].",
"For each theme, we calculated the Count (the number of methods that include this theme), Percent (the percentage of all methods that include this theme), and Mean Percent (the average percentage of methods containing this theme across all projects).",
The Mean Percent helps determine whether a high overall percentage is driven by a few outlier projects or represents a more generalizable trend.,
We now describe the themes and show example methods (CodeShovel generated JSON files) associated with the themes.,
These example files can be viewed in our public repository.,
"11 11https://github.com/SQMLab/ExtremelyBuggyPublicData/tree/main/ManualAnalysisExamples 16 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury exception/error handling and logging (47.2%) exception and error handling (46%) null ptr (27.2%) logging (4.9%) avoiding too many logs (0.4%) error handling (7.5%) error message (6.4%) exception handling (15.8%) logging practices (4.9%) more specific exception (1.1%) Fig.",
The taxonomy of one of the themes we identified in the bug themes category during our manual analysis of the ExtremelyBuggy methods.,
"The right column shows initial codes, the middle shows more general themes, and the left contains the higher-order theme.",
Percentages are the number of ExtremelyBuggy methods containing that code/theme divided by the total number of ExtremelyBuggy methods.,
"The complete taxonomies for all three themes (Bug, Context, and Visual) can be found in the shared repository.",
4.4.2 Visual Themes.,
Table 4 shows the final identified themes for the visual/syntactical dimension.,
These are themes that were identified just by looking at the methods.,
Final themes from the manual analysis of syntactic and visual characteristics of ExtremelyBuggy methods.,
Theme Count Percent Mean Percent Confusing Control Flow 155 58.5 50.8 Abnormal Size 93 35.1 31.7 Other 90 34.0 40.2 Poor Readability 86 32.5 19.5 Drastic Change in Size 80 30.2 37.6 SATD 47 17.7 13.4 Sketchy Exception/Error Handling 35 13.2 14.8 • Confusing Control Flow.,
"This theme refers to methods where it is hard to follow the control flow of the method, for example if the method has many if/else statements or deeply nested code.",
An example is the processScope() method (105.json) from the intellij-community repository that has many many if/else statements and deeply nested code resulting in complicated bugs related to its control flow.,
• Abnormal Size.,
These methods are visually very large.,
This reinforces the common belief that very large methods are substantially more susceptible to bugs than smaller ones.,
The processEntry() method (35.json) from the jgit project is an example of a method that is abnormally large.,
These are methods that did not show any unique visual characteristics or did not have repeated examples to be grouped into a theme.,
An example is the disassemble() method (166.json) from the eclipseJdt project.,
This means that it may be difficult to detect these methods at inception using source code metrics alone.,
• Poor Readability.,
Poor Readability refers to methods that are difficult to read or are formatted very poorly.,
"For example, if a method has bad indentation as seen in the search() method (2.json) from the weka project.",
The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 17 • Drastic Change in Size.,
These are methods that started with a few lines of code but got much larger over time.,
"These methods are often unimplemented fully at inception and undergo frequent, large changes that result in bugs being introduced later on.",
An example is the visitSetOperation() method (18.json) from the presto project.,
"Self-admitted technical debt (SATD) [54] occurs when developers acknowledge that there is future work that needs to be done on a method (e.g., with a TODO comment).",
"An example is the parseTransition() method (21.json) from the argouml repository, where unresolved technical debts directly caused bugs in the future.",
• Sketchy Exception/Error Handling.,
"These methods handled errors and exceptions in dubious ways, often by using many or incomplete try/catch blocks.",
A notable example is the invoke() method (88.json) that contains many try/catch blocks and try blocks without catch blocks.,
4.4.3 Context themes.,
Table 5 contains the final 8 themes we identified to explain the contextual purpose of the ExtremelyBuggy methods.,
Final themes from the manual analysis of the contextual purpose of ExtremelyBuggy methods.,
Theme Count Percent Mean Percent Core Logic and Algorithms 70 26.4 20.6 Data Handling and Transformation 56 21.1 22.1 Handling External Resources 44 16.6 28.8 Abstract Code Operations 41 15.5 9.6 Error/Exception Handling and Logging 36 13.6 9.8 Initialization and Setup 27 10.2 6.3 UI and Presentation 24 9.1 11.0 Other 18 6.8 7.6 • Core Logic and Algorithms.,
These methods represent some core logical component of a project that determines how that project works.,
These methods often have complicated logic with requirements that change as the project changes.,
"For example, the method isInUseableZone() (249.json), from the mage repository performs specific logic to check if an object is in a zone where it can be used.",
This method gets updated with more and more logic requirements as the project matures which ultimately result in bugs from incorrect logic.,
• Data Handling and Transformation.,
"These methods deal with or transform data in some way, for example, by encoding/decoding low-level data or parsing a document.",
"An example is the scanExpr() method (178.json) from the xerces2-j repository, which parses an XPath expression and breaks it up into tokens.",
• Handling External Resources.,
External resources such as databases and HTTP messages are handled by these methods.,
"External resources often have specific requirements for how messages are sent and received, which can cause issues if not handled correctly.",
A notable example is the doPost() method (161.json) from the tomcat project that has bugs resulting from the incorrect formatting of an HTTP POST message.,
• Abstract Code Operations.,
"These methods perform operations on abstract code representations, such as building and navigating abstract syntax trees (ASTs).",
"However, most of these methods come from a few projects that deal heavily with ASTs.",
An example is the visit() method (237.json) from the pmd project.,
• Error/Exception Handling and Logging.,
The main purpose of these methods is to handle some type of exception/error or to log for debugging purposes.,
The method handlePackError() (80.json) from the jgit repository is an example of this theme.,
"Note that this theme represents the purpose of the method dealing with 18 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury error/exception handling, whereas the previous visual theme Sketchy Exception/Error Handling describes how the methods performed exception/error handling.",
• Initialization and Setup.,
"These methods configure and set up objects, which can often result in developers forgetting some parts that they need to configure or initialize fully.",
"An example is the buildClassifier() method (181.json) from the weka project, where the developers did not account for missing values during configuration, causing bugs.",
• UI and Presentation.,
These methods involve the creation and handling of the visual components in a project.,
One example is the createDropDownList() method (97.json) from the dbeaver repository.,
4.4.4 Bug Themes.,
"In this dimension of themes, we focus on what was done to fix a bug, not what the method was doing (context) or how it looks (visual).",
Table 6 contains the final 10 themes we identified to explain types of bugs in ExtremelyBuggy methods.,
Final themes from the manual analysis of the reasons for bugs in ExtremelyBuggy methods.,
"Theme Count Percent Mean Percent Conditional Logic 155 58.5 54.5 Exception/Error Handling and Logging 125 47.2 43.3 External Interaction 73 27.5 28.8 Variable Misuse and Naming 54 20.4 19.1 Behavioral Oversights 43 16.2 15.2 Other 28 10.6 11.1 Incorrect Statement Order/Location 25 9.4 2.9 Syntax, Type and Keyword Issues 22 8.3 8.6 Data Lifecycle Mismanagement 21 7.9 6.3 Looping, Indexing and Recursion 20 7.5 4.4 • Conditional Logic.",
These methods had bugs related to incorrect or missing conditional logic.,
An example is the update() method (109.json) from the mindustry repository.,
• Exception/Error Handling and Logging.,
"These methods generated incorrect error messages, did not properly handle null pointer exceptions, or used the wrong types of exceptions.",
An example is getViewSize() (155.json) from the wicket repository.,
• External Interaction.,
These types of bugs occur when a method interacts with other methods and resources.,
"These bugs commonly occur from calling the wrong method, often due to it being overloaded or having a similar name to the one it is trying to call.",
An example is the method addDefaultAttributes() (296.json) from the xerces2-j project that calls toString() instead of stringValue().,
• Variable Misuse and Naming.,
"For these methods, bug-fixes were related to incorrect use and naming of variables.",
The method lookupValues() (44.json) from the pentaho-kettle repository provides an example where the wrong variable was passed to a method.,
• Behavioral Oversights.,
This theme represents bugs that were caused by some missed edge case or behavior that was not handled properly.,
"For example, an edge case was missed in the getHadoopUser() method (7.json) from the flink repository.",
• Incorrect Statement Order/Location.,
"This type of bug occurs when the correct statements are present, but are in the wrong order or location in the method.",
"This can often occur due to deep nesting, resulting in a The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 19 statement being in the wrong set of brackets.",
The method readHeaders() (145.json) from the netty project is one example.,
"• Syntax, Type and Keyword Issues.",
"These bugs occur due to typing issues, usually only requiring one keyword or type to be changed to be fixed.",
"We also noticed that variables were often cast to the incorrect type, which caused the wrong methods to be called or being unable to be called.",
An example is the method supresses() (242.json) from the pmd repository.,
• Data Lifecycle Mismanagement.,
"These are bugs related to the creation, destruction, and handling of data.",
"One example is in the failed() (59.json) method from the pulsar repository that missed releasing resources, causing a memory leak.",
"• Looping, Indexing and Recursion.",
These are bugs related to any kind of looping or recursion and indexing problems.,
"For example, in the replaceEvent() method (328.json) from the mage repo, there is a missing break statement in a loop.",
"RQ4 Summary: ExtremelyBuggy methods often exhibit distinct visual characteristics (e.g., code smells such as large size or poor readability) and have frequent contextual roles (e.g., handling core logic).",
The underlying causes of bugs—such as missing edge cases or incorrect exception handling—are also commonly shared among these methods.,
These observations can help practitioners identify areas that warrant greater attention and support researchers in designing more effective features for future machine learning–based bug prediction models.,
"5 Discussion Due to the importance of method-level bug prediction, significant research has been done in recent years [13, 19, 20, 41, 50, 65].",
"However, these earlier studies treated all bug-prone methods as the same, regardless of how many times a method was involved in bug fixes.",
"In this study, we choose to explore the ExtremelyBuggy methods as understanding and capturing these methods would enable more optimized resource allocation.",
We found that ExtremelyBuggy methods are very small in numbers but can often account for the majority of bugs in a project (RQ1).,
This is encouraging because early optimization of these small numbers of methods can significantly reduce future maintenance burdens.,
"Our analysis showed that ExtremelyBuggy methods are significantly larger, more complex, and less readable than both Buggy and NotBuggy methods (RQ2).",
"However, ExtremelyBuggy methods proved very hard to predict at their inception, as machine learning models showed poor prediction performance (RQ3).",
"Upon manual analysis, we found that the ExtremelyBuggy methods have common visual characteristics, contextual purposes, and similar fix types (RQ4).",
"5.1 Implications for Researchers For the research community, we provide a dataset of over 1.25 million Java methods originating from 98 popular open-source projects.",
"To the best of our knowledge, this is the largest dataset on method-level bugs.",
"This enormous dataset not only enables replicating our results, but also can help answer more research questions.",
"While manually analyzing the ExtremelyBuggy methods (RQ4), we discovered that over 45% of the bugs in the ExtremelyBuggy methods were introduced after modification to the original code.",
"This not only explains the poor prediction performance at the inception of the ExtremelyBuggy methods, but also suggests that future research should incorporate the change history of methods while building the prediction models.",
"20 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury Many of the common themes we observed are not detectable from code metrics alone—e.g., HTTP and database operations, SATD, and abstract code operations.",
This implies that incorporating code embeddings as additional features might help improve the accuracy of these models.,
"We also noticed that prediction performance is not uniform across all projects, as some projects exhibited unusually high or low accuracy.",
Future work should focus on optimal project selection for training models based on the project under test.,
"Previous work in similar studies has shown the promise of optimal project selection during model training [13, 71].",
5.2 Implications for Practitioners Practitioners seeking optimization opportunities should take into account the contextual and visual themes identified in this paper.,
"For instance, methods associated with a project’s core logic and algorithms are significantly more likely to become ExtremelyBuggy.",
"When designing solutions for such methods, conducting a focused brainstorming session can help identify the most effective implementation strategies and anticipate potential future requirement changes related to the core logic that might introduce bugs.",
"Notably, conditional logic emerged as one of the most bug-prone elements in methods.",
"Therefore, when developers encounter complex conditional structures—especially in high-risk contexts—they should consider breaking them down into simpler, smaller logical components.",
"Additionally, practitioners are encouraged to use code smell detectors, as many extremely buggy methods exhibit common code smells.",
Practitioners should also address self-admitted technical debts (SATD) as early as possible.,
"In addition, dedicated effort to the design and implementation of test code can help prevent bugs related to unhandled edge cases and incorrect exception handling that we observed frequently with these methods.",
6 Threats to Validity We have identified multiple multiple threats that may impact the validity of our findings.,
Construct validity is impacted by our choice of the CodeShovel tool for extracting method histories.,
"Although CodeShovel has been known to show poor performance for very small methods [12], it has still outperformed other tools, including FinerGit and IntelliJ / git log [24].",
"A more recent tool, CodeTracker [33], which has been shown to outperform CodeShovel, can be used in future work to see how our results hold up.",
"We used a variety of popular product metrics for evaluating the ExtremelyBuggy methods, however, other metrics could be considered.",
"For example, developer-centric metrics could be used, although we chose not to include developer- centric metrics as they are not very useful at the beginning of a project, and we wanted our findings to be applicable at a projects inception.",
"To detect the bug-proneness of a method, we used a keyword-based approach, which is known to suffer from false positives [3].",
"However, this threat was mitigated by the construction of three different datasets: one with high precision, one with high recall, and one aimed at a balance between precision and recall.",
"To identify themes in the ExtremelyBuggy methods, we used a manual labeling approach, which in inherently susceptible to human bias and error.",
"To mitigate this threat, we employed the help of two graduate students, with 2 and 10 years of industry experience respectively.",
"After two of the authors labeled 10 methods, these graduate students analyzed the results, and verified the correctness of the assigned labels.",
The use of two separate authors for manual labeling of methods may also be a threat to validity if they don’t have agreement between the assignment of labels.,
"To curb this threat, the two authors, labeled 10 of the same methods independently, then discussed together and resolved The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 21 any disagreements that arose.",
"They did this again for another 10 methods, and, after finding almost no disagreements, conducted the rest of the analysis separately.",
"For the manual analysis of the methods, the high precision dataset was used to reduce the number of false positives during labeling.",
"However, the high precision dataset can only consider a commit to be a bug-fix if there was no tangled changes, that is, if only one method was changed in that commit.",
This may introduce bias towards certain types of bugs that only affect one method.,
Future work can be done to investigate ExtremelyBuggy methods bugs that affect multiple methods to validate our results.,
"Additionally, the ExtremelyBuggy methods in the high precision dataset are not evenly distributed, with over half of all ExtremelyBuggy methods coming from just 5 projects.",
"However, upon removing these projects we found that none of the main takeaways from the paper changed, in turn mitigating this threat.",
"Internal validity is hampered due to our selection of statistical tests such as Kendall’s 𝜏, the Wilcoxon Rank Sum test and Cliffs d. However, all three are well established and used in software engineering research [11, 12, 21, 31].",
"External validity is affected due to our use of only open source, java repositories, and it is unclear how our findings would extend to closed source projects or projects in different languages.",
We also ensured that the descriptions for our repositories were written in english.,
This was necessary to ensure bug-fix commits were able to identified using a english language keyword based approach and to ensure we could use commit messages and comments to assist in manual labeling.,
Future work is needed to find explore the differences in repositories with commit messages written in different languages.,
Conclusion validity of this study is influenced by all previous threats to validity.,
"Additionally, our selection of a five- year threshold for age-normalization was chosen arbitrarily to allow for more change history of methods.",
"Consequently, we repeated most of our experiments using a two and eight year threshold, and none of the major results from the first three RQs changed.",
"However, future work is needed to see how the results may change for the manual analysis of ExtremelyBuggy methods using different age-normalization thresholds.",
"7 Conclusion This study provides the first large-scale, method-level investigation into ExtremelyBuggy methods, those repeatedly involved in bug-fix activities, using a dataset of over 1.25 million methods from 98 open-source Java systems.",
"Our empirical analysis demonstrates that although ExtremelyBuggy methods constitute only a small fraction of a project’s methods, they frequently account for a disproportionately large share of bugs.",
"These methods exhibit clear distinguishing characteristics at their inception: they are typically larger, more complex, less readable, and less maintainable than both simple buggy and non-buggy methods.",
"Yet, despite these measurable differences, our evaluation of multiple machine learning models reveals that early prediction of ExtremelyBuggy methods remains highly challenging due to severe class imbalance, cross-project variability, and the fact that many defects arise through later code evolution rather than initial implementation.",
These findings highlight limitations in existing metric-based prediction approaches and emphasize the need for richer representations of code and project history.,
"To complement the quantitative analyses, our manual thematic examination of 265 ExtremelyBuggy methods reveals recurring visual, contextual, and defect-related patterns.",
"Visually, these methods often exhibit clear code smells, including confusing control flow, abnormal size growth, poor readability, self-admitted technical debt, and fragile exception-handling structures.",
"Many ExtremelyBuggy methods implement core logic, transform complex data structures, or interface with external resources—roles inherently prone to evolving requirements and error-prone interactions.",
"Common root causes of their repeated failures include missing or incorrect conditional logic, improper exception handling, misuse of variables, and interaction errors stemming from related APIs.",
"These observations carry practical 22 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury implications: developers may benefit from additional scrutiny and testing of methods that embody these high-risk characteristics, while researchers should explore hybrid models that integrate code embeddings, change-history signals, and contextual semantic information.",
"Overall, this work deepens the understanding of extreme bug-prone methods and establishes a foundation for future research toward more robust, early identification of methods likely to cause recurring bugs.",
"References [1] Abdul Ali Bangash, Hareem Sahar, Abram Hindle, and Karim Ali.",
On the time-based conclusion stability of cross-project defect prediction models.,
"Empirical Software Engineering 25, 6 (2020), 5047–5083.",
"Basili, L.C.",
"Briand, and W.L.",
A validation of object-oriented design metrics as quality indicators.,
"IEEE Transactions on Software Engineering 22, 10 (Oct. 1996), 751–761.",
"doi:10.1109/32.544352 [3] Christian Bird, Adrian Bachmann, Eirik Aune, John Duffy, Abraham Bernstein, Vladimir Filkov, and Premkumar Devanbu.",
Fair and balanced?,
bias in bug-fix datasets.,
In Proceedings of the 7th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering.,
[4] Raymond PL Buse and Westley R Weimer.,
Learning a metric for code readability.,
"IEEE Transactions on software engineering 36, 4 (2009), 546–558.",
[5] Jürgen Börstler and Barbara Paech.,
The Role of Method Chains and Comments in Software Readability and Comprehension—An Experiment.,
"IEEE Transactions on Software Engineering 42, 9 (Sept. 2016), 886–898.",
"doi:10.1109/TSE.2016.2527791 [6] M Ariel Cascio, Eunlye Lee, Nicole Vaudrin, and Darcy A Freedman.",
A team-based approach to open coding: Considerations for creating intercoder consensus.,
"Field methods 31, 2 (2019), 116–130.",
[7] Celerity.,
The True Cost of a Software Bug: Part One.,
https://www.celerity.com/insights/the-true-cost-of-a-software-bug.,
[Online; last accessed 01-Sep-2022].,
"[8] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang, Tao Wei, and Long Lu.",
Savior: Towards bug-driven hybrid testing.,
In 2020 IEEE Symposium on Security and Privacy (SP).,
"IEEE, 1580–1596.",
[9] Shaiful Chowdhury.,
"The Good, the Bad, and the Monstrous: Predicting Highly Change-Prone Source Code Methods at Their Inception.",
"ACM Transactions on Software Engineering and Methodology (Jan. 2025), 3715006. doi:10.1145/3715006 [10] Shaiful Chowdhury, Stephanie Borle, Stephen Romansky, and Abram Hindle.",
Greenscaler: training software energy models with automatic test generation.,
"Empirical Software Engineering 24 (2019), 1649–1692.",
"[11] Shaiful Chowdhury, Reid Holmes, Andy Zaidman, and Rick Kazman.",
Revisiting the debate: Are code metrics useful for measuring maintenance effort?,
"Empirical Software Engineering 27, 6 (Nov. 2022), 158. doi:10.1007/s10664-022-10193-8 [12] Shaiful Chowdhury, Hisham Kidwai, and Muhammad Asaduzzaman.",
Evidence is All We Need: Do Self-Admitted Technical Debts Impact Method-Level Maintenance?.,
In 2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR).,
"IEEE, 813–825.",
"[13] Shaiful Chowdhury, Gias Uddin, Hadi Hemmati, and Reid Holmes.",
Method-level Bug Prediction: Problems and Promises.,
"ACM Transactions on Software Engineering and Methodology 33, 4 (May 2024), 1–31.",
"doi:10.1145/3640331 [14] Shaiful Alam Chowdhury, Gias Uddin, and Reid Holmes.",
An empirical study on maintainable method size in java.,
In Proceedings of the 19th International Conference on Mining Software Repositories.,
[15] D. S. Cruzes and T. Dyba.,
Recommended Steps for Thematic Synthesis in Software Engineering.,
In 2011 International Symposium on Empirical Software Engineering and Measurement.,
"IEEE Comp Soc, IEEE, LOS ALAMITOS, 275–284.",
"[16] Marco D’Ambros, Michele Lanza, and Romain Robbes.",
An extensive comparison of bug prediction approaches.,
In 2010 7th IEEE working conference on mining software repositories (MSR 2010).,
"IEEE, 31–41.",
"[17] Khaled El Emam, Saïda Benlarbi, Nishith Goel, and Shesh N. Rai.",
The confounding effect of class size on the validity of object-oriented metrics.,
"IEEE Transactions on Software Engineering 27, 7 (2001), 630–650.",
[18] Jennifer Fereday and Eimear Muir-Cochrane.,
Demonstrating rigor using thematic analysis: A hybrid approach of inductive and deductive coding and theme development.,
"International journal of qualitative methods 5, 1 (2006), 80–92.",
"[19] Rudolf Ferenc, Péter Gyimesi, Gábor Gyimesi, Zoltán Tóth, and Tibor Gyimóthy.",
An automatically created novel bug dataset and its validation in bug prediction.,
"Journal of Systems and Software 169 (Nov. 2020), 110691. doi:10.1016/j.jss.2020.110691 [20] Emanuel Giger, Marco D’Ambros, Martin Pinzger, and Harald C. Gall.",
Method-level bug prediction.,
In Proceedings of the ACM-IEEE international symposium on Empirical software engineering and measurement (ESEM ’12).,
"Association for Computing Machinery, New York, NY, USA, 171–180.",
doi:10.1145/2372251.2372285 [21] Yossi Gil and Gal Lalouche.,
On the correlation between size and metric validity.,
"Empirical Software Engineering 22, 5 (Oct. 2017), 2585–2611.",
"doi:10.1007/s10664-017-9513-5 [22] Todd L Graves, Alan F Karr, James S Marron, and Harvey Siy.",
Predicting fault incidence using software change history.,
"IEEE Transactions on software engineering 26, 7 (2002), 653–661.",
"The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods 23 [23] Abraham Grosfeld-Nir, Boaz Ronen, and Nir Kozlovsky.",
The Pareto managerial principle: when does it apply?,
"International Journal of Production Research 45, 10 (2007), 2317–2325.",
"[24] Felix Grund, Shaiful Alam Chowdhury, Nick C. Bradley, Braxton Hall, and Reid Holmes.",
CodeShovel: Constructing Method-Level Source Code Histories.,
In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).,
"IEEE, Madrid, ES, 1510–1522.",
doi:10.1109/ICSE43902.2021.,
"00135 [25] Hideaki Hata, Osamu Mizuno, and Tohru Kikuno.",
Bug prediction based on fine-grained module histories.,
In 2012 34th international conference on software engineering (ICSE).,
"IEEE, 200–210.",
"[26] Peng He, Bing Li, Xiao Liu, Jun Chen, and Yutao Ma.",
An empirical study on software defect prediction with a simplified metric set.,
"Information and Software Technology 59 (2015), 170–190.",
"[27] Steffen Herbold, Alexander Trautsch, Benjamin Ledel, Alireza Aghamohammadi, Taher A Ghaleb, Kuljit Kaur Chahal, Tim Bossenmaier, Bhaveet Nagaria, Philip Makedonski, Matin Nili Ahmadabadi, et al.",
A fine-grained data set and analysis of tangling in bug fixing commits.,
"Empirical Software Engineering 27, 6 (2022).",
[28] Melinda R Hess and Jeffrey D Kromrey.,
Robust confidence intervals for effect sizes: A comparative study of Cohen’sd and Cliff’s delta under non-normality and heterogeneous variances.,
"In annual meeting of the American Educational Research Association, Vol.",
"[29] Abram Hindle, Michael W Godfrey, and Richard C Holt.",
Reading beside the lines: Indentation as a proxy for complexity metric.,
In 2008 16th IEEE International Conference on Program Comprehension.,
"IEEE, 133–142.",
"[30] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant.",
Applied logistic regression.,
John Wiley & Sons.,
[31] Laura Inozemtseva and Reid Holmes.,
Coverage is not strongly correlated with test suite effectiveness.,
In Proceedings of the 36th international conference on software engineering.,
"[32] Sigma Jahan, Saurabh Singh Rajput, Tushar Sharma, and Mohammad Masudur Rahman.",
Why Attention Fails: A Taxonomy of Faults in Attention-Based Neural Networks.,
arXiv preprint arXiv:2508.04925 (2025).,
[33] Mehran Jodavi and Nikolaos Tsantalis.,
Accurate method and variable tracking in commit history.,
In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022).,
"Association for Computing Machinery, New York, NY, USA, 183–195.",
"doi:10.1145/3540250.3549079 [34] John Johnson, Sergio Lubo, Nishitha Yedla, Jairo Aponte, and Bonita Sharif.",
An empirical study assessing source code readability in comprehension.,
In 2019 IEEE International conference on software maintenance and evolution (ICSME).,
"IEEE, 513–523.",
"[35] Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M German, and Daniela Damian.",
The promises and perils of mining github.,
In Proceedings of the 11th working conference on mining software repositories.,
"[36] Davy Landman, Alexander Serebrenik, and Jurgen Vinju.",
Empirical analysis of the relationship between CC and SLOC in a large corpus of Java methods.,
In 2014 IEEE International Conference on Software Maintenance and Evolution.,
"IEEE, 221–230.",
"[37] Michele Lanza, Andrea Mocci, and Luca Ponzanelli.",
"The tragedy of defect prediction, prince of empirical software engineering research.",
"IEEE Software 33, 6 (2016), 102–105.",
"[38] Valentina Lenarduzzi, Alberto Sillitti, and Davide Taibi.",
Analyzing forty years of software maintenance models.,
In 2017 IEEE/ACM 39th international conference on software engineering companion (ICSE-C).,
"IEEE, 146–148.",
[39] Thomas J McCabe.,
A complexity measure.,
"IEEE Transactions on software Engineering 4 (1976), 308–320.",
[40] Carma L McClure.,
A model for program complexity analysis.,
In Proceedings of the 3rd international conference on Software engineering.,
"[41] Ran Mo, Shaozhi Wei, Qiong Feng, and Zengyang Li.",
An exploratory study of bug prediction at the method level.,
"Information and software technology 144 (2022), 106794.",
[42] Mockus and Votta.,
Identifying reasons for software changes using historic databases.,
In Proceedings International Conference on Software Maintenance ICSM-94.,
IEEE Comput.,
"Press, Victoria, BC, Canada, 120–130.",
"doi:10.1109/ICSM.2000.883028 [43] Manishankar Mondal, Banani Roy, Chanchal K. Roy, and Kevin A. Schneider.",
Investigating the relationship between evolutionary coupling and software bug-proneness.,
In Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering (CASCON ’19).,
"IBM Corp., USA, 173–182.",
[44] Fionn Murtagh.,
Multilayer perceptrons for classification and regression.,
"Neurocomputing 2, 5-6 (1991), 183–197.",
"[45] Suman Nakshatri, Maithri Hegde, and Sahithi Thandra.",
Analysis of exception handling patterns in Java projects: An empirical study.,
In Proceedings of the 13th International Conference on Mining Software Repositories.,
"[46] Alberto S Nuñez-Varela, Héctor G Pérez-Gonzalez, Francisco E Martínez-Perez, and Carlos Soubervielle-Montalvo.",
Source code metrics: A systematic mapping study.,
"Journal of Systems and Software 128 (2017), 164–197.",
[47] Paul Oman and Jack Hagemeister.,
Metrics for assessing a software system’s maintainability.,
In Proceedings Conference on Software Maintenance 1992.,
"IEEE Computer Society, 337–338.",
"[48] Md Nahidul Islam Opu, Shaowei Wang, and Shaiful Chowdhury.",
LLM-Based Detection of Tangled Code Changes for Higher-Quality Method-Level Bug Datasets.,
arXiv preprint arXiv:2505.08263 (2025).,
"[49] Fabio Palomba, Andy Zaidman, Rocco Oliveto, and Andrea De Lucia.",
An Exploratory Study on the Relationship between Changes and Refactoring.,
"In Proceedings of the 25th International Conference on Program Comprehension (Buenos Aires, Argentina).",
"24 Ethan Friesen, Sasha Morton-Salmon, Md Nahidul Islam Opu, Shahidul Islam, and Shaiful Chowdhury [50] Luca Pascarella, Fabio Palomba, and Alberto Bacchelli.",
On the performance of method-level bug prediction: A negative result.,
"Journal of Systems and Software 161 (March 2020), 110493. doi:10.1016/j.jss.2019.110493 [51] Fabiano Pecorelli, Gemma Catolino, Filomena Ferrucci, Andrea De Lucia, and Fabio Palomba.",
Testing of mobile applications in the wild: A large-scale empirical study on android apps.,
In Proceedings of the 28th international conference on program comprehension.,
"[52] Daryl Posnett, Abram Hindle, and Premkumar Devanbu.",
A simpler model of software readability.,
In Proceedings of the 8th working conference on mining software repositories.,
[53] A. Potdar and E. Shihab.,
An exploratory study on self-admitted technical debt.,
"In Software Maintenance and Evolution (ICSME), 2014 IEEE International Conference on.",
"IEEE, 91–100.",
[54] Aniket Potdar and Emad Shihab.,
An exploratory study on self-admitted technical debt.,
In 2014 IEEE International Conference on Software Maintenance and Evolution.,
"IEEE, 91–100.",
[55] Foyzur Rahman and Premkumar Devanbu.,
"How, and why, process metrics are better.",
In 2013 35th international conference on software engineering (ICSE).,
"IEEE, 432–441.",
[56] Md Saidur Rahman and Chanchal K Roy.,
On the relationships between stability and bug-proneness of code clones: An empirical study.,
In 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation (SCAM).,
"IEEE, 131–140.",
[57] Paul Ralph and Ewan Tempero.,
Construct validity in software engineering research and software metrics.,
In Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018.,
"[58] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and Premkumar Devanbu.",
"On the ""naturalness"" of buggy code.",
In Proceedings of the 38th International Conference on Software Engineering (ICSE ’16).,
"Association for Computing Machinery, New York, NY, USA, 428–439.",
"doi:10.1145/2884781.2884848 [59] Nornadiah Mohd Razali, Yap Bee Wah, et al.",
"Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests.",
"Journal of statistical modeling and analytics 2, 1 (2011), 21–33.",
[60] Steven J Rigatti.,
Random forest.,
"Journal of insurance medicine 47, 1 (2017), 31–39.",
[61] Daniele Romano and Martin Pinzger.,
Using source code metrics to predict change-prone Java interfaces.,
In 2011 27th IEEE International Conference on Software Maintenance (ICSM).,
doi:10.1109/ICSM.2011.6080797 ISSN: 1063-6773.,
"[62] Simone Scalabrino, Mario Linares-Vasquez, Denys Poshyvanyk, and Rocco Oliveto.",
Improving code readability models with textual features.,
In 2016 IEEE 24th International Conference on Program Comprehension (ICPC).,
"IEEE, 1–10.",
[63] Robert E Schapire.,
Explaining adaboost.,
In Empirical inference: festschrift in honor of vladimir N. Vapnik.,
"[64] Emad Shihab, Ahmed E. Hassan, Bram Adams, and Zhen Ming Jiang.",
An industrial study on the risk of software changes.,
In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering (FSE ’12).,
"Association for Computing Machinery, New York, NY, USA, 1–11.",
"doi:10.1145/2393596.2393670 [65] Thomas Shippey, Tracy Hall, Steve Counsell, and David Bowes.",
So You Need More Method Level Datasets for Your Software Defect Prediction?,
(ESEM ’16).,
"[66] Shivkumar Shivaji, E James Whitehead, Ram Akella, and Sunghun Kim.",
Reducing features to improve code change-based bug prediction.,
"IEEE Transactions on Software Engineering 39, 4 (2012), 552–569.",
"[67] Dag IK Sjøberg, Aiko Yamashita, Bente CD Anda, Audris Mockus, and Tore Dybå.",
Quantifying the effect of code smells on maintenance effort.,
"IEEE Transactions on Software Engineering 39, 8 (2012), 1144–1156.",
[68] Yan-Yan Song and Ying Lu.,
Decision tree methods: applications for classification and prediction.,
Shanghai archives of psychiatry (2015).,
"[69] Davide Spadini, Fabio Palomba, Andy Zaidman, Magiel Bruntink, and Alberto Bacchelli.",
On the Relation of Test Smells to Software Code Quality.,
In 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME).,
"IEEE, Madrid, 1–12.",
doi:10.1109/ICSME.2018.00010 [70] Anselm Strauss and Juliet Corbin.,
Basics of qualitative research techniques.,
"[71] Zhongbin Sun, Junqi Li, Heli Sun, and Liang He.",
CFPS: Collaborative filtering based source projects selection for cross-project defect prediction.,
"Applied Soft Computing 99 (2021), 106940.",
[72] Chakkrit Tantithamthavorn and Ahmed E Hassan.,
An experience report on defect modelling in practice: Pitfalls and challenges.,
In Proceedings of the 40th International conference on software engineering: Software engineering in practice.,
"[73] Zixu Wang, Weiyuan Tong, Peng Li, Guixin Ye, Hao Chen, Xiaoqing Gong, and Zhanyong Tang.",
BugPre: an intelligent software version-to- version bug prediction system using graph convolutional neural networks.,
"Complex & Intelligent Systems 9, 4 (2023), 3835–3855.",
"[74] Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Hideaki Hata, and Kenichi Matsumoto.",
Predicting defective lines using a model-agnostic technique.,
"IEEE Transactions on Software Engineering 48, 5 (2020), 1480–1496.",
"[75] Yuming Zhou, Baowen Xu, and Hareton Leung.",
On the ability of complexity metrics to predict fault-prone classes in object-oriented systems.,
"Journal of Systems and Software 83, 4 (April 2010), 660–674.",
doi:10.1016/j.jss.2009.11.704,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs Extended Version ALEXANDRE MOINE, New York University, USA SAM WESTRICK, New York University, USA JOSEPH TASSAROTTI, New York University, USA Nondeterminism makes parallel programs challenging to write and reason about.",
"To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way.",
Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order.,
"However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.",
"To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety.",
"A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe.",
We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety.,
"Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.",
"Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal deter- minism.",
"MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set.",
"Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.",
All results in this paper have been verified in Rocq using the Iris separation logic framework.,
CCS Concepts: • Theory of computation →Program verification; Separation logic; • Software and its engineering →Parallel programming languages.,
"Additional Key Words and Phrases: program verification, separation logic, parallelism, determinism ACM Reference Format: Alexandre Moine, Sam Westrick, and Joseph Tassarotti.",
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs: Extended Version.,
ACM Program.,
"10, POPL, Article 26 (January 2026), 32 pages.",
https://doi.org/10.1145/3776668 1 Introduction One of the most challenging aspects of concurrent and parallel programming is dealing with nondeterminism.,
Nondeterminism complicates almost every aspect of trying to make programs correct.,
Bugs often arise because programmers struggle to reason about the set of all possible non- deterministic outcomes and interleavings.,
"Finding those bugs becomes more difficult, as testing can only cover a subset of possible outcomes.",
"Even when bugs are found, nondeterminism makes them Authors’ Contact Information: Alexandre Moine, alexandre.moine@nyu.edu, New York University, New York, USA; Sam Westrick, shw8119@nyu.edu, New York University, New York, USA; Joseph Tassarotti, jt4767@nyu.edu, New York University, New York, USA.",
This work is licensed under a Creative Commons Attribution 4.0 International License.,
© 2026 Copyright held by the owner/author(s).,
ACM 2475-1421/2026/1-ART26 https://doi.org/10.1145/3776668 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
"Publication date: January 2026. arXiv:2511.23283v1 [cs.PL] 28 Nov 2025 26:2 Alexandre Moine, Sam Westrick, and Joseph Tassarotti harder to reproduce and debug.",
"These challenges also extend to formal methods for such programs, where nondeterminism makes various analyses and verification techniques more complex.",
"For these reasons, there has long been interest in methods for deterministic parallel programming.",
A range of algorithmic techniques [Blelloch et al.,
"2012], language designs [Blelloch et al.",
1994; Kuper et al.,
"2014a], type systems [Bocchino Jr. et al.",
"2009], specialized operating systems and runtimes [Aviram et al.",
"2010], and various other approaches have been developed for making parallel programs deterministic.",
"Researchers in this area have long noted that determinism is not simply a binary property, and in fact there is a spectrum of degrees of determinism.",
"On one end of the spectrum is external determinism, which simply says that the input/output behavior of a program is deterministic.",
"However, in an externally deterministic program, even if the final output is deterministic, the manner in which the computation takes place may be highly nondeterministic and vary across runs.",
"As a result, external determinism does not eliminate all of the programming challenges associated with nondeterminism.",
"For example, a programmer who attaches a debugger to an externally deterministic program may still see different internal behaviors across different runs, complicating efforts to understand the program’s behavior.",
"A stronger property, called internal determinism, requires in addition that the structure and internal steps of a computation are deterministic.",
"More formally, in an internally deterministic program, for a given input, every execution will generate the same computation graph, a trace that captures the dependencies of operations and their results.",
"With this strong form of determinism, we can reason about the program’s behavior by considering any one sequential traversal of operations in the computation graph.",
"This is useful, because as Blelloch et al.",
"[2012] put it: In addition to returning deterministic results, internal determinism has many advan- tages including ease of reasoning about the code, ease of verifying correctness, ease of debugging, ease of defining invariants, ease of defining good coverage for testing, and ease of formally, informally and experimentally reasoning about performance.",
"Although ensuring internal determinism might seem expensive, Blelloch et al.",
"[2012] have shown that by using a core set of algorithmic techniques and building blocks, it is possible to develop fast and scalable internally deterministic algorithms for a range of benchmark problems.",
"In this paper, we explore the meaning and benefits of internal determinism from the perspective of program verification.",
"If one of the advantages of internal determinism is that it simplifies reasoning about programs, then it should be possible to exploit this property in the form of new reasoning rules in a program logic.",
"To do so, we first define a property we call schedule-independent safety, which holds for a parallel program 𝑒if, to verify that every execution of 𝑒is safe (i.e.",
"never triggers undefined behavior or a failing assert), it suffices to prove that at least one interleaving of operations in 𝑒is terminating and safe.",
"Internal determinism implies schedule-independent safety, and it is this property that makes reasoning about internally deterministic programs simpler.",
"Schedule- independent safety recalls the motto of Dumas’ Three Musketeers, “all for one and one for all”: the safety of all interleavings amounts to the safety of one of them.",
"Building on this observation, we develop Musketeer, a separation logic for proving that a program satisfies schedule-independent safety.",
"Although Musketeer is formulated as a unary program logic, schedule-independent safety is a ∀∀hyperproperty [Clarkson and Schneider 2010], since it relates safety of any chosen execution of a program 𝑒to all other executions of 𝑒.",
"Thus, to prove the soundness of Musketeer, we encode Musketeer triples into a new relational logic called ChainedLog.",
"In contrast to most prior relational concurrent separation logics, which are restricted to ∀∃hyperproperties, ChainedLog supports ∀∀ hyperproperties using a judgement we call a chained triple.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:3 Intuitively, the high-level reasoning rules of Musketeer restrict the user to verify only internally- deterministic programs.",
"However, while internally deterministic programs always satisfy schedule- independent safety, the converse is false: a program may be nondeterministic because it observes actions from concurrent tasks, but it may do so without jeopardizing safety.",
"In order to verify such programs (§7.2, §7.3), we use the fact that Musketeer is defined in terms of the more flexible and more complex ChainedLog, and drop down to this low-level logic to conduct the proof.",
We next explore how to exploit schedule-independent safety to simplify verification of programs.,
"To that end, we present a logic called Angelic that allows one to angelically select and verify one sequential ordering of operations in a parallel program.",
Angelic is sound to apply to programs that satisfy schedule-independent safety because the safety and termination of the one ordering verified during the proof will imply safety for all other executions.,
"This is in contrast to standard concurrent separation logics, in which one must consider all possible orderings during a proof.",
"Using these logics, we verify a number of examples from the literature on internal determinism and related properties.",
"First, we show how to use Musketeer to prove properties about language- based approaches for enforcing internal determinism.",
"In particular, because Musketeer is a higher- order impredicative logic, Musketeer can encode logical relations models for type systems that are designed to enforce internal determinism.",
We start by applying this to a simple ownership-based affine type system we call MiniDet.,
The resulting logical relations model for MiniDet shows that every well-typed program satisfies schedule-independent safety.,
"Next we use Musketeer to prove specifications for priority writes and deterministic concurrent hash sets, two of the core primitives that Blelloch et al.",
[2012] use in several of their examples of internally deterministic algorithms.,
"Using these specifications, we extend MiniDet and its logical relations model with typing rules for priority writes and hash sets, showing that schedule-independent safety is preserved.",
"Finally, putting these pieces together, we turn to parallel array deduplication, one of the example benchmark problems considered by Blelloch et al.",
"We first show that an implementation of the algorithm they propose for this problem can be syntactically-typed in MiniDet, thereby showing that it is schedule-independent safe.",
"Next, we use Angelic to verify a correctness property for this algorithm.",
"Although the algorithm is written using a parallel for-loop that does concurrent insertions into a hash set, by using Angelic, we can reason as if the parallel loop was a standard, sequential loop, thereby simplifying verification.",
Contributions.,
The contributions of this paper are the following: • We identify schedule-independent safety as a key property of deterministic parallel programs.,
"• We present Musketeer, a separation logic for proving that a program satisfies schedule- independent safety, meant to be used as a tool for proving automatic approaches correct.",
"• We present Angelic, a separation logic for proving that one interleaving safely terminates.",
"• We use Musketeer to verify properties of MiniDet, an affine type system guaranteeing schedule-independent safety.",
"• We verify that priority writes and a deterministic concurrent hash set satisfy schedule- independent safety using Musketeer, and then use this property to verify a deduplication algorithm using Angelic.",
• We formally verify all the presented results [Moine et al.,
"2025], including the soundness of the logics and the examples, in the Rocq prover using the Iris framework [Jung et al.",
"2 Key Ideas In this section, we first give a simple motivating example (§2.1), describe some of the core concepts behind how Musketeer guarantees schedule-independent safety (§2.2), and conclude by showing some of the rules of Angelic that allow for reasoning sequentially about a parallel program (§2.3).",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:4 Alexandre Moine, Sam Westrick, and Joseph Tassarotti M-Assert {⊤} assert𝑣{𝜆𝑤_.",
⌜𝑤= () ∧𝑣= true⌝} M-KSplit counter𝑣(𝑞1 + 𝑞2) (𝑖1 + 𝑖2) ⊣⊢counter𝑣𝑞1 𝑖1 ∗counter𝑣𝑞2 𝑖2 M-KRef {⊤} ref 𝑖{𝜆𝑣_.,
counter𝑣1𝑖} M-KAdd {counter𝑣𝑞𝑖} atomic_add𝑣𝑗{𝜆_ _. counter𝑣𝑞(𝑖+ 𝑗)} M-KGet {counter𝑣1𝑖} get𝑣{𝜆𝑤_.,
⌜𝑤= 𝑖⌝∗counter𝑣1𝑖} M-Par {𝑃1} 𝑒1 {𝑄1} {𝑃2} 𝑒2 {𝑄2} {𝑃1 ∗𝑃2} par𝑒1 𝑒2 {𝜆𝑣𝑥.,
∃𝑣1 𝑣2 𝑥1 𝑥2.,
"⌜𝑣= (𝑣1, 𝑣2) ∧𝑥= (𝑥1,𝑥2)⌝∗𝑄1 𝑣1 𝑥1 ∗𝑄2 𝑣2 𝑥2} Fig.",
Reasoning Rules for a Concurrent Counter and Key Reasoning Rules of Musketeer 2.1 A Motivating Example Our example program is named dumas and appears below: dumas ≜ 𝜆𝑛.,
let𝑟= ref 0 in par (𝜆_.,
atomic_add𝑟1802) (𝜆_.,
atomic_add𝑟42); assert (get𝑟== 𝑛) The dumas program takes an argument 𝑛.,
"It first allocates a reference 𝑟initialized to 0, and then calls in parallel two closures, one that atomically adds 1802 to 𝑟, and the other that atomically adds 42.",
"After the parallel phase, the function asserts that the content of 𝑟is equal to 𝑛.",
"Imagine we wish to prove that (dumas 1844) is safe—that is, for every interleaving, the program will never get stuck, and in particular the assertion will succeed.",
"Of course, many existing concurrent separation logics can easily prove this.",
"In such logics, one can use an invariant assertion to reason about the shared access to 𝑟by the two parallel threads.",
"This invariant would ensure that, no matter which order the threads perform their additions, after both have finished 𝑟will contain 1844.",
We propose an alternate approach that simplifies reasoning by exploiting the internal determinism in programs like dumas.,
"In our approach, we first prove in a light-weight way that, for any given value of 𝑛, the order of the parallel additions in (dumas𝑛) does not affect the outcome of the assert.",
"Then, to prove safety of (dumas𝑛) for the specific value of 𝑛= 1844, we can just pick one possible ordering and verify safety of that ordering.",
"2.2 Verifying Schedule-Independent Safety with Musketeer Our first contribution is Musketeer, a logic for proving that a program satisfies schedule-independent safety, i.e.",
that safety of any one complete execution implies safety of all possible executions.,
"Although Musketeer is itself a program logic, we stress that Musketeer is not meant to be used directly.",
"Rather, Musketeer is a kind of intermediate logic designed for proving the soundness of other light-weight, automatic approaches of ensuring schedule-independent safety such as type systems.",
"For instance, our main case study focuses on using Musketeer to show the soundness of an affine type system guaranteeing schedule-independent safety (§7).",
"Nevertheless, for the sake of explaining the ideas behind Musketeer, here we explain the reasoning rules that would allow one to verify manually the schedule-independent safety of (dumas𝑛) for all 𝑛.",
Key reasoning rules.,
"Musketeer takes the form of a unary separation logic with triples written {𝑃} 𝑒{𝑄}, where 𝑃is a precondition, 𝑒the program being verified and 𝑄the postcondition.",
The postcondition 𝑄is of the form 𝜆𝑣𝑥.,
"𝑅, where 𝑣is the value being returned by the execution of 𝑒and 𝑥is a ghost return value.",
"We explain ghost return values in detail later, but for now, they can be thought of as a special way to existentially quantify variables in the postcondition.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:5 Musketeer triple guarantees the following hyper-property: “if one execution of 𝑒is safe starting from a heap satisfying 𝑃and terminates, then every execution of 𝑒is safe starting from a heap satisfying 𝑃and all terminating executions will end in a heap satisfying 𝑄”.",
"The upper part of Figure 1 shows the main reasoning rules we use for our example (in these rules, the horizontal bar is an implication in the meta-logic).",
"While the assertions and rules of Musketeer are similar to standard separation logic rules, there are two key differences.",
"First, Musketeer does not provide the usual disjunction or existential elimination rules from separation logic.",
"That is, to prove a triple of the form {𝑃1 ∨𝑃2} 𝑒{𝑄}, we cannot in general do case analysis on the precondition and reduce this to proving {𝑃1} 𝑒{𝑄} and {𝑃2} 𝑒{𝑄}.",
"As we will see later, this restriction is necessary because the imprecision in disjunctions and existentials can encode nondeterministic behavior, where different executions pick different witnesses.",
"Second, unlike traditional separation logic rules, rules in Musketeer do not guarantee safety.",
"Rather, they guarantee that safety is independent of scheduling.",
"Thus, these rules often have weaker preconditions than standard separation logic rules.",
The rule M-Assert illustrates this unusual aspect of Musketeer.,
"This rule applies to an expression assert𝑣, for an arbitrary value 𝑣, and has a trivial precondition.",
"The postcondition has the pure facts that the return value 𝑤is () and that 𝑣equals true, i.e.",
that the assert did not fail.,
"In contrast, the standard separation logic rule for assert𝑣requires the user to prove that 𝑣= true!",
This is because the expression assert𝑣is safe only if the value 𝑣= true (§3.2).,
"So in conventional separation logic, where a triple implies safety, the obligation is to show that the assert will be safe.",
"However, in Musketeer, the rule M-Assert corresponds exactly to the “motto” of Musketeer triples: if one execution of assert𝑣is safe and terminates with value 𝑤such that 𝑤= () and 𝑣= true, then every execution of assert𝑣is safe and terminates with value 𝑤= (), and 𝑣= true in those executions too.",
"This property is true in a trivial way: since the argument 𝑣in assert𝑣is already a value, there is only one possible safe execution for assert𝑣, and such an execution is possible only if 𝑣= true.1 On the contrary, M-Par has a standard shape.",
This rule allows for verifying the parallel primitive par𝑒1 𝑒2.,
"It requires the user to split the precondition into two parts 𝑃1 and 𝑃2, and to establish the two triples {𝑃1} 𝑒1 {𝑄1} and {𝑃2} 𝑒2 {𝑄2}.",
"The postcondition of the rule asserts that the value 𝑣 being returned is an immutable pair (𝑣1, 𝑣2) and the ghost return value 𝑥is itself a pair of two ghost return values 𝑥1 and 𝑥2, such that 𝑄𝑣1 𝑥1 and 𝑄𝑣2 𝑥2 hold.",
Verifying dumas.,
The other rules in Figure 1 are the reasoning rules for the concurrent counter we use in dumas.,
"They make use of a predicate counter𝑣𝑞𝑖, asserting that 𝑣is a concurrent counter with fractional ownership 𝑞∈(0; 1].",
"When 𝑞= 1 the assertion represents exclusive ownership of the counter, in which case 𝑖is the value stored in the counter.",
"Otherwise, it asserts ownership of a partial share of the counter, and 𝑖is the contribution added to the counter with this share.",
M-KSplit shows that counter can be split into several shares.,
"M-KRef verifies ref 𝑖, has a trivial precondition and returns a counter initialized to 𝑖with fraction 1.",
"M-KAdd verifies atomic_add𝑣𝑗, where the share may have an arbitrary fraction.",
"M-KGet verifies get𝑣, requiring that counter𝑣1𝑖holds.",
"The fraction is 1, preventing a concurrent add to 𝑣.",
"Such a concurrent add would introduce nondeterminism based on the relative ordering of the add and get, thereby breaking schedule-independent safety.",
"Using the above rules, we can show that for any 𝑛, {⊤} (dumas𝑛) {𝜆_ _.",
"⊤}, that is, without precondition, the safety of (dumas𝑛) is scheduling independent.",
"To do so, we use M-KRef to 1Note that Musketeer supports a bind rule (M-Bind, Figure 7) that allows the user to reason under an evaluation context.",
"Hence, Musketeer supports reasoning on an expression (assert𝑒), for an arbitrary expression 𝑒.",
"To conduct such a proof, the user should first apply M-Bind and focus on 𝑒, show that 𝑒itself satisfies schedule-independent safety, and then for any value 𝑣to which 𝑒may reduce, apply M-Assert on the remaining expression (assert 𝑣).",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:6 Alexandre Moine, Sam Westrick, and Joseph Tassarotti ⊤⊢run (assert true) {𝜆𝑣.",
⌜𝑣= ()⌝} A-Assert run𝑒1 {𝜆𝑣1.,
"run𝑒2 {𝜆𝑣2.𝜓(𝑣1, 𝑣2)}} ⊢run (par𝑒1 𝑒2) {𝜓} A-ParSeqL run𝑒2 {𝜆𝑣2.",
"run𝑒1 {𝜆𝑣1.𝜓(𝑣1, 𝑣2)}} ⊢run (par𝑒1 𝑒2) {𝜓} A-ParSeqR ⊤⊢run (ref 𝑖) {𝜆𝑣.",
⌜𝑣= ℓ⌝∗ℓ↦→𝑖} A-Ref ℓ↦→𝑖⊢run (atomic_add𝑣𝑗) {𝜆_.,
ℓ↦→(𝑖+ 𝑗)} A-Add ℓ↦→𝑖⊢run (get ℓ) {𝜆𝑣.,
⌜𝑣= 𝑖⌝∗ℓ↦→𝑖} A-Get Fig.,
"Reasoning Rules for a Concurrent Counter and Key Reasoning Rules of Angelic initialize the counter, getting counter𝑟1 0, which we split into counter𝑟(1/2) 0 ∗counter𝑟(1/2) 0, and then use M-Par.",
"The counter𝑟(1/2) 0 given to each thread is sufficient to reason about the add they each perform, and when we combine the shares they give back, we get counter𝑟1 1844.",
"Using M-KRef, we know that the get𝑟returns 1844, leaving us to show {⊤} assert (1844 == 𝑛) {𝜆_ _.",
"At this point, we would get stuck in a standard separation logic proof, because the standard rule for assert would require us to prove that (1844 == 𝑛) evaluates to true.",
"However, that would only be the case if 𝑛was in fact 1844.",
"Instead, in Musketeer, we can use a rule showing that (1844 == 𝑛) will evaluate to some Boolean 𝑏, regardless of what value 𝑛is.",
"At that point, we can use M-Assert to conclude, even though we don’t know which value 𝑏will take.",
"2.3 Verifying That One Interleaving is Safe and Terminates with Angelic Now that we know that for all 𝑛, (dumas𝑛) satisfies schedule-independent safety, we can prove that (dumas 1844) is safe just by showing that one interleaving is safe and terminates.",
"For such a simple example, it would suffice at this point to simply execute (dumas 1844) once and observe one safe, terminating execution.",
We would then be able to conclude that all possible executions are safe.,
"However, for more complex examples (for example, programs that are parameterized by an argument from an infinite type), we propose Angelic, a program logic for verifying that one interleaving is safe and terminates.",
"Angelic uses a form of weakest-precondition reasoning, with specifications taking the form 𝜑⊢run𝑒{𝜓}, where 𝜑is the precondition, 𝑒the program being verified, and 𝜓the postcondition, of the form 𝜆𝑣.𝜑′, where 𝑣is the value being returned.",
"In order to guarantee termination, Angelic’s WP is defined as a total weakest precondition, that is, the WP is defined as a least fixpoint and does not mention the so-called later modality.",
"Such a construction is standard, Krebbers et al.",
"[2025, §4] describes the differences between a WP for partial and total correctness.",
"Hence, run𝑒{𝜆_.",
⊤} guarantees that one execution of 𝑒is safe and terminates.,
Figure 2 presents a few reasoning rules for Angelic.,
"It is helpful to read these rules backwards, applying the rule to a goal that matches the right side of the turnstile ⊢and ending up with a goal of proving the left side.",
"A-Assert verifies an assertion, for which the argument must be the Boolean true.2 Indeed, since Angelic guarantees safety, the proof burden is now to show that the assert will succeed.",
"A-ParSeqL says that to verify par𝑒1 𝑒2, it suffices to verify sequentially 𝑒1 and then 𝑒2.",
"A-ParSeqR lets us verify the reverse order instead, reasoning first about 𝑒2 and then 𝑒1.",
"As we will explain later on (§6.2), Angelic more generally allows for selecting any interleaving of steps within 𝑒1 and 𝑒2 by “jumping” between the two expressions during a proof.",
"Finally, A-Ref, A-Add and A-Get shows how to reason on a concurrent counter.",
"First, these rules do not involve any new predicate, and manipulate the plain points-to assertion linked with the counter.",
"Second, no fractions or invariants are involved.",
"Indeed, in Angelic, there is no need to split and join assertions, as the parallel primitive can be verified sequentially in any order.",
"2Angelic supports a bind rule (A-Bind, Figure 11).",
"As in Musketeer, A-Bind allows for reasoning under an evaluation context.",
"In combination with A-Assert, the user may reason about an expression (assert𝑒) for an arbitrary expression 𝑒. Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:7 Values V 𝑣::= () | 𝑏∈{true, false} | 𝑖∈Z | ℓ∈L | (𝑣, 𝑣) | ˆ𝜇𝑓𝑥.𝑒 Primitives ⊲⊳::= + | −| × | ÷ | mod | == | < | ≤| > | ≥| ∨| ∧ Expressions 𝑒::= 𝑣,𝑤 value 𝑥 variable let𝑥= 𝑒in𝑒 sequencing if 𝑒then𝑒else𝑒 conditional 𝜇𝑓𝑥.𝑒 abstraction 𝑒𝑒 call 𝑒⊲⊳𝑒 primitive operation prod𝑒𝑒 product proj𝑘∈{1,2} 𝑒 projections assert𝑒 assertion alloc 𝑒 array allocation 𝑒[𝑒] array load 𝑒[𝑒] ←𝑒 array store length𝑒 array length par𝑒𝑒 parallelism 𝑒|| 𝑒 active parallel tuple CAS𝑒𝑒𝑒𝑒 compare-and-swap Contexts 𝐾::= let𝑥= □in𝑒 | if □then𝑒else𝑒 | alloc □ | length □ | assert □ | 𝑒[□] | □[𝑣] | 𝑒[𝑒] ←□ | 𝑒[□] ←𝑣 | □[𝑣] ←𝑣 | 𝑒⊲⊳□ | □⊲⊳𝑣 | 𝑒□ | □𝑣 | CAS𝑒𝑒𝑒□ | CAS𝑒𝑒□𝑣 | CAS𝑒□𝑣𝑣 | CAS □𝑣𝑣𝑣 | prod𝑒□ | prod □𝑣 | proj𝑘□ Fig.",
"Syntax of MusketLang Using these rules, we can verify that ⊢run (dumas 1844) {𝜆_.",
"⊤} holds, which implies that there exists one interleaving that is safe and terminates.",
"Combined with the fact that this program has schedule-independent safety, we conclude that (dumas 1844) is always safe.",
3 Syntax and Semantics MusketLang is a call-by-value lambda calculus with mutable state and parallelism.,
We first present its syntax (§3.1) and then its semantics (§3.2).,
"MusketLang is similar to HeapLang, the language that ships with Iris, except that it implements structured parallelism instead of fork-based concurrency.",
3.1 Syntax Figure 3 presents the syntax of MusketLang.,
"A value 𝑣∈V is either the unit value (), a Boolean 𝑏∈ {true, false}, an idealized integer 𝑖∈Z, a location ℓfrom an infinite set of locations L, an immutable product (𝑣1, 𝑣2) of two values, or a recursive function ˆ𝜇𝑓𝑥.𝑒.",
An expression 𝑒describe a computation in MusketLang.,
Recursive functions are written 𝜇𝑓𝑥.𝑒.,
"For non-recursive functions, we write 𝜆𝑥.𝑒≜𝜇_𝑥.𝑒.",
We define functions with multiple arguments as a chain of function constructors.,
Mutable state is available through arrays.,
"Parallelism is available through a primitive par𝑒1 𝑒2, which evaluates to an active parallel tuple 𝑒1 || 𝑒2.",
Such a tuple evaluates the two expressions in parallel and returns their result as an immutable product.,
"MusketLang also has a primitive compare-and-swap instruction CAS𝑒1 𝑒2 𝑒3 𝑒4, which targets an array entry and has 4 parameters: the array location, the offset into the array, the old value and the new value.",
References are defined as arrays of size 1 with the following operations: ref ≜𝜆𝑥.,
let𝑟= alloc 1 in𝑟[0] ←𝑥; 𝑟 get ≜𝜆𝑟.𝑟[0] set ≜𝜆𝑟𝑣.𝑟[0] ←𝑣 An evaluation context 𝐾describes an expression with a hole □and dictates the right-to-left evaluation order of MusketLang.,
"3.2 Semantics Figure 4 presents the head reduction relation 𝑒\𝜎 head −−−→𝑒′ \𝜎′, describing a single step of expres- sion 𝑒with store 𝜎into expression 𝑒′ and store 𝜎′.",
"A store is a map from location to arrays, modeled as a list of values.",
We write ∅for the empty store and 𝜎(ℓ) for the list of values at location ℓin 𝜎.,
"To insert or update a location ℓwith array ®𝑣in store 𝜎, we write [ℓ:= ®𝑣]𝜎, and similarly write Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:8 Alexandre Moine, Sam Westrick, and Joseph Tassarotti HeadIfTrue if true then𝑒1 else𝑒2 \𝜎 head −−−−→𝑒1 \𝜎 HeadIfFalse if false then𝑒1 else𝑒2 \𝜎 head −−−−→𝑒2 \𝜎 HeadCallPrim 𝑣1 ⊲⊳𝑣2 pure −−−→𝑣 𝑣1 ⊲⊳𝑣2 \𝜎 head −−−−→𝑣\𝜎 HeadAbs 𝜇𝑓𝑥.𝑒\𝜎 head −−−−→ˆ𝜇𝑓𝑥.𝑒\𝜎 HeadLetVal let𝑥= 𝑣in𝑒\𝜎 head −−−−→[𝑣/𝑥]𝑒\𝜎 HeadAlloc 0 ≤𝑖 ℓ∉dom(𝜎) alloc 𝑖\𝜎 head −−−−→ℓ\ [ℓ:= ()𝑖]𝜎 HeadLoad 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| ®𝑤(𝑖) = 𝑣 ℓ[𝑖] \𝜎 head −−−−→𝑣\𝜎 HeadStore 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| ℓ[𝑖] ←𝑣\𝜎 head −−−−→() \ [ℓ:= [𝑖:= 𝑣] ®𝑤]𝜎 HeadAssert assert true \𝜎 head −−−−→() \𝜎 HeadProduct prod𝑣1 𝑣2 \𝜎 head −−−−→(𝑣1, 𝑣2) \𝜎 HeadProj 𝑘∈{1; 2} proj𝑘(𝑣1, 𝑣2) \𝜎 head −−−−→𝑣𝑘\𝜎 HeadLength 𝜎(ℓ) = ®𝑤 𝑖= | ®𝑤| length ℓ\𝜎 head −−−−→𝑖\𝜎 HeadCASSucc 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| ®𝑤(𝑖) = 𝑣 CAS ℓ𝑖𝑣𝑣′ \𝜎 head −−−−→true \ [ℓ:= [𝑖:= 𝑣′] ®𝑤]𝜎 HeadCASFail 𝜎(ℓ) = ®𝑤 0 ≤𝑖< | ®𝑤| ®𝑤(𝑖) = 𝑣0 𝑣0 ≠𝑣 CAS ℓ𝑖𝑣𝑣′ \𝜎 head −−−−→false \𝜎 HeadCall ( ˆ𝜇𝑓𝑥.𝑒) 𝑣\𝜎 head −−−−→[( ˆ𝜇𝑓𝑥.𝑒)/𝑓][𝑥/𝑣]𝑒\𝜎 HeadFork par𝑒1 𝑒2 \𝜎 head −−−−→𝑒1 || 𝑒2 \𝜎 HeadJoin 𝑣1 || 𝑣2 \𝜎 head −−−−→(𝑣1, 𝑣2) \𝜎 Fig.",
Head Reduction Relation StepHead 𝑒\𝜎 head −−−−→𝑒′ \𝜎′ 𝑒\𝜎−→𝑒′ \𝜎′ StepCtx 𝑒\𝜎−→𝑒′ \𝜎′ 𝐾⟨𝑒⟩\𝜎−→𝐾⟨𝑒′⟩\𝜎′ StepParL 𝑒1 \𝜎−→𝑒′ 1 \𝜎′ 𝑒1 || 𝑒2 \𝜎−→𝑒′ 1 || 𝑒2 \𝜎′ StepParR 𝑒2 \𝜎−→𝑒′ 2 \𝜎′ 𝑒1 || 𝑒2 \𝜎−→𝑒1 || 𝑒′ 2 \𝜎′ Fig.,
Main Reduction Relation [𝑖:= 𝑤]®𝑣to update offset 𝑖with value 𝑤in array ®𝑣.,
"The length of an array ®𝑣is written as |®𝑣|, and 𝑣𝑖 represents an array of size 𝑖initialized with value 𝑣.",
Most of the reduction rules are standard.,
"For example, HeadAlloc allocates an array initialized with the unit value and returns its location, which is selected nondeterministically.",
"HeadLoad and HeadStore perform loads and stores, respectively.",
HeadCASSucc and HeadCASFail performs an atomic compare-and-swap at an offset in an array.,
HeadAssert reduces an assert statement to a unit if the asserted value is true; asserts of false are stuck expressions.,
"HeadFork performs a fork, converting a primitive par operation into an active parallel tuple.",
HeadJoin takes an active parallel tuple where both sides have reached a value and converts it into an immutable product.,
"Figure 5 presents the main reduction relation 𝑒\𝜎−→𝑒′ \𝜎′, describing a parallel step of computation, potentially under an evaluation context.",
StepHead performs a head step.,
StepCtx performs a computation step under an evaluation context.,
"StepParL and StepParR implement parallelism: these two rules allow for the main reduction relation to perform nondeterministically a step to the left or right side of an active parallel tuple, respectively.",
We write the reflexive-transitive closure of the reduction relation as 𝑒\𝜎−→∗𝑒′ \𝜎′.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:9 RedHead 𝑒\𝜎 head −−−−→𝑒′ \𝜎′ Red𝑒𝜎 RedCtx Red𝑒𝜎 Red (𝐾⟨𝑒⟩) 𝜎 RedPar 𝑒1 ∉V ∨𝑒2 ∉V 𝑒1 ∉V =⇒Red𝑒1 𝜎 𝑒2 ∉V =⇒Red𝑒2 𝜎 Red (𝑒1 || 𝑒2) 𝜎 Notstuck𝑒𝜎≜𝑒∈V ∨Red𝑒𝜎 Safe𝑒≜∀𝑒′ 𝜎′.,
(𝑒\ ∅−→∗𝑒′ \𝜎′) =⇒Notstuck𝑒′ 𝜎′ SISafety 𝑒≜∀𝑣𝜎.,
(𝑒\ ∅−→∗𝑣\𝜎) =⇒Safe𝑒 Fig.,
"Definition of the Red, Notstuck, Safe, and SISafety Predicates 4 A Separation Logic for Proving Schedule-Independent Safety In this section, we present Musketeer in more detail.",
"First, we define schedule-independent safety (§4.1).",
"Next, we introduce our notations for triples and assertions (§4.2) and then present the reasoning rules of Musketeer (§4.3).",
"We conclude with one of the main technical challenges in working with Musketeer, the absence of a rule for eliminating existentials, and explain how we overcame this with the novel concept of ghost return values (§4.4).",
"4.1 Definition of Schedule-Independent Safety Let us make formal the definition of schedule-independent safety, that is, the property guaranteeing our motto “if one execution of 𝑒is safe and terminates, then every execution of 𝑒is safe”.",
What does it mean for a parallel program to be safe?,
"We say that the configuration 𝑒\𝜎is not stuck if either 𝑒is a value, or every parallel task in 𝑒that has not reached a value can take a step—in the latter case, we call the configuration reducible.",
A program is defined to be safe if every configuration it can reach is not stuck.,
"In particular, if a program 𝑒is safe, then no assertion in 𝑒can fail, since an assert of a false value is not reducible.",
Figure 6 gives the formal definitions.,
"The upper part of Figure 6 defines the property Red𝑒𝜎, asserting that the configuration 𝑒\𝜎is reducible.",
"RedHead asserts that if 𝑒can take a head step, then it is reducible.",
"RedCtx asserts that the reducibility of an expression 𝐾⟨𝑒⟩follows from reducibility of 𝑒. RedPar asserts that an active parallel tuple 𝑒1 || 𝑒2 is reducible if at least one sub-expression is not a value (otherwise, a join is possible), and each sub-expression that is not a value is reducible.",
The lower part of Figure 6 asserts that the property Notstuck𝑒𝜎holds if and only if either 𝑒is a value or Red𝑒𝜎holds.,
"Then, Safe𝑒says that if 𝑒\ ∅can reach 𝑒′ \𝜎′ in zero or more steps, then Notstuck𝑒′ 𝜎′.",
"Finally, the main property SISafety 𝑒, asserting that the safety of 𝑒 is schedule-independent, is defined.",
"The property says that if some execution of 𝑒reaches a value 𝑣, then 𝑒is safe.",
"The soundness Theorem 4.1 of Musketeer guarantees that, for a verified program 𝑒, the property SISafety 𝑒holds.",
"4.2 Triples and Assertions As we saw, Musketeer is a separation logic whose main judgement takes the form of a triple {𝑃} 𝑒{𝑄}.",
"In this triple, 𝑃is the precondition, 𝑒the program being verified, and 𝑄the postcondition.",
The postcondition is of the form 𝜆𝑣𝑥.,
"𝑃′, where 𝑣is the value being returned by the execution of 𝑒and 𝑥is a ghost return value returned by the verification of 𝑒.",
"Both 𝑃and 𝑃′ are separation logic assertions, and can be understood as heap predicates: they describe the content of a heap.",
"We write 𝑃∗𝑃′ for the separating conjunction, 𝑃−∗𝑃′ for the separating implication and ⌜𝑃⌝when the property 𝑃holds in the meta-logic (i.e.",
Musketeer offers fractional [Bornat et al.,
2005; Boyland 2003] points-to assertions ℓ↦→𝑞®𝑣.,
This assertion says that the location ℓpoints to the array ®𝑣with fraction 𝑞∈(0; 1].,
When 𝑞= 1 we simply write ℓ↦→®𝑣.,
We use the term vProp for the type of assertions that can be used in Musketeer pre/post-conditions.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:10 Alexandre Moine, Sam Westrick, and Joseph Tassarotti M-If ( 𝑣= true =⇒{𝑃} 𝑒1 {𝑄} ) ( 𝑣= false =⇒{𝑃} 𝑒2 {𝑄} ) {𝑃} if 𝑣then𝑒1 else𝑒2 {𝑄} M-Conseq 𝑃−∗𝑃′ {𝑃′} 𝑒{𝑄′} ∀𝑣𝑥.",
"𝑄𝑣𝑥−∗𝑄′ 𝑣𝑥 {𝑃} 𝑒{𝑄} M-Val 𝑃−∗𝑄𝑣𝑥 {𝑃} 𝑣{𝑄} M-Alloc {⊤} alloc 𝑤{𝜆𝑣(ℓ,𝑖).",
⌜𝑣= ℓ∧𝑤= 𝑖∧0 ≤𝑖⌝∗ℓ↦→()𝑖} M-Load {ℓ↦→𝑞®𝑣} ℓ[𝑤] {𝜆𝑣′ 𝑖.,
⌜𝑤= 𝑖∧0 ≤𝑖< |®𝑣| ∧®𝑣(𝑖) = 𝑣′⌝∗ℓ↦→𝑞®𝑣} M-Store {ℓ↦→®𝑣} ℓ[𝑤] ←𝑣′ {𝜆𝑣′′ 𝑖.,
⌜𝑣′′ = () ∧𝑤= 𝑖∧0 ≤𝑖< |®𝑣|⌝∗ℓ↦→[𝑖:= 𝑣′]®𝑣} M-Bind {𝑃} 𝑒{𝜆𝑣𝑥.,
𝑄′ 𝑣𝑥} ∀𝑣𝑥.,
{𝑄′ 𝑣𝑥} 𝐾⟨𝑣⟩{𝑄} {𝑃} 𝐾⟨𝑒⟩{𝑄} M-Frame {𝑃} 𝑒{𝑄} {𝑃∗𝑃′} 𝑒{𝜆𝑣𝑥.,
𝑄𝑣𝑥∗𝑃′} Fig.,
"Selected Reasoning Rules of Musketeer (extends Figure 1) As described before, the Musketeer triple {𝑃} 𝑒{𝑄} can be intuitively read as implying the following hyper-property: “if one execution of 𝑒is safe starting from a heap satisfying 𝑃and terminates, then every execution of 𝑒is safe starting from a heap satisfying 𝑃and all terminating executions will end in a heap satisfying 𝑄”.",
"If 𝑃and 𝑄are trivial, then this implies the SISafety property.",
This is captured formally in the soundness theorem of the logic.,
Theorem 4.1 (Soundness of Musketeer).,
If {⊤} 𝑒{𝜆_ _.,
"⊤} holds, then SISafety 𝑒holds.",
"At first, this soundness theorem might seem weak, since it focuses on safety of all executions.",
What if we instead want to show that every terminating execution satisfies a stronger postcondi- tion 𝑄?,
"In general, Theorem 4.1 does not directly imply such a stronger property, but recall that in MusketLang, safety implies that no assert fails.",
"Thus, by annotating a program with appropriate assert statements, we can encode various specifications in terms of safety.",
"We illustrated this aspect in our dumas example (§2.1), where safety implied that the return value across all executions would equal a particular number.",
"Although Musketeer is a unary logic with judgements referring to a single program 𝑒, the above statement reveals that the judgements are relating together multiple executions of that program.",
"To make this work, under the hood, Musketeer’s vProp assertions describe not one but two heaps, corresponding to two executions of the program.",
This has ramifications for some proof rules (§4.4).,
"Later, we will see how vProp assertions can be encoded into assertions in a relational logic that makes these two different heaps more explicit.",
4.3 Reasoning Rules for Musketeer Figure 7 presents selected reasoning rules of Musketeer.,
"Recall that because Musketeer triples do not imply safety, these rules differ from familiar separation logic rules.",
We have previously seen this in the rule M-Assert.,
"A similar phenomenon happens in M-If, which targets the expression if 𝑣then𝑒1 else𝑒2.",
"In standard separation logic, one must prove that 𝑣is a Boolean, since otherwise the if-statement would get stuck.",
"However, in M-If, the user does not have to prove that 𝑣is a Boolean.",
"Instead, the rule requires the user to verify the two sides of the if-statement under the hypothesis that 𝑣was the Boolean associated with the branch.",
"M-Alloc, M-Load and M-Store are similar to their standard separation logic counterparts, except that they do not require the user to show that the allocation size or the loaded or stored offset are valid integers.",
M-Alloc targets the expression alloc 𝑤and has a trivial pre-condition.,
"The postcondition asserts that the value being returned is a location ℓand that 𝑤is a non-negative integer–recall that we can think of the ghost return value (ℓ,𝑖) as if it were just a special way of Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:11 existentially quantifying the variables ℓand 𝑖in the postcondition.,
The postcondition additionally contains the points-to assertion ℓ↦→()𝑖asserting that ℓpoints to the array of size 𝑖initialized with the unit value.,
M-Load and M-Store follow the same pattern.,
"M-Alloc might surprise the reader, since based on the interpretation of triples we described above, the postcondition seems to imply that every execution of the allocation will return the same location ℓ.",
Yet allocation in MusketLang is not deterministic.,
"The resolution of this seeming contra- diction, is that because MusketLang does not allow for “constructing” a location (e.g.",
"transforming an integer into a location), there is no way for the program to observe the nondeterminism of allocations.",
"Hence, from the reasoning point-of-view we can conduct the proof as if allocations were made deterministically.",
This subtlety will appear in the model of Musketeer (§5.2).,
"M-Bind allows for reasoning under a context, and is very similar to the standard separation logic Bind rule, except that in the second premise, we quantify over not just the possible return values 𝑣, but also the ghost return value 𝑥. M-Val allows for concluding a proof about a value, allowing the user of the rule to pick an arbitrary ghost return value 𝑥. M-Frame shows that Musketeer supports framing.",
M-Conseq is the consequence rule of Musketeer: it allows for weakening the precondition and strengthening the postcondition.,
"4.4 Existential Reasoning with Ghost Return Values In separation logic, existential quantification is essential for modularity.",
"Among other things, it allows for concealing intermediate pointers behind an abstraction barrier.",
"To see how this is typically done, let us consider an example making use of the following indirection function that creates a reference to a reference: indirection ≜𝜆𝑣.",
"ref (ref 𝑣) Without using ghost return value, a possible specification for indirection𝑣would be: {⊤} indirection𝑣{𝜆𝑤_.",
⌜𝑤= ℓ⌝∗∃ℓ′.,
"ℓ↦→[ℓ′] ∗ℓ′ ↦→[𝑣]} In the above specification, the first existential quantification on ℓdoes not hide or abstract over anything, since the returned value 𝑤uniquely characterizes ℓ.",
"However, the existential quantifica- tion on ℓ′ is more interesting, as it forms an abstraction barrier: it hides this intermediate location.",
"Let us now focus on a client of indirection, and try to verify the following triple: {⊤} get (indirection𝑣) {𝜆_ _.",
"⊤} Making use of M-Bind and then applying the above specification for indirection, we obtain: {∃ℓ.",
⌜𝑤= ℓ⌝∗∃ℓ′.,
ℓ↦→[ℓ′] ∗ℓ′ ↦→[𝑣]} get𝑤{𝜆_ _.,
⊤} (intermediate) We now need to eliminate the existentials on ℓand ℓ′ in the precondition by introducing universally- quantified variables in the meta-logic.,
"More precisely, we would like to apply the following standard separation logic rule: ∀𝑥.",
{𝑃𝑥} 𝑒{𝑄} {∃𝑥.,
"𝑃𝑥} 𝑒{𝑄} However, Musketeer does not support this rule.",
"Indeed, although Musketeer is formulated as a unary logic, it relates two executions of the same program.",
"As we previously alluded to (§4.2), Musketeer’s vProp assertions are, under the hood, tracking not one, but two heaps: one for each execution of the same program.",
The fact that preconditions describe two heaps implies that the precondition ∃𝑥.,
𝑃𝑥has two interpretations—one for each heap of the two executions of 𝑒being tracked by the triple.,
"Although the precondition holds in both heaps, the witness 𝑥might differ between the two.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:12 Alexandre Moine, Sam Westrick, and Joseph Tassarotti Whereas, in the premise of the above rule, quantifying over 𝑥at the meta-level means that 𝑥is treated as the same in both executions.",
We present a detailed example of such a case in Appendix A.,
"As a result, Musketeer only supports the weaker rule M-ElimExist, allowing an existential to be eliminated when the precondition guarantees that the witness is unique.",
M-ElimExist (∀𝑥.,
𝑃𝑥−∗⌜𝑈𝑥⌝) (∀𝑥𝑦.𝑈𝑥∧𝑈𝑦=⇒𝑥= 𝑦) (∀𝑥.,
{𝑃𝑥} 𝑒{𝑄}) {∃𝑥.,
"𝑃𝑥} 𝑒{𝑄} For example, in the above intermediate triple, M-ElimExist would allow to eliminate the quantification on ℓ, since it is uniquely characterized by 𝑤.",
"However, M-ElimExist is tedious to use in practice.",
"Moreover, sometimes objects are not uniquely characterized by the precondition, and yet are chosen deterministically, so that the witnesses ought to be the same in both executions.",
"For example, in the above intermediate triple, M-ElimExist cannot be used to eliminate the quantification on ℓ′.",
"To solve this issue, we introduce ghost return values.",
In a Musketeer triple {𝑃} 𝑒{𝜆𝑣𝑥.,
"𝑄𝑣𝑥}, the ghost return value 𝑥is an object (of an arbitrary type, which is formally a parameter of the triple) that will eventually be chosen by the user when they apply M-Val.",
"We think of the bound variable 𝑥 as if it were existentially quantified, but the key is that the eventual “witness” selected when using M-Val will be the same across the two executions under consideration.",
"As a result, instead of having to use the weak M-ElimExist to eliminate 𝑥, the ghost return value is automatically eliminated in a strong way by M-Bind.",
Let us go back to our indirection example.,
"We prove a specification for indirection in which ℓ′ is bound in a ghost return value, instead of as an existential: {⊤} indirection𝑣{𝜆𝑣ℓ′.",
"⌜𝑣= ℓ⌝∗ℓ↦→[ℓ′] ∗ℓ′ ↦→[𝑣]} As we use this specification to reason about (get (indirection𝑣)), M-Bind will eliminate ℓ′ auto- matically, and we can use M-ElimExist to eliminate ℓ, reducing the proof to: {ℓ↦→[ℓ′] ∗ℓ′ ↦→[𝑣]} get ℓ{𝜆_ _.",
"⊤} allowing us to proceed and conclude, since there is no longer an existential to eliminate.",
"We extensively use ghost return values for the verification of MiniDet, our case study (§7).",
"For instance, we use a ghost return value to record the content of references in the typing environment.",
"5 Unchaining the Reasoning with Chained Triples For an expression 𝑒, a Musketeer triple guarantees the property “if one execution of 𝑒is safe and terminates, then every execution of 𝑒is safe”.",
"In order to justify the validity of the reasoning rules for Musketeer triples, we generalize the above property and define an intermediate logic called ChainedLog which targets two expressions 𝑒𝑙and 𝑒𝑟and guarantees the property “if one execution of 𝑒𝑙is safe and terminates, then every execution of 𝑒𝑟is safe”.",
We first present chained triples (§5.1) and present some associated reasoning rules (§5.2).,
"Finally, we explain how we encode Musketeer triples using chained triples (§5.3).",
"5.1 Chained Triples as a Generalization of Musketeer Triples In ChainedLog, a chained triple takes the form: {𝜑𝑙} 𝑒𝑙{𝜓𝑙| 𝜑𝑟} 𝑒𝑟{𝜓𝑟} The assertions 𝜑𝑙and 𝜑𝑟are the preconditions of 𝑒𝑙and 𝑒𝑟, respectively.",
"The assertions𝜓𝑙and𝜓𝑟are both of the form 𝜆𝑣.𝜑, where 𝑣is a return value, and are the postconditions of 𝑒𝑙and 𝑒𝑟, respectively.",
"Intuitively, the above chained triple says that, if there exists a reduction of 𝑒𝑙starting from a heap Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:13 satisfying 𝜑𝑙, that is safe and terminates on a final heap with a value 𝑣𝑙satisfying 𝜓𝑙𝑣𝑙, then every reduction of 𝑒𝑟starting from a heap satisfying 𝜑𝑟is safe and if it terminates, it does so on a final heap with a value 𝑣𝑟satisfying 𝜓𝑟𝑣𝑟.",
"Moreover, chained triples guarantee determinism (for simplicity, see our commentary of C-Par), that is, 𝑣𝑙= 𝑣𝑟.",
"Formally, we have the following soundness theorem: Theorem 5.1 (Soundness of Chained Triples).",
If {⊤} 𝑒1 {_.,
⊤| ⊤} 𝑒2 {𝜆_.,
"⊤} holds, and if there exists a value 𝑣and a store 𝜎such that 𝑒1 \ ∅−→∗𝑣\𝜎, then for every 𝑒′ and 𝜎′ such that 𝑒2 \ ∅−→∗𝑒′ \𝜎′, the property Safe𝑒′𝜎′ holds.",
"In particular, chained triples do not guarantee safety for 𝑒𝑙, but they do guarantee safety for 𝑒𝑟.",
We call the triples “chained” because enjoy the following rule that allows us to chain facts from one execution to the other: C-Chain {𝜑𝑙} 𝑒𝑙{𝜆𝑣𝑙.𝜓𝑙𝑣𝑙∗𝜑| 𝜑𝑟} 𝑒𝑟{𝜆𝑣𝑟.𝜓𝑟} {𝜑𝑙} 𝑒𝑙{𝜆𝑣𝑙.𝜓𝑙𝑣𝑙| 𝜑−∗𝜑𝑟} 𝑒𝑟{𝜆𝑣𝑟.𝜓𝑟} It is best to read this rule from the bottom up.,
"Below the line, using the precondition for 𝑒𝑟requires showing 𝜑.",
"Above the line, the rule allows us to discharge this assumption by showing that 𝜑holds in the postcondition of 𝑒𝑙.",
"That is, if some knowledge 𝜑is needed in order to verify the safety of 𝑒𝑟, then this knowledge can be gained from an execution of 𝑒𝑙.",
Assertions 𝜑of ChainedLog are ground Iris assertions of type iProp.,
"As previously intuited (§4.2), they include two forms of points-to assertions, one for each side of the triple.",
"We write ℓ↦→𝑙 𝑞®𝑣 the points-to assertion for the left expression, and ℓ↦→𝑟 𝑞®𝑣for the right expression.",
"Moreover, ChainedLog makes use of a left-allocation token, written leftalloc ℓ.",
This (non-persistent) assertion witnesses that ℓhas been allocated by the left expression and plays a key role for allocations.,
5.2 Reasoning Rules for Chained Triples Figure 8 presents selected reasoning rules for chained triples.,
"Before commenting on these rules, let us underline a caveat of chained triples, explaining in part why we only use them as a model for Musketeer: chained triples do not support a Bind rule.3 Hence, non-structural rules for chained triples explicitly mentions a stack of contexts, written ®𝐾.",
Let us again start with the rules for reasoning about an assertion.,
C-AssertL allows for reasoning about assert𝑣on the left-hand side.,
"Because this rule targets the left hand-side, there is no safety- related proof obligation, hence the premise of the rule allows the user to suppose that 𝑣= true.",
"C-AssertR is, on the contrary, similar to a standard separation logic rule for assertions: the assertion must target a Boolean, and this Boolean must be true.",
C-AllocL allows for reasoning about an allocation of an array on the left-hand side.,
"Again, there is no safety proof obligation, so the user gets to suppose that the argument of the allocation is a non-negative integer.",
The precondition is then augmented with a points-to assertion to a universally quantified location ℓas well as the allocation token leftalloc ℓ.,
"This latter assertion plays a role in C-AllocR, which allows for reasoning about an allocation on the right-hand side.",
"Indeed, the assertion leftalloc ℓappears in the precondition of the right-hand side.",
This assertion allows for predicting the location allocated on the right-hand side.,
"As a result, the premise of C-AllocR does not universally quantify over the location allocated–the name ℓis reused.",
The user can transmit a leftalloc ℓassertion from the left-hand side to the right-hand side using C-FrameL and C-Chain.,
"This rule may seem surprising, since allocation is nondeterministic in MusketLang, yet this rule appears to ensure that the right-hand side allocation returns the same location as the left-hand 3The absence of a Bind rule comes from the chaining intention of these triples: the user needs to terminate the reasoning on the whole left-hand side expression before reasoning on the right-hand side.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:14 Alexandre Moine, Sam Westrick, and Joseph Tassarotti C-AssertL 𝑣= true =⇒{𝜑𝑙} ®𝐾𝑙⟨()⟩{𝜓𝑙| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} {𝜑𝑙} ®𝐾𝑙⟨assert𝑣⟩{𝜓𝑙| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} C-AssertR {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟} ®𝐾𝑟⟨()⟩{𝜓𝑟} {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟} ®𝐾𝑟⟨assert true⟩{𝜓𝑟} C-AllocL ∀ℓ𝑖.",
𝑣= 𝑖∧0 ≤𝑖=⇒{𝜑𝑙∗ℓ↦→𝑙()𝑖∗leftalloc ℓ} ®𝐾𝑙⟨ℓ⟩{𝜓𝑟| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} {𝜑𝑙} ®𝐾𝑙⟨alloc 𝑣⟩{𝜓𝑟| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} C-AllocR 0 ≤𝑖 {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟∗ℓ↦→𝑟()𝑖} ®𝐾𝑟⟨ℓ⟩{𝜓𝑟} {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟∗leftalloc ℓ} ®𝐾𝑟⟨alloc 𝑖⟩{𝜓𝑟} C-LoadL ∀𝑖𝑤.,
𝑣= 𝑖∧0 < 𝑖≤|®𝑣| ∧𝑤= ®𝑣(𝑖) =⇒{𝜑𝑙∗ℓ↦→𝑙 𝑞®𝑣} ®𝐾𝑙⟨®𝑤⟩{𝜓𝑟| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} {𝜑𝑙∗ℓ↦→𝑙 𝑞®𝑣} ®𝐾𝑙⟨ℓ[𝑣]⟩{𝜓𝑟| 𝜑𝑟} ®𝐾𝑟⟨𝑒𝑟⟩{𝜓𝑟} C-LoadR 0 < 𝑖≤|®𝑣| ∧𝑤= ®𝑣(𝑖) {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟∗ℓ↦→𝑟 𝑞®𝑣} ®𝐾𝑟⟨𝑤⟩{𝜓𝑟} {𝜑𝑙} ®𝐾𝑙⟨𝑒𝑙⟩{𝜓𝑙| 𝜑𝑟∗ℓ↦→𝑟 𝑞®𝑣} ®𝐾𝑟⟨ℓ[𝑖]⟩{𝜓𝑟} C-Par {𝜑𝑙1} 𝑒𝑙1 {𝜓𝑙1 | 𝜑𝑟1} 𝑒𝑟1 {𝜓𝑟1} {𝜑𝑙2} 𝑒𝑙2 {𝜓𝑙2 | 𝜑𝑟2} 𝑒𝑟2 {𝜓𝑟2} ∀𝑣1 𝑥1 𝑣2 𝑥2.,
"{𝜓𝑙1𝑣1 𝑥1 ∗𝜓𝑙2 𝑣2 𝑥2} ®𝐾𝑙⟨(𝑣1, 𝑣2)⟩{𝜓𝑙| 𝜓𝑟1𝑣1 𝑥1 ∗𝜓𝑟2 𝑣2 𝑥2} ®𝐾𝑟⟨(𝑣1, 𝑣2)⟩{𝜓𝑟} {𝜑𝑙1 ∗𝜑𝑙2} ®𝐾𝑙⟨par𝑒𝑙1 𝑒𝑙2⟩{𝜓𝑙| 𝜑𝑟1 ∗𝜑𝑟2} ®𝐾𝑟⟨par𝑒𝑟1 𝑒𝑟2⟩{𝜓𝑟} C-FrameL {𝜑𝑙} 𝑒𝑙{𝜓𝑙| 𝜑𝑟} 𝑒𝑟{𝜓𝑟} {𝜑𝑙∗𝜑0} 𝑒𝑙{𝜓𝑙∗𝜑0 | 𝜑𝑟} 𝑒𝑟{𝜓𝑟} C-FrameR {𝜑𝑙} 𝑒𝑙{𝜓𝑙| 𝜑𝑟} 𝑒𝑟{𝜓𝑟} {𝜑𝑙} 𝑒𝑙{𝜓𝑙| 𝜑𝑟∗𝜑0} 𝑒𝑟{𝜆𝑣𝑟.𝜓𝑟𝑣𝑟∗𝜑0} Fig.",
Selected Reasoning Rules for Chained Triples side.,
The key is that a right-hand points-to assertion of the form ℓ↦→𝑟 𝑞®𝑣does not mean that the specific location ℓhas that value in the right-hand side execution.,
"Rather, it means that there exists some location which points to ®𝑣on the right-hand side, and we can reason as if that location were equivalent to ℓ, under some implicit permutation renaming of locations.",
"In other words, as we alluded to earlier in Section 4.3 when discussing the nondeterminism of allocation in Musketeer, the logic ensures that the specific location of an allocation does not matter, since we do not support casting integers to pointers.",
Our approach of using the leftalloc ℓassertion has two consequences.,
"First, as we will see (§5.3), it will allow us to define Musketeer triples in terms of chained triples where both the left- and right-hand side coincide; such a definition would be impossible if the allocation on the left and on the right-hand side could return different names.",
"Second, it bounds the number of allocations on the right-hand side by the number of allocations on the left-hand side.",
"We posit that this limitation can be lifted by distinguishing between synchronized locations, whose name come from the left-hand side, and unsynchronized one.",
We were able to conduct our case studies without such a feature.,
"C-LoadL and C-LoadR follow the same spirit as the previous rules: the rule for the left-hand side has no safety proof obligation, but the right-hand size has a standard separation logic shape.",
C-Par targets a parallel primitive and is a synchronization point: both the left- and right-hand side must face a parallel primitive.,
The rule mimics a standard Par rule on both sides at once.,
"In particular, it requires the user to split the preconditions of the left- and right-hand sides, which will be given to the corresponding side of the active parallel pair.",
"The bottom premise of C-Par requires the user to verify the continuation, after the execution of the parallel primitive ended.",
"This premise also show the (external) determinism guaranteed by chained triple: the execution is resumed on both sides with the same result of the parallel execution: the immutable pair (𝑣1, 𝑣2).",
Note also that both sides agree on the ghost return values.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:15 vProp ≜B →iProp ∀𝑥.,
𝑃𝑥𝑏 𝑃1 ∗𝑃2 ≜𝜆𝑏.,
𝑃1 𝑏∗𝑃2 𝑏 ∃𝑥.,
𝑃𝑥𝑏 𝑃1 −∗𝑃2 ≜𝜆𝑏.,
𝑃1 𝑏−∗𝑃2 𝑏 ℓ↦→𝑞®𝑣≜𝜆𝑏.,
if 𝑏then ℓ↦→𝑙 𝑞®𝑣else ℓ↦→𝑟 𝑞®𝑣 Fig.,
Definition of vProp assertions {𝑃} 𝑒{𝑄} ≜∀®𝐾𝜑𝑙𝜑𝑟𝜓𝑙𝜓𝑟.,
{𝑄𝑣𝑥true ∗𝜑𝑙} ®𝐾⟨𝑣⟩{𝜓𝑙| 𝑄𝑣𝑥false ∗𝜑𝑟} ®𝐾⟨𝑣⟩{𝜓𝑟}  −∗ {𝑃true ∗𝜑𝑙} ®𝐾⟨𝑒⟩{𝜓𝑙| 𝑃false ∗𝜑𝑟} ®𝐾⟨𝑒⟩{𝜓𝑟} Fig.,
Definition of Musketeer Triples 5.3 Encoding Musketeer in ChainedLog We now discuss how to encode Musketeer into ChainedLog.,
Recall that Musketeer’s assertions have the type vProp.,
"We encode these as functions from Booleans to iProp, the ground type of ChainedLog assertions.",
"The idea is that the vProp tracks two heaps, and we use the Boolean parameter of the function to indicate which side of the ChainedLog the assertion is being interpreted to: true indicates the left side, and false the right side.",
The formal definition of vProp assertions appears in Figure 9.,
"The Boolean parameter is threaded through the separating star and implication, and similarly for the ∀and ∃quantifier.",
The points-to assertion simply cases over the Boolean and returns the left or right version of the points-to.,
Entailment is defined as 𝑃1 ⊢𝑃2 ≜∀𝑏.,
"Next, we can encode Musketeer triples as shown in Figure 10.",
A Musketeer triple {𝑃} 𝑒{𝑄} is mapped to a chained triple where both sides refer to the expression 𝑒use the precondition 𝑃 instantiated with Booleans corresponding to the appropriate side.,
"Because chained triples do not support a bind rule, the encoding is written in a continuation passing style: rather than having 𝑄 in the post-condition of the chained triple, we instead quantify over an evaluation context ®𝐾that represents an arbitrary continuation to run after 𝑒.",
This continuation is assumed to satisfy a chained tripled in which 𝑄occurs in the preconditions.,
"We additionally quantify over several assertions 𝜑𝑙, 𝜑𝑟, 𝜓𝑙, and 𝜓𝑟that are used to represent additional resources used by the continuation.",
"6 A Separation Logic for Verifying One Interleaving We now return to Angelic, our program logic verifying that one interleaving of a MusketLang program is safe and terminates.",
We first present the assertions of Angelic (§6.1) and then present selected reasoning rules (§6.2).,
"6.1 Assertions of Angelic Assertions of Angelic are Iris assertions of type iProp, written 𝜑.",
"The fractional points-to assertion of Angelic takes the form ℓ↦→𝑞®𝑣(while we reuse the syntax of the points-to assertion from Musketeer, the two assertions are different—recall that Angelic and Musketeer are totally disjoint).",
A key aspect of Angelic is that this logic has two reasoning modes.,
"First, the running mode takes the form run𝑒{𝜓}, where 𝑒is the expression being logically “run” and 𝜓is a postcondition, The assertion run𝑒{𝜓} is close to a weakest-precondition (WP).",
"In fact, it enjoys all the rules of a standard separation logic WP.",
"However, the running mode enjoys additional rules that allow one to dynamically “select” and verify just one interleaving.",
"This selection is made possible thanks to a second mode, that we call the scheduler mode.",
The scheduler mode involves two key assertions.,
"First, 𝔤𝔬𝔞𝔩is an opaque assertion, intuitively representing the proof obligation to verify the whole program.",
"Second, the assertion yielded 𝜋𝑒asserts the ownership of the task 𝜋, and that this task yielded facing expression 𝑒. Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:16 Alexandre Moine, Sam Westrick, and Joseph Tassarotti A-Alloc ⌜0 ≤𝑖⌝ run (alloc 𝑖) {𝜆𝑣.",
ℓ↦→()𝑖} A-Load ⌜0 ≤𝑖< |®𝑣|⌝ ℓ↦→𝑞®𝑣 run (ℓ[𝑖]) {𝑤.,
⌜𝑤= ®𝑣(𝑖)⌝∗ℓ↦→𝑞®𝑣} A-Bind run𝑒{𝜆𝑣.,
run (𝐾⟨𝑣⟩) {𝜓}} run (𝐾⟨𝑒⟩) {𝜓} A-Store ⌜0 ≤𝑖< |®𝑣|⌝ ℓ↦→®𝑣 run (ℓ[𝑖] ←𝑣′) {𝑤.,
⌜𝑤= ()⌝∗ℓ↦→[𝑖:= 𝑣′]®𝑣} A-Call run ([ ˆ𝜇𝑓𝑥.𝑒/𝑓][𝑣/𝑥]𝑒) {𝜓} run ( ˆ𝜇𝑓𝑥.𝑒) 𝑣{𝜓} A-Conseq run𝑒{𝜓′} ( ∀𝑣.,
𝜓′ 𝑣−∗𝜓𝑣) run𝑒{𝜓} Yield ∀𝜋.,
yielded 𝜋𝑒−∗(∀𝑣.,
yielded 𝜋𝑣−∗𝜓𝑣−∗𝔤𝔬𝔞𝔩) −∗𝔤𝔬𝔞𝔩 run𝑒{𝜓} Resume yielded 𝜋𝑒 run𝑒{𝜆𝑣.,
yielded 𝜋𝑣−∗𝔤𝔬𝔞𝔩} 𝔤𝔬𝔞𝔩 Fork ∀𝜋1 𝜋2.,
yielded 𝜋1 𝑒1 −∗yielded 𝜋2 𝑒2 −∗(∀𝑣1 𝑣2.,
"yielded 𝜋1 𝑣1 −∗yielded 𝜋2 𝑣2 −∗𝜓(𝑣1, 𝑣2) −∗𝔤𝔬𝔞𝔩) −∗𝔤𝔬𝔞𝔩 run (par𝑒1 𝑒2) {𝜓} Fig.",
Selected Reasoning Rules of Angelic (extends Figure 2) The logic satisfies the following soundness theorem: Theorem 6.1 (Soundness of Angelic).,
If run𝑒{𝜆_.,
"⊤} holds, then there exists a value 𝑣and a store 𝜎such that 𝑒\ ∅−→∗𝑣\𝜎.",
6.2 Reasoning Rules of Angelic Figure 11 presents the key reasoning rules allowing the user to select and verify an interleaving.,
"These inference rules are at the iProp level: their premises are implicitly separated by ∗, and the implication between the premise and the conclusion is stated as the entailement ⊢.",
"The upper part of Figure 11 showcases that the run mode of Angelic is, for its sequential part, similar to a standard separation logic.",
"A-Alloc performs an allocation, A-Load a load and A-Store a store—here, the allocation size and various offsets must be valid.",
A-Call verifies a function call.,
A-Conseq shows that the user can make the postcondition stronger.,
A-Bind allows for reasoning under an evaluation context.,
The lower part of Figure 11 focuses on the scheduler mode of Angelic.,
"Yield asserts (reading the rule from bottom to top) that the proof of run𝑒{𝜓} can pause, and switch to the scheduler mode—that is, a proof where the target is 𝔤𝔬𝔞𝔩.",
"To prove this target, the user gets to assume that some (universally quantified) task 𝜋yielded with expression 𝑒, and that when this expression will have reduced to a value 𝑣satisfying 𝜓, then 𝔤𝔬𝔞𝔩will hold.",
"Resume is the companion rule of Yield: it asserts that in order to prove 𝔤𝔬𝔞𝔩, the user has to give up the ownership of a task 𝜋facing an expression 𝑒and switch back to the running mode to verify that run𝑒{𝜆𝑣.",
yielded 𝜋𝑣−∗𝔤𝔬𝔞𝔩}.,
Fork shows the real benefit of the scheduler mode.,
"This rule asserts that, for verifying the parallel primitive par𝑒1 𝑒2, the user can switch to the scheduler mode.",
"In this mode, the user gets to suppose that two tasks 𝜋1 and 𝜋2 yielded at 𝑒1 and 𝑒2, respectively.",
"Moreover, the user can suppose that, when these two tasks would have completed their execution and reached values 𝑣1 and 𝑣2 such that 𝜓(𝑣1, 𝑣2) hold, the 𝔤𝔬𝔞𝔩will hold.",
"At this point, the user can choose which of 𝑒1 and 𝑒2 to begin verifying using Resume.",
Recall in Section 2.3 we saw rules A-ParSeqL and A-ParSeqR allowing one to verify a parallel composition by picking either a left-then-right or right-then-left sequential ordering.,
These two rules can be derived from the more general constructs of Angelic that we have now seen.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:17 𝜏≜⊥| empty | unit | bool | int | 𝜏→𝜏| (𝜏× 𝜏) | ref𝜏 Γ ∈Var ⇀𝜏 empty · empty ≜empty int · int ≜int unit · unit ≜unit (𝜏1 × 𝜏2) · (𝜏′ 1 × 𝜏′ 2) ≜((𝜏1 · 𝜏′ 1) × (𝜏2 · 𝜏′ 2)) bool · bool ≜bool (𝜏1 →𝜏2) · (𝜏′ 1 →𝜏′ 2) ≜if (𝜏1 = 𝜏′ 1 ∧𝜏2 = 𝜏′ 2) then𝜏1 →𝜏2 else ⊥ Fig.,
"Syntax of MiniDet Type System example, in order to show that A-ParSeqL holds, we first apply Fork, then use Resume for the expression 𝑒1.",
We then use A-Conseq with Resume for expression 𝑒2 and conclude.,
"7 Case Studies To showcase Musketeer, we start by using it to prove the soundness of a simple affine type system that ensures schedule-independent safety (§7.1).",
We then extend this type system with two core algorithmic primitives proposed by Blelloch et al.,
[2012] for ensuring internal determinacy: priority writes (§7.2) and deterministic hash sets (§7.3).,
"Interestingly, while these primitives appear to be internally deterministic to their client, their implementation is not: they observe internal state of shared data structures that may be concurrently modified.",
"Yet, we show that they satisfy schedule- independent safety.",
"Because all well-typed programs in this system have schedule-independent safety, we can use Angelic to reason about them, as we demonstrate by verifying a parallel list deduplication example (§7.4).",
"7.1 MiniDet: An Affine Type System for Determinism This section presents MiniDet, an affine type system for MusketLang that ensures determinism.",
"Like many other substructural type systems, the types in MiniDet can be thought of as tracking ownership of resources such as array references, thereby preventing threads from accessing shared resources in a way that would introduce nondeterministic behaviors.",
The syntax of types in MiniDet appears in Figure 12.,
"A type 𝜏is either the invalid type ⊥ (used only internally), the empty type, describing an unknown value without ownership, the unit type unit, the Boolean type bool, the integer type int, the arrow type 𝜏1 →𝜏2, the immutable product (𝜏1 × 𝜏2) or the reference type ref𝜏.",
A typing environment Γ is a finite map from variables to types.,
We write dom(Γ) for its domain.,
"The type system is affine meaning that, when splitting a typing context in two, a variable can only appear in one sub-context at a time.",
"However, variables with types whose inhabitants have no associated notion of ownership, or variables with types with fractional reasoning, can be split and joined.",
"In order to capture this notion, we equip types with a monoid operation _ · _ taking two types as arguments and producing a new type.",
"In particular, when 𝜏· 𝜏= 𝜏, it means that a variable of type 𝜏can be duplicated.",
The definition of the monoid operation appears in the lower part of Figure 12.,
The missing cases are all sent to ⊥.,
In particular these definitions prevent a reference from being duplicated.,
"We extend the monoid operation to typing environments by defining Γ1 · Γ2 as the function that maps the variable 𝑥to 𝜏1 if Γ1(𝑥) = 𝜏1 and 𝑥is not in the domain of Γ2, 𝜏2 if Γ2(𝑥) = 𝜏2 and 𝑥is not in the domain of Γ1, and 𝜏1 · 𝜏2 if Γ1(𝑥) = 𝜏1 and Γ2(𝑥) = 𝜏2.",
"The typing judgement of MiniDet takes the form Γ ⊢𝑒: 𝜏⊣Γ′, and asserts that 𝑒has type 𝜏 and transforms the typing environment Γ into Γ′.",
Typing rules.,
Selected typing rules appear in Figure 13.,
T-Var types variable 𝑥at type 𝜏if 𝑥 has type 𝜏in the typing environment.,
The returned environment is empty.,
"T-Unit, T-Bool and rule T-Int type unboxed values.",
T-Assert types an assert primitive.,
T-Let types a let-binding Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:18 Alexandre Moine, Sam Westrick, and Joseph Tassarotti T-Var {𝑥:= 𝜏} ⊢𝑥: 𝜏⊣∅ T-Unit ∅⊢() : unit ⊣∅ T-Bool ∅⊢𝑏: bool ⊣∅ T-Int ∅⊢𝑖: int ⊣∅ T-Assert Γ ⊢𝑒: bool ⊣Γ′ Γ ⊢assert𝑒: unit ⊣Γ′ T-Let Γ1 ⊢𝑒1 : 𝜏1 ⊣Γ′ 1 [𝑥:= 𝜏1]Γ′ 1 ⊢𝑒2 : 𝜏2 ⊣Γ2 Γ1 ⊢let𝑥= 𝑒1 in𝑒2 : 𝜏2 ⊣del𝑥Γ2 T-Weak Γ1 ⊆Γ′ 1 Γ2 ⊆Γ′ 2 Γ′ 1 ⊢𝑒: 𝜏⊣Γ′ 2 Γ1 ⊢𝑒: 𝜏⊣Γ2 T-Abs Γ = Γ · Γ [𝑓:= 𝜏→𝜏′][𝑥:= 𝜏]Γ ⊢𝑒: 𝜏′ ⊣∅ Γ ⊢𝜇𝑓𝑥.𝑒: 𝜏→𝜏′ ⊣∅ T-App Γ1 ⊢𝑒1 : 𝜏⊣Γ2 Γ2 ⊢𝑒2 : 𝜏→𝜏′ ⊣Γ3 Γ1 ⊢𝑒2 𝑒1 : 𝜏′ ⊣Γ3 T-Ref Γ ⊢𝑒: 𝜏⊣Γ′ Γ ⊢ref 𝑒: ref𝜏⊣Γ′ T-Get {𝑥:= ref𝜏} ⊢get𝑥: 𝜏⊣{𝑥:= ref empty} T-Set Γ ⊢𝑒: 𝜏⊣{𝑥:= ref empty} · Γ′ Γ ⊢set𝑥𝑒: unit ⊣{𝑥:= ref𝜏} · Γ′ T-Par Γ1 ⊢𝑒1 : 𝜏1 ⊣Γ′ 1 Γ2 ⊢𝑒2 : 𝜏2 ⊣Γ′ 2 Γ1 · Γ2 ⊢par𝑒1 𝑒2 : (𝜏1 × 𝜏2) ⊣Γ′ 1 · Γ′ 2 T-Frame Γ ⊢𝑒: 𝜏⊣Γ′ Γ0 · Γ ⊢𝑒: 𝜏⊣Γ0 · Γ′ Fig.",
"Selected Typing Rules of MiniDet let𝑥= 𝑒1 in𝑒2 at type𝜏2 with initial context Γ1 if 𝑒1 has type𝜏1 under the same context and produces context Γ2, and if 𝑒2 has type 𝜏2 under the context Γ1 in which 𝑥has type 𝜏1.",
The produced context of the let-binding is Γ2 from which 𝑥has been deleted.,
"T-Abs types a function with recursive name 𝑓, argument 𝑥and body 𝑒, of type 𝜏→𝜏′ and with typing environment Γ.",
"This environment must be duplicable, that is Γ = Γ · Γ.",
"This duplicability implies that Γ contains no types with ownership, that is, for now, no references.",
"The precondition requires that 𝑒has type 𝜏′ in Γ, augmented with 𝑓 of type 𝜏→𝜏′ and 𝑥of type 𝜏. T-App types a function call and is straightforward.",
T-Ref types a reference allocation.,
T-Get types a get operation on a variable 𝑥.,
"This rule requires that 𝑥is of some type ref𝜏, returns a type 𝜏and updates the binding of 𝑥to ref empty.",
"This is because get returns the ownership of the content of the cell—meaning that the cell does not hold recursive ownership of its contents anymore.4 T-Set is the dual, and types the expression set𝑥𝑒.",
"This rule requires that 𝑒is of some type 𝜏and that, in the resulting environment, 𝑥is of type ref empty.",
"The set operation returns unit and updates the type of 𝑥to ref𝜏, “filling” the cell.",
"T-Par types a parallel primitive, and is similar to the related separation logic rules.",
"Indeed, T-Par requires splitting the context in two parts, that will be used to type separately the two sub-tasks, whose result typing context will be merged in the result typing context of the rule.",
"Finally, T-Frame allows for framing a part of the context for local reasoning, and T-Weak allows for removing bindings from the input and output typing environments.",
Soundness of MiniDet.,
"The above system prevents data-races, and hence guarantees that well- typed programs have schedule-indepedent safety, as formalized by the following lemma.",
Lemma 7.1 (Soundness of MiniDet).,
"If ∅⊢𝑒: 𝜏⊣∅holds, then SISafety 𝑒holds.",
"To prove this theorem, we use program logic-based semantic typing [Timany et al.",
"With this technique, we associate a triple (in our case, a Musketeer triple) to a typing judgement, and 4We could have derived another rule for get on a reference whose content is not tied to any ownership.",
We follow this approach when extending the type system with priority writes (§7.2).,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:19 𝑠≜sinvalid | snone | sprod𝑠𝑠| sref𝑣𝑠| sarrow𝛾 𝑀∈Var ⇀𝜏 𝑉∈Var ⇀V J empty | snone | 𝑣K ≜⊤ J bool | snone | 𝑣K ≜⌜∃𝑏.,
𝑣= 𝑏⌝ J unit | snone | 𝑣K ≜⌜𝑣= ()⌝ J int | snone | 𝑣K ≜⌜∃𝑖.,
𝑣= 𝑖⌝ J (𝜏1 × 𝜏2) | sprod𝑠1 𝑠2 | 𝑣K ≜∃𝑣1 𝑣2.,
"⌜𝑣= (𝑣1, 𝑣2)⌝∗J𝜏1 | 𝑠1 | 𝑣1 K ∗J𝜏2 | 𝑠2 | 𝑣2 K J ref𝜏| sref𝑤𝑠| 𝑣K ≜∃ℓ.",
⌜𝑣= ℓ⌝∗ℓ↦→[𝑤] ∗J𝜏| 𝑠| 𝑤K J𝜏→𝜏′ | sarrow𝛾| 𝑣K ≜∃𝑃.𝛾Z⇒𝑃∗ 𝑃∗ onlyleft ( ∀𝑤𝑠.,
{⊲𝑃∗J𝜏| 𝑠| 𝑤K} (𝑣𝑤) {𝜆𝑣′ 𝑠′.,
J𝜏′ | 𝑠′ | 𝑣′ K}) J Γ | 𝑀| 𝑉K ≜⌜dom(Γ) = dom(𝑀) = dom(𝑉)⌝∗∗𝑥∈dom(Γ) J Γ(𝑥) | 𝑀(𝑥) | 𝑉(𝑥) K where onlyleft (𝑃) ≜𝜆𝑏.,
if 𝑏then (𝑃true) else ⊤ J Γ ⊢𝑒: 𝜏⊣Γ′ K ≜∀𝑀𝑉.,
"{J Γ | 𝑀| 𝑉K} ([𝑉/]𝑒) {𝜆𝑣(𝑠, 𝑀′).",
⌜Γ ≈Γ′ ∧𝑀≈𝑀′⌝∗J𝜏| 𝑠| 𝑣K ∗J Γ′ | 𝑀′ | 𝑉|dom(Γ′) K} Fig.,
"Semantic Interpretation of MiniDet show that whenever the typing judgement holds, the corresponding triple is valid.",
The soundness theorem of the type system is then derived from the soundness of the underlying logic.,
The Musketeer triple associated to a typing judgement makes use of a logical relation.,
"Typically, when using program logic-based semantic typing, a logical relation is a relation expressed in the assertions of the underlying logic that relates a type to a value it inhabits.",
"In our case, however, the logical relation involves three parameters: a type, a value, and a shape.",
The shape captures the “determinism” of each type and will be used in connection with ghost return values.,
"For example, the shape of a reference is the actual value stored in this reference, and the shape of a function records that the function’s environment is deterministic.",
Figure 14 defines the format of shapes.,
"A shape 𝑠as either an invalid shape (whose purpose is similar to the invalid type, as we equip shapes with a monoid operation), the none shape, storing no information, the product shape sprod𝑠1 𝑠2, the reference shape sref𝑣𝑠, where 𝑣represents the content of the reference and 𝑠the shape associated with 𝑣and finally the arrow shape sarrow𝛾, where 𝛾is the name of an Iris ghost cell [Jung et al.",
"The logical relation J𝜏| 𝑠| 𝑣K, shown in Figure 14 then relates a type 𝜏, a shape 𝑠, and a value 𝑣.",
This relation is itself of type vProp.,
"Unboxed types are interpreted as expected, associated the with snone shape.",
"Products must be associated to the product shape and a product value, and the interpretation must recursively hold.",
"For the reference type ref𝜏, the shape must be a reference shape sref𝑤𝑠, 𝑣must be a location ℓsuch that ℓpoints to 𝑤and that recursively J𝜏| 𝑠| 𝑤K holds.",
Note that the interpretation of a reference expresses the ownership of the associated points-to.,
"Besides, the content of the reference 𝑤is not existentially quantified, but rather given by the shape.",
The case of an arrow 𝜏→𝜏′ is subtle and differs from the approach used in other program logic based logical relations.,
"In the usual approach, the interpretation of 𝜏→𝜏′ says that 𝑣is in the relation if for any 𝑤in the interpretation of 𝜏, a Hoare triple of a certain form holds for the application 𝑣𝑤.",
"Unfortunately, this approach cannot be used directly with Musketeer.",
"The reason is that the usual approach exploits the fact that the underlying logic is higher-order and impredicative, so that a Hoare triple is itself an assertion that can appear in the pre/post-condition of another triple.",
"In contrast, in Musketeer, the assertions appearing in pre/post-conditions are vProps, but the triple itself is not a vProp, it is an iProp in the underlying chained logic, as we saw in §5.",
"To work around this, we define an operation onlyleft that takes an iProp and coerces it into a vProp by requiring the proposition to only hold for the left-hand side.",
"Using this, the logical relation asserts that, only in the left case, for any value 𝑤and shape 𝑠, a Hoare triple holds for 𝑣𝑤.",
"In this triple, the precondition requires J𝜏| 𝑠| 𝑤K, and the postcondition says that the result will satisfy Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:20 Alexandre Moine, Sam Westrick, and Joseph Tassarotti the interpretation of 𝜏′.",
The precondition additionally requires 𝑃to hold for some existentially quantified vProp predicate 𝑃.,
"( Technically, 𝑃is assumed to hold under a later modality ⊲, but this detail can be ignored.)",
This 𝑃will correspond to the resources associated with whatever variables from a typing environment the function closes over.,
"Thus, 𝑃is required to hold under the Iris persistent modality , ensuring that the proposition is duplicable—recall that the typing rule T-Abs requires functions to close over only duplicable environments.",
"Finally, there is one last trick: to ensure that this existential quantification over 𝑃can later be eliminated using M-ElimExist, the witness is made unique by using an Iris saved predicate assertion, lifted at the vProp level and written 𝛾Z⇒𝑃, which states that 𝛾is the name of a ghost variable that stores the assertion 𝑃.",
The 𝛾 here is bound as part of the shape sarrow𝛾.,
"Since a ghost variable can only store one proposition, only one 𝑃can satisfy this assertion.",
"Figure 14 then defines the interpretation of a typing environment Γ, a shape environment 𝑀 and a value environment 𝑉, written J Γ | 𝑀| 𝑉K as the lifting per-variable 𝑥of the logical relation.",
"Using this, we obtain the interpretation of the typing judgement Γ ⊢𝑒: 𝜏⊣Γ′.",
"This interpretation universally quantifies over a shape environment 𝑀and a variable environment 𝑉, and asserts a Musketeer triple.",
"The precondition is the interpretation of the environments, and targets an expression [𝑉/]𝑒, that is, the expression 𝑒with variables replaced by values as specified by 𝑉.",
The postcondition binds a return value 𝑣as well as a ghost return value consisting of a shape 𝑠and a shape environment 𝑀′.,
"The postcondition asserts that the two typing environment Γ and Γ′ are similar, written Γ ≈Γ′ and that the shape environments 𝑀and 𝑀′ are also similar, with (overloaded) notation 𝑀≈𝑀′.",
Intuitively these relations guarantee that variables did not change in nature in environments (e.g.,
"a reference stayed a reference, and a reference shape stayed a reference shape, even if the content may have changed).",
We formally define these statements in Appendix B.,
"The postcondition finally asserts that the return value is related to 𝜏and 𝑠and that the returned environment Γ′ is correct with 𝑀′ and the same variables 𝑉, dropping unneeded bindings.",
"With these definitions, we state the fundamental lemma of the logical relation.",
Lemma 7.2 (Fundamental).,
If Γ ⊢𝑒: 𝜏⊣Γ′ holds then J Γ ⊢𝑒: 𝜏⊣Γ′ K holds too.,
"From this lemma, it is easy to prove the soundness of MiniDet (Lemma 7.1).",
Let us suppose that ∅⊢𝑒: 𝜏⊣∅holds.,
We apply Lemma 7.2 and learn that J ∅⊢𝑒: 𝜏⊣∅K holds too.,
"Unfolding definitions and applying M-Conseq, this fact implies that {⊤} 𝑒{𝜆_ _.",
We conclude by applying the soundness of Musketeer (Theorem 4.1).,
"7.2 Priority Writes In this section, we extend MiniDet with rules for priority writes [Blelloch et al.",
A priority write targets a reference 𝑟on an integer 𝑥and atomically updates the content 𝑦of 𝑟to 𝑥max𝑦.,
"As long as there are no concurrent reads, priority writes can happen in parallel: because max is associative and commutative, the order in which the parallel write operations happen does not matter.",
"Conversely, so long as there are no ongoing concurrent writes, reads from the reference will be safe and deterministic—and such reads can also happen in parallel.",
"Thus, priority writes are deterministic so long as they are used in a phased manner, alternating between concurrent writes in one phase, and concurrent reads in the next.",
"For simplicity, we consider priority writes on integers equipped with the max function.",
Implementation of priority writes.,
Figure 15 shows the implementation of priority references.,
Allocating a priority reference with palloc just allocates a reference.,
The priority read pread is just a plain get operation.,
"A priority write pwrite is a function with recursive name 𝑓taking two arguments: 𝑟, the reference to update, and 𝑥, the integer to update the reference with.",
The function Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:21 palloc ≜𝜆𝑛.,
ref 𝑛 pread ≜𝜆𝑟.,
get 𝑟 pwrite ≜𝜇𝑓𝑟𝑥.,
let𝑦= get𝑟in if 𝑥< 𝑦then () else if CAS𝑟0𝑥𝑦then () else 𝑓𝑟𝑥 Fig.,
Implementation of Priority Writes 𝜏≜· · · | pwrite𝑞| pread𝑞 pwrite𝑞1 · pwrite𝑞2 ≜pwrite (𝑞1 + 𝑞2) pread𝑞1 · pread𝑞2 ≜pread (𝑞1 + 𝑞2) T-PAlloc Γ ⊢𝑒: int ⊣Γ′ Γ ⊢palloc𝑒: pwrite 1 ⊣Γ′ T-PWrite Γ ⊢𝑒: int ⊣Γ′ Γ′(𝑥) = pwrite𝑞 Γ ⊢𝑒: pwrite 𝑥𝑒⊣Γ′ T-PRead {𝑥:= pread𝑞} ⊢pread𝑥: int ⊣{𝑥:= pread𝑞} T-Update Γ { Γ′ Γ′ ⊢𝑒: 𝜏⊣Γ′′ Γ ⊢𝑒: 𝜏⊣Γ′′ U-Refl 𝜏{ 𝜏 U-Pair 𝜏1 { 𝜏′ 1 𝜏2 { 𝜏′ 2 (𝜏1 × 𝜏2) { (𝜏′ 1 × 𝜏′ 2) U-R2W pread 1 { pwrite 1 U-W2R pwrite 1 { pread 1 Fig.,
Extension of MiniDet with Priority Writes tests if the content 𝑦of the reference is greater than 𝑥.,
"If 𝑥< 𝑦, the function returns, because 𝑥max𝑦= 𝑦.",
"Else, the function attempts to overwrite 𝑦with 𝑥in 𝑟with a CAS, and loops if it fails.",
As noted by Blelloch et al.,
"[2012], if we break the abstractions of the priority reference, the implementation of pwrite is not internally deterministic: because pwrite reads 𝑟, a location that can be written by a parallel task, different interleavings might see different values.",
"However, because pwrite is carefully designed, these nondeterministic observations are not externally visible and do not impact the safety of the program.",
"As we will see, this latter fact allow us to derive a Musketeer triple API to priority writes.",
"However, because nondeterminism is involved internally in the implementation, we conduct the proof at the level of ChainedLog.",
Extension of MiniDet.,
Figure 16 shows how we extend our type system.,
"We add two new type constructors, pwrite𝑞and pread𝑞, asserting that the reference is in a write phase with fraction 𝑞 or a read phase with fraction 𝑞, respectively.",
The monoid on types is extended to sum fractions.,
"This definition implies, as we will see, that writes can happen in parallel with writes, and reads can happen in parallel with reads.",
The lower part of Figure 16 shows the new typing rules.,
T-PAlloc allocates a priority reference and returns a type pwrite 1.,
T-PWrite types a priority write on some reference 𝑥bound to the type pwrite𝑞.,
"In particular, this rule does not require the full fraction 1, meaning that the write operation can happen in parallel of other write operations.",
T-PRead types a read similarly.,
Again this rule does not require the full fraction.,
T-Update allows for updating a typing context Γ into Γ′ as long as Γ { Γ′.,
This relation is defined pointwise over the elements of the environments as the update relation 𝜏{ 𝜏′ which is defined last in Figure 16.,
"U-Refl asserts that a type can stay the same, U-Pair distributes over pairs, U-R2W transforms a read type into a write one, if the fraction is the full permission 1.",
This precondition on the fraction is important: it asserts that no parallel task use the priority reference.,
U-W2R is symmetrical.,
Extending the soundness proof.,
"To extend the soundness proof to support these new rules, we first prove specifications for the priority reference operations in Musketeer, shown in the upper part of Figure 17.",
"These specifications involve two predicates: ispw ℓ𝑞𝑖, asserting that ℓis a priority reference, and that ℓis in its concurrent phase with fraction 𝑞and stores (at least) 𝑖. Symmetrically, ispr ℓ𝑞𝑖asserts that ℓis in its read phase.",
The specification of palloc𝑖asserts that this function Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:22 Alexandre Moine, Sam Westrick, and Joseph Tassarotti {⊤} palloc𝑖 {𝜆𝑟_.",
ispw ℓ1𝑖} {ispw ℓ𝑞𝑖} pwrite ℓ𝑗{𝜆𝑟ℓ.,
⌜𝑣= ℓ⌝∗ispw ℓ𝑞(𝑖max 𝑗)} {ispr ℓ𝑞𝑖} pread ℓ {𝜆𝑟_.,
⌜𝑣= 𝑖⌝∗ispr ℓ1𝑖} ispw ℓ(𝑞1 + 𝑞2) (𝑖max 𝑗) ⊣⊢ispw ℓ𝑞1 𝑖∗ispw ℓ𝑞2 𝑗 ispr ℓ(𝑞1 + 𝑞2) 𝑖⊣⊢ispr ℓ𝑞1 𝑖∗ispr ℓ𝑞2 𝑖 ispw ℓ1𝑖⊣⊢ispr ℓ1𝑖 𝑠≜· · · | spwrite𝑖| spread𝑖 J pwrite𝑞| spwrite𝑖| 𝑣K ≜∃ℓ.,
⌜𝑣= ℓ⌝∗ispw ℓ𝑞𝑖 J pread𝑞| spwrite𝑖| 𝑣K ≜∃ℓ.,
⌜𝑣= ℓ⌝∗ispr ℓ𝑞𝑖 Fig.,
Specifications of Priority Writes and Logical Interpretation alloc_fill ≜ 𝜆𝑛𝑣.,
fill (alloc 𝑛) 𝑣 init ≜ 𝜆ℎ𝑛.,
"assert (𝑛≥0); let𝑑= ref () in let𝑎= alloc_fill𝑛𝑑in (𝑎,𝑑,ℎ) elems ≜ 𝜆(𝑎,𝑑,ℎ).",
"filter_compact𝑎𝑑 add ≜ 𝜆(𝑎,𝑑,ℎ) 𝑥. let𝑝𝑢𝑡= 𝜇𝑓𝑥𝑖.",
let𝑦= 𝑎[𝑖] in if𝑥== 𝑦then () else if𝑥== 𝑑then (if CAS𝑎𝑖𝑑𝑥then () else 𝑓𝑥𝑖) else let 𝑗= (𝑖+ 1) mod (length𝑎) in if𝑥< 𝑦then 𝑓𝑥𝑗else (if CAS𝑎𝑖𝑦𝑥then 𝑓𝑦𝑗else 𝑓𝑥𝑖) in 𝑝𝑢𝑡𝑥((ℎ𝑖) mod (length𝑎)) Fig.,
Implementation of a Deterministic Concurrent Hash Set call returns a location ℓsuch that ispw ℓ𝑞1 holds.,
The specification of pwrite ℓ𝑗updates a share ispw ℓ𝑞𝑖into ispw ℓ𝑞(𝑖max 𝑗).,
"The specification of pread ℓasserts that this function call returns the content of a priority reference, if this reference is in its read phase.",
The central part of Figure 17 shows the splitting and joining rules of the ispw and ispr assertions.,
"It also shows that one can update a ispw assertion into a ispr assertion, and vice-versa, as long as the fraction in 1 (formally, these conversions involve the so-called ghost updates [Jung et al.",
The lower part of Figure 17 intuits how we extend the logical relation backing the soundness of our type system.,
"We add two shapes, one for each phase.",
"We then extend the logical relation as expected, making use of the previous assertions.",
"7.3 Deterministic Concurrent Hash Sets Next, we extend MiniDet with a deterministic concurrent hash set, inspired by Shun and Blelloch [2014].",
"This hash set allows for concurrent, lock-free insertion, and offers a function elems that returns an array with the inserted elements in some arbitrary but deterministic order.",
"This hash set is implemented as an array, and makes use of open addressing and linear probing to handle collision.",
The key idea to ensure determinism is that neighboring elements in the array are ordered according to a certain total order relation.,
"As we will see, insertion preserves the ordering, which in turn ensures determinism of the contents of the array.",
"Shun and Blelloch [2014] also propose a deletion function, which we do not verify.",
The hash set usage must be phased: insertion is allowed to take place in parallel as long as no task calls the function elems.,
Implementation of our hash set.,
Figure 18 presents the implementation of the deterministic hash set.,
"While in our mechanization we support a hash set over arbitrary values, for space constraints we present here an implementation specialized to integers, equipped with the comparison function <.",
"A new hash set is initialized with the function initℎ𝑛, which returns a tuple (𝑎,𝑑,ℎ), where 𝑎is the underlying array, 𝑑is a dummy element (in our case, a fresh reference containing the unit value) representing an empty slot in the array.",
The function ℎis the hash function.,
"The implementation uses a helper routine, alloc_fill𝑛𝑑, which allocates an array and fills it with the value 𝑣using a Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:23 𝜏≜· · · | intarray𝑞| intset𝑞 intarray𝑞1 · intarray𝑞2 ≜intarray (𝑞1 + 𝑞2) intset𝑞1 · intset𝑞2 ≜intset (𝑞1 + 𝑞2) T-AAlloc Γ1 ⊢𝑒1 : int ⊣Γ2 Γ2 ⊢𝑒2 : int ⊣Γ3 Γ ⊢alloc_fill𝑒2 𝑒1 : intarray 1 ⊣Γ3 T-ALoad Γ1 ⊢𝑒: int ⊣Γ2 Γ2(𝑥) = intarray𝑞 Γ1 ⊢𝑥[𝑒] : int ⊣Γ2 T-AStore Γ1 ⊢𝑒1 : int ⊣Γ2 Γ2 ⊢𝑒2 : int ⊣Γ3 Γ3(𝑥) = intarray 1 Γ ⊢𝑥[𝑒2] ←𝑒1 : unit ⊣Γ3 T-SAlloc Γ ⊢𝑒: int ⊣Γ′ (∀𝑥.,
{⊤} ℎ𝑥{𝜆𝑣_.,
⌜𝑣= ℎ𝑎𝑠ℎ(𝑥)⌝}) Γ ⊢initℎ𝑒: intset 1 ⊣Γ′ T-SAdd Γ ⊢𝑒: int ⊣Γ′ Γ′(𝑥) = intset𝑞 Γ ⊢add𝑥𝑒: unit ⊣Γ′ T-SElems Γ ⊢𝑒: intset 1 ⊣Γ′ Γ ⊢elems𝑒: intarray 1 ⊣Γ′ Fig.,
"Extension of MiniDet with Integer Arrays and Hash Set function fill, which we omit for brevity.",
"The function elems (𝑎,𝑑,ℎ) returns a fresh array containing the elements of 𝑎obtained by filtering those equal to the dummy element 𝑑.",
"The key challenge in the design is to ensure that this operation will be deterministic: in conventional linear probing hash tables, the order of elements in the array would depend on the order of insertions, so concurrent insertions would lead to nondeterministic orders.",
"To avoid this nondeterminism, the function add (𝑎,𝑑,ℎ) 𝑥, which inserts 𝑥in the hash set (𝑎,𝑑,ℎ), enforces an ordering on elements in the array according to the comparison function <.",
"The code makes use of a recursive auxiliary function 𝑝𝑢𝑡, parameterized by an element 𝑥and an index 𝑖, which tries to insert 𝑥at 𝑖.",
The function 𝑝𝑢𝑡loads the content of the array 𝑎at offset 𝑖and names it 𝑦.,
"If 𝑦is equal to 𝑥, then 𝑥is already in the set and the function returns.",
"If 𝑦is equal to the dummy element, the function tries a CAS to replace 𝑦with 𝑥, and loops in case the CAS fails.",
"Otherwise, 𝑦 is an element distinct from 𝑥.",
The function names the next index 𝑗= (𝑖+ 1) mod (length𝑎) and tests if 𝑥< 𝑦.,
"If 𝑦is greater than 𝑥, the function tries to insert 𝑥at the next index 𝑗by doing a recursive call of 𝑓𝑥𝑗.",
"If 𝑥is greater than 𝑦, the function tries to replace 𝑦with 𝑥with a CAS, and loops if the CAS fails.",
"If the CAS succeeds, the function removed 𝑦from the hash set, and must hence insert it again by doing a recursive call 𝑓𝑦𝑗.",
The function add then simply calls 𝑝𝑢𝑡to insert 𝑥at the initial index (ℎ𝑥) mod (length𝑎).,
Extension of MiniDet.,
Figure 19 presents the extension of MiniDet with this hash set.,
"To avoid issues related to ownership of the elements in the set, we consider a hash set containing integers.",
We add two new types: intarray𝑞describing an array of integers with a fraction 𝑞and intset𝑞a hash set of integers with a fraction 𝑞.,
The monoid on types is extended to sum the fractions.,
T-AAlloc types the allocation of an array filled with a default element.,
T-ALoad types a load operation on an array bound to the variable 𝑥.,
This operation requires any fraction of intarray.,
"T-AStore types a store operation but requires full ownership of the array—that is, the fraction 1.",
T-SAlloc allocates a hash set.,
"This rule has one non-syntactical precondition, which cannot be handled by a type system.",
"It requires that the hash functionℎ, the first parameter of add, implements some arbitrary pure function ℎ𝑎𝑠ℎ: V →𝑍.",
"This proof can be derived in Musketeer, and ensures that calls to the hash function are deterministic.",
T-SAlloc returns a intset type with fraction 1.,
"T-SAdd types an add operation on a hash set 𝑥with an arbitrary fraction 𝑞, meaning that this operation can happen in parallel.",
"T-SElems types the elems operation, requiring the full ownership Proc.",
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:24 Alexandre Moine, Sam Westrick, and Joseph Tassarotti (∀𝑥.",
{⊤} ℎ𝑥{𝜆𝑣_.,
⌜𝑣= ℎ𝑎𝑠ℎ(𝑥)⌝}) {⊤} initℎ𝑖{𝜆𝑣_.,
hashset𝑣1 ∅} {hashset𝑣𝑞𝑋} add𝑣𝑖{𝜆𝑟_.,
"hashset𝑣𝑞({𝑖} ∪𝑋)} {hashset𝑣1𝑋} elems𝑣{𝜆𝑣′ (ℓ, ®𝑤).",
⌜𝑣′ = ℓ⌝∗ℓ↦→®𝑤} hashset𝑣(𝑞1 + 𝑞2) (𝑋1 ∪𝑋2) ⊣⊢hashset𝑣𝑞1 𝑋1 ∗hashset𝑣𝑞2 𝑋2 𝑠 ≜ · · · | sintset𝑋 J intset𝑞| sintset𝑋| 𝑣K ≜ hashset𝑣𝑞𝑋 Fig.,
Specifications of a Deterministic Hash Set and Logical Interpretation parfor ≜ˆ𝜇𝑓.,
if (𝑗−𝑖) ==0 then () else if (𝑗−𝑖) == 1 then 𝑘𝑖 else let 𝑚𝑖𝑑= 𝑖+ ((𝑗−𝑖)/2) in par (𝑓𝑖𝑚𝑖𝑑𝑘) (𝑓𝑚𝑖𝑑𝑗𝑘) dedup ≜𝜆ℎ𝑎.,
let𝑠𝑡𝑎𝑟𝑡= 0 in let𝑙𝑒𝑛= length𝑎in let𝑠= initℎ(𝑙𝑒𝑛+ 1) in parfor𝑠𝑡𝑎𝑟𝑡𝑙𝑒𝑛(𝜆𝑖.,
add𝑠(𝑎[𝑖])); prod𝑎(elems𝑠) Fig.,
"Implementation of parfor and dedup Functions of a hash set, and producing a fresh array.",
This operation consumes the hash set argument; this is for simplicity: the hash set is only read and is in fact preserved by the operation.,
Extending the soundness proof.,
The upper part of Figure 20 presents the Musketeer specifications of the hash set operations.,
"These specifications make use of an assertion hashset𝑣𝑞𝑋asserting that 𝑣is a hash set with fraction 𝑞and content at least 𝑋, a set of values.",
"When 𝑞= 1, then 𝑋is exactly the set of values in the set.",
"The specification of initℎ𝑖returns a fresh set with fraction 1 and no elements, provided that the parameter ℎbehaves correctly.",
"The specification of add𝑣𝑖verifies the insertion of an integer 𝑖in a hash set 𝑣with an arbitrary fraction 𝑞and current content 𝑋, which the function call updates to ({𝑖} ∪𝑋).",
"Since we specialize to hash sets of integers, we know that the inserted value will not be the dummy element.",
"In our mechanization, we offer a more general specification, allowing the user to insert other pointers as long as they ensure that the inserted pointer is not the dummy element.",
"Perhaps most importantly, the specification of elems𝑣consumes an assertion hashset𝑣1𝑋with fraction 1 and produces an array ℓwith a deterministic content ®𝑤.",
"Figure 20 then gives the reasoning rule for splitting a hashset assertion, enabling parallel use.",
The lower part of Figure 20 shows how we extend the logical relation.,
"We add a shape sintset𝑋, where 𝑋a set of integers.",
The interpretation of intset𝑞with shape sintset𝑋and value 𝑣is then simply hashset𝑣𝑞𝑋.,
"7.4 Deduplication via Concurrent Hashing For our last example, we consider array deduplication, one of the parallel benchmark problems proposed by Blelloch et al.",
The task is to take an array of elements and return an array containing the same elements but with duplicates removed.,
The solution proposed by Blelloch et al.,
[2012] is to simply insert all the elements in parallel into a deterministic hash set and then return the elements of the hash set.,
"Figure 21 presents dedup, an implementation of this algorithm in MusketLang.",
"To do the parallel inserts, it uses a helper routine called parfor𝑖𝑗𝑘, which runs (𝑘𝑛) in parallel for all 𝑛between 𝑖and 𝑗.",
"Our goal is to prove that dedup satisfies schedule-independent safety, and then prove a specification in Angelic.",
"Throughout this proof, we assume that we have some hash function ℎsuch that ∀𝑥.",
{⊤} (ℎ𝑥) {𝜆𝑣_.,
⌜𝑣= ℎ𝑎𝑠ℎ𝑥⌝} and ∀𝑥.,
run (ℎ𝑥) {𝜆𝑣.,
"⌜𝑣= ℎ𝑎𝑠ℎ𝑥⌝}, where ℎ𝑎𝑠ℎis some function in the meta-logic.",
Our first step is to show that dedup can be typed in MiniDet.,
"This follows by using a typing rule for parfor (given in Appendix C.1), and the earlier typing rules we derived for the hash set.",
Using Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:25 these, we derive ∅⊢dedupℎ: intarray𝑞→(intarray𝑞× intarray 1) ⊣∅.",
"Thus, for a well-typed input array 𝑎, dedupℎ𝑎satisfies schedule-independent safety.",
We then verify dedup using Angelic.,
"The proof uses Angelic reasoning rules for the hash set, shown in Appendix C.2, which are similar to the earlier Musketeer specifications (§7.3), except for three key points.",
"First, the Angelic specification shows that, for a set 𝑣with content 𝑋, elems𝑣 returns an array ®𝑤which contains just the elements of the set 𝑋.",
"Second, the representation predicate for the hash set has no fraction: there is never a need for splitting it in Angelic.",
"Third, as we require the user to prove termination, the representation predicate tracks how many elements have been inserted, and does not allow inserting into a full table.",
"Finally, we use a derived specification for parfor𝑖𝑗𝑘that allows us to reason about it as if it were a sequential for-loop: forspec𝑖𝑗𝑘𝜑⊢run (parfor𝑖𝑗𝑘) {𝜆𝑣.",
"⌜𝑣= ()⌝∗𝜑} Here, forspec𝑖𝑗𝑘𝜑is defined recursively as forspec𝑖𝑗𝑘𝜑≜  ⌜𝑖≥𝑗⌝∗𝜑 ∨  ⌜𝑖< 𝑗⌝∗run (𝑘𝑖) {𝜆𝑣.",
"⌜𝑣= ()⌝∗forspec (𝑖+ 1) 𝑗𝑘𝜑}  In this definition, either 𝑖≥𝑗and the postcondition holds (since there are no recursive calls to be done), or 𝑖< 𝑗, and the user has to verify 𝑘𝑖, and show that forspec (𝑖+ 1) 𝑗𝑘𝜑holds afterward.",
"Essentially, this generalizes the idea we saw earlier in A-ParSeqL, by having us verify an interleaving that executes each task sequentially from 𝑖to 𝑗.",
"With these specifications, we deduce the following Angelic specification for dedup: ℓ↦→𝑞®𝑣⊢run (dedupℎℓ) {𝜆𝑣.",
ℓ↦→𝑞®𝑣∗ℓ′ ↦→®𝑤∗⌜deduped ®𝑤®𝑣⌝} 8 Related Work Deterministic parallel languages.,
"As shown in Section 7.1, Musketeer can be used to prove the soundness of language-based techniques for enforcing determinism.",
"A large body of such techniques exist, and it would be interesting to apply Musketeer to some of these.",
"In general, these languages typically ensure determinism by restricting side effects (e.g., in purely functional languages) or by providing the programmer with fine-grained control over scheduling of effects (e.g., in the form of a powerful type-and-effect system).",
Examples include seminal works such as Id [Arvind et al.,
1989] and NESL [Blelloch et al.,
1994] as well as related work on Deterministic Parallel Java [Bocchino Jr. et al.,
"2009, 2011], parallelism in Haskell [Jones et al.",
2008; Keller et al.,
2010; Chakravarty et al.,
"2011, 2001; Marlow et al.",
"2011], the LVars/LVish framework [Kuper et al.",
"2014a,b; Kuper and Newton 2013], Liquid Effects [Kawaguchi et al.",
Manticore [Fluet et al.,
"2007], SAC [Scholz 2003], Halide [Ragan-Kelley et al.",
"2013], Futhark [Henriksen et al.",
"2017], and many others.",
"It is typically challenging to formally prove sequentialization or determinization results for these kinds of languages, particularly in an expressive language with features like higher-order state and recursive types.",
"For example, Krogh-Jespersen et al.",
"[2017] point out that it took 25 years for the first results proving that in a type-and-effect system, appropriate types can ensure that a parallel pair is contextually equivalent to a sequential pair.",
"They show how a program-logic based logical relation, like the one we used in Section 7, can vastly simplify such proofs.",
Musketeer provides a program logic that is well-suited for constructing models to prove whole-language determinism properties.,
"Although not discussed in this paper, we have already completed a proof of schedule-independent safety for a simplified model of the LVars framework.",
We believe similar results may be possible for other deterministic-by-construction languages.,
Logic for hyperproperties.,
So-called relational program logics have been developed to prove hyperproperties.,
Naumann [2020] provides an extensive survey of these logics.,
A number of such Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:26 Alexandre Moine, Sam Westrick, and Joseph Tassarotti logics support very general classes of hyperproperties [D’Osualdo et al.",
2022; Sousa and Dillig 2016].,
"However, most of the relational logics building on concurrent separation logic have been restricted ∀∃hyperproperties [Liang and Feng 2016; Frumin et al.",
2021; Gäher et al.,
2022; Timany et al.,
"Because schedule-independent safety is a ∀∀property, it falls outside the scope of these logics, which motivated our development of ChainedLog.",
"In the world of ∀∀hyperproperties, several Iris-based relational logics have been proposed.",
"For example, Frumin et al.",
[2021] and Gregersen et al.,
[2021] verify variations of non-interference in a sequential setting.,
Both works require that both executions terminate.,
"However, their underlying relational logics do not support Musketeer’s distinctive feature: the chaining rule C-Chain.",
"In another setting, Timany et al.",
[2017] present a logical relation showing that Haskell’s ST monad [Launchbury and Peyton Jones 1995] properly encapsulates state.,
"They show such a result using a state-independence property which intuitively asserts that, for a well-typed program, if one execution terminates with a particular initial heap, then every execution terminates with any other initial heap.",
Most logics for hyperproperties are structured as relational logics.,
"However, some, like Musketeer, prove a hyperproperty through unary reasoning.",
"For example, Dardinier and Müller [2024], target arbitrary hyperproperties for a pure language, with a triple referring to a single expression, but with pre/post-conditions describing multiple executions.",
Eilers et al.,
"[2023] present CommCSL, a concurrent separation logic for proving abstract commutativity, that is, where two operations commute up-to some abstract interface.",
"This idea appears for example in the API for priority writes, which implies that writes commutes (§7.2).",
"In contrast with our approach, CommCSL is globally parameterized by a set of specifications the logic ensures commute.",
"In Musketeer, no such parameterization is needed: proof obligations are entirely internalized.",
Commutativity-Based Reasoning.,
Schedule-independent safety reduces the problem of verifying safety for all executions of a program to just verifying safety of any one terminating execution.,
"This can be seen as an extreme form of a common technique in concurrent program verification, in which the set of possible executions of a program is partitioned into equivalence classes, and then a representative element of each equivalence class is verified [Farzan 2023].",
"This approach has its origins in the work of Lipton [1975], and typically uses some form of analysis to determine when statements in a program commute in order to restructure programs into an equivalent form that reduces the set of possible nondeterministic outcomes [Elmas et al.",
2009; Kragl and Qadeer 2021; von Gleissenthall et al.,
2019; Farzan et al.,
"For programs satisfying schedule-independent safety, there is effectively only one equivalence class, allowing a user of Angelic to dynamically select one ordering to verify.",
9 Conclusion and Future Work Schedule-independent safety captures the essence of why internal determinism simplifies reasoning about parallel programs.,
"In this paper, we have shown how Musketeer provides an expressive platform for proving that language-based techniques ensure schedule-independent safety, and how Angelic can take advantage of schedule-independent safety.",
One limitation of schedule-independent safety is that it is restricted to safety properties.,
"In future work, it would be interesting to extend Musketeer for proving that liveness properties, such as termination, are also schedule-independent.",
Acknowledgments This work was supported by the National Science Foundation through grant no.,
2318722 and 2319168.,
The authors thank the anonymous reviewers of the paper and associated artifact for their feedback.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:27 unsafe ≜let𝑟= ref true in par (set𝑟true) (set𝑟false); assert (get𝑟) Fig.,
"An Unsafe Code A A Counter-Example to the Existantial Elimination Rule in Musketeer In this section, we explain in more detail why adding the standard separation logic rule for elimi- nating an existential to Musketeer would be unsound.",
Consider the unsafe program presented in Figure 22.,
"This program allocates a reference 𝑟 initialized to false, then executes in parallel two writes, the left one setting 𝑟to true, and the right one to false.",
"After the parallel phase, the program asserts that the content of 𝑟is true.",
This program does not satisfy schedule-independent safety.,
"Indeed, if the left write is scheduled before the right one, 𝑟will contain false and the assert will fail.",
"Hence, Musketeer must reject the unsafe program.",
"Yet, let us attempt a Musketeer proof, and let us show that this proof would succeed if the user is able to eliminate an existential without restriction.",
Let us attempt to verify the following triple {⊤} unsafe {𝜆_ _.,
"⊤} After the allocation of 𝑟, we have to verify {𝑟↦→false} par (set𝑟true) (set𝑟false); assert (get𝑟) {𝜆_ _.",
"⊤} Because the program next performs a concurrent write on the same location 𝑟, we have to use an invariant in order to share the ownership of 𝑟between the two tasks.",
Let us pick the invariant (∃𝑥.𝑟↦→𝑥).,
We now have to verify { ∃𝑥.𝑟↦→𝑥} par (set𝑟true) (set𝑟false); assert (get𝑟) {𝜆_ _.,
"⊤} Using M-Store and standard rules for invariants, we can easily establish the intermediate triple ∀𝑛.",
{ ∃𝑥.𝑟↦→𝑥} set𝑟𝑛{𝜆_ _.,
"Using this intermediate triple, the fact that we can duplicate invariants and rules M-Par and M-Bind, we are left with proving { ∃𝑥.𝑟↦→𝑥} assert (get𝑟) {𝜆_ _.",
"⊤} We apply M-Bind to focus on the sub-expression (get𝑟) with the weakest possible intermediate postcondition 𝑄′, that is we instantiate 𝑄′ with (𝜆_ _.⊤).",
The two preconditions of M-Bind are { ∃𝑥.𝑟↦→𝑥} get𝑟{𝜆_ _.,
{⊤} assert𝑣{𝜆_ _.,
⊤} The right triple immediately follows from M-Assert.,
"Using the opening rule of invariants on the left triple, we have to verify {∃𝑥.𝑟↦→𝑥} get𝑟{𝜆_ _.",
"∃𝑥.𝑟↦→𝑥} In order to conclude, one has to eliminate the existential, before using M-Load.",
"Thankfully, Muske- teer prevents the user from eliminating the existential, and the proof ultimately fails.",
What happened here?,
The existential quantification on𝑥hid the fact that there are two possible scheduling-dependent witnesses (true and false).,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:28 Alexandre Moine, Sam Westrick, and Joseph Tassarotti S-Empty empty ≈empty S-Unit unit ≈unit S-Bool bool ≈bool S-Int int ≈int S-HashSet intset𝑞1 ≈intset𝑞2 S-Array intarray𝑞1 ≈intarray𝑞2 S-Ref ref𝜏1 ≈ref𝜏2 S-Arrow 𝜏1 →𝜏2 ≈𝜏1 →𝜏2 S-Prod 𝜏1 ≈𝜏′ 1 𝜏2 ≈𝜏′ 2 (𝜏1 × 𝜏2) ≈(𝜏′ 1 × 𝜏′ 2) S-PRead 𝜏= pread𝑞2 ∨𝜏= pwrite𝑞2 pread𝑞1 ≈𝜏 S-PWrite 𝜏= pread𝑞2 ∨𝜏= pwrite𝑞2 pwrite𝑞1 ≈𝜏 Fig.",
Similar Predicate on Types S-SNone snone ≈snone S-SHashSet sintset𝑋1 ≈sintset𝑋2 S-SArray sintarray ®𝑣1 ≈sintarray ®𝑣2 S-SRef sref𝑣1 ≈ref𝑣2 S-SArrow sarrow𝛾≈sarrow𝛾 S-SProd 𝑠1 ≈𝑠′ 1 𝑠2 ≈𝑠′ 2 sprod𝑠1 𝑠2 ≈sprod𝑠′ 1 𝑠′ 2 S-SPRead 𝑠= spread𝑖2 ∨𝑠= spwrite𝑖2 spread𝑖1 ≈𝑠 S-SWrite 𝑠= spread𝑖2 ∨𝑠= spwrite𝑖2 spwrite𝑖1 ≈𝑠 Fig.,
Similar Predicate on Shapes T-ParFor Γ(𝑥) = Γ(𝑦) = int Fractional Γ Γ𝑓 ∀𝑞.,
[𝑖:= int](Γ𝑓𝑞) ⊢𝑒: unit ⊣Γ𝑓𝑞 Γ ⊢parfor𝑥𝑦(𝜆𝑖.,
𝑒) : unit ⊣Γ Fig.,
"A MiniDet Type for parfor B Definition of the Similarity between Typing and Shape Environments Figure 23 shows the definition of 𝜏1 ≈𝜏2, asserting that the two MiniDet types 𝜏1 and 𝜏2 are similar (§7.1).",
This property asserts that both types have the same structure except that functions must be equal and that priority reads and priority writes are identified.,
"Figure 24 shows the definition of 𝑠1 ≈𝑠2, asserting that the two shapes 𝑠1 and 𝑠2 are similar.",
"This property asserts that both shapes have the same structure, except that function shapes must be equal and that priority reads and priority writes are identified.",
We extend these two predicates to maps 𝑚1 ≈𝑚2 as the trivial predicate for the keys not in the intersection of dom(𝑚1) and dom(𝑚2) and the similar predicate when a key appears in both maps.,
"C Additional Explanations on the Concurrent Hash Set Example C.1 A Typing Rule for parfor In order to give a type in MiniDet to dedup (§7.4), we first give parfor a type, which we prove sound by dropping to the semantic model.",
"T-ParFor, which appear in Figure 25, requires the two indices to be variables bound to integers, for simplicity.",
"It then requires the environment Γ to be fractional, that is, to contain only fractional assertion.",
"This is witnessed by the precondition Fractional Γ Γ𝑓which is defined as (∀𝑛.𝑛≠0 =⇒Γ = ·𝑛(Γ𝑓𝑛)), that is, for every positive integer 𝑛, Γ𝑓𝑛represents a n-th share of Γ.",
"Finally, T-ParFor requires to type the last argument of parfor, which must be a function of the form 𝜆𝑖.𝑒.",
The precondition requires that 𝑒is typeable while borrowing a share Γ𝑓𝑛of the environment.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:29 A-HAlloc (∀𝑥.,
run (ℎ𝑥) {𝜆𝑣.⌜𝑣= ℎ𝑎𝑠ℎ𝑥⌝}) run (initℎ𝑖) {𝜆𝑣.,
ahashset𝑖𝑣∅} A-HAdd ⌜𝑠𝑖𝑧𝑒𝑋< 𝑖⌝ ahashset𝑖𝑣𝑋 run (add𝑣𝑥) {𝜆𝑤.,
⌜𝑤= ()⌝∗ahashset𝑖𝑣({𝑥} ∪𝑋)} A-HElems ahashset𝑖𝑣𝑋 run (elems𝑣) {𝜆𝑣′.,
⌜𝑣′ = ℓ⌝∗ℓ↦→®𝑤∗⌜deduped𝑋®𝑤⌝} Fig.,
Angelic Specifications for a Concurrent Hash Set C.2 Angelic Reasoning Rules for our Concurrent Hash Set Figure 26 presents the Angelic reasoning rules for our councurrent hash set (§7.3).,
"These spec- ifications involve a representation predicate ahashset𝑖𝑣𝑋, where 𝑖is the capacity (that is, the maximum number that can be contained in the set), 𝑣is the hash set and 𝑋the logical set with the inserted element.",
"Note that this predicate is not fractional, as there is no need to ever split it.",
A-HAlloc verifies initℎ𝑖.,
The precondition requires that the hash function ℎimplements a hash function in the meta-logic.,
A-HAdd verifies add𝑣𝑥.,
The precondition requires that 𝑣is a set with content 𝑋and capacity 𝑖.,
"The user must ensure that the size of the set 𝑋is less than the capacity, in order to guarantee termination.",
The postcondition returns the set with an updated model.,
A-HElems verifies elems𝑣.,
The precondition requires 𝐶3 𝑖that 𝑣is a set with content 𝑋.,
The postcondition returns a fresh array ℓpointing to ®𝑤such that deduped𝑋®𝑤holds.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:30 Alexandre Moine, Sam Westrick, and Joseph Tassarotti References Arvind, Rishiyur S. Nikhil, and Keshav Pingali.",
I-Structures: Data Structures for Parallel Computing.,
"11, 4 (1989).",
"doi:10.1145/69558.69562 Amittai Aviram, Shu-Chun Weng, Sen Hu, and Bryan Ford.",
Efficient System-Enforced Deterministic Parallelism.,
"In 9th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2010, October 4-6, 2010, Vancouver, BC, Canada, Proceedings, Remzi H. Arpaci-Dusseau and Brad Chen (Eds.).",
USENIX Association.,
"http://www.usenix.org/ events/osdi10/tech/full_papers/Aviram.pdf Guy E. Blelloch, Jeremy T. Fineman, Phillip B. Gibbons, and Julian Shun.",
Internally deterministic parallel algorithms can be fast.,
"In PPoPP ’12 (New Orleans, Louisiana, USA).",
"Guy E. Blelloch, Jonathan C. Hardwick, Jay Sipelstein, Marco Zagha, and Siddhartha Chatterjee.",
Implementation of a Portable Nested Data-Parallel Language.,
Parallel Distributed Comput.,
"21, 1 (1994).",
"doi:10.1006/JPDC.1994.1038 Robert L. Bocchino Jr., Vikram S. Adve, Danny Dig, Sarita V. Adve, Stephen Heumann, Rakesh Komuravelli, Jeffrey Overbey, Patrick Simmons, Hyojin Sung, and Mohsen Vakilian.",
A type and effect system for deterministic parallel Java.,
"In Proceedings of the 24th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2009, October 25-29, 2009, Orlando, Florida, USA, Shail Arora and Gary T. Leavens (Eds.).",
"doi:10.1145/1640089.1640097 Robert L. Bocchino Jr., Stephen Heumann, Nima Honarmand, Sarita V. Adve, Vikram S. Adve, Adam Welc, and Tatiana Shpeisman.",
Safe nondeterminism in a deterministic-by-default parallel language.,
"In Proceedings of the 38th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL, Thomas Ball and Mooly Sagiv (Eds.).",
"doi:10.1145/1926385.1926447 Richard Bornat, Cristiano Calcagno, Peter O’Hearn, and Matthew Parkinson.",
Permission accounting in separation logic.,
In Principles of Programming Languages (POPL).,
http://www.cs.ucl.ac.uk/staff/p.ohearn/papers/permissions_ paper.pdf John Boyland.,
Checking Interference with Fractional Permissions.,
"In Static Analysis Symposium (SAS) (Lecture Notes in Computer Science, Vol.",
"Springer, 55–72.",
"https://doi.org/10.1007/3-540-44898-5_4 Manuel M. T. Chakravarty, Gabriele Keller, Roman Lechtchinsky, and Wolf Pfannenstiel.",
Nepal - Nested Data Parallelism in Haskell.,
"In Euro-Par 2001: Parallel Processing, 7th International Euro-Par Conference Manchester, UK August 28-31, 2001, Proceedings (Lecture Notes in Computer Science, Vol.",
"2150), Rizos Sakellariou, John A. Keane, John R. Gurd, and Len Freeman (Eds.).",
"doi:10.1007/3-540-44681-8_76 Manuel M. T. Chakravarty, Gabriele Keller, Sean Lee, Trevor L. McDonell, and Vinod Grover.",
Accelerating Haskell array codes with multicore GPUs.,
"In Proceedings of the POPL 2011 Workshop on Declarative Aspects of Multicore Programming, DAMP, Manuel Carro and John H. Reppy (Eds.).",
doi:10.1145/1926354.1926358 Michael R. Clarkson and Fred B. Schneider.,
Hyperproperties.,
"18, 6 (2010).",
doi:10.3233/JCS-2009-0393 Thibault Dardinier and Peter Müller.,
Hyper Hoare Logic: (Dis-)Proving Program Hyperproperties.,
ACM Program.,
"8, PLDI (2024).",
"doi:10.1145/3656437 Emanuele D’Osualdo, Azadeh Farzan, and Derek Dreyer.",
Proving hypersafety compositionally.,
ACM Program.,
"6, OOPSLA2 (2022).",
"doi:10.1145/3563298 Marco Eilers, Thibault Dardinier, and Peter Müller.",
CommCSL: Proving Information Flow Security for Concurrent Programs using Abstract Commutativity.,
ACM Program.,
"7, PLDI (June 2023).",
"doi:10.1145/3591289 Tayfun Elmas, Shaz Qadeer, and Serdar Tasiran.",
A calculus of atomic actions.,
"In Proceedings of the 36th ACM SIGPLAN- SIGACT Symposium on Principles of Programming Languages, POPL, Zhong Shao and Benjamin C. Pierce (Eds.).",
doi:10.1145/1480881.1480885 Azadeh Farzan.,
Commutativity in Automated Verification.,
"In 38th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS.",
"doi:10.1109/LICS56636.2023.10175734 Azadeh Farzan, Dominik Klumpp, and Andreas Podelski.",
Sound sequentialization for concurrent program verification.,
"In PLDI ’22: 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Ranjit Jhala and Isil Dillig (Eds.).",
"doi:10.1145/3519939.3523727 Matthew Fluet, Mike Rainey, John H. Reppy, Adam Shaw, and Yingqi Xiao.",
Manticore: a heterogeneous parallel language.,
"In Proceedings of the POPL 2007 Workshop on Declarative Aspects of Multicore Programming, DAMP, Neal Glew and Guy E. Blelloch (Eds.).",
"doi:10.1145/1248648.1248656 Dan Frumin, Robbert Krebbers, and Lars Birkedal.",
ReLoC Reloaded: A Mechanized Relational Logic for Fine-Grained Concurrency and Logical Atomicity.,
"Logical Methods in Computer Science 17, 3 (2021).",
"https://arxiv.org/abs/2006.13635v3 Lennard Gäher, Michael Sammler, Simon Spies, Ralf Jung, Hoang-Hai Dang, Robbert Krebbers, Jeehoon Kang, and Derek Dreyer.",
Simuliris: a separation logic framework for verifying concurrent program optimizations.,
"Proceedings of the ACM on Programming Languages 6, POPL (2022), 1–31.",
"https://doi.org/10.1145/3498689 Simon Oddershede Gregersen, Johan Bay, Amin Timany, and Lars Birkedal.",
Mechanized logical relations for termination-insensitive noninterference.,
ACM Program.,
"5, POPL (Jan. 2021).",
doi:10.1145/3434291 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs 26:31 Troels Henriksen, Niels G. W. Serup, Martin Elsman, Fritz Henglein, and Cosmin E. Oancea.",
Futhark: purely functional GPU-programming with nested parallelism and in-place array updates.,
"In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI, Albert Cohen and Martin T. Vechev (Eds.).",
"doi:10.1145/3062341.3062354 Simon L. Peyton Jones, Roman Leshchinskiy, Gabriele Keller, and Manuel M. T. Chakravarty.",
Harnessing the Multicores: Nested Data Parallelism in Haskell.,
"In IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (LIPIcs, Vol.",
"2), Ramesh Hariharan, Madhavan Mukund, and V. Vinay (Eds.).",
Schloss Dagstuhl - Leibniz-Zentrum für Informatik.,
"doi:10.4230/LIPICS.FSTTCS.2008.1769 Ralf Jung, Robbert Krebbers, Jacques-Henri Jourdan, Aleš Bizjak, Lars Birkedal, and Derek Dreyer.",
Iris from the ground up: A modular foundation for higher-order concurrent separation logic.,
"Journal of Functional Programming 28 (2018), e20.",
"https://people.mpi-sws.org/~dreyer/papers/iris-ground-up/paper.pdf Ming Kawaguchi, Patrick Maxim Rondon, Alexander Bakst, and Ranjit Jhala.",
Deterministic parallelism via liquid effects.,
"In ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’12, Jan Vitek, Haibo Lin, and Frank Tip (Eds.).",
"doi:10.1145/2254064.2254071 Gabriele Keller, Manuel M. T. Chakravarty, Roman Leshchinskiy, Simon L. Peyton Jones, and Ben Lippmeier.",
"Regular, shape-polymorphic, parallel arrays in Haskell.",
"In Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming, ICFP, Paul Hudak and Stephanie Weirich (Eds.).",
doi:10.1145/1863543.1863582 Bernhard Kragl and Shaz Qadeer.,
The Civl Verifier.,
"In Formal Methods in Computer Aided Design, FMCAD 2021, New Haven, CT, USA, October 19-22, 2021.",
"doi:10.34727/2021/ISBN.978-3-85448-046-4_23 Robbert Krebbers, Luko van der Maas, and Enrico Tassi.",
Inductive Predicates via Least Fixpoints in Higher-Order Separation Logic.,
"In 16th International Conference on Interactive Theorem Proving (ITP 2025) (Leibniz International Proceedings in Informatics (LIPIcs), Vol.",
"352), Yannick Forster and Chantal Keller (Eds.).",
"Schloss Dagstuhl – Leibniz- Zentrum für Informatik, Dagstuhl, Germany.",
"doi:10.4230/LIPIcs.ITP.2025.27 Morten Krogh-Jespersen, Kasper Svendsen, and Lars Birkedal.",
A relational model of types-and-effects in higher-order concurrent separation logic.,
"In Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL, Giuseppe Castagna and Andrew D. Gordon (Eds.).",
doi:10.1145/3009837.3009877 Lindsey Kuper and Ryan R. Newton.,
LVars: lattice-based data structures for deterministic parallelism.,
"In Proceedings of the 2nd ACM SIGPLAN workshop on Functional high-performance computing, Clemens Grelck, Fritz Henglein, Umut A. Acar, and Jost Berthold (Eds.).",
"doi:10.1145/2502323.2502326 Lindsey Kuper, Aaron Todd, Sam Tobin-Hochstadt, and Ryan R. Newton.",
Taming the parallel effect zoo: extensible deterministic parallelism with LVish.,
"In ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’14, Michael F. P. O’Boyle and Keshav Pingali (Eds.).",
"doi:10.1145/2594291.2594312 Lindsey Kuper, Aaron Turon, Neelakantan R. Krishnaswami, and Ryan R. Newton.",
Freeze after writing: quasi- deterministic parallel programming with LVars.,
"In The 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL, Suresh Jagannathan and Peter Sewell (Eds.).",
doi:10.1145/2535838.2535842 John Launchbury and Simon Peyton Jones.,
State in Haskell.,
"LISP and Symbolic Computation 8, 4 (1995), 293–341.",
http://dx.doi.org/10.1007/BF01018827 Hongjin Liang and Xinyu Feng.,
A program logic for concurrent objects under fair scheduling.,
SIGPLAN Not.,
"51, 1 (Jan. 2016).",
doi:10.1145/2914770.2837635 Richard J. Lipton.,
Reduction: A Method of Proving Properties of Parallel Programs.,
"ACM 18, 12 (1975).",
"doi:10.1145/361227.361234 Simon Marlow, Ryan Newton, and Simon L. Peyton Jones.",
A monad for deterministic parallelism.,
"In Proceedings of the 4th ACM SIGPLAN Symposium on Haskell, Koen Claessen (Ed.).",
"doi:10.1145/2034675.2034685 Alexandre Moine, Sam Westrick, and Joseph Tassarotti.",
All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs (Artifact).,
doi:10.5281/zenodo.17653510 David A. Naumann.,
Thirty-Seven Years of Relational Hoare Logic: Remarks on Its Principles and History.,
"In 9th International Symposium on Leveraging Applications of Formal Methods (Lecture Notes in Computer Science, Vol.",
"12477), Tiziana Margaria and Bernhard Steffen (Eds.).",
"doi:10.1007/978-3-030-61470-6_7 Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo Durand, and Saman P. Amarasinghe.",
"Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines.",
"In ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’13, Hans-Juergen Boehm and Cormac Flanagan (Eds.).",
doi:10.1145/2491956.2462176 Sven-Bodo Scholz.,
Single Assignment C: efficient support for high-level array operations in a functional setting.,
"13, 6 (2003).",
doi:10.1017/S0956796802004458 Julian Shun and Guy E. Blelloch.,
Phase-concurrent hash tables for determinism.,
"In 26th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA, Guy E. Blelloch and Peter Sanders (Eds.).",
doi:10.1145/2612669.2612687 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"26:32 Alexandre Moine, Sam Westrick, and Joseph Tassarotti Marcelo Sousa and Isil Dillig.",
Cartesian hoare logic for verifying k-safety properties.,
"In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI, Chandra Krintz and Emery D. Berger (Eds.).",
"doi:10.1145/2908080.2908092 Amin Timany, Simon Oddershede Gregersen, Léo Stefanesco, Jonas Kastberg Hinrichsen, Léon Gondelman, Abel Nieto, and Lars Birkedal.",
Trillium: Higher-Order Concurrent and Distributed Separation Logic for Intensional Refinement.,
ACM Program.,
"8, POPL (Jan. 2024).",
"doi:10.1145/3632851 Amin Timany, Robbert Krebbers, Derek Dreyer, and Lars Birkedal.",
A Logical Approach to Type Soundness.,
"J. ACM 71, 6 (Nov. 2024).",
"doi:10.1145/3676954 Amin Timany, Léo Stefanesco, Morten Krogh-Jespersen, and Lars Birkedal.",
A logical relation for monadic encapsulation of state: proving contextual equivalences in the presence of runST.,
ACM Program.,
"2, POPL (Dec. 2017).",
"doi:10.1145/3158152 Klaus von Gleissenthall, Rami Gökhan Kici, Alexander Bakst, Deian Stefan, and Ranjit Jhala.",
Pretend synchrony: synchronous verification of asynchronous distributed programs.,
ACM Program.,
"3, POPL (2019).",
doi:10.1145/ 3290372 Received 2025-07-10; accepted 2025-11-06 Proc.,
ACM Program.,
"Lang., Vol.",
"POPL, Article 26.",
Publication date: January 2026.,
"Springer Nature 2021 LATEX template Learning Curves for Decision Making in Supervised Machine Learning: A Survey Felix Mohr1* and Jan N. van Rijn2 1Universidad de La Sabana, Ch´ıa, Cundinamarca, Colombia.",
"2Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands.",
*Corresponding author(s).,
"E-mail(s): felix.mohr@unisabana.edu.co; Contributing authors: j.n.van.rijn@liacs.leidenuniv.nl; Abstract Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learn- ing algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations.",
"Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection.",
"For instance, learning curves can be used to model the per- formance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process.",
Var- ious learning curve models have been proposed to use learning curves for decision making.,
"Some of these models answer the binary deci- sion question of whether a given algorithm at a certain budget will outperform a certain reference performance, whereas more complex mod- els predict the entire learning curve of an algorithm.",
"We contribute a framework that categorises learning curve approaches using three cri- teria: the decision-making situation they address, the intrinsic learning curve question they answer and the type of resources they use.",
We survey papers from the literature and classify them into this framework.,
"Keywords: learning curves, supervised machine learning 1 arXiv:2201.12150v2 [cs.LG] 28 Jan 2025 Springer Nature 2021 LATEX template 2 Learning Curves for Decision Making 1 Introduction Learning curves describe a system’s performance on a task as a function of some resource to solve that task.",
"There can be a pre-defined budget of that resource, limiting the amount of resources that can be spent.",
"In other cases, the goal can be to obtain reasonable results while minimising the spent budget of that resource.",
Typical types of budgets are the number of examples the learner has observed before performing the task or the number of iterations or time the learner spends in an environment.,
"The performance measure expresses the quality of the obtained model, e.g., error rate or F1 measure.",
Learning curves are an important source of information for making decisions on the following matters in machine learning: • Data Acquisition determines how many data points should reasonably be acquired to obtain a desired performance.,
The top right plot in Fig.,
1 visu- alises a scenario where we have already observed performance up to a certain amount of data (the blue learning curve).,
"We can extrapolate this and make a prediction of what the performance would be if more data was available, i.e., the value of the orange extrapolation at different vertical pink lines in the figure (see, e.g., Last, 2009; Weiss and Tian, 2008).",
• Early Stopping of training a model.,
"If we are committed to some specific learner (a learning algorithm and its hyperparameters), we might want to minimise the training time (John and Langley, 1996; Provost et al, 1999) or avoid over-fitting (Bishop, 1995; Goodfellow et al, 2016).",
The middle right plot in Fig.,
"1 visualises a scenario where we have already observed performance up to a certain amount of budget, and based on the progression of the learning curves on recent iterations, a decision can be made whether to continue the learning or terminate it.",
• Early Discarding in model selection.,
"If we want to select from various mod- els, we want to stop the evaluation of a candidate when we are reasonably certain that it is not competitive to the best-known solution (Domhan et al, 2015; Mohr and van Rijn, 2023; Swersky et al, 2014).",
The bottom right plot of Fig.,
1 visualises a scenario where we have already observed performance up to a certain amount of budget (the blue learning curve) and already an incumbent performance obtained by an earlier configuration (horizon- tal dashed pink line).,
"By using learning curve extrapolation techniques, we can determine whether the current configuration can surpass the incumbent configuration; if not, discarding the current training process early (as in the case shown) would be justified.",
Many techniques with varying complexity and required resources have been proposed to address either of these problems.,
"The complexity ranges from approaches that simply recognise whether an already observed part of a learn- ing curve has converged (Bishop, 1995; Provost et al, 1999) to the creation of parametric learning curve models, which capture a belief model for the behaviour of various learners at any possible budget (Klein et al, 2017b).",
While simple approaches may only rely on the observations made so far for a learner Springer Nature 2021 LATEX template Learning Curves for Decision Making 3 Fig.,
1: The three types of decision-making situations in which learning curves are typically used.,
"The x-axis of each figure represents the budget in the appli- cable unit, and the y-axis represents the performance.",
"on the dataset of interest, more complex approaches may rely on additional resources such as learning curves or features of other datasets (see, e.g., Leite and Brazdil, 2010) or learners (see, e.g., Chandrashekaran and Lane, 2017) or both (see, e.g., Ruhkopf et al, 2023).",
Our contribution is a unified framework of the usage of learning curves for decision making in machine learning and an extensive review of the literature of approaches that fall within this framework.,
This framework categorises the existing literature along the following three axes: 1.,
"The type of decision-making situation, i.e., whether it is used to make decisions about data acquisition, early stopping, or early discarding.",
4.1 for more details.,
"The type of technical question that can be answered with an approach, e.g., some approaches can only answer the binary question whether a model has converged, whereas other approaches are able to answer ques- tions about the behaviour of any part of the learning curve.",
4.2 for more details.,
The data resources that are used to model the learning curve.,
"For exam- ple, in some cases, data from different algorithms on the same dataset is being used, whereas in other cases, data from the same algorithm on other datasets.",
4.3 for more details.,
We perform an extensive literature survey in which we categorise published learning curve extrapolation models along the various axes of this framework.,
This literature survey is subject of Sec.,
"5, which is then summarised in Table 1.",
"We focus specifically on supervised machine learning, in which learning curves describe the predictive performance of a model produced by a learning algo- rithm either as a function of the number of training instances or of the time or iterations spent for learning on a given dataset.",
"We explicitly exclude learning Springer Nature 2021 LATEX template 4 Learning Curves for Decision Making curves that describe the performance of a learning agent in an environment over time, i.e., the learning curve of an agent in a reinforcement learning setup (Waltz and Fu, 1965).",
"Similarly, we briefly contrast learning curves to other performance curves, such as active learning curves, feature curves, and capacity curves, and explain why we consider these out of scope for the litera- ture review.",
"Still, we aim to survey exhaustively the literature that introduces approaches that use learning curves in supervised learning.",
Contributions: Our contributions are the following.,
• We present a unified framework of the usage of learning curves for decision making in machine learning and an extensive review of the literature on approaches that fall within this framework.,
"This framework contains three axes, i.e., (i) the type of decision-making situation, see also Fig.",
"1 (ii) the type of question that can be answered, see also Fig.",
"10 and (iii) the data resources that are used to model the learning curve, see also Fig.",
"While the first axis of this framework is also used in other literature (see, e.g., Viering and Loog, 2023), to the best of our knowledge, the other two axes have not yet been explicitly identified.",
• We conducted a literature survey in which we categorise methods presented in the literature along the various axes of this framework.,
"5 lists all these methods (where each subsection represents a type of question being answered), and Table 1 overviews all methods along the three axes of our framework.",
"• Based on the framework, we identify unexplored routes for further research.",
"Most notably, we note that there is a mismatch between the research ques- tions being answered and the learning curve modelling method being used; in many cases, a high-level modelling technique is used to answer a low- level question.",
We speculate that matching the level of the question being answered with the appropriate level of the modelling technique can further improve the obtained results.,
"Relation to other literature reviews on learning curves: Another prominent literature review that centres around learning curves is the highly complementary work by Viering and Loog (2023), which has been developed in parallel.",
"While both works have identified the three types of decision-making situations that are referred to in the literature, Viering and Loog (2023) survey more theoretical work that analyses the shape of learning curves, whereas this work surveys work that is more oriented towards methods that extrapolate learning curves, thereby supporting the data scientists in various decision-making situations.",
Structure: This paper is structured into three main parts.,
"2 presents relevant back- ground knowledge on learning curves, including formal definitions and the Springer Nature 2021 LATEX template Learning Curves for Decision Making 5 terminology relevant for the remainder.",
3 presents relevant important con- cepts that relate to how learning curves are generally modelled.,
4 contains our main contribution by introducing our framework for categorising methods that utilise learning curves for decision making in supervised learning.,
"Follow- ing this framework, Sec.",
5 exhaustively reviews approaches that explicitly or implicitly answer questions related to learning curves to make or recommend decisions in the context of supervised machine learning.,
6 concludes our findings.,
"Finally, Appendix A presents a table that overviews the most critical notation used throughout this paper.",
2 Background on Learning Curves This section gives a conceptual background on learning curves.,
It first provides an idealised formal definition in Sec.,
2.1 followed by a definition of empirical learning curves in Sec.,
2.2 that can be computed in practice.,
The concept of utility curves is introduced in Sec.,
"2.4 introduces important terminol- ogy such as anchor points, limit performance, and the saturation point.",
"Finally, Sec.",
2.5 contrasts the learning curves covered in this survey with other types of performance curves used in machine learning.,
2.1 Sample-Wise and Iteration-Wise Learning Curves We consider learning curves in the context of supervised machine learning.,
"Formally, in the supervised learning context, we assume some instance space X and a label space Y.",
"A dataset d ⊂{(x, y) | x ∈X, y ∈Y} is a finite relation between the instance space and the label space.",
We denote as D the set of all possible datasets.,
"A learning algorithm is a function a : D × Ω→H , where H = {h | h : X →Y} is the space of hypotheses and Ωis a source of randomness.",
Note that learning curves can also be considered in other machine learning setups.,
"In fact, learning curves appeared first in reinforcement learning (Waltz and Fu, 1965) and have also been used for unsupervised learning (Meek et al, 2002).",
"However, to give this survey focus, we consider learning curves for supervised learning.",
"The performance of a hypothesis is typically expressed as risk, which is also often called out-of-sample error: Rout(h) = Z X,Y loss(y, h(x)) dPX×Y.",
"(1) Here, loss(y, h(x)) ∈R is the penalty for predicting h(x) for instance x ∈X when the true label is y ∈Y, and PX×Y is a joint probability measure on X × Y from which the available dataset d has been generated.",
"As such, the out-of-sample error represents the weighted summed error that hypothesis h makes on all possible instance-label pairs, weighted by their probabilities.",
Springer Nature 2021 LATEX template 6 Learning Curves for Decision Making The performance of a learning algorithm is simply the performance of the hypothesis it produces.,
"In contrast to the performance of a hypothesis, the performance of a learner depends on its input, i.e., on the data provided for learning.",
"The average performance of learner a for a number n of training examples can then be expressed as C(a, n) = Z ω∈Ω,dtr∈D,|dtr|=n Rout(a(dtr, ω))dPX×YdPΩ, (2) where dtr ∈D is the dataset of size n used to induce a model using learner a.",
It is generally assumed that d is a collection of i.i.d.,
samples from PX×Y.,
When we consider Eq.,
"(2) as a function of the number of training samples for a fixed learner a, we obtain the sample-wise curve of learner a.",
"That is, the sample-wise curve is the function C(a, ·) : N →R; so it is a sequence of performances, one for each training size.",
2 (left) visualises this by means of the green line and compares this to two other types of learning curves with the error rate as the loss (see Sec.,
"Alternatively, many learning algorithms implement an iterative internal optimisation process, which allows describing the learning progress over time or a number of iterations.",
"For example, neural network training produces a new hypothesis after every batch or epoch, ensemble learners like bagging or boosting produce a new hypothesis after every added ensemble member, and support vector machine optimizers yield updated attribute or instance coeffients in iteration.",
"In the formal framework, a learner can be seen more generally as a function a : D × Ω→H + that maps a dataset to a sequence of hypotheses, one for each of its iterations.",
"The above error function for learners can then be written as C(a, n, t) = Z ω∈Ω,dtr∈D,|dtr|=n Rout(a(dtr, ω)t)dPX×YdPΩ, (3) Here, t expresses some budget, for example, time or a number of iterations over the dataset, often expressed in epochs.",
"Based on this notion, the iteration-wise curve of a learner a is defined for a fixed dataset size n (often between 70% and 90% of the available data) and is then the function C(a, n, ·) : N →R.",
2 (right) visualises an example of two iteration-wise curves.,
It can be seen that these iteration-wise curves are also influenced by the number of samples.,
The sample-wise curve in this example (visualised by the dashed line) is assumed to utilise the maximal number of iterations.,
"Examples of such learning curves occur above all in the analysis of deep learning models (Domhan et al, 2015; Goodfellow et al, 2016).",
"The two types of learning curves seem to be related and indeed look simi- lar when visualised, but they have different semantics.",
"The crucial difference is that iteration-wise curves are usually visualized for a fixed and finite number of training samples (expressed by n), no matter how large t becomes.",
"In fact, Springer Nature 2021 LATEX template Learning Curves for Decision Making 7 sample size error rate Sample-Wise Learning Curve sample- optimized curve stream curve iterations/time Iteration-Wise Learning Curve Fig.",
2: Left: (Standard) sample-wise curve (green) for a single learner on a particular data source together with learning curves under sample optimisation (pink) and learning curves on streams (blue).,
Right: iteration-wise curves of a single learner on a particular data source for two different dataset sizes n1 < n2.,
"since iterative learners typically automatically stop the learning process as soon as no progress is observed, it often holds that C(a, n) = limt→∞C(a, n, t).",
"But this is not necessarily the case, specifically if an algorithm stops early (in the iteration-wise curve), e.g., to avoid over-fitting (as the neural net- work in Fig.",
"Note that as t grows, each training instance is considered an unlimited number of times; hence, iteration-wise curves show how much the learner can make out of a constant number of training instances.",
"Instead, sample-wise curves show the performance of the learner as the number of exam- ples grows.",
"The latter typically means, for an infinite input space, that the information basis available to the learner is growing strictly bigger, while the information is constant in the case of iteration-wise curves.",
"Note that while the learning success can be expressed in a metric that ought to be maximised or minimised, in this paper we assume for simplic- ity that they are to be minimised.",
This is why the performance is expressed through a loss such as the error rate.,
"In this case, learning curves are (usually) decreasing.",
"However, more generally, one can also be interested in increas- ing learning curves, e.g., when considering accuracy or the F1 measure.",
"Since learning curves can be simply mirrored at the x-axis, every approach discussed in this paper is applicable to both increasing or decreasing learning curves.",
"For simplicity, this survey assumes that lower performance values are better (error rate, log-loss/cross-entropy, Brier score, mean-square-error, etc.).",
2.2 Empirical Learning Curves The above definitions of learning curves are purely theoretical.,
This is because we cannot evaluate equations (1-3) in practice.,
"First, the out-of-sample error Rout, i.e., Eq.",
(1) cannot be computed in practice since the measure PX×Y is unknown.,
"Relying on this error, the learning curve values cannot be computed either.",
The necessity to average over the oftentimes uncountable set of all possible train sets can add additional problems.,
Springer Nature 2021 LATEX template 8 Learning Curves for Decision Making 0 1000 2000 3000 Size n of the training set dtr 0.125 0.150 0.175 0.200 error rate Empirical Sample-Wise Learning Curves RF NN SVM 0 50 100 150 200 250 Iteration t (trees/epochs/optimization iterations) error rate Empirical Iteration-Wise Learning Curves RF NN SVM Fig.,
3: Empirical learning curves for the waveform dataset.,
Left: sample-wise curves at different training set sizes up to 80% of the data.,
The remaining 20% are used to compute the error.,
"Right: iteration-wise curves with one entry for each forest size (RF), epoch (NN), or optimization iteration (SVM) when a fixed set of 80% of the available data is used in each iteration for training and the rest to compute the error.",
"To compute learning curves in practice, we rely on empirical estimates of the above quantities.",
"We estimate the out-of-sample error by the internal error: Rin(h) = 1 |d| X (x,y)∈d loss(y, h(x)), (4) where d is the dataset used for assessing the performance of hypothesis h. Note that the dataset d may or may not contain instances used to create the hypothesis h. In most practical applications, dataset d consists of instances that have not been seen during the creation of hypothesis h (i.e., the test set).",
"However, in theory, the internal error can also be estimated based on the train set dtr or a combination of instances from the train set and test set.",
We consider an empirical learning curve any set of estimates of a true learning curve for different sizes or iterations.,
"We can use various estima- tion procedures to estimate the performance using a given training size, such as using a regular holdout set or cross-validation.",
"The latter leads to vari- ous estimates, and averaging over these estimates yields an estimate of the sample-wise curve in Eq.",
(2) at size n. To obtain an empirical estimate of the iterative learning curve in Eq.,
"(3), we do the same except that we stop the learning algorithm after t iterations.",
"3 shows empirical learning curves for a Random Forest (RF), a Neural Network (NN) with 100 neurons in a hidden layer, an a support vector machine (SVM) with RBF kernel on a concrete and widely used benchmark dataset (the waveform dataset).",
The empirical curves are the scatter points; the lines here are only a visual aid.,
"The error rate is here obtained from a single validation fold, i.e., without averaging, which explains the rather unsmooth behavior.",
The iteration-wise curves were created using 80% of the data for training.,
"Since empirical learning curves are the only way to gain insights about true learning curves, quite some studies have been published with the sole goal of Springer Nature 2021 LATEX template Learning Curves for Decision Making 9 sharing empirical learning curves with the community and thereby improv- ing the understanding of how they behave.",
Perlich et al (2003) contrast the learning curves on decision trees and logistic regression on different datasets.,
"Notably, the authors also compare learning curves, e.g., they report whether one curve is below the other (dominates it) on all considered training set sizes or whether the two learning curves cross.",
Several other studies report learning curves for specific learners.,
Ng and Jordan (2001) compare logistic regression and naive Bayes.,
Mørch et al (1997) conduct a study similar to the one by Perlich et al (2003) to compare linear vs. non-linear classifiers on a smaller scale.,
"Recently, a number of learning curve databases have been published, i.e., learning curves of different network architectures on typical image classi- fication datasets (Bornschein et al, 2020; Dong and Yang, 2020; Siems et al, 2020), learning curves of different machine learning algorithms on tabular data (Mohr et al, 2022), and for mixtures of these tasks (Eggensperger et al, 2021; Pfisterer et al, 2022).",
Empirical studies of this type do not answer generalising questions about learning curves but rather report experimental results.,
"This is different from contributions in which certain model assumptions are made and data is com- pared to those models, e.g., with the aim to compute the goodness of fit of that model.",
"3, we briefly discuss some of such models.",
"2.3 Utility Curves The concept of learning curves can be further generalised to a utility curve (Last, 2007, 2009; Weiss and Tian, 2008).",
The utility usually involves a trade-off between the performance and the computational cost of training a model.,
The specific details can be different per task.,
The utility is connected to the learning curve in so far as the utility is also a function of the budget and is directly influenced by the predictive performance.,
"Therefore, one could argue that the utility curve U is obtained by passing the learning curve to the utility function alongside other parameters that influence the utility, most notably the cost of acquiring new instances and the cost to train a model on the respective dataset size.",
The learning curves associated with utility costs are visualised in Fig.,
4 (orange) and compared to a normal learning curve.,
"Assuming there is a linear cost associated with further training the classifier, we can see that the utility curve (which makes a trade-off between performance and cost) peaks at a given point, and deteriorates afterwards.",
"2.4 Terminology of Learning Curves While this survey is not primarily about the shapes of learning curves, the shapes of learning curves play an important role when using them to make deci- sions.",
"Hence, we consider it necessary to convey some of the most important insights about the basics of the shapes of learning curves.",
"However, we refer to a recent exhaustive survey on the shapes of learning curves (Viering and Loog, 2023) for details on this topic.",
5 visualises some important concepts.,
Springer Nature 2021 LATEX template 10 Learning Curves for Decision Making error rate budget Utility Curve Fig.,
4: A utility curve with the corresponding learning curve.,
"Anchor Points In this survey, we adopt the term anchor to refer to a point for which the empirical learning curve carries a performance estimate.",
There is no established name for such points in literature.,
"They are called sample sizes in (John and Langley, 1996; Leite and Brazdil, 2007; Provost et al, 1999), and those authors refer to a collection of such samples sizes as a schedule (Figueroa et al, 2012; John and Langley, 1996; Meek et al, 2002; Provost et al, 1999).",
"However, for iteration-wise curves, the term ‘sample’ is misleading because the curve plots performance against the number of times all instances (out of a fixed set) are presented to the learner, which is not the same as sample size.",
"Besides, the term sample size is quite overloaded in the context of machine learning, because this field deals with various types of samples in different contexts, e.g., train and validation samples, etc.",
"Another terminology observed sometimes is the one of sample landmarks (F¨urnkranz and Petrak, 2001; Leite and Brazdil, 2005).",
"However, this term is also slightly confusing, since landmarks are generally understood as the performances of cheap-to-evaluate learners, which was also the motivation for this terminology by F¨urnkranz and Petrak (2001).",
"A less used terminology is the term anchor (Kielh¨ofer et al, 2024; Kolachina et al, 2012; Mohr and van Rijn, 2021, 2023), which is not ambiguous in the machine learning context and captures the idea that analysis is based on some selected points.",
It serves well to immediately create an association with a particular size of a sampled training data set or a number of visited instances that is used in the context of building an empirical learning curve.,
"Throughout this paper, we formally use the symbol b to refer to an anchor.",
"Thereby, we abstract away from sample sizes n or iteration t. In other words, the symbol b is used to indicate points on both sample-wise curves or iteration-wise curves, and it should be clear from the context which one is meant (if the difference is relevant).",
"For example, in Fig.",
"3 above, we have the following anchors.",
"For the sample-wise curve, a geometric schedule with n ∈{⌈2i/2⌉| i ∈N} is cho- sen.",
"In this concrete case, the anchors are {1, 2, 3, 4, 6, 8, 12, 16, 23, 32, 46, 64, 91, 128, 182, 256, 363, 512, 725, 1024, 1449, 2048, 2897, 3750}.",
"In the iteration-wise curve, every iteration is used as an anchor, so t ∈{1, .., 200}.",
Springer Nature 2021 LATEX template Learning Curves for Decision Making 11 train set size |dtr| or number of iterations t 0.18 0.20 0.22 0.24 0.26 0.28 Error Rate Learning Curve (C) Observations of an Empirical LC at anchors Limit Performance Saturation Point/Performance (bsat/psat) Pre-Exponential Point/Performance Fig.,
5: Concepts related to a learning curve.,
Red: Limit performance.,
Green: Saturation point bsat (vertical line) and saturation performance psat (hori- zontal line).,
Orange: Pre-exponential point (vertical line) and pre-exponential performance (horizontal line).,
The curve plateaus at a performance of 0.2; the plateau is not visualized here to emphasise the difference between the pre- exponential point and saturation point.,
Limit Performance It is generally assumed that learning curves converge to some value.,
"In the case of iterative learning curves, there are sometimes oscillations in the curve, but even in such cases, the curve usually converges to some (in those cases, typically a bad) value eventually.",
We are not aware of a particular term that is used to describe the score to which a learner converges.,
"Cortes et al (1994) describes the limiting performance or the asymptotic performance of the data; i.e., it is not the property of a particular learner but the best achievable perfor- mance among all learners under consideration (even though only tested with two model types in the paper).",
"In this paper, we adopt the term limit perfor- mance of the learner (on a fixed number of training instances in the case of iteration-wise curves), and we denote this performance as plim.",
"Saturation Point Intuitively, the saturation point is the anchor after which the performance convergences.",
"That is, the anchor after which all values are in a distance of less than some pre-defined and typically very small ε.",
"In early works, this point has been called stopping point (Figueroa et al, 2012; Leite and Brazdil, 2003, 2004; Meek et al, 2002; Provost et al, 1999).",
"Provost et al (1999) characterise this point as follow: “Models built with smaller training sets [than smin] have lower accuracy than models built with from training sets of size smin, and models built with larger training sets have no higher accuracy.” In the context of those works, namely progressive sampling, the term stopping point makes sense because they progressively sample until they reach the convergence region, and then stop the sampling procedure.",
"In the absence of such a mechanism, the term appears a bit odd.",
"The notion of saturation in the context of learning Springer Nature 2021 LATEX template 12 Learning Curves for Decision Making curves was proposed by Tomanek (2010) and seems appropriate since the limit performance has not been reached, but it has almost been reached.",
The curve is saturated up to a mistake of ε.,
"We will denote the saturation point itself as bsat, emphasising that the saturation point is an anchor.",
The saturation performance is simply the value of the learning curve at that point.,
Both are defined in the context of a fixed learner a.,
"Hence, we can write the saturation performance as psat := C(a, bsat) in the context of a sample-wise curve or psat := C(a, n, bsat) in the context of an iteration-wise curve.",
"Pre-Exponential Point A related concept is the pre-exponential point and, correspondingly, the pre- exponential performance.",
The saturation point may be expensive to reach in the sense that a lot of training data is necessary to obtain the satura- tion performance.,
"We call the smallest anchor point for which an increase by a factor of q leads to a performance improvement of less than some δ, i.e., C(a, n) −C(a, q · n) < δ.",
"Reasonable candidates for q can be 2 or 10, while candidates for δ can be 0.01 or 0.001 if the metric is the error rate.",
"Its seman- tic is that from the pre-exponential point on, one needs more than qk (i.e., an exponentially increasing number of) training samples or iterations to improve by a low margin of kδ.",
"Correspondingly, the pre-exponential performance is the performance that can be obtained by a comparably small anchor point.",
Utility-Based Stopping Point The utility-based stopping point is the point at which the acquisition of fur- ther data points has a negative impact on the utility of the data analysing entity.,
"Therefore, this concept is associated with utility curves.",
"The utility- based stopping point is not related to the saturation point but, if at all, rather to the pre-exponential point; we denote it as bu sat.",
"4, this is the peak of the yellow curve.",
Plateau The right-sided open interval bounded by the saturation point from the left is consistently called the plateau of the curve.,
"However, a curve can also have intermediate plateaus, i.e., intervals of (almost) constant performance without being the final plateau.",
"Well-behaved Learning Curves To our knowledge, the notion of a well-behaved learning curve is first used by Provost et al (1999).",
A learning curve is said to be well behaved if its slope is monotonically non-decreasing (for error-based learning curves).,
"An even stricter criterion demanding convexity of the curve, which implies monotonic- ity, has been introduced recently by Mohr and van Rijn (2021, 2023).",
The property of being well behaved is one of the true learning curve.,
"The (linear interpolation of an) empirical learning curve can often violate this condition, Springer Nature 2021 LATEX template Learning Curves for Decision Making 13 specifically when the number of validations conducted at the anchors is small or when the learning curve has reached a plateau.",
"While it is known that not all learning curves are well behaved (Loog and Duin, 2012; Loog et al, 2019), empirical studies suggest that such curves are rather an exception and that most sample-wise curves are well behaved.",
"Learn- ing curves that are not well behaved are found above all in the context of deep learning, where a double descent or peaking phenomenon can be observed (at times) for both sample-wise curves and iteration-wise curves (Nakkiran et al, 2020); the effect was also observed decades ago for other learners (Val- let et al, 1989).",
"However, extensive empirical studies have shown that most sample-wise curves are even convex (Mohr and van Rijn, 2023) and hence well behaved.",
"Some recent works suggest that potential ill behaviour can be mit- igated by appropriate configuration or wrapping of learners (Mhammedi and Husain, 2021; Nakkiran et al, 2021; Viering et al, 2020).",
2.5 Relation to Other Types of Performance Curves We briefly discuss the relation of learning curves to other types of curves and problem settings.,
"These curves are fundamentally different from learning curves, and therefore, a more detailed coverage is beyond our scope.",
"2.5.1 Learning Curves in Active Learning Active learning is a setting where the data scientist can acquire the label of arbitrary instances (Settles, 2009) and hence can actively increase the training set.",
"On a concrete data source with a concrete initial dataset, a specific active learning strategy creates a deterministically extended dataset for any arbitrary anchor.",
"While this allows drawing a curve that plots performance against the number of training instances, this curve is not the one described in Eq.",
"(2), because the datasets are not sampled i.i.d.",
from PX×Y but dictated by the active learning strategy; they are sample-optimised.,
2 displays how this theoretically relates to the normal sample-wise curve.,
We do not consider this type of learning curve in this survey.,
"2.5.2 Learning Curves under Optimal Class Distribution Similarly, we obtain such a biased learning curve if we do not preserve the class distribution.",
Weiss and Provost (2003) have shown that it can be advantageous to over-sample instances of a minority class if they occur substantially more seldom than instances of a majority class.,
One can then ask for the best class distribution for a certain anchor.,
"Similar to the active learning case, this creates a new distribution of datasets that does not coincide with PX×Y anymore.",
"If we optimise over the class distribution at each anchor, we obtain a curve with the same axis labels as a sample-wise curve but a different semantics (and most likely different values).",
"For the case of two classes, this type of learning curve is obtained by taking the budget-wise maximum of a performance surface as proposed by Forman and Cohen (2004).",
Springer Nature 2021 LATEX template 14 Learning Curves for Decision Making num features error rate Feature Curve Fig.,
"6: Feature curve example for a single learner, once for a fixed and finite dataset size and once for an infinite dataset size.",
2.5.3 Learning Curves on Data Streams Another type of learning curve that violates the implicit assumptions made in Eq.,
"(2) is obtained when learning from data streams, a scenario in which training instances are coming in sequentially and need to be processed under strict time and memory constraints (Bifet et al, 2018).",
"Incremental learners like Hoeffding trees (Domingos and Hulten, 2000) and models induced by stochastic gradient descent are natural solutions to this problem domain.",
"When applied to a data stream, these algorithms explicitly forget an instance once it has been processed.",
"This is not only to free memory but also to address the problem of concept drift, i.e., the fact that PX×Y changes over time.",
"In such a case, we do not have that C(a, n) = limt→∞C(a, n, t) but rather that C(a, n) = C(a, n, n), where the learner is updated at every incoming training instance, and each of the n instances was considered exactly once; for example, similar to training a neural network for one epoch with batch size 1.",
"While this produces a kind of sample-wise curve, the result is clearly different from the learning curve received in the normal batch setting.",
2 displays how these type of curves theoretically relate to the normal sample-wise curves.,
"Even though syntactically equivalent to a sample-wise curve, data stream learning curves are substantially different and need different treatment.",
"Due to the (potential) concept drift over the time dimension, the i.i.d.",
"assumption is not guaranteed (da Costa et al, 2016).",
"From a theoretical viewpoint, extrap- olating the learning curve over time to meaningfully predict future behaviour becomes impossible without the i.i.d.",
assumption.,
"Due to its special nature, the data stream setting is beyond our scope.",
2.5.4 Feature Curves Learning curves always consider a fixed number of features.,
"Instead, one can fix the number of training instances and consider the performance as a function of the number of features.",
"This yields so-called feature curves (Hughes, 1968; Viering and Loog, 2023).",
Defining meaningful feature curves is conceptually more difficult than learning curves because of the importance that single features can have.,
"For Springer Nature 2021 LATEX template Learning Curves for Decision Making 15 simplicity, consider only sample-wise curves for this comparison.",
"In such learn- ing curves, every point n is associated with the expected performance when using n training instances.",
These n training instances are assumed to be drawn independently and identically distributed from the underlying distribution.,
According to the aforementioned definitions of a learning curve (per Eq.,
"3), there is no notion of a more informative instance (even though this notion clearly exists in the field of active learning).",
"In particular, the order in which instances are drawn is irrelevant.",
"However, in the context of features, some features are often more informative than others (and again, other fea- tures that have not been measured may be even more informative).",
"Therefore, in order to get an adequate overview, feature curves require some kind of aver- aging over all possible sets of features of a fixed size that can be formed from a base set of (available) features, as done by Hughes (1968).",
"The fact that some features might be more important than other features makes it hard to model and extrapolate feature curves, as there is no reasonable set of assump- tions to build these models on.",
6 displays two theoretical examples of feature curves.,
"When having a finite number of samples, the performance of these curves will, in the limit (when more features are added), deteriorate due to the curse of dimensionality.",
"Of course, when having infinite samples, the performance of such feature curves will go to perfect performance.",
Note that feature curves and learning curves can be integrated.,
"For exam- ple, Strang et al (2018) look at the combination of the number of instances and the number of features.",
"Since the effect of the number of features and the number of instances on the overall performance is clearly not independent, considering both together is sensible.",
"At the same time, due to the ambigu- ous semantics of feature curves already discussed above, using such combined curves is not necessarily straightforward for decision making, and we are not aware that the combined curve has been used for decision making so far.",
2.5.5 Capacity Curves Cortes et al (1994) introduce a curve that plots the performance of a config- urable learner as a function of the complexity of its instantiation.,
"For example, the learner could be a neural network, and the complexity would then be the number of hidden layers.",
"In doing this, a fixed dataset size is assumed.",
"In that paper, this type of curve has no specific name, but we dub it the capacity curve because they plot the performance as a function of capacity.",
7 displays examples of capacity curves.,
Capacity curves are interesting from a theoretical viewpoint as they allow us to analyse the intrinsic noise level of the given data.,
"More precisely, one can ask for the performance of a learner of some complexity level on, perhaps, an infinite number of data points.",
"If this value can be computed for every com- plexity level, then we obtain a performance curve over the model complexity.",
"If the number of data points is large enough, this curve can be assumed to be monotonically decreasing.",
"If we have a maximally flexible learner (such as a neural network) that can, in principle, assimilate any function, then the curve Springer Nature 2021 LATEX template 16 Learning Curves for Decision Making model capacity Capacity Curve Fig.",
"7: Capacity curve example for a learner class whose complexity can be increased (e.g., a neural network), once for a fixed and finite dataset size and once for the theoretical case of an infinite dataset size.",
will converge towards the intrinsic noise of the data.,
"That is, no learner can improve over that performance.",
"2.5.6 Curriculum Learning Curriculum learning is a paradigm inspired by human learning strategies, par- ticularly how humans learn complex tasks by gradually increasing the difficulty of the examples they are exposed to (Wang et al, 2022).",
"In curriculum learning, instead of randomly presenting training instances to the model, instances are presented in a meaningful order, typically from simpler to more complex sets of training instances.",
This can help the model learn more effectively and converge faster by initially focusing on easier instances that are simpler to learn and gradually introducing more difficult instances.,
"Learning curves related to curriculum learning come with all sorts of novel challenges, such as interpreting the learning curve in case more complex test instances are provided.",
"Therefore, it is hard to make assumptions about what a well-behaved curriculum learning curve would look like.",
"For this reason, we consider curriculum learning to be out of scope.",
3 Modelling a Learning Curve A learning curve model is a characterisation of the true learning curve derived from an empirical learning curve.,
The empirical learning curve is the result of sampling from a stochastic process that underlies noise stemming from randomness in data splits and the learning algorithm itself.,
"It is typically assumed (Domhan et al, 2015; Figueroa et al, 2012; Klein et al, 2017a,b; Mohr and van Rijn, 2023; Swersky et al, 2014) that, for any learner a and any budget b ∈N, this stochastic process follows the distribution f (a, b) ∼N(µa,b, σ2 a,b) = µa,b + N(0, σ2 a,b), (5) where µa,b is either C(a, b) as per Eq.",
"(2) if modelling a sample-wise curve or C(a, n, b) for some (implicit and not further specified) training set size n as per Eq.",
(3) when modelling an iteration-wise curve.,
"It assumes a noise that follows a Gaussian distribution with zero mean and dispersion σ2 a,b that may Springer Nature 2021 LATEX template Learning Curves for Decision Making 17 vary over different anchor sizes.",
"The assumption of a Gaussian noise is rea- sonable, because most loss functions form an average over sample-wise scores, which implies a Gaussian distribution through the Central Limit Theorem.",
"For simplicity, we will not make a difference between the two types of learning curves, so that anchors are always denoted as budgets b (regardless of whether this refers to training set size or iterations, epochs, trees, etc.).",
The task of forming a learning curve model for one or multiple learners is inherently one of supervised machine learning and requires the ability to generalise across anchors (and possibly even learners).,
"In practice, observations are only available for a finite number of learner-anchor combinations O = {(a1, b1), (a2, b2), (a3, b3), .",
"}, and the task is to learn, for each learner a, a model ˆfa(b) that expresses the belief about f (a, b) for any budget b, not only those in O.",
"Therefore, it is not enough to simply create an explicit estimate of µa,bi for the anchors (a, bi) ∈O, but some general pattern must be learned.",
Observe that a here is not a parameter of ˆfa(b) since one often does not generalise across learners.,
"However, some approaches advocate a single model ˆf (a, b) that estimates µa,b for any learner-budget combination, where a ∈A and A is the set of all possible learners (Klein et al, 2017a,b; Swersky et al, 2014).",
"During this process of building a learning curve model, one generally needs to cope with two types of uncertainty.",
"First, the aleatoric uncertainty is σ2 a,b, which is intrinsic and averaged out in the true learning curve.",
"Again, this is the uncertainty arising from randomness in the learner itself (if applicable) and random effects in the splits (or more general: data collection) when computing the empirical learning curve.",
"Second, the epistemic uncertainty is the one the learning curve model itself has about the estimate of the mean value µa,b.",
"This uncertainty can be removed by gathering more observations (i.e., extending observation set O).",
It is important to understand that epistemic uncertainty generally does not indicate model quality.,
"Epistemic uncertainty is not related to correctness: A model can have no epistemic uncertainty (be absolutely sure) about an actually wrong prediction, and similarly, it can be uncertain about a prediction that is actually correct.",
"Also, epistemic uncertainty gives no indication about whether the class from which the predictive model is inferred is suitable for the task, i.e., whether the true curve can be captured by the model that is fitted (e.g., a power law).",
H¨ullermeier and Waegeman (2021) discuss this for the more general case of selecting an appropriate machine learning model.,
"Recent results suggest that many learning curves are not adequately captured even by a very flexible parametric model, i.e., the 4-parameter MMF model (Kielh¨ofer et al, 2024), which motivates other, possibly non-parametric approaches for modelling, such as the one used in freeze-thaw Bayesian optimisation (Swersky et al, 2014).",
"Therefore, the uncertainty at the meta-level about whether a model class is suitable for a task cannot be captured in epistemic uncertainty and must be studied independently.",
"Springer Nature 2021 LATEX template 18 Learning Curves for Decision Making Modelling uncertainty in learning curve models typically implies modelling the epistemic uncertainty about the curve mean µa,b.",
"This uncertainty refers to arbitrary anchors b, both those from the observed data O as well as the anchors that were not part of this.",
"The three common patterns to define beliefs about µa,b are (i) point estimates (no uncertainty is expressed), (ii) range estimates, e.g., confidence intervals for µa,b, or (iii) distribution estimates, which quantify a full belief model over the true value of µa,b.",
"The aleatoric uncertainty can also be quantified and modelled, e.g., by taking many different samples per anchor.",
8 illustrates all of these concepts together.,
"The blue line µa,· repre- sents the true learning curve as per Eq.",
3 for all possible values of b.,
"As this integrates out all possible data splits as well as random factors from the algorithm, each point is an average from a distribution of many possi- ble performance values.",
"The variance of this distribution is expressed as σ2 a,·.",
"This variance stems from the aleatoric uncertainty, in the sense that it is high when the aleatoric uncertainty is high, and vice versa.",
"As the budget b (rep- resented on the x-axis) increases, the aleatoric uncertainty typically decreases, and thereby the variance naturally decreases as well.",
The orange elements are related to observations of the learners’ performance and the learning curve model.,
"The orange points (which all together form O) show observations made on the empirical learning curve (as a sample from the blue distributions, this can be one or more per point).",
"Note that these do not necessarily need to align with µa,· or fall within the variance bandwidth.",
"The orange solid line shows a point estimate defined by a parametric model obtained from the observations, and in this example, it substantially deviates from the true curve (cf.",
"The orange shaded area is a range estimate modelling epistemic uncertainty, which grows as one moves away from the available data (cf.",
"Finally, the dashed orange lines are distribution estimates for different budgets b (cf.",
Each of these dashed lines can be seen as a probability density func- tion (rotated 90 degrees) for a certain budget b.,
"Each curve shown sketches the distribution of the belief about where the true mean may be situated, in this figure modelled through Gaussian distributions.",
"As we move away from the observations, the shape of these bells grows bigger, indicating higher epistemic uncertainty.",
"In the following sections, we will explain the three model types in more depth.",
"We explain the concept for models of the type ˆfa(b), i.e., models for a specific learner a, because this is the most common case.",
"Generalising a curve model across learners requires additional logic, which we discuss along with the models that utilise such generalised model in Sec.",
"3.1 Point Estimates of the Learning Curve The simplest type of learning curve model for a learner a just estimates the mean curve values µa,b for any possible budget b and ignores uncertainty aspects (solid orange line in Fig.",
"Given an empirical learning curve in the form of some finite samples from this process at different anchors B = Springer Nature 2021 LATEX template Learning Curves for Decision Making 19 µa,· σ2 a,· Observations ˆfa (Point Estimate) ˆfa (Range Estimate) ˆfa (Distribution Estimate) Fig.",
8: Visualisation of various forms of uncertainty in learning curve mod- elling.,
"The blue line µa,· represents the true learning curve; the dot · means that it is for all budgets b.",
"The variance over all possible curves that could be sampled is expressed as σ2 a,·.",
The variance stems from the aleatoric uncer- tainty.,
Orange points are anchors at which a learner’s performance has been observed.,
"From this, an empirical learning curve can be modelled.",
"The orange solid line, area, and dashed lines are point estimate, range estimate, and dis- tribution estimate models, respectively.",
The latter two models also express epistemic uncertainty.,
"{b1, .., bn}, a regression model ˆfa(·|θ) : N →R is trained with respect to some model class with parameters θ.",
A considerable number of different parametric models have been proposed over time for this task.,
"To our knowledge, the first proposal of such classes was made, apparently independently, by Cortes et al (1993) and John and Langley (1996) with the three-parametric inverse power law (IPL) µa,b = α + βb−γ, (6) where the parameters α, β, γ > 0 need to be optimised to fit the learning curve for learner a. Frey and Fisher (1999) took a simplified variant of that model (αb−β) and compared it to a logarithmic (α log b + β), and an exponential model (α · 10−βb).",
"While it has been argued, at least for the power-law family, that there is a theoretical foundation for it (Seung et al, 1992), the considered model classes are typically not theoretically motivated but rather pop up in an ad-hoc manner.",
"For example, Gu et al (2001) extended the above three classes, without a specific motivation, by a vapor pressure model, the Morgan- Mercer-Flodin (MMF) model, and a Weibull model.",
"Depending on the purpose of the model, it is essential to distinguish between best-fitting and best-predictive models.",
"As was pointed out by Gu et al (2001), the model class that can best accommodate a given set of anchor points is not always the one that will make the best predictions on a high anchor when having been fit only on some initial anchors.",
"To understand the learning curve of a learner on a given dataset, one is interested in a best-fitting model class.",
"For extrapolation, one is interested in a best-predictive one.",
Springer Nature 2021 LATEX template 20 Learning Curves for Decision Making Our work does not seek to give a broad overview of different model classes but rather about the usage of such models.,
"We expose the inverse power law model because it is arguably the most prominent model class and has been advocated by many authors as a good fit for nearest neighbours, SVMs, deci- sion trees, and neural networks (Frey and Fisher, 1999; Gu et al, 2001; Hess and Wei, 2010; Richter and Khoshgoftaar, 2019).",
"However, other models have been proposed, e.g., based on differential equations (Boonyanunta and Zeep- hongsekul, 2004) or other physical laws (Gu et al, 2001), and authors have argued that other models, such as logarithmic shape can be a better fit (Gu et al, 2001; Singh, 2005).",
"For an updated and exhaustive overview of used model classes, we refer to the work by Viering and Loog (2023).",
"3.2 Range Estimates of the Learning Curves It has been recognised that incorporating some notion of uncertainty into the model itself is important (Mukherjee et al, 2003).",
"Formally, this amounts to learn a range estimate function ˆfa : N →R2 such that ˆfa(b) ≡[u, v] with some pre-defined semantic relationship between µa,b and the interval [u, v].",
"8, this type of estimate is visualised through the orange area.",
"The semantics of the interval [u, v] depend on what exactly is being mod- elled, which also implies how the model is created.",
"In the earliest known attempt on this matter, Mukherjee et al (2003) model the (believed) interquar- tile range of the actual distribution f (a, b) with this interval, i.e., values that are expected to be observed with a certain probability if sampling at a specific anchor (aleatoric uncertainty).",
"In this approach, two parametric (i.e., inverse power law) models are built, one from the 25 and one from the 75 quantile for each observed anchor b.",
"Even though the mean does not necessarily lie between these quartiles in general, this is the case in a Gaussian distribution, which is a sensible assumption as explained above.",
"In contrast, Figueroa et al (2012) use it to model a confidence interval of µa,b (epistemic uncertainty).",
"In this specific case, only one parametric model is learned, and the confidence interval around the curve is obtained through analytical rather than stochastic techniques.",
"The confidence interval-based approach can also be thought of as putting a probabilistic bound on the gap between the predicted performance ˆfa(b) and the true value µa,b.",
"Again, the mere presence of interval-based predictions that express epis- temic uncertainty should not lead to the conclusion or belief that there is a necessary relationship to correctness.",
"In particular, an epistemic uncertainty of 0 does not imply a correct prediction.",
"Suppose the model ˆfa is chosen from a class of which the mean curve µa,· is not a member.",
"In that case, it is guaranteed that there will be wrong predictions, regardless of the epistemic uncertainty expressed by the model.",
"But even if µa,· is among the models from which ˆfa can be built, it can still (and usually will) happen that, based on insufficient observations, a wrong ˆfa will be picked.",
"In such a case, it is still conceivable that, depending on the probabilistic model on which ˆfa rests, the epistemic uncertainty would be 0 for anchors b whereas f (a, b) ̸= ˆfa(b); here ˆfa(b) is just Springer Nature 2021 LATEX template Learning Curves for Decision Making 21 a value since the epistemic uncertainty of 0 means that the interval ˆfa(b) has only one value.",
"3.3 Distribution Estimates of Learning Curves In a more ambitious case, we can try to learn a full belief model of the learn- ing curve C. Formally, this amounts to learn a distribution estimate function ˆfa : N →{p | p is a distribution in the domain of the performance measure}.",
"8, this model corresponds to the sequence of dashed orange distributions (in this figure, displayed for only 14 values of budget b).",
"As with the previous approaches, one typically uses parametric functions (e.g., the inverse power law) as a basis but specifies distributions over their parameters instead of a single (based on the maximum likelihood) assignment.",
The distribution over parameters then induces a distribution of the space of learning curves.,
"Such a belief model is, for example, well-defined in a Bayesian framework that defines the posterior distribution of models given the observed data and assumes a certain model class.",
"This posterior distribution cannot be efficiently computed exactly but approximate it through sampling (Domhan et al, 2015; Klein et al, 2017b).",
"Clearly, distribution estimates are the most flexible way of modelling uncer- tainty and allow many interesting operations.",
"In particular, one can quantify the probability that the limit performance of a learning curve will be above or below some threshold τ, which is very useful for confidence-based early discarding (Domhan et al, 2015).",
4 A Framework to Categorise Learning Curves Methods for Decision Making Based on the common ground of learning curves and their models introduced in Sec.,
"3, this section presents a framework for categorising decision- making methods that use learning curves.",
We identify three orthogonal criteria along which those approaches can be categorised.,
The first criterion relates to the decision-making situation in which learning curves are used.,
We discuss these situations exhaustively in Sec.,
The second dimension covers the technical question that is answered about a learning curve to support the decision.,
"For example, are we interested in the saturation point or a complete model?",
These technical questions are sketched in Sec.,
"4.2, and we structure the literature review of Sec.",
5 according to this axis.,
"Finally, different data resources can be used to conduct an analysis with learning curves, e.g., other learning curves or features describing the datasets or the learning algorithms.",
These resources are covered in Sec.,
4.1 Types of Decision-making Situations Learning curves are an important resource in at least three types of decision- making situations: Springer Nature 2021 LATEX template 22 Learning Curves for Decision Making 1.,
"Quantitative Data Acquisition Consider the situation where a data sci- entist has a model trained on a set of observations, the performance is known, and there is the option to spend additional resources (e.g., money or labelling effort) to obtain additional training observations.",
The decision that needs to be made is: The acquisition of how many more labels is eco- nomically reasonable?,
"This question has an obvious connection to the field of active learning, which addresses the question of which instances should be labelled next (qualitative acquisition).",
Another question is whether we should acquire other features instead.,
Early Stopping (of training an independently considered model).,
"In the situation where one is committed to some specific learner (a learning algorithm and its hyperparameters), minimising the training effort is a reasonable goal.",
"Specifically, if large amounts of data are available and training is costly, the aim is to train until the saturation point is reached.",
Being able to detect or predict whether a learner’s performance saturates after a given number of observations or iterations can support making decisions on this.,
Early Discarding (in model selection).,
"Similarly, if we want to select from various models, we want to stop the evaluation of a candidate when we are sure that it is not competitive to the current best solution.",
We com- pare the learner performance to that of another learner instead of its own performance on more training investment.,
"For example, consider the situation where the learning curve of an algorithm seems to approach the saturation point, and we have already seen a superior model before, of which it is unlikely that the current algorithm will improve over.",
"In this case, we can discard the performance of this learner based on the performance in relation to other models.",
There is a large methodological overlap in creating a decision basis among all these decision-making situations.,
"For example, whether more data points would be helpful to improve performance is related to the question of the training size that should be chosen to minimise training effort.",
"Both questions, at their core, ask for the saturation point of the learning curve.",
"In the following sections, we will discuss each of the three decision-making situations in more depth.",
"4.1.1 Quantitative Data Acquisition Quantitative data acquisition focuses on the question of how many training examples should be considered, given that they are all sampled i.i.d.",
from the same source.,
"Quantitative data acquisition does not consider or pay attention to the possibility of acquiring specific instances, which would be considered in qualitative data acquisition.",
Qualitative data acquisition is mainly studied in the field of active learning and does not ask whether or how many instances should be acquired but for which instances a label should be acquired.,
Since active learning undermines the i.i.d.,
"assumption, it generates a different type of learning curve and is not covered in this survey.",
Springer Nature 2021 LATEX template Learning Curves for Decision Making 23 The relevance of learning curves for quantitative data acquisition rises from their ability to give insights into intrinsic properties of the data source as well as into the relationship between the number of training examples and the utility of having that number of samples.,
"On the one hand, intrinsic properties refer to the intrinsic noise of the data (Cortes et al, 1994), which tells us about the best possible performance of any learner no matter how much data would be available from the source.",
"If we know that, with the given data, we already achieve a performance close to the intrinsic noise, then data acquisition should focus on acquiring additional features instead of new instances.",
"On the other hand, the utility is mainly determined by the cost of acquiring (additional) examples, the performance obtained with a certain number of samples, and the cost to train a model with a given number of instances (Last, 2007, 2009; Weiss and Tian, 2008).",
"In this economic context, there are mainly five questions that can be considered: 1.",
Possibility.,
Can the classification performance be improved by more data?,
What is the best possible predictive performance given unlim- ited training observations?,
Maximization Principle.,
By how much can the predictive performance be improved if there is a budget for a fixed number of additional data points?,
Minimization Principle.,
How many instances are necessary to obtain a certain degree of predictive performance?,
Utility maximization.,
Which sample size maximises a given utility func- tion?,
"In the context of data acquisition, we typically deal with sample-wise curves.",
"Furthermore, one is often not committed to a particular learner; therefore, the performance measure in the above questions is implicitly the best one of a portfolio of learners.",
"That is, one assumes a set of learners that is considered admissible for the prediction task due to external restrictions.",
"In general, due to the ability to parameterise learners, this set is usually infinite.",
"When referring to the portfolio’s performance at a specific anchor point, we are interested in its best-performing algorithm at that specific point at the learning curve (Mohr and van Rijn, 2023).",
"Of course, if one is committed to one particular learner, then the situation simplifies to a portfolio of size 1.",
"Since data acquisition is not for free, it is sensible to relate potential pre- dictive performance improvements with the costs to collect the additional labels.",
"Therefore, instead of looking only at predictive performance, one looks at utility of an anchor point.",
"While performance typically only improves with an increased number of observations, the utility also considers acquisition costs, which negatively affect the utility.",
"Therefore, the goal is to decide how many instances should be labelled to maximise utility, i.e., how many addi- tional instances are justified before the added value no longer outweighs the additional costs (Last, 2007; Weiss and Tian, 2008).",
Springer Nature 2021 LATEX template 24 Learning Curves for Decision Making 4.1.2 Early Stopping Early stopping means interrupting the training process of a learner if the learn- ing curve has converged.,
"The term ‘early’ refers to the fact that the learning process would normally be continued, e.g., because more data is available or other stopping criteria are not yet satisfied.",
"That is, one uses the learning curve to judge that, despite more available resources or other criteria that would encourage further training, investing more time will not improve the performance of the considered model class any further.",
"The training process can then be stopped early, i.e., earlier than if that criterion would not be used.",
9 shows this logic in the left (red) part in which the blue learning curve of learner a is used to detect that not all the data is necessary and only 500 training instances are used (to save training time).,
Early stopping can be applied to both sample-wise curves and iteration-wise curves.,
Early stopping in sample-wise curves means retraining a model on different training set sizes to create an empirical learning curve with training set size as the budget.,
This can make sense if we do not already know that the saturation point is larger than the available dataset size; oth- erwise we should immediately train on the complete dataset.,
"We can then try to analyse the sample-wise curve of the learner for increasing training sizes and stop as soon as we find that performances do not change significantly between two anchors (John and Langley, 1996; Provost et al, 1999).",
"For iter- ative learners (such as neural networks), an iteration-wise curve is usually a by-product that can cheaply be created in parallel to learning.",
Hence it might seem more appropriate to do early stopping based on an iteration-wise curve rather than the sample-wise curve.,
"An additional advantage of early stopping in iteration-wise curves is that it can help avoid over-fitting, e.g., in neural networks (Bishop, 1995; Goodfellow et al, 2016) or gradient boosting.",
"While it is conceivable that building sample-wise curves, even for iterative learners, could be useful in some cases, we are not aware of any such work being done for early stopping (or any other purpose).",
The early stopping problem can be addressed retrospectively and pro- jectively.,
Retrospective early stopping means to stop after observing the saturation point.,
Projective early stopping means to predict the saturation point before it is reached and stop precisely at the (believed) saturation point.,
The projective approach is particularly important in the case of sample-wise curves.,
Early stopping in sample-wise curves and data acquisition might seem sim- ilar since both define a sample size at which a process should be stopped.,
"However, the concepts are fundamentally different in three ways: 1.",
Stopped Process: Early stopping means to stop a training process (at the saturation point) run in a machine.,
"In contrast, the decision-making situation in data acquisition is to stop the data acquisition process (at the economic saturation point).",
The latter is sometimes carried out by humans.,
Springer Nature 2021 LATEX template Learning Curves for Decision Making 25 500 1000 1500 2000 2500 3000 3500 4000 0.20 0.25 0.30 Early Stopping stops here avaiable data Data Acquisition stops here Early Stopping vs.,
"Stopping in Data Acquisition µa,· µ(arg mina∈A µa,∞,·) Matter of Early Stopping Matter of Data Acqusition Fig.",
9: Early stopping with sample-wise curves stops the training process of a single learner a at its saturation point using its (actually empirical) learning curve.,
"It shows the learning curve of two learners, the blue curve and the green dashed curve.",
The latter has the best saturation performance.,
The available data marks a restriction to this process.,
"In contrast, data acquisition considers the available data as the decision variable, and it stops collecting data when all learners have reached their saturation performance.",
"Role of Available Data: In early stopping, the available amount of training data is a given constraint under which early stopping operates, and data acquisition precisely seeks to control this quantity in an economically optimal fashion.",
Used Performance Curve: Early Stopping uses a single learning curve of learner a to decide upon early stopping of the training of a.,
"Data acqui- sition uses the learning curves of all learners under consideration (i.e., a finite set A) and considers the budget-wise best performance achievable (by any learner).",
"To clarify this difference, consider also the right (green) part of Fig.",
"The learner a may be the best solution available given the 1500 data points (in this case, a barely needs 500 of them to attain saturation performance).",
"However, if more data were available, then at least one other learner could take advantage of that additional data and outperform a.",
"The figure shows the curve of an optimal learner a∗∈arg mina∈A µa,∞that has the best performance if no limit is posed on the available training data (as in Cortes et al (1994)).",
"Only after 3000 instances, no learner will improve the overall possible performance anymore; therefore, at this point, the data acquisition process stops.",
"4.1.3 Early Discarding In many setups, the learner itself is a matter of optimisation.",
"Consider the situation where we have a (possibly infinite) set of learners, e.g., a finite set of algorithms, each of which can be instantiated with a possibly infinite number of hyper-parametrisations.",
"The task is to find the (hyper-parametrised) learner which performs best for the given data of size n in the sense that it creates, Springer Nature 2021 LATEX template 26 Learning Curves for Decision Making on average, the best model.",
"Formally, if A is the (infinite) set of parametrised learners, the goal is to find arg min a∈A C(a, n).",
(7) This task is commonly known as model selection.,
"While it is uncommon in literature to be so explicit and describe the model selection problem through the value of the learning curve at some sample size, this formulation is rather precise and insightful.",
It emphasises that which learner is best might depend on the number of available training points.,
Note that one needs to separate some portion of the data for validation in practice to estimate model performances.,
"In other words, most approaches in practice do not even address the above problem but instead arg min a∈A C(a, ⌈αn⌉), (8) where α ∈]0, 1[ (open interval) is the training portion, typically between 70% and 90%, where the remaining portion of 1 −α is used to estimate C(a, ⌈αn⌉), typically in some (possibly repeated) hold-out validation.",
"We could conceive that this procedure of estimating C(a, ⌈αn⌉) might involve the construction of an empirical learning curve as a sub-routine or on-the-fly.",
"First, if a is an iterative learner, then the model performance C(a, ⌈αn⌉) = C(a, ⌈αn⌉, t∗) is the performance of the iteration-wise curve at some point t∗where the learning process is stopped.",
"Therefore, the whole iteration-wise curve C(a, ⌈αn⌉, t∗) is available for analysis.",
"Second, even if a is not incremental, one could create an schedule {α1, .., αk} of increasing αi ≤α and thereby create a sample-wise curve (Mohr and van Rijn, 2023).",
"Such a sample-wise curve would also offer the perspective, via extrapolation, to address Eq.",
(7) rather than just Eq.,
"In the light of the availability of such a (partial) empirical learning curve, early discarding is the practice of aborting the performance estimation proce- dure of a candidate as soon as it becomes apparent from that curve that the candidate cannot be the solution to the above optimisation problem.",
"Formally, this is to drop a candidate a as soon as the criterion C(a, bref ) > min a∗∈A C(a∗, bref ) (9) can be verified, where bref is usually n or ⌈αn⌉.",
"In other words, as soon as it can be shown that a is not the best learner of all possible learners A, no further resources should be committed to training learner a.",
"Note that, even though the terms are frequently mixed up in literature, this is very different from early stopping, in which the convergence of the curve of a single learner is considered in isolation (cf.",
"Early discarding has been applied to both observation (Adriaensen et al, 2023; Mohr and van Rijn, 2021, 2023; Ruhkopf et al, 2023) and iteration (Adri- aensen et al, 2023; Domhan et al, 2015; Klein et al, 2017b; Ruhkopf et al, 2023; Springer Nature 2021 LATEX template Learning Curves for Decision Making 27 Swersky et al, 2014) learning curves.",
"In the first case, one is sampling from C(a, ·) at different anchors n and hopes to be able to drop sub-optimal can- didates at n ≪bref anchors (much) smaller than the target size bref .",
"In the second case, one always uses the complete dataset (or at least all data desig- nated for training, say n), observes samples of C(a, n, ·) for different anchors (maybe epochs) t, and seeks to avoid convergence if it can be foreseen that the convergence performance will be sub-optimal.",
Early discarding is more aggressive than early stopping because it does not need to wait until the learning curve converges.,
"On the contrary, one tries to avoid reaching convergence since this is considered a waste of resources in the case that the learner performs sub-optimal.",
In an extreme case and depending on available knowledge about learners (cf.,
"4.3), one could only use a single point of an empirical learning curve to discard a candidate.",
Situations in which early discarding plays a role can be further classified into horizontal and vertical scenarios (and a mixture of the two): 1.,
Horizontal Model Selection.,
"Horizontal model selection implies an apriori fixed finite set of learning algorithms, from which one has to be selected.",
Empirical learning curves are grown iteratively for the whole set or shrink- ing subsets of it.,
"Successive halving and related works are a prominent example of horizontal decision making (Van den Bosch, 2004; Jamieson and Talwalkar, 2016).",
"However, these approaches only consider the last anchor point (rather than the complete learning curve).",
Vertical Model Selection.,
"Vertical means that the set of learners is gener- ally not limited to a finite set, and the set of evaluated learner candidates evolves over time (i.e., not fixed apriori).",
Learners are evaluated one after another.,
Each learner is evaluated in an iterative fashion to grow a learn- ing curve and allow for early discarding.,
"Examples are the early discarding routine for deep networks by Domhan et al (2015) or, more generally, for learning curve cross-validation (Mohr and van Rijn, 2021, 2023).",
Diagonal Model Selection.,
This case is similar to the vertical decision- making situation with the difference that one does allow to continue the evaluation of a candidate at a later point.,
"Hence, candidates are not evaluated one after another, but the evaluation of different candidates can be interleaved.",
"Examples are Bayesian optimisation-based approaches to pause and continue evaluations of (not necessarily iterative) learn- ers (Klein et al, 2017a; Swersky et al, 2014).",
Non-iterative learners must be trained from scratch with the increased budget.,
"Another approach that addresses this type of decision-making situa- tion is Hyperband (Li et al, 2017) and Bayesian optimisation based on progressive sampling (Zeng and Luo, 2017).",
"However, neither of these approaches considers learning curves even though they implicitly con- struct them.",
Decisions are taken based on the observations of the largest anchor point considered so far.,
Springer Nature 2021 LATEX template 28 Learning Curves for Decision Making 4.2 Technical Questions Asked About Learning Curves A plethora of questions can be asked about learning curves.,
10 gives an overview of these questions.,
The figure is organised in three layers (depth dimension) corresponding to the three types of estimates discussed in Sec.,
Each layer consists of a set of questions that can be posed about learning curves.,
"From bottom to top, the questions are ordered by complexity, and an arrow from one question to another indicates that the question with the incoming arrow is more general.",
Answering the more general question also implies answering the less general question.,
"In the simplest case, we can answer a binary question.",
"There are four relevant questions, i.e., (i) whether some specific anchor point, e.g., the dataset size, is beyond the saturation point (bsat ≤bref ), (ii) whether the performance of a learner at the saturation point is better than some baseline τ (psat ≤ τ), (iii) whether the performance pref := C(a, bref ) at some reference point bref is better than some threshold τ, or (iv) whether a specific anchor point is beyond the utility-based stopping point (bu sat < bref ).",
"To our knowledge, the only approaches in this category are those implicitly answering question (iii) by discarding candidates that are not believed to be competitive (see, e.g., Jamieson and Talwalkar, 2016; Petrak, 2000; Zeng and Luo, 2017); here τ = mina∈A C(a, bref ) is the (unknown) best performance of any learner on the target size.",
A family of slightly more general questions tries to order a set A of learning algorithms w.r.t.,
their performance at some (future) anchor bref .,
"We denote this ordering as πa∈A ∼C(a, bref ).",
"Here, bref is typically the maximum avail- able training data, even if an iteration-wise curve is considered because then this is the termination performance of that curve.",
"In the simplest case, we could ask for a concrete pair of two learning algorithms a1, a2 whether C(a1, bref ) ≥ C(a2, bref ), i.e., which will perform better at some reference point.",
"For example, the work by Leite and Brazdil (2005) answers this question.",
"If this question is simultaneously asked for a set of or even all possible pairs of algorithms, one asks for a partial or even the full ranking πa∈A ∼C(a, bref ) of algorithms.",
"A particular case is to ask only for the best algorithm a∗= arg mina C(a, bref ), which implicitly answers that C(a∗, bref ) ≤C(a, bref ) for any learner a but without explicitly asking for any other comparisons.",
"Still, the comparison is merely qualitative, and answering this question does not necessarily require to quantify any aspect of any learning curve.",
"The above questions are merely qualitative and not quantitative, which gives rise to a third level of complexity, where the concrete values of bsat (Provost et al, 1999), psat (Cortes et al, 1993), pref (Baker et al, 2018; Chandrashekaran and Lane, 2017; Leite and Brazdil, 2003, 2004), or bu sat (Weiss and Tian, 2008) are being modelled.",
"Here, the reference performance pref is the performance at a fixed reference point, often the number of training samples available for cross-validation.",
"All questions up to this point produce closed answers in the sense that the answer is either a boolean value, a number, or a finite ranking of candidates.",
"Springer Nature 2021 LATEX template Learning Curves for Decision Making 29 Portfolio of curves What is C What is U Curves of learner What is C(a, ·) or C(a, ·, ·) What is U(a, ·) or U(a, ·, ·) Conservative bound on curve Determine C(a, ·) or C(a, ·, ·) Determine U(a, ·) or U(a, ·, ·) Value What is bsat What is psat What is pref What is bu sat Ranking What is πa∈A ∼C(a, bref ) Binary bsat ≤bref psat ≤τ pref ≤τ bu sat ≤bref Distribution estimates Range estimates Point estimates Fig.",
10: Technical questions that can be asked on learning curves.,
"At the fourth level, the task is to make assertions about arbitrary points of a learning curve.",
"However, the answers do not yet refer to the value of the learning curve itself but only a bound on those values.",
"At a fifth level, we could eventually ask for a model of the whole learning curve of a learner a, i.e., C(a, ·) for sample-wise curves (Figueroa et al, 2012; Frey and Fisher, 1999; Gu et al, 2001; John and Langley, 1996; Mukherjee et al, 2003) or C(a, n, ·) for iteration-wise curves given a fixed (sample) anchor n (Cortes et al, 1993; Domhan et al, 2015).",
We can ask a similar question for the utility curve.,
"While the question is on the same level, it is more general, as it is based on the learning curve itself (Last, 2007, 2009; Weiss and Tian, 2006) and combines it with other information such as acquisition costs.",
"Finally, at the sixth and most general level, we could ask for a model of the whole performance function C, i.e., the model of the learning curves across all learners (Klein et al, 2017a,b; Swersky et al, 2014).",
"Analogously, this question could be asked for the whole utility function U, which is arguably the most complex and general question that can be asked, although we are not aware of any works that have done so.",
"Springer Nature 2021 LATEX template 30 Learning Curves for Decision Making Since all of the above questions are answered based on observational statis- tics, we can also consider noise and uncertainty aspects for the quantitative questions.",
"In the simplest case (blue layer, cf.",
"3.1), we only get point esti- mates, i.e., the estimate of C at one or a set of points.",
"Since these estimates are always afflicted with uncertainty, it is reasonable to ask for quantifica- tions of this uncertainty.",
"One form is to express strict bounds as in the fourth layer, which may be derived from assumptions about the learning curve shape, e.g., convexity (Mohr and van Rijn, 2021, 2023).",
"Another form is to express probabilistic bounds like confidence intervals around C (Figueroa et al, 2012; Koshute et al, 2021) (cf.",
"In the most general form, we can ask for a full belief model of the learning curves, specifying a probability distribution over the values of C at one point or a set thereof (Domhan et al, 2015; Klein et al, 2017a,b; Swersky et al, 2014) (cf.",
It is common practice to solve relatively simple questions by implicitly answering more complex ones.,
"For example, a typical question in the context of model selection is whether the performance of a candidate learner at some given data or in the limit will beat a known baseline (Domhan et al, 2015; Leite and Brazdil, 2005; van Rijn et al, 2015).",
"This is the binary question of pref ≤p∗, where p∗is the best-known performance.",
"Often, this question is answered by estimating pref (maybe plim) explicitly and then comparing it to p∗(Leite and Brazdil, 2005; van Rijn et al, 2015), which is an answer to a slightly more complicated question.",
"Domhan et al (2015); Swersky et al (2014) built an explicit curve model, for estimating plim and pref , respectively.",
The approach answers this binary question by building an entire learning curve model and then derives the binary answer from it.,
"The rationale behind this is the notion that one often needs rather complex models to find a high-quality answer to a simple question, and it is just a side effect that one can then even answer other questions with those models.",
"4.3 Used Data Resources for Inference Above we have discussed a series of questions that can be asked about learning curve properties, which in turn are important for decision making in a spe- cific context.",
"Of course, answering these questions requires specific informative resources.",
"In a concrete decision-making situation, we are typically confronted with a dataset and a learner or a portfolio of learners.",
We call this the target dataset and the current learner.,
"That is, we want to say something about the learning curve of the current learner in a domain in which we have a finite (the target) dataset d available.",
11 shows the types of data resources that can be used to answer ques- tions about learning curves.,
"We can utilise empirical learning curves gathered on the target dataset, empirical learning curves gathered on other datasets, dataset meta-features, and features describing the learners.",
"Learning curves gathered on the target dataset come at a particular computational cost, as they need to be generated during the process.",
"Typically, when modelling the current learner on the target dataset, a partial empirical learning curve is constructed, Springer Nature 2021 LATEX template Learning Curves for Decision Making 31 Data Resources Dataset Meta- Features Learner Features Target Dataset Other Datasets Curves on Current Learner Curves on Other Learners Curves on Current Learner Curves on Other Learners Fig.",
"11: Taxonomy of data resources for learning curve analysis which can then be step-wise extended or discarded (Domhan et al, 2015; Leite and Brazdil, 2003; Provost et al, 1999).",
"When using learning curves of other datasets, those are usually available up to a large portion of the dataset size (with a specific schedule of anchors) in the case of sample-wise curves or until convergence in the case of iteration-wise curves.",
"This is because these curves could be prepared offline before the target dataset became available (Leite and Brazdil, 2003, 2010; van Rijn et al, 2015).",
Both learning curves of the current learner and other learners can be utilised for this.,
"In the context of model selection, various learners are usually evaluated.",
"Therefore we can acquire var- ious learning curves of other learners on the target dataset (Baker et al, 2018; Chandrashekaran and Lane, 2017; Klein et al, 2017a,b; Swersky et al, 2014).",
Other types of data resources that can be used are meta-features on the datasets and learner features.,
"These are measurable qualities of the dataset and learner, respectively, and these can indicate how similar specific datasets (or learners) are.",
These give the decision-making algorithm a sense of which learning curves are more informative for the current learner and tar- get dataset.,
"To the best of our knowledge, the only line of research utilising meta-features for learning curve modelling is the work of Leite and Brazdil (2008, 2010); Ruhkopf et al (2023).",
"The description of learners through fea- tures for the sake of model prediction is specifically prevalent in the analysis of iteration-wise curves (Baker et al, 2018; Klein et al, 2017b; Swersky et al, 2014).",
"The development and analysis of meta-features is a research field in its own right; for more information, we refer the reader to Brazdil et al (2022).",
Springer Nature 2021 LATEX template 32 Learning Curves for Decision Making 5 Literature Review on learning curve extrapolation methods This section presents the literature review that covers methods to model and utilise learning curves.,
We organise approaches based on the key problem they resolve on a rather abstract level and independent of the purpose or type of decision-making situation in which they were presented.,
We organise it along the framework presented in Fig.,
"10, particularly along the axis that reflects the technical question asked about a learning curve.",
"The motivation to not use the type of decision-making situation, which is also a prominent property of these methods, is that the same approach can be used in different decision- making situations.",
"For example, Leite and Brazdil (2004) present and motivate an approach to identify the portion of some given data that should be used for training (i.e., determine the saturation point), but the same approach could be used to determine how much more data would be needed to obtain saturation performance.",
"Similarly, Domhan et al (2015) present an approach that aims to decide during training whether a neural network will become competitive (ask for saturation performance); however, they did this by modelling the complete learning curves.",
12 gives an overview of all the methods categorised in this framework.,
We make the following two observations.,
• Most learning curve methods address the early discarding / model selection decision-making situation.,
"This implies that there is an opportunity for more research on, for example, data acquisition or early stopping.",
"We note that research towards active learning provides many approaches that handle data acquisition (which we do not cover), which might serve as a basis for a literature search.",
"• Second, most methods are centred around the middle levels of problem com- plexity they address, i.e., predicting the actual value of a learner at a certain point and predicting the complete curves of a learner.",
"It seems logical that there are benefits for exploiting the situation where either the complete port- folio is modelled (e.g., the benefit of parameter sharing, the opportunity of model acquisition) or the binary problem is solved (because of the simplicity of the problem definition).",
"Methods addressing the binary situation, such as Successive Halving and Hyperband, have attracted quite some attention.",
"To further structure the overview, we divide the whole literature on methods for learning curves into two roughly even groups according to the usage of a learning curve model as described in Sec.",
Approaches in the first group do not employ a learning curve model.,
These approaches address the ques- tions defined in the four lowest levels of the framework (see Sec.,
"In contrast, approaches of the second group, i.e., which employ a learning curve model, address questions in the top two levels of the framework (see Sec.",
"Approaches with a curve model are more general, but that does not neces- sarily mean they give better answers to simpler questions.",
"In fact, Kielh¨ofer Springer Nature 2021 LATEX template Learning Curves for Decision Making 33 Data Acquisition Early Stopping Early Discarding / Model Selection Unclear / Mixed Portfolio Swersky et al (2014); Wistuba and Pedapati (2019) Klein et al (2017a,b) Curves of Learner Richter and Khosh- goftaar (2019) Cortes et al (1993); John and Langley (1996); Gu et al (2001); Mukherjee et al (2003); Figueroa et al (2012); Domhan et al (2015); Cardona- Escobar et al (2017); Adriaensen et al (2023); Kielh¨ofer et al (2024); Egele et al (2024) Frey and Fisher (1999); Boonyanunta and Zeephongsekul (2004); Singh (2005); Hess and Wei (2010); Kolachina et al (2012); Koshute et al (2021) Conservative bound on curve Sabharwal et al (2016); Mohr and van Rijn (2023) Value / Quantitative prediction Weiss and Tian (2006, 2008) Meek et al (2002); Bishop (1995); John and Langley (1996); Provost et al (1999); Leite and Brazdil (2003, 2004); Ng and Dash (2006) Chandrashekaran and Lane (2017); Baker et al (2018) Last (2007, 2009); Cortes et al (1994) Ranking / Qualitative prediction Leite and Brazdil (2005, 2007, 2008, 2010); van Rijn et al (2015); Ruhkopf et al (2023) Binary Petrak (2000); Van den Bosch (2004); Jamieson and Talwalkar (2016); Li et al (2017); Zeng and Luo (2017) Fig.",
12: Overview of the methods (indicated by the bibliographic reference) covered and categorised in this framework.,
Each method is categorised along the problem type they solve (vertical axis) and the type of decision-making situation they explicitly address (horizontal axis).,
Some methods are employed to address multiple decision-making situations.,
"Some papers do not explicitly state which decision-making situation is being addressed, or address multiple.",
These are categorised in the last column.,
Several models can be used for more decision-making situations than the original paper evaluated them on.,
"For example, the portfolio approaches could in theory be used for any decision- making situation.",
et al (2024) show that the model-free MDS approach discussed in Sec.,
5.1.2 in specific situations outperforms a parametric model such as the ones presented by Gu et al (2001) discussed in Sec.,
"In total, the approaches cover 10 of the questions discussed in Sec.",
"4.2, which are organized in the following sub-sections: 5.1.",
Approaches without learning curve model: 1.,
"Is the target performance of a learner worse than the one of the best learner, i.e., pref > mina∈A C(a, bref )?",
"What is the ordering πa∈A ∼C(a, bref ) of the learning algorithms w.r.t.",
their performance at some target anchor?,
What is the saturation performance (psat)?,
What is the saturation point (bsat)?,
What is the utility-based stopping point (bu sat)?,
Springer Nature 2021 LATEX template 34 Learning Curves for Decision Making 6.,
What is the value of the learning curve at a specific (fixed and known) point?,
"(C(a, n) or C(a, n, t)) 7.",
What is a lower/upper bound of the learning curve at any point?,
"(C(a, ·) or C(a, n, ·)) 5.2.",
Approaches with a model for the learning curve: 1.,
What is the value of the learning curve at any (queryable) point?,
"(C(a, ·) or C(a, n, ·)) 2.",
What is the utility at an arbitrary anchor point?,
"(U(a, ·) or U(a, n, ·)) 3.",
What is the value of the learning curve at any queryable point of any queryable learner?,
"(C(·, ·)) We organise the rest of this section exactly according to this scheme.",
"For each approach, we always consider the most general problem it solves, independently of how this solution is used in the context of a paper.",
"For example, Domhan et al (2015) decide whether the saturation performance of a learner beats some threshold (question at the binary level) but develop a learning curve model and are hence discussed alongside the approaches for question 1 in Sec.",
5.1 Approaches Without Learning Curve Models Many interesting questions related to learning curves can be addressed without even building an explicit learning curve model.,
None of the questions in the lower layers in Fig.,
10 necessarily requires a learning curve model.,
"13 shows a summary of all the approaches we are aware of, which make significant assertions or decisions related to learning curves without building a learning curve model.",
"5.1.1 Prediction of Candidate Competitiveness (Binary) In this section, we discuss approaches that answer the early stopping criterion posed in Eq.",
9 without using a learning curve model.,
"The early discarding criterion can be seen as an instantiation of the binary question pref ≤τ, where τ = mina∗∈A C(a∗, bref ) is the best value that any learner on the available resources.",
This (conceptually simple) question is often answered by applying sophisticated learning curve models.,
"The usage of learning curve models is understandable since, at the time of the decision, neither pref nor mina∗∈A C(a∗, bref ) is known; therefore, extrapolating learning curves offers a possibility to make assessments about these quantities.",
"However, it is also pos- sible to say something about the early discarding condition without a learning curve model.",
"As far as we know, the only type of approach in this category is hori- zontal early discarding with an implicit affirmation of the early discarding criterion that only uses the last anchor of the empirical learning curve.",
"That is, during the model selection process, all remaining members of a candi- date set A are trained to some budget.",
"This budget can be either a sample size (sample-wise curves), learning iterations, or time (iteration-wise curves).",
"Springer Nature 2021 LATEX template Learning Curves for Decision Making 35 Cortes et al (1994) Bishop (1995) John and Langley (1996) Provost et al (1999) Petrak (2000) Meek et al (2002) Leite and Brazdil (2003) Leite and Brazdil (2004) Van den Bosch (2004) Leite and Brazdil (2005) Ng and Dash (2006) Weiss and Tian (2006) Leite and Brazdil (2007) Leite and Brazdil (2008) Weiss and Tian (2008) Leite and Brazdil (2010) van Rijn et al (2015) Jamieson and Talwalkar (2016) Sabharwal et al (2016) Li et al (2017) Zeng and Luo (2017) Chandrashekaran and Lane (2017) Baker et al (2018) Ruhkopf et al (2023) Mohr and van Rijn (2023) 1994 1995 1996 1999 2000 2002 2003 2004 2005 2006 2007 2008 2010 2015 2016 2017 2018 2022 2023 Approaches Without Explicit Learning Curve Model ssat su sat pref > τ πa∈A ∼C(a, sref) C(a, ·) C(a, sref) plim Fig.",
13: Citations across approaches without learning curve models.,
The colours indicate the question addressed about learning curves.,
"Normal arrows indicate that the paper cited the other paper, and coloured arrows indicate an experimental comparison with previous approaches.",
"Then, some portion of candidates A−is removed from A based on the last value of their respective curve.",
"The implicit assumption is that C(a, bref ) > mina∗∈A C(a∗, bref ) for every a ∈A−.",
"To our knowledge, the first algorithm in this line was introduced in the Wrapped Progressive Sampling procedure (WPS) by Van den Bosch (2004).",
"In this approach, a dynamically computed subset of the candidate set A is discarded (instead of a constant fraction as 50%).",
"WPS creates a histogram with ten bins b1, .., b10 of the candidates in A based on their validation accuracy to decide which candidates are discarded.",
"Then WPS identifies the largest index i−= max{i ∈N, < 10 : |bi| < |bi+1|} of a bin that has fewer elements than its successor.",
"Then, all candidates in bins with an index lower or equal i− are dropped.",
"In an extreme case, all except one candidate might be dropped right in the first iteration, e.g., if |b10| = 1 and |b9| = 0.",
"More recently, successive halving (Jamieson and Talwalkar, 2016) and hyperband (Li et al, 2017) have been introduced, which are both adequate methods based on a simple concept.",
"Successive halving considers a set of can- didate models A, which all receive an initial budget.",
It iteratively drops 50% of the candidate set A while doubling the remaining candidates’ budget until a single candidate model remains.,
"Hyperband is a series of successive halving brackets, where each bracket is initialised with an increased initial budget and a new initial set of candidates.",
Zeng and Luo (2017) proposed an extended version in which all the candidates that perform worse than the best candi- date by some constant and pre-defined margin are discarded.,
"Interestingly, the above approaches operate without a learning curve model for extrapolation and refrain from using the existing observations other than comparing the last seen values.",
"Therefore, neither predictions nor observations of recent trends are being utilised.",
"Nonetheless, these methods perform well empirically and come with theoretical guarantees.",
Springer Nature 2021 LATEX template 36 Learning Curves for Decision Making 5.1.2 Candidate Ranking The methods in this section aim to rank the learning algorithms with respect to their (expected) performance at the full dataset size.,
In our scheme in Sec.,
4.2 and Fig.,
"10, we denote this question as πa∈A ∼C(a, bref ).",
Depending on the available resources (cf.,
"4.3), such a ranking can be based on the other learning curves available from other contexts and the explicit characterisation of such contexts, e.g., features that describe datasets or algorithms.",
"Ranking Without Context Description The first approach we are aware of to address this problem was metalearning on data samples (MDS) presented by Leite and Brazdil (2005) through the notion of comparing two learners, i.e., a ranking of two.",
"Given a dataset, MDS decides which of two algorithms is the better choice on a given dataset.",
"Therefore, it can be used for early discarding.",
The authors specifically use an SVM and a C5 decision tree but rightfully claim that any algorithm could be used.,
"The formal basis of the work is the same as the one introduced in their previous work (Leite and Brazdil, 2003, 2004).",
"Similar to previous work, it assumes that empirical learning curves (with standardised anchor sizes) for the learner under examination are already known for other datasets.",
"Additionally, it builds upon the idea of quantifying the distance between the target and the other datasets based on the sum of squared distances over the already- known performances at anchors of the target dataset.",
"Once the most similar k learning curves have been identified, MDS assigns a score to each learner that is the mean accuracy of its k nearest neighbours (at the final anchor).",
It then selects the algorithm with the higher score.,
Leite and Brazdil (2005) acknowledge that this method may result in poor rankings because even the closest learning curve on other datasets can still be substantially different and propose learning curve adaption as a remedy.,
"Instead of forming the mean directly over the target anchors of the nearest neighbour empirical learning curves, the authors first scale those curves to make them more similar to the shape already observed on the target dataset.",
"To this end, they compute a scaling constant under which the overall anchor- wise distance is minimised and then multiply all the scores with this constant.",
This version of the MDS algorithm is called AMDS (probably for Adaptive MDS).,
"Interestingly, one can argue that this adaption technique could be applied either before or after determining the k nearest neighbours.",
"Doing it before could lead to other (and better) nearest neighbours because then the neigh- bours are determined more with respect to the shape of the learning curve, and the offset plays much less of a role.",
"However, in the above paper, the adaption is done after retrieving the neighbours.",
"The authors extended the approach by creating an online sampling scheme with the SetGen algorithm (Leite and Brazdil, 2007).",
"SetGen is an online adaption of AMDS in that, after each acquired anchor, it is decided whether and which anchor should be evaluated with each algorithm; this considers both Springer Nature 2021 LATEX template Learning Curves for Decision Making 37 the (believed) accuracy and the runtime.",
This procedure can be seen as a way of racing between the algorithms.,
The potential of each additional amount of budget is judged based on the metalearning database.,
An implicit assumption of all approaches in this line of research is that the datasets in the database from which performances are extracted need to be at least as big as the target dataset.,
"This issue was first explicitly treated in the Pairwise Curve Comparison approach (PCC) (van Rijn et al, 2015).",
"This algorithm builds upon the works of Leite and Brazdil (2010) and implements a voting scheme to identify the best learning algorithm of a portfolio; votes are distributed based on wins, which are determined based on the predicted performance at the complete dataset d. van Rijn et al (2015) explicitly discuss the issue if the sizes of datasets in the database and the target dataset are not identical.",
"In general, the point for which predictions must be made is typically not one of the anchors; it is typically not a power of 2 but rather 90% of the given dataset size (due to the holdout scheme).",
A remedy is to resort to the closest available anchor in the schedule.,
"However, if the highest anchor available for another dataset is much smaller than the required training size of the target dataset, then it is unclear how that curve should be used.",
"Ranking With Context Description The first work we are aware of that realises an explicit context description was proposed by Leite and Brazdil (2008, 2010).",
"Similar to the methods discussed earlier, these methods select the k nearest datasets to measure the relevance of known complete learning curves of other datasets for performance predic- tion on the target dataset.",
"The main difference is that, in addition to the contribution of the partial learning curve itself to the distance, they also use the distance between the datasets in terms of their meta-features.",
"More pre- cisely, they compute the Manhattan distance between seven range-normalised dataset meta-features, e.g., dataset size, number of symbolic and numerical attributes, etc.",
The overall distance between the datasets is then the sum of the distance between the partial learning curves and the distance in terms of meta-features.,
"This work was marginally refined in the Selection of Algorithms using Metalearning approach (SAM), which applies the same logic but assigns a weight to each of the two distance sources (Leite and Brazdil, 2010); the weight is however implicitly assumed to be set to 0.5.",
"A recent and entirely different approach to candidate ranking is the MASIF transformer framework (Ruhkopf et al, 2023).",
"This approach takes partial learning curves of different learners on the current task, which may have poten- tially different lengths and combines them with dataset meta-features in order to predict latent utility values of each learner as expected for the complete dataset.",
The transformer is trained based on previous experiences on datasets for which true rankings among the learners have been computed for the com- plete dataset.,
"It is unclear to which degree the utility values predicted by the transformer resemble the actual performance of the learners at the target Springer Nature 2021 LATEX template 38 Learning Curves for Decision Making size, but the paper suggests that the ordering πa∈A ∼C(a, bref ) of the learn- ers according to these scores is relatively faithful to the true ordering induced by the actual performances.",
"5.1.3 Identification of Saturation Performance Estimating the limit or saturation performance psat is helpful in two decision- making situations: 1. for sample-wise curves, it can be used for data acquisition by checking whether the availability of more data promises to improve predictive performance significantly.",
"2. for iteration-wise curves, it can be used for early discarding.",
"Given a threshold p, we can determine whether training a model to convergence will achieve a generalisation performance of at least p. To the best of our knowledge, the question on estimates of psat is the only of the above questions that received a substantial amount of theoretical con- tributions.",
This is not surprising since psat is an asymptotic quantity that is arguably suited for theoretical analysis.,
"The root of this line of research is the statistical mechanics framework (Seung et al, 1992).",
"This and related research (Amari and Murata, 1993; Fine and Mukherjee, 1999; Murata et al, 1992) consider a type of capacity curve in which asymptotic properties of the learning curve are expressed in terms of the number of parameters, usually those of a neural network.",
"However, a side observation of these works is that there is a kind of symmetrical behaviour between the train error and the validation error (sometimes called generalisation error).",
"Cortes et al (1994) take these observations to use the mean of the two empirical curves to estimate psat as soon as the training error starts to rise, i.e., as soon as the model cannot accommodate the training data perfectly anymore.",
"Although this work considers capacity curves to identify the intrinsic noise level of the data, i.e., the minimum error necessarily made by any learner, they also report the asymptotic performance of a neural network on a single data set.",
"To the best of our knowledge, this is the only approach that estimates psat without building an explicit learning curve model.",
"While several other methods are capable of estimating psat of iteration-wise curves of neural net- works (Domhan et al, 2015; Swersky et al, 2014), these rely on full learning curve models, which are therefore discussed in Sec.",
5.1.4 Identification of Saturation Point Identifying the saturation point bsat of sample-wise curves is useful in the following cases: 1.,
Early-Stopping with sample-wise curves: Which portion of the available data is necessary to obtain saturation performance?,
This is relevant if |d| > bsat or the relationship between |d| and bsat is unknown and training on full d is potentially undesirable.,
Springer Nature 2021 LATEX template Learning Curves for Decision Making 39 2.,
Early-Stopping with iteration-wise curves: How many iterations are nec- essary until performance converges?,
"This applies to incremental learners, such as neural networks.",
Data acquisition: How many additional labelled observations are neces- sary to obtain (near-optimal) performance?,
This applies if |d| < bsat.,
The question can be posed for a specific learner or a portfolio.,
Retrospective Approaches The simplest way of determining the saturation point bsat is to incrementally build a learning curve and stop as soon as it is believed that the saturation point has been exceeded.,
"If we do this, we can estimate that the saturation point lies between the last two anchors.",
"For iteration-wise curves, determining bsat comes for free as a side-product of the training procedure.",
"It is com- monly used for training neural networks (Bishop, 1995; Goodfellow et al, 2016).",
"On the other hand, it requires restarting and is potentially costly for sample-wise curves.",
John and Langley (1996) define a dynamic sampling approach to determine the bsat for sample-wise curves.,
A straightforward approach mentioned in that paper is to observe whether the performance has become worse on the last sampled anchor.,
"If so, one might consider gathering empirical evidence that the saturation point has been exceeded, i.e., it should be somewhere between the last two anchors.",
"However, the authors argue that preliminary results indicate that this approach often stops too early.",
"This is mainly caused by high aleatoric uncertainty, which implies noisy empirical learning curves.",
"On the other hand, one could argue that this approach stops far too late because it can require quite some iterations until, by chance, the observed performance is worse than the one of the last iteration.",
"Therefore, John and Langley (1996) propose also a model-based approach to avoid this problem, which we discuss in Sec.",
"Of course, when having access to such a model, we can query the expected performance and compare it to the performance at the last anchor.",
Provost et al (1999) address the stability issues and also (some of) the effi- ciency issues of the above trivial approach in a scheme they call progressive sampling.,
"Similar to dynamic sampling (John and Langley, 1996), progressive sampling induces models for each anchor in the schedule until the convergence of the learning curve is detected.",
There are two main differences between the two approaches.,
"First, the authors propose to use geometrical instead of arith- metic schedules, i.e., a schedule of the form bk instead of bk, where b is a constant and k is the position of an anchor in the schedule.",
"They prove that every geometric schedule is asymptotically optimal in terms of runtime; that is, every such schedule has the same asymptotic runtime as the schedule that eval- uates only on bsat.",
"This optimality proof only holds in the asymptotic calculus; in practice, there are better and less good geometric schedules.",
"For example, Provost et al (1999) propose a dynamic programming approach (called DP), which efficiently computes the cost-optimal schedule based on a prior distribu- tion on bsat and a given training runtime model.",
"The second difference is that Springer Nature 2021 LATEX template 40 Learning Curves for Decision Making Provost et al (1999) check whether the saturation point has been reached using a method called linear regression with local sampling (LRLS), which samples not only at but also closely around anchors to estimate the slope at an anchor and stop if the slope is close to zero.",
The practical benefit of the LRLS scheme is not entirely clear for sample-wise curves.,
"First, since the slope is also based on empirical values, from a theoretical viewpoint, it is not clear that the criterion is necessarily bet- ter than the naive approach suggested by John and Langley (1996).",
"Second, the approach is more expensive than the naive approach because more obser- vations need to be sampled; this can be a substantial factor, especially for large anchors.",
"LRLS becomes relevant when applied to learners that have a compu- tational complexity for training that scales worse than linear in the number of training points (e.g., Gaussian processes or decision trees).",
"For learners with training complexity linear in the number of observations, it will often be more expensive than evaluating a learner’s performance on the complete dataset.",
"This can be seen with a simple calculation, in which we assume roughly linear training time complexity: Suppose a costly anchor at 40% of the overall data size.",
"If we draw only two additional samples around this anchor, then the run- time is around 120% of the runtime we would have had if we had trained on the complete dataset once.",
"While Provost et al (1999) argue that LRLS only adds a constant factor to the runtime, Sarkar et al (2015) reasonably argue that this factor is often prohibitive in practice.",
The approaches in this section explicitly assume that more data than bsat is available.,
This implies that they can be used primarily for early stopping scenarios rather than data acquisition scenarios.,
"Still, if a learner does not attain saturation performance on the complete dataset, the approaches can detect this at the cost of the additional evaluations at the non-final anchors.",
"Concerning the stability of estimates, Beleites et al (2013) point out the necessity to have an estimate for the confidence interval not only for the per- formance at the anchors in the training schedule but also on the test data.",
They argue that confidence intervals are essential when deciding whether rea- sonable generalisation statements can be made for a classifier.,
"This changes the notion of the stopping point to, perhaps, a confident stopping point.",
"The optimal stopping point may be reached early, but the validation fold sizes may still be too small to assure stable assertions.",
"Typically, the confidence inter- vals are large on small anchors and then contract for increasing anchor sizes.",
"Based on credible intervals, the authors propose choosing the anchor point that achieves a sufficiently narrow interval on the test data.",
Ng and Dash (2006) address the impact of class imbalance on the per- formance of a learner.,
"That approach hypothesises that, without further knowledge, the class distribution in which all classes have the same number of observations is optimal.",
The authors modified the aforementioned progressive sampling scheme by creating train sets at each anchor such that all classes have the same distribution.,
"For anchors of sizes that would require more instances of a class than are available in the existing data, random instances of that class Springer Nature 2021 LATEX template Learning Curves for Decision Making 41 are replicated until the class balance is established again.",
Note that while the work is based on the findings by Weiss and Provost (2003) (cf.,
"2.5.2), they apply a different strategy.",
"Instead of optimising over the class distribution for a given anchor size, they try to find the stopping point under the premise that the training set will always be balanced.",
"Regarding the stopping point of iteration-wise curves, a common technique is to separate some data that is not used for training but to compute an iteration-wise curve online to detect convergence (Bishop, 1995).",
"Projective Approaches A different idea to obtain the saturation point bsat was proposed by Leite and Brazdil (2003, 2004) through the notion of metalearning (Brazdil et al, 2022).",
"Similar to Provost et al (1999), a geometric schedule is used.",
The assumption is that we already know the performances of the current learner at all anchors in the schedule on other datasets.,
"The idea is to compute, on the target dataset, the performances only for the very first anchors and then to predict the satura- tion point by aggregating the (known) saturation point on the k most similar learning curves of the other datasets.",
"The distance measure here is the sum of differences between the curves at the initial anchors; the concrete anchors used in their paper are (91, 128, 181, 256, .",
"), corresponding to the powers of √ 2.",
"The authors consider different aggregation measures such as mean and minimum (Leite and Brazdil, 2003) and the median (Leite and Brazdil, 2004).",
"The authors discuss the potential issue that, among the k nearest neigh- bour curves, some or even all of the curves can be substantially different from the partial learning curve on the target dataset.",
Using the k nearest learning curves to predict the stopping point would not work in such cases.,
"Follow-up work (Leite and Brazdil, 2007) proposes a remedy to this problem, in which the curves are not used directly but are adjusted via a concept called curve adaptation (discussed in Sec.",
"5.1.5 Finding the Utility-Based Stopping Point The problem of identifying the utility-based stopping point was, to our knowledge, first addressed by Meek et al (2002) and was also independently investigated by Weiss and Tian (2006).",
"In these papers, a retrospective approach is applied.",
"The idea is similar to the aforementioned concept of pro- gressive sampling (Provost et al, 1999), except that the analysis is done for utility rather than learning curves.",
"In contrast to the learning curve, the utility curve does not plateau but starts to deteriorate after its peak (see Fig.",
The main difficulty with the concept of utility in the context of learning curves is to find a unifying scale for (i) the costs of data acquisition and training time and (ii) the model performance.,
Meek et al (2002) avoid this problem by adopting the notion of implicit utility through the comparison with a baseline.,
They stop the algorithm when the ratio between the benefit improvement and the augmented runtime drops below a pre-defined threshold.,
"Springer Nature 2021 LATEX template 42 Learning Curves for Decision Making In contrast, Weiss and Tian (2006) compute an explicit utility, and the algo- rithm stops as soon as the observed utility decreases for the first time, which is taken as the indicator that the utility-based stopping point has been passed.",
"The authors adopt the concept of the net utility of a potential classifier, which is the difference (in utility) between predictive performance (under a hypo- thetical number of training instances) and the cost to acquire the (additional) instances.",
"Notably, the assumption is that the user has no control over the instances for which labels will be acquired next, which contrasts the approach from active learning.",
"To merge different types of inconveniences (i.e., acquisi- tion costs and prediction errors) into a single utility measure, the user has to define costs per unit, e.g., costs for acquiring a single usable training instance and costs for making a wrong prediction.",
"Furthermore, the authors consider the problem of deciding online whether or not to acquire more data and, in the affirmative case, how many instances should be considered in the acquisi- tion batch before reconsidering.",
"The latter effectively corresponds to deciding upon a progressive sampling scheme (Provost et al, 1999).",
"However, the paper does not analyse the effects of fixed costs per batch (such as the computational costs of training a model), which implies that one could set batch sizes to 1 without consequence.",
"A consecutive version of that paper also adds the CPU cost for model induction to the costs of a point on the learning curve (Weiss and Tian, 2008).",
The original paper only considered acquisition costs and the prediction error.,
"This model also seems suitable for iteration-wise curves, where there would be no data acquisition costs.",
"In all of the above approaches, the usage of the empirically gathered learn- ing curves for decision making is minimal.",
"Moreover, the approaches ignore all except the last two points on the learning curve.",
"In this sense, and in terms of the stopping point approaches, the above works are retrospective in nature.",
"No model of the learning curve is built, and no projections of errors on bigger training sizes are made, which, for example, could make sense to predict that this utility peak event will occur in the future or even only in the next iteration.",
"Last (2007) proposes such an approach, which we discuss in Sec.",
"5.1.6 Performance Prediction at Fixed Point The problem of predicting the learning curve value is naturally a regression problem, where the goal is to predict µa,b for a fixed budget b.",
"Essentially, given a fixed learner a and the budget expressed in either observations n or iterations t, it is about predicting C(a, n) for sample-wise curves, and pre- dicting C(a, n, t) for sample-wise curves.",
The attributes are the performance values at different (cheap) anchors and potentially additional contextualising attributes.,
"Using these attributes, one explicitly or implicitly generalises over datasets or learning algorithms (or both).",
"Models that generalise over datasets are typically called meta-models and rather aim at model ranking (Brazdil et al, 2022; Ruhkopf et al, 2023), which we discuss in Sec.",
This is because Springer Nature 2021 LATEX template Learning Curves for Decision Making 43 it is difficult to generalise exact performance across datasets.,
"When general- ising over learners, one typically trains a single model for the target dataset, trained from learning curves on the same dataset belonging to other learning algorithms.",
"Models that generalise over learners are typically called surrogate models (Eggensperger et al, 2018).",
We organise this section by how explicit the generalisation is made over one of the two concepts.,
"If no additional attributes are available, one implic- itly assumes that all entries in the database are to a degree suitable to predict values in a new situation.",
"There is no explicit context in this case, and we are generalising implicitly over datasets or learners.",
"Since no explicit contextual- isation exists, those approaches can be used for both purposes, regardless of their original purpose.",
"In contrast, additional contextualising attributes can describe the dataset, i.e., we have meta-features of the datasets available, or they can describe the learning algorithms to which the learning curve values belong.",
"In principle, one could utilise both types of additional attributes, but we are not aware of any approaches that adopt both.",
"Generalization Without Explicit Context Generalisation from learning curves without explicit context means to pre- dict the performance of a given learner on some dataset based on previously acquired learning curves that are not equipped with additional information, i.e., features describing the dataset or the algorithm used to produce them.",
"To justify the prediction model, the existing empirical learning curves either stem from the same algorithm on other datasets, or other algorithms on the same dataset.",
Either of these implicitly qualifies them to be relevant for the new task.,
"In other words, one simply uses a set of unannotated existing learn- ing curves as aids to predict the behaviour of a new, only partially known, learning curve.",
Chandrashekaran and Lane (2017) developed an approach that explicitly used regression to predict the target performance without context.,
"Probably without noticing, the approach mainly re-invents the approaches previously developed by Leite and Brazdil (2005, 2007, 2010); van Rijn et al (2015), since it computes the most similar other learning curves in the portfolio and obtains a prediction based on the average over those curves.",
"The three dif- ferences are that Chandrashekaran and Lane (2017) (i) consider uncertainty in the prediction based on the variance in the neighbourhood, (ii) adopt an affine transformation (instead of a linear transformation) of the existing learn- ing curves and apply this before selecting the most similar ones, and (iii) that they do not use a fixed schedule but, due to the focus on iteration-wise curves, simply a continuous schedule that is stopped as soon as there is enough evi- dence that the target performance will not be better than a current threshold.",
The Euclidean norm between the vectors describing the performances at the anchors is used as the distance function between two curves.,
The reason why the approach is discussed here and not in Sec.,
5.1.2 together with the others is subtle and worth being discussed.,
"In both lines of research, Springer Nature 2021 LATEX template 44 Learning Curves for Decision Making target performance values from related curves are averaged to estimate the per- formance of the current learner.",
"However, Chandrashekaran and Lane (2017) explicitly treat this as a performance prediction, which is then used for early discarding by comparing against a threshold.",
"In contrast, the works in the line of Leite and Brazdil (2005) do not treat this value as an actual prediction but simply as some score used to order a pair or a set of learners.",
"While the approach generalises across algorithm configurations, it ignores the configurations and generalises from learning curves without explicit con- text.",
"In other words, there is no reason why the approach could not be used also for generalisation across datasets.",
Cardona-Escobar et al (2017) presented an approach that predicts values at all anchors.,
"The authors adopt a series of support vector regression models, one for each anchor not evaluated so far.",
"It is not entirely clear with which data the models are trained, but we presume that it follows the same logic as Chandrashekaran and Lane (2017) and uses the fully known learning curves of previously evaluated neural network configurations to do so.",
"Interestingly, similar to chaining (Gkioxari et al, 2016) in classification, they use as inputs for the j-th future anchor not only the known partial learning curve values but also use the predictions for the anchor points predicted before j. Generalization With an Explicit Algorithm Context Generalisation across algorithms only considers the target dataset and assumes that a number of (complete) empirical learning curves on that dataset are already available for different algorithms.",
The explicit generalisation requires that the previous learning curves are explicitly associated with features describing the algorithm to which they belong.,
Baker et al (2018) propose a method that uses features describing both the learning curve (including up to second-order differences) and the algorithm.,
"The approach predicts the performance of neural networks based on features that describe the architecture (number of layers and weights) as well as the hyperparameters of the learning algorithm (such as learning rate, learning rate decay, etc.).",
"They adopt linear and kernel-based support vector regression machines, random forests, and simple linear regression based on ordinary least squares.",
"Even though the authors suggest using kernel-based support vector regression machines, they find that simple linear regression also often compares highly competitive for this prediction task.",
"Following this idea, Long et al (2020) additionally add textual descriptions of the architecture to predict the learning curves of neural networks.",
"Indeed, the architecture description by Baker et al (2018) is rather simplistic and only immediately well suited if all layers are of the same type, e.g., dense layers, and have the same number of neurons.",
Long et al (2020) report substantial performance improvements for convolutional network architectures compared to the approach taken by Baker et al (2018).,
Springer Nature 2021 LATEX template Learning Curves for Decision Making 45 5.1.7 Performance Bounding Performance bounding tries to give explicit lower or upper bounds on the per- formance value at some specific or arbitrary budget.,
Answering this question is essential to make high-confidence decisions on early discarding as discussed in Sec.,
"4.1.3; formally, we denote it as C(a, ·).",
"Typically, one is interested in C(a, bref ), bref being the size of the dataset intended for training or the limit performance of an iteration-wise curve, but the models we discuss here are more general.",
Performance bounding is intuitively a simpler problem than performance prediction (Sec.,
"5.1.6), but there are usually also higher expectations with respect to the accuracy of the assertion.",
"When a performance bound is expressed, one would expect that the true value is at least as high as specified with high probability.",
"One approach that addresses this problem is Data Allocation using Upper Bounds (DAUB) (Sabharwal et al, 2016).",
"Given a set of configurations, it first runs all configurations on two anchors of the dataset, effectively building the initial segment of the learning curve.",
"Based on this initial segment per configuration, it determines an optimistic performance bound for each learning curve that likely will not exceeded (i.e., an upper bound for measures that need to be maximised, such as accuracy, and a lower bound for measures that need to be minimised, such as error rate).",
This performance bound is determined by calculating the linear regression slope of the last two segments.,
The performance on the last anchor is extrapolated to the full size of the dataset according to this slope.,
"Therefore, there is an optimistic upper bound on the performance that a configuration can obtain.",
"After that, it goes into the following loop: It runs the most promising configuration on a larger sample size and updates the performance bound.",
It reevaluates which configuration has the most potential at that budget and assigns more budget to the most promising configuration until one configuration has been run on the entire dataset.,
"Therefore, this is an example of horizontal model selection.",
"Alternatively, learning curve cross-validation uses a similar approach but addresses this in a more flexible, vertical setting (Mohr and van Rijn, 2021, 2023).",
The method explicitly assumes that sample-wise curves have a convex behaviour.,
The convexity of the curve allows for deriving a best-case extrap- olation from a partial empirical learning curve.,
The convexity assumption is used to linearly extrapolate the empirical learning curve and prune learners when they can no longer improve on the best-known solution.,
"5.2 Approaches With Learning Curve Models This section covers learning curve approaches that utilise an explicit model to model the entire learning curve, as described in Sec.",
14 shows an overview of all the approaches we discuss and how they cite each other.,
"In the base form, there is a model for a specific learner a that is able to predict the performance C(a, ·) or C(a, n, ·) at any sample size or iteration respectively Springer Nature 2021 LATEX template 46 Learning Curves for Decision Making Cortes et al (1993) John and Langley (1996) Frey and Fisher (1999) Gu et al (2001) Mukherjee et al (2003) Boonyanunta and Zeephongsekul (2004) Singh (2005) Last (2007) Last (2009) Hess and Wei (2010) Kolachina et al (2012) Figueroa et al (2012) Swersky et al (2014) Domhan et al (2015) Klein et al (2017a) Klein et al (2017b) Cardona-Escobar et al (2017) Wistuba and Pedapati (2019) Richter and Khoshgoftaar (2019) Koshute et al (2021) Adriaensen et al (2023) Egele et al (2024) Kielh¨ofer et al (2024) 1993 1996 1999 2001 2003 2004 2005 2007 2009 2010 2012 2014 2015 2017 2019 2021 2023 2024 Approaches With Explicit Learning Curve Model C(a, ·) C(·, ·) u(C(a, ·)) Fig.",
14: Citations across approaches with a learning curve model.,
The colours indicate the question addressed about learning curves.,
"Normal arrows indicate that the paper cited the other paper, and coloured arrows indicate an experi- mental comparison with previous approaches.",
"(yellow), discussed in Sec.",
"On top of such curves, a utility curve U can be defined (green), discussed in Sec.",
"Finally, the curve models can even generalise across the learners, leading to generalised curve models C(·, ·) (beige).",
"One additional benefit of these types of models is that they can also perform model acquisition, i.e., assess the performance of a learner for which we have no performance evaluations yet.",
These models are discussed in Sec.,
"In this section, we describe the learning curve model according to the notion of budget, using the variable b to avoid having to distinguish between sample sizes n or iterations t. In the spirit of Sec.",
"3, we use the notation f (a, b) to refer to the original random variable that generates observations of the performance of learner a at budget b (independently of whether f here is a sample-wise curve with b being the sample size or whether f is an iteration-wise curve with some implicit sample size n and b being the itera- tion).",
"Accordingly, µa,b = E[f (a, b)] is the mean value of the curve of learner a at budget b.",
5.2.1 Performance Prediction at Any Point We will discuss approaches that build a full learning curve model ˆfa(b) for a specific learner a.,
"That is, they model the curve mean µa,b for a fixed learner a at any budget b (effectively modelling µa,·).",
"We divide the approaches into three groups, corresponding to the three model types for ˆf discussed in Sec.",
"3, which are also reflected in the three layers of Fig.",
"Accordingly, we first discuss approaches that provide point estimates of the curve, i.e., ˆfa : N →R, then approaches that explicitly treat those estimates with uncer- tainty and introduce a notion of bounds on them, i.e., ˆfa : N →R2, and finally approaches that create entire probabilistic belief models over curves, i.e., ˆfa : N →{p | p is a distribution of a real-valued random variable}.",
"Springer Nature 2021 LATEX template Learning Curves for Decision Making 47 Point Estimates To our knowledge, the first approach that used observed data to fit a learn- ing curve model was presented by Cortes et al (1993).",
"In that paper, the three-parametric inverse power law shown in Eq.",
(6) was used to build an iteration-wise curve.,
"The usage of the power law model is justified with the findings in the statistical mechanic framework (Seung et al, 1992) and used to predict the predictive performance on the complete dataset (for two neural network architectures on the NIST dataset).",
The authors find that the predic- tive performance on 60k instances can be almost perfectly predicted using the inverse power law.,
"Unfortunately, it is not entirely clear on which anchors the estimates are built.",
"Notably, this is the only paper we are aware of that fits both a train- and test-error curve.",
The paper reports some information about noise through boxplots on the curve but does not explicitly incorporate them into the model.,
John and Langley (1996) proposed a similar approach.,
"Similar to the work of Cortes et al (1993), the model is used to estimate the performance of a learner on the complete dataset with the goal of stopping the training proce- dure early.",
One difference is that the model is adopted for a sample-wise curve instead of an iteration-wise curve.,
"Concerning convergence detection, John and Langley (1996) employ the Probably Close Enough criterion, which detects convergence if the probability that the accuracy of a model trained on the complete dataset will be at most some ε worse than the current model’s per- formance is less than some δ.",
"However, the paper itself then does not adopt a notion of probabilities but stops if the performance on the complete dataset predicted by an inverse power law model is not by at least ε better than the currently observed performance; they call the approach Extrapolation of Learning Curves.",
"In other words, the uncertainty is not quantified.",
The inverse power law has been used in many applications.,
"For example, several approaches have used the inverse power law to model the performance of neural networks in different domains (Alwosheel et al, 2018; Cho et al, 2015).",
"It is noteworthy that recent works show evidence against the usage of any commonly used models for neural networks, at least in the initial parts of the curve, due to the (sample-wise) double descent (Nakkiran et al, 2020).",
"At least for certain combinations of architectures, datasets, and training procedures, there is empirical evidence that the learning curve exhibits non-monotonic behaviour, which contradicts all existing learning curve models like the inverse power law model.",
Range Estimates Mukherjee et al (2003) built inverse power law models in the domain of DNA data.,
The main contribution of that paper is to analyse the appropriateness of the inverse power model on eight medical datasets.,
"To this end, they construct uncertainty bounds around the mean learning curve consisting of the q25 and q75 curves fitted from those statistics, respectively.",
"This way, a learning curve model including information about dispersion is obtained.",
Experiments are Springer Nature 2021 LATEX template 48 Learning Curves for Decision Making conducted for a support vector machine on eight medical datasets in which the leave-one-out validation result (estimate of the learning curve on |d| −1 data points for training) is compared to the boundaries suggested by the model.,
"Having an explicit model for the q25 and q75 curves, one can obtain for an arbitrary anchor b not only a point estimate of µa,b but an estimate of the inter-quartile range of f (a, b) itself, which is arguably more informative given that, assuming Gaussian noise, the interval should also contain µa,b.",
Figueroa et al (2012) modify the aforementioned approach in two ways.,
"First, different anchors are associated with different weights, usually to assign higher weights to larger anchors since they are more informative.",
"Second, implicitly assuming a standard Gaussian distribution of the observations as in Eq.",
"(5), they compute a 95% confidence interval to describe the uncertainty rather than the interquartile range.",
"Based on this information, for a query point b, they predict a confidence interval instead of a point estimate.",
"Figueroa et al (2012) also applied this approach to medical data, just as Mao et al (2016) for EEG data.",
"More recently, it was also successfully used for sensor commu- nication (Oyedare and Park, 2019).",
"This work aims to predict a reasonable sample size, which is perhaps more reasonably addressed by the utility-based approaches discussed in Sec.",
"Recently, Koshute et al (2021) have used the inverse power law to predict the minimum anchor point on which a learner must be trained to reach near- psat performance with a given desired confidence.",
This approach can be seen as a combination of the above two approaches.,
"Similar to Figueroa et al (2012), they compute the confidence interval at all anchors.",
"However, instead of using these to estimate confidence intervals at arbitrary points, they fit a single curve on the lower bounds of the confidence intervals at the known anchors.",
"The resulting model is not used to make predictions on arbitrary anchors but to compute the cheapest anchor that will obtain with a pre-defined probability (size of confidence-interval) a performance that is ε-close to psat, where ε is a hyperparameter controlled by the user.",
"The idea of computing confidence intervals is also adopted by learning curve cross-validation (Mohr and van Rijn, 2021, 2023).",
A Morgan-Mercer-Flodin model is created to decide whether or not to skip intermediate anchors and evaluate the learner on the full dataset size.,
"However, the confidence intervals are used differently than in the above cases and are not used for the inverse power law model itself.",
"In contrast, the confidence bounds are used to compute the range of possible slopes of the learning curve between two anchors.",
Distribution Estimates The first approach to predict distribution estimates for any anchor point was presented by Domhan et al (2015).,
"The approach assumes learning curves to be instances of a parametric model that is a linear combination of known model classes, such as the inverse power law, and others (Gu et al, 2001).",
"The main difference to the above approaches is that, instead of estimating the parameters through a maximum likelihood approach, they estimate, for each Springer Nature 2021 LATEX template Learning Curves for Decision Making 49 parameter, the whole posterior distribution adopting Monte-Carlo Markov Chains (MCMC).",
"The approach is successfully used to early discard neural network architectures by predicting the saturation performance psat of an iteration-wise curve, thereby discarding learners as soon as the probability that it is competitive drops below a pre-defined threshold.",
"The Bayesian model pro- posed by Domhan et al (2015) also explicitly estimates the noise σ2 a,b, which is assumed to be homoscedastic, i.e., identical for all budgets b.",
"In this sense, the approach quantifies both epistemic and aleatoric uncertainty (cf.",
"A recent alternative for estimating distributions has been proposed by Adriaensen et al (2023) through the notion of prior fitted neural networks (PFN) (Hollmann et al, 2023).",
"Based on the approach by Domhan et al (2015), Adriaensen et al (2023) consider a set of basis functions, over which they define a prior.",
"They then sample a large number of curves from this prior and train a variation of a transformer neural network with it, which is able to predict distributions for a target anchor based on a partial learning curve.",
"Since no sampling from the posterior is required anymore, compared to the MCMC approach by Domhan et al (2015), predictions can be obtained much faster, and the authors claim that the prediction performance is comparable or even better, which could however not be confirmed in subsequent experiments discussed below.",
The latest approach we are aware of is the robust estimation RoBER presented by Egele et al (2024).,
The approach follows the idea of MCMC introduced by Domhan et al (2015) but applies a different sampling algorithm to obtain more stable estimates.,
The results suggest that classical learn- ing extrapolation significantly outperforms PFN-based extrapolations at the current state of research.,
5.2.2 Utility Prediction at Any Point Utility prediction combines learning curve models as discussed in Sec.,
5.2.1 with utility models as discussed in Sec.,
"The learning curve model ˆf is used as a basis to estimate the performance at any point, and the utility at budget b is then computed as a function of the modelled performance ˆfa(b) at and the associated costs for budget b.",
The first approach in this direction was presented by Last (2007).,
This work is very similar to the works of Weiss and Tian (2006) but makes util- ity forecasts rather than looking back.,
"Therefore, it is projective instead of retrospective.",
"The error rate that serves as input to the model is obtained from a parametric model (i.e., a power-law) trained on the empirical values obtained at earlier budgets.",
This framework enables one to analytically com- pute the optimal dataset size.,
"The main advantage of the approach over the one of Weiss and Tian (2006) is that one does not need to go through sev- eral acquisition iterations, which is a benefit if those are associated with fixed costs.",
"Therefore, the learning curve has become a resource for decision mak- ing.",
"While this work assumes the empirical learning curve to be available, Last Springer Nature 2021 LATEX template 50 Learning Curves for Decision Making (2009) embeds his idea into an algorithm that follows a progressive sampling scheme in a follow-up work.",
"One use case in which the above techniques have been adopted has been reported in the context of automated software configuration (Sarkar et al, 2015).",
"The context of that paper is that every instance is a parametrisation of a software library, and obtaining its label requires the costly execution of a benchmark on such a configuration.",
The goal is to understand how many observations must be acquired to learn a reliable prediction model.,
"To this end, the authors adopt the projective sampling approach of Last (2007, 2009).",
The latter work raises an important issue by stating that knowing the saturation point is (often) not enough.,
"Instead, we often also need to know the performance at the saturation point.",
"Sarkar et al (2015) argue that if the user is unaware of the expected performance at that point, substantial resources might be required to obtain the observations to reach the saturation point.",
"However, if the actual performance at that point is known to be mediocre, the user could anticipate this and not invest the required resources.",
"To this end, they also incorporate the utility model proposed by Weiss and Tian (2008).",
"5.2.3 Performance at Any Point for Any Learner The approaches discussed in this section are the most general ones developed to date regarding learning curves in that they create a model for the complete function C, i.e., generalising both over both budgets and learners.",
"Such a model is so versatile that it can be used in all types of decision-making situations, e.g., data acquisition, early stopping, and early discarding.",
"Additionally, these can be used for model acquisition (selecting a yet unseen promising model).",
"Freeze-thaw Bayesian optimisation models the behaviour of learning curves through Gaussian processes (Swersky et al, 2014).",
An important contribution of that work is a non-stationary kernel for Gaussian processes that supports exponentially decaying learning curve models; it can easily be checked that standard kernels like a linear or Gaussian kernel do not lead to meaning- ful learning curve models.,
"Assuming that the kernel reflects the model class appropriately, one additional benefit of using a Gaussian process is that one automatically obtains estimates for the noise σ2 a,b at an arbitrary anchor b.",
"Using their kernel and the current set of observations, Swersky et al (2014) estimate the asymptotic mean performance psat.",
"Since the learning curves are combined with Bayesian optimisation, the uncertainty for a specific future anchor is one of the required inputs for computing their acquisition func- tion.",
"In a rather thin evaluation, the approach was successfully applied to Online Latent Dirichlet Allocation, Logistic Regression, and Probabilistic Matrix Factorization, considering one dataset per learner.",
"While the paper focused on iteration-wise curves, the modelling technique can also be used for sample-wise curves.",
Klein et al (2017a) presented a similar approach dubbed FABOLAS.,
"Simi- lar to Freeze-thaw Bayesian optimisaion, a Gaussian process is used to model Springer Nature 2021 LATEX template Learning Curves for Decision Making 51 the learning curve of the learners across different hyperparameter configura- tions.",
There are three main differences between the two approaches as far as learning curves are concerned.,
"First, FABOLAS considers sample-wise curves, while freeze-thaw Bayesian optimisation considers iteration-wise curves.",
"Sec- ond, and related to this, FABOLAS uses a kernel different from freeze-thaw Bayesian optimisation to model the behaviour of the learning curve using the Gaussian process, which is defined by the relative dataset size in [0, 1] instead of absolute sizes.",
"Third, FABOLAS tries to explicitly learn the complete learning curve, while freeze-thaw Bayesian optimisation focuses only on the saturation performance.",
"Similar to freeze-thaw Bayesian optimisation, the uncertainty about the performance estimates of the learning curve is not explicitly used.",
"Still, the fact that a Gaussian process is fitted from the data allows one to make assertions about the certainty of the learning curve value at any point.",
"Parallel to their work on FABOLAS, Klein et al (2017b) proposed an approach to estimate both the mean and the noise (aleatoric uncertainty) of a learning curve through the notion of Bayesian neural networks.",
The neu- ral network predicts the parameters of a set of basis functions.,
"These basis functions incorporate prior knowledge into the network, which is necessary to extrapolate away from the data.",
The main difference between this approach and the aforementioned approaches is that this approach models the behaviour of the learning curve through a neural network.,
"This network has d + 1 input units (d for the algorithm description and one for the anchor), one output unit for the estimated performance and, optionally, one output unit for the estimate of the variance of the performance (which relates to aleatoric uncer- tainty).",
"To our knowledge, this approach and the learning curve extrapolation proposed by Domhan et al (2015) are the only approaches in this area that explicitly model the variance of the performance of the learning curve (i.e., which can also be seen as noise).",
"An important difference between the two is that Klein et al (2017b) assume heteroscedastic noise, i.e., noise that changes with both different hyperparameters and anchors, while Domhan et al (2015) assume homoscedastic noise across anchor sizes (not across configurations, because the model does not generalise over different configurations).",
"While the approach presented in the paper does not explicitly consider the uncertainty about the parameter estimates, the parameters are essentially sampled from a posterior distribution.",
"Therefore, the uncertainty is at least implicitly avail- able.",
"However, it should be noted that the number of parameters describing the model here, namely the network weights, is potentially much larger than in the approach taken by Domhan et al (2015).",
"Wistuba and Pedapati (2019) propose to use biased matrix factorisation to model C(·, ·).",
The approach is settled in the context of neural architecture search.,
"Knowledge from previous datasets and different architectures is used to estimate the performance of new architectures on the target dataset, and this estimate is used to drive a Bayesian optimisation approach.",
"Springer Nature 2021 LATEX template 52 Learning Curves for Decision Making Table 1: Overview of the discussed learning curve approaches, ordered along the most general question they address, the learning curve type and the data resources used (4 columns).",
"In the header, ‘LC’ stands for learning curve, ‘DS’ stands for dataset, and ‘AL’ stands for algorithm.",
All of them use partial empirical curves on the target dataset of the current learner(s).,
"Estimate type: p,r,d are point, range and distribution estimates, respectively.",
Question Type LC other DS DS Meta-Feat.,
LC other AL AL Meta-Feat.,
Utility Estimate Type Contributions pref > τ obs.,
✗✗✗✗✗ p Van den Bosch (2004); Jamieson and Talwalkar (2016); Petrak (2000); Zeng and Luo (2017) pref > τ both ✗✗✗✗✗ p Li et al (2017) πa∈A obs.,
"✓✗✓✗✗ p Leite and Brazdil (2005, 2007); van Rijn et al (2015) πa∈A obs.",
"✓✓✗✗✗ p Leite and Brazdil (2008, 2010) πa∈A both ✓✓✗✗✗ p Ruhkopf et al (2023) bsat iter.",
✗✗✗✗✗ p Bishop (1995) bsat obs.,
✗✗✗✗✗ p John and Langley (1996); Ng and Dash (2006); Provost et al (1999) bsat obs.,
"✓✗✗✗✗ p Leite and Brazdil (2003, 2004) plim obs.",
✗✗✗✗✗ p Cortes et al (1994) bu sat obs.,
✗✗✗✗✗ p Meek et al (2002) bu sat obs.,
"✗✗✗✗✓p Weiss and Tian (2006, 2008) C(a, dtr ) obs.",
"✓✗✗✗✗ r Chandrashekaran and Lane (2017) C(a, dtr ) iter.",
"✗✗✓✓✗ p Baker et al (2018) C(a, ·) obs.",
"✗✗✗✗✗ p Sabharwal et al (2016) C(a, ·) obs.",
"✗✗✗✗✗ r Mohr and van Rijn (2023) C(a, ·) iter.",
"✗✗✗✗✗ p Cortes et al (1993) C(a, ·) obs.",
"✗✗✗✗✗ p Boonyanunta and Zeephongsekul (2004); Frey and Fisher (1999); Gu et al (2001); Hess and Wei (2010); John and Langley (1996); Kolachina et al (2012); Richter and Khoshgoftaar (2019); Singh (2005) C(a, ·) obs.",
"✗✗✗✗✗ r Figueroa et al (2012); Koshute et al (2021); Mukherjee et al (2003) C(a, ·) iter.",
"✗✗✗✗✗ d Domhan et al (2015) C(a, ·) obs.",
"✓✗✗✗✗ p Cardona-Escobar et al (2017) C(a, ·) both ✓✗✗✗✗ d Adriaensen et al (2023) C(a, ·) both ✗✗✗✗✗ d Egele et al (2024) C(a, ·) obs.",
"✓✓✗✗✗ p Kielh¨ofer et al (2024) U(a, ·) obs.",
"✗✗✗✗✓p Last (2007, 2009) C(·, ·) both ✗✗✓✓✗ d Klein et al (2017b); Swersky et al (2014) C(·, ·) obs.",
"✗✗✓✓✗ d Klein et al (2017a) C(·, ·) both ✓✗✓✓✗ d Wistuba and Pedapati (2019) 6 Summary and Open Research Directions Learning curves have been a vital resource for decision making in machine learning for several decades, and they have gained significant attention over the last years.",
"Learning curves have proven to be a suitable solution for different Springer Nature 2021 LATEX template Learning Curves for Decision Making 53 types of decision-making situations, i.e., data acquisition, early stopping, and early discarding for model selection.",
We have provided a formal definition of various types of learning curves (Sec.,
"There are two predominant types of learning curves in the machine learning literature, i.e.",
": the sample-wise curve (i.e., the type of learning curve that one obtains when giving a learner more training instances) and the iteration-wise curve (i.e., the type of learning curve that one obtains when allowing an algorithm to process the data multiple times, see for example the number of epochs of a neural network).",
"While both types of learning curves seem similar, they have distinct semantic meanings and characteristics.",
"Both types of learning curves can be extended to a utility curve, which considers the cost of computational resources or data acquisition.",
"Additionally, we have con- trasted these against other types of (learning) curves, such as feature curves, capacity curves, and curves obtained by data-centric models, such as active learning or curriculum learning.",
We have described the basic concepts of modelling a learning curve (Sec.,
"There are various parametric models that incorporate domain knowledge about what we already know about the shape of learning curves (e.g., the three- parameter inverse power law-model).",
"Even when the performance of a learner is observed at very few anchors, the learning curve can already be extrapo- lated to make predictions about larger anchors.",
"Additionally, one can decide to also model a degree of uncertainty, either as a range estimate or as a distribution.",
"We distinguish between two types of uncertainty, i.e., epistemic uncertainty and aleatoric uncertainty, and relate these concepts to the lit- erature on modelling learning curves.",
"Typically, when uncertainty is being modelled, the epistemic uncertainty is being modelled, but in some cases, the aleatoric uncertainty is being modelled (see, e.g., Klein et al, 2017b).",
We have provided a unified framework for methods that utilise learn- ing curves for decision making in machine learning (Sec.,
"This framework categorises these methods along three axes: the decision situation that they address, the questions that can be addressed with learning curves, and the data resources that can be used to model the learning curves.",
"Notably, Fig.",
10 shows an overview of all questions that can be addressed by learning curves.,
"There are various ways to address decision situations with learning curves; for example, questions about the saturation point of a given learner or whether a learner will perform better than another learner at a given amount of data.",
"These questions can be further generalised, eventually ranging in complexity from binary questions to questions that address how any learner behaves at any budget.",
"We have done an extensive literature survey, categorising all learning curve methods that we are aware of into this framework (Sec.",
"Table 1 shows an overview of the methods we have discussed in this survey, contextualising them according to these criteria.",
This table can be seen as an extension of Fig.,
"Based on this literature survey, we describe several directions for future work.",
Springer Nature 2021 LATEX template 54 Learning Curves for Decision Making More experimental databases for learning curve research to sup- port the full complexity of learning curve methods.,
Doing relevant research on learning curves requires extensive computing power.,
"Exploring a sample-wise curve inherently requires many models to restart the learning pro- cess at different anchors, whereas exploring an iteration-wise curve is often done on neural networks that come with their distinct layers of complexity (see, e.g., White et al, 2023).",
A common way to address this is by experimental databases or surrogate benchmarks that store certain experimental results.,
"This allows for fast experimentation and, therefore, faster development cycles.",
While several of these experimental databases for learning curves exist (as outlined in Sec.,
"2.2), these currently do not capture the full scale of learning curves resources or questions that can be answered using learning curves.",
A quantitative benchmark on learning curve extrapolation meth- ods.,
"Many models have been proposed to extrapolate learning curves and make predictions about the performance of a learning at a higher budget (see, e.g., Gu et al, 2001).",
"However, these models have only been subject to limited com- parison.",
"While Gu et al (2001) compared various parametric models against each other, and Kielh¨ofer et al (2024) compared a representative parametric model against a representative metalearning model across many different set- tings, more research is needed.",
13 and Fig.,
"14 already show that, while many papers are aware of other methods and cite those methods (grey arrows), only very few actively compare against each other (blue arrows).",
"Moreover, many of these learning curve models are used as a small component in a larger system, e.g., an AutoML system.",
"In such a case, the predictive performance of the learning curve model might not even be measured, as eventually, one often measures the quality of the complete system; in the case of an AutoML system, the performance of the final selected model.",
"Due to this modular nature, improvements on the learning curve extrapolation method would then be orthogonal to improvements on the AutoML system.",
Tighter integration of learning curve extrapolation methods with AutoML systems.,
We already noted a clear opportunity for AutoML sys- tems in Sec.,
"In situations where multiple learners are being compared against each other, the training set needs to be further split into an actual training set and a validation set to select the best learner to be tested on the test set (which can only be seen once).",
"The existence of this validation set already shows an opportunity for learning curve methods; while a learner is being selected based on its performance after being trained on the split-off training set, what is relevant is its performance after being trained on the orig- inal training set (i.e., the split-off training set plus validation set).",
It is not necessarily the case that the same learner performs best on both.,
Learning curve extrapolation models can predict which learner will eventually perform best on an anchor of the size of the original training set.,
More learning curve methods that operate on low-level questions.,
A prominent question that arises from the literature survey is whether sim- ple questions can be treated more simplistically.,
10 shows four binary Springer Nature 2021 LATEX template Learning Curves for Decision Making 55 questions.,
"While approaches aim to answer these questions, most do so by implicitly answering a more difficult question (see Table 1 and Fig.",
Some of these questions are really at the core of the discussed approaches.,
"For exam- ple, will the learning curve intersect with the learning curve of the currently best model?",
"Most approaches create a learning curve model for this, implic- itly solving a much more difficult problem.",
"While those models, if appropriate, have the potential to provide additional interesting insights, the question arises whether simpler approaches could reliably solve those problems while needing much less online data.",
Approaches that remain faithful to this question level (such as successive halving and hyperband) have proven effective and received considerable attention.,
"Additionally, we see a clear opportunity for incorpo- rating uncertainty into methods that address the binary question, effectively providing the chance that a particular learner will be better than another learning at a given budget.",
A learning curve method that makes use of all types of data resources.,
Learning curve methods can make use of various resources to model the learning curve (see Fig 11).,
"For example, the inverse power law model uses the current learner’s learning curve on the same dataset.",
"In contrast, metalearning models often also make use of learning curves of either the current learner or other learners on other datasets (see, e.g., Leite and Brazdil, 2005).",
A reasonable assumption is that the methods that utilise more types of data resources would be more accurate.,
Table 1 reveals that no learning curve model utilises all types of data resources.,
"Indeed, combining anchors across learners and datasets might be a complex task, but when this is done successfully, it will enormously increase our understanding of learning curves.",
Acknowledgements.,
"We thank Pavel Brazdil, Isabelle Guyon, Aaron Klein, and Marco Loog for their insightful discussions and feedback on this survey.",
Springer Nature 2021 LATEX template 56 Learning Curves for Decision Making A Notation Table 2 contains an overview of the notation used throughout this paper.,
Table 2: Overview of notation.,
"term description D The space of all possible datasets d An instantiation of a dataset dtr The instances from a given dataset based upon which a given hypothesis h is trained X The space of all possible input values for a given dataset d Y The space of all possible labels of a given dataset d H The space that a given model or hypothesis h can take h Model or hypothesis h, induced based on a given train set dtr a An algorithm that given a training set dtr induces a hypothesis h A Set of all possible learners under consideration Rout The theoretical performance of a hypothesis under the true distribution of the data (note that this true distribution is typically unknown) Rin The empirical risk of a hypothesis under some sample d (i.e., a dataset) from the true distribution C (a, n) true mean performance of learner a when trained on n samples (related to observation learning curve) C (a,n,t) true mean performance of learner a when trained on n samples with t iterations (related to iteration learning curve) f (a, b) The performance of a given learner a trained on a dataset of sample size b (in case of observation curve) or iteration b (in case of iteration curve).",
"In contrast to C. f (a, b) is a random variable with C as mean value µa,b The mean of f (a, b) σ2 a,b The variance of f (a, b) O Set of performance observations from anchors of historic learning curves, pos- sibly for different learners, e.g., O = {(a1, b1), (a2, b2), (a3, b3), .",
"}, where ai are learners and bj are budgets.",
"ˆf (a, b) The performance estimated by a learning curve model for learner a at budget b.",
"Maybe a point, range, or distributional estimate.",
ˆfa(b) The performance estimated by a learning curve model for learner a at budget b if the model does not generalize across learners.,
"Maybe a point, range, or distributional estimate.",
"n anchor size, indicating the size of a subsample of the dataset t number of iterations, e.g., in the case of neural networks, the number of epochs b generic symbol for an anchor, stands either for n or t, depending on the context.",
"bsat The anchor size at which the performance of a learner saturates psat The performance of the learner at the saturation point bu sat The anchor size at which the utility (a performance measure divided by a cost measure) is maximized bref Used in the framework for the binary question as a threshold on the budget pref Used in the framework for the binary question as a threshold on the performance θ The parameters of a parametric function, for example, the parameters of the IPL-model are (α, β, γ) Springer Nature 2021 LATEX template Learning Curves for Decision Making 57 Declarations Funding.",
Felix Mohr was supported through the project ING-312-2023 from Universidad de La Sabana.,
Conflicts of interest.,
Not Applicable Ethics approval.,
Not Applicable Consent to participate.,
Not Applicable Consent for publication.,
Not Applicable Availability of data and material.,
Not Applicable Code availability.,
Not Applicable Authors’ contributions.,
Both authors were involved in the redaction of all parts of the document.,
had the lead in all sections; J.N.v.R.,
contributed to all sections by actively writing and critically reviewing the text.,
"References Adriaensen S, Rakotoarison H, M¨uller S, et al (2023) Efficient bayesian learning curve extrapolation using prior-data fitted networks.",
"In: Advances in Neural Information Processing Systems 36, pp 19,858–19,886 Alwosheel A, van Cranenburgh S, Chorus CG (2018) Is your dataset big enough?",
sample size requirements when using artificial neural networks for discrete choice analysis.,
"Journal of choice modelling 28:167–182 Amari S, Murata N (1993) Statistical theory of learning curves under entropic loss criterion.",
"Neural Computation 5(1):140–153 Baker B, Gupta O, Raskar R, et al (2018) Accelerating neural architecture search using performance prediction.",
"In: 6th International Conference on Learning Representations, ICLR’18 Beleites C, Neugebauer U, Bocklitz T, et al (2013) Sample size planning for classification models.",
"Analytica chimica acta 760:25–33 Bifet A, Gavald`a R, Holmes G, et al (2018) Machine learning for data streams: with practical examples in MOA.",
MIT press Bishop C (1995) Regularization and complexity control in feed-forward networks.,
"In: Proceedings International Conference on Artificial Neural Networks ICANN’95, pp 141–148 Boonyanunta N, Zeephongsekul P (2004) Predicting the relationship between the size of training sample and the predictive power of classifiers.",
"In: Knowledge-Based Intelligent Information and Engineering Systems, 8th International Conference, KES 2004. pp 529–535 Springer Nature 2021 LATEX template 58 Learning Curves for Decision Making Bornschein J, Visin F, Osindero S (2020) Small data, big decisions: Model selection in the small-data regime.",
In: Proceedings of the 37th International Conference on Machine Learning.,
pp 1035–1044 Van den Bosch A (2004) Wrapped progressive sampling search for optimizing learning algorithm parameters.,
"In: Proceedings of the 16th Belgian-Dutch Conference on Artificial Intelligence, pp 219–226 Brazdil P, van Rijn JN, Soares C, et al (2022) Metalearning: Applications to Automated Machine Learning and Data Mining, 2nd edn.",
"Springer Cardona-Escobar AF, Giraldo-Forero AF, Castro-Ospina AE, et al (2017) Efficient hyperparameter optimization in convolutional neural networks by learning curves prediction.",
"In: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications.",
"pp 143–151 Chandrashekaran A, Lane IR (2017) Speeding up hyper-parameter optimiza- tion by extrapolation of learning curves using previous builds.",
"In: Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2017. pp 477–492 Cho J, Lee K, Shin E, et al (2015) How much data is needed to train a medi- cal image deep learning system to achieve necessary high accuracy?",
CoRR abs/1511.06348.,
"https://arxiv.org/abs/1511.06348 Cortes C, Jackel LD, Solla SA, et al (1993) Learning curves: Asymptotic values and rate of convergence.",
"In: Advances in Neural Information Processing Systems 6. pp 327–334 Cortes C, Jackel LD, Chiang W (1994) Limits in learning machine accuracy imposed by data quality.",
"In: Advances in Neural Information Processing Systems 7. pp 239–246 da Costa FG, Rios RA, de Mello RF (2016) Using dynamical systems tools to detect concept drift in data streams.",
"Expert Systems with Applications 60:39–50 Domhan T, Springenberg JT, Hutter F (2015) Speeding up automatic hyper- parameter optimization of deep neural networks by extrapolation of learning curves.",
"In: Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015. pp 3460–3468 Domingos P, Hulten G (2000) Mining High-Speed Data Streams.",
"In: Proceed- ings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pp 71–80 Springer Nature 2021 LATEX template Learning Curves for Decision Making 59 Dong X, Yang Y (2020) Nas-bench-201: Extending the scope of reproducible neural architecture search.",
"In: 8th International Conference on Learning Representations, ICLR 2020 Egele R, Mohr F, Viering T, et al (2024) The unreasonable effectiveness of early discarding after one epoch in neural network hyperparameter optimization.",
"Neurocomputing 597:127,964 Eggensperger K, Lindauer M, Hoos HH, et al (2018) Efficient benchmarking of algorithm configurators via model-based surrogates.",
"Machine Learning 107(1):15–41 Eggensperger K, M¨uller P, Mallik N, et al (2021) HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO.",
"In: Proceed- ings of the Neural Information Processing Systems Track on Datasets and Benchmarks Figueroa RL, Zeng-Treitler Q, Kandula S, et al (2012) Predicting sample size required for classification performance.",
"BMC Medical Informatics Decis Mak 12:8 Fine T, Mukherjee S (1999) Parameter convergence and learning curves for neural networks.",
"Neural Comput 11(3):747–769 Forman G, Cohen I (2004) Learning from little: Comparison of classifiers given little training.",
"In: Knowledge Discovery in Databases: PKDD 2004, 8th European Conference on Principles and Practice of Knowledge Discovery in Databases.",
"pp 161–172 Frey LJ, Fisher DH (1999) Modeling decision tree performance with the power law.",
"In: Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics, AISTATS 1999 F¨urnkranz J, Petrak J (2001) An evaluation of landmarking variants.",
"In: Work- ing Notes of the ECML/PKDD 2000 Workshop on Integrating Aspects of Data Mining, Decision Support and Meta-Learning, pp 57–68 Gkioxari G, Toshev A, Jaitly N (2016) Chained predictions using convolutional neural networks.",
"In: Leibe B, Matas J, Sebe N, et al (eds) Computer Vision - ECCV 2016 - 14th European Conference.",
"pp 728–743 Goodfellow I, Bengio Y, Courville A (2016) Deep Learning.",
"MIT Press, http: //www.deeplearningbook.org Gu B, Hu F, Liu H (2001) Modelling classification performance for large data sets.",
"In: Advances in Web-Age Information Management, Second International Conference, WAIM 2001. pp 317–328 Springer Nature 2021 LATEX template 60 Learning Curves for Decision Making Hess KR, Wei C (2010) Learning curves in classification with microarray data.",
"Seminars in oncology 37(1):65–68 Hollmann N, M¨uller S, Eggensperger K, et al (2023) Tabpfn: A transformer that solves small tabular classification problems in a second.",
"In: The Eleventh International Conference on Learning Representations, ICLR 2023 Hughes GF (1968) On the mean accuracy of statistical pattern recognizers.",
"IEEE Trans Inf Theory 14(1):55–63 H¨ullermeier E, Waegeman W (2021) Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods.",
"Machine Learning 110(3):457–506 Jamieson KG, Talwalkar A (2016) Non-stochastic best arm identification and hyperparameter optimization.",
"In: Proceedings of the 19th Interna- tional Conference on Artificial Intelligence and Statistics, AISTATS 2016. pp 240–248 John GH, Langley P (1996) Static versus dynamic sampling for data min- ing.",
In: Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96).,
"pp 367–370 Kielh¨ofer L, Mohr F, van Rijn JN (2024) Learning curve extrapolation meth- ods across extrapolation settings.",
In: Advances in Intelligent Data Analysis XXII.,
"pp 145–157 Klein A, Falkner S, Bartels S, et al (2017a) Fast bayesian optimization of machine learning hyperparameters on large datasets.",
"In: Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017. pp 528–536 Klein A, Falkner S, Springenberg JT, et al (2017b) Learning curve prediction with bayesian neural networks.",
"In: 5th International Conference on Learning Representations, ICLR’17 Kolachina P, Cancedda N, Dymetman M, et al (2012) Prediction of learning curves in machine translation.",
"In: Proceedings of the 50th Annual Meet- ing of the Association for Computational Linguistics, Proceedings of the Conference.",
"pp 22–30 Koshute P, Zook J, McCulloh I (2021) Recommending training set sizes for classification.",
CoRR abs/2102.09382.,
https://arxiv.org/abs/2102.09382 Last M (2007) Predicting and optimizing classifier utility with the power law.,
In: Workshops Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007).,
pp 219–224 Springer Nature 2021 LATEX template Learning Curves for Decision Making 61 Last M (2009) Improving data mining utility with projective sampling.,
In: Pro- ceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.,
"pp 487–496 Leite R, Brazdil P (2003) Improving progressive sampling via meta-learning.",
"In: Progress in Artificial Intelligence, 11th Protuguese Conference on Artificial Intelligence, EPIA 2003. pp 313–323 Leite R, Brazdil P (2004) Improving progressive sampling via meta-learning on learning curves.",
"In: Machine Learning: ECML 2004, 15th European Conference on Machine Learning.",
"pp 250–261 Leite R, Brazdil P (2005) Predicting relative performance of classifiers from samples.",
"In: Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005).",
"pp 497–503 Leite R, Brazdil P (2007) An iterative process for building learning curves and predicting relative performance of classifiers.",
"In: Progress in Artificial Intelligence, 13th Portuguese Conference on Aritficial Intelligence, EPIA 2007. pp 87–98 Leite R, Brazdil P (2008) Selecting classifiers using metalearning with sampling landmarks and data characterization.",
"In: Proceedings of the 2nd Planning to Learn Workshop (PlanLearn) at ICML/COLT/UAI, pp 35–41 Leite R, Brazdil P (2010) Active testing strategy to predict the best classi- fication algorithm via sampling and metalearning.",
In: ECAI 2010 - 19th European Conference on Artificial Intelligence.,
"pp 309–314 Li L, Jamieson KG, DeSalvo G, et al (2017) Hyperband: A novel bandit-based approach to hyperparameter optimization.",
"Journal of Machine Learning Research 18:185:1–185:52 Long D, Zhang S, Zhang Y (2020) Performance prediction based on neural architecture features.",
"Cognitive Computation and Systems 2(2):80–83 Loog M, Duin RP (2012) The dipping phenomenon.",
"In: Joint IAPR Interna- tional Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), pp 310–317 Loog M, Viering TJ, Mey A (2019) Minimizers of the empirical risk and risk monotonicity.",
"In: Advances in Neural Information Processing Systems 32, pp 7476–7485 Mao Z, Jung T, Lin C, et al (2016) Predicting EEG sample size required for classification calibration.",
"In: Foundations of Augmented Cognition: Neuroer- gonomics and Operational Neuroscience - 10th International Conference, AC Springer Nature 2021 LATEX template 62 Learning Curves for Decision Making 2016. pp 57–68 Meek C, Thiesson B, Heckerman D (2002) The learning-curve sampling method applied to model-based clustering.",
"Journal of Machine Learning Research 2:397–418 Mhammedi Z, Husain H (2021) Risk-monotonicity in statistical learning.",
"In: Advances in Neural Information Processing Systems 34, pp 10,732–10,744 Mohr F, van Rijn JN (2021) Towards model selection using learning curve cross-validation.",
"In: 8th ICML Workshop on Automated Machine Learning (AutoML) Mohr F, van Rijn JN (2023) Fast and informative model selection using learning curve cross-validation.",
"IEEE Transactions on Pattern Analysis and Machine Intelligence 45(8):9669–9680 Mohr F, Viering TJ, Loog M, et al (2022) LCDB 1.0: An extensive learning curves database for classification tasks.",
"In: Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD.",
"pp 3–19 Mørch NJS, Hansen LK, Strother SC, et al (1997) Nonlinear versus lin- ear models in functional neuroimaging: Learning curves and generalization crossover.",
"In: Information Processing in Medical Imaging, 15th International Conference, IPMI’97.",
"pp 259–270 Mukherjee S, Tamayo P, Rogers S, et al (2003) Estimating dataset size require- ments for classifying DNA microarray data.",
"Journal of Computational Biology 10(2):119–142 Murata N, Yoshizawa S, Amari S (1992) Learning curves, model selection and complexity of neural networks.",
"In: Advances in Neural Information Processing Systems 5. pp 607–614 Nakkiran P, Kaplun G, Bansal Y, et al (2020) Deep double descent: Where big- ger models and more data hurt.",
"In: 8th International Conference on Learning Representations, ICLR’20 Nakkiran P, Venkat P, Kakade SM, et al (2021) Optimal regularization can mitigate double descent.",
"In: 9th International Conference on Learning Representations, ICLR 2021 Ng AY, Jordan MI (2001) On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.",
"In: Advances in Neural Information Processing Systems 14. pp 841–848 Springer Nature 2021 LATEX template Learning Curves for Decision Making 63 Ng W, Dash M (2006) An evaluation of progressive sampling for imbal- anced data sets.",
In: Workshops Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 2006).,
"pp 657–661 Oyedare T, Park JJ (2019) Estimating the required training dataset size for transmitter classification using deep learning.",
"In: 2019 IEEE International Symposium on Dynamic Spectrum Access Networks, DySPAN 2019. pp 1–10 Perlich C, Provost FJ, Simonoff JS (2003) Tree induction vs. logistic regression: A learning-curve analysis.",
Journal of Machine Learning Research 4:211–255 Petrak J (2000) Fast subsampling performance estimates for classification algorithm selection.,
"In: Proceedings of the ECML-00 Workshop on Meta- Learning: Building Automatic Advice Strategies for Model Selection and Method Combination, pp 3–14 Pfisterer F, Schneider L, Moosbauer J, et al (2022) YAHPO gym - an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization.",
"In: International Conference on Automated Machine Learning, AutoML.",
"pp 3/1–39 Provost FJ, Jensen DD, Oates T (1999) Efficient progressive sampling.",
In: Pro- ceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.,
"pp 23–32 Richter AN, Khoshgoftaar TM (2019) Approximating learning curves for imbalanced big data with limited labels.",
"In: 31st IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2019. pp 237–242 van Rijn JN, Abdulrahman SM, Brazdil P, et al (2015) Fast algorithm selection using learning curves.",
In: Advances in Intelligent Data Analysis XIV.,
"pp 298–309 Ruhkopf T, Mohan A, Deng D, et al (2023) Masif: Meta-learned algo- rithm selection using implicit fidelity information.",
"Transactions on Machine Learning Research 2023 Sabharwal A, Samulowitz H, Tesauro G (2016) Selecting near-optimal learners via incremental data allocation.",
"In: Proceedings of the AAAI Conference on Artificial Intelligence Sarkar A, Guo J, Siegmund N, et al (2015) Cost-efficient sampling for per- formance prediction of configurable systems (T).",
"In: 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015. pp 342–352 Springer Nature 2021 LATEX template 64 Learning Curves for Decision Making Settles B (2009) Active learning literature survey.",
"rep., University of Wisconsin Seung HS, Sompolinsky H, Tishby N (1992) Statistical mechanics of learning from examples.",
"Physical Review A 45(8):6056 Siems J, Zimmer L, Zela A, et al (2020) Nas-bench-301 and the case for sur- rogate benchmarks for neural architecture search.",
CoRR abs/2008.09777.,
https://arxiv.org/abs/2008.09777 Singh S (2005) Modeling performance of different classification methods: devi- ation from the power law.,
"Project Report, Department of Computer Science, Vanderbilt University, USA Strang B, van der Putten P, van Rijn JN, et al (2018) Don’t rule out simple models prematurely: A large scale benchmark comparing linear and non- linear classifiers in openml.",
In: Advances in Intelligent Data Analysis XVII.,
"pp 303–315 Swersky K, Snoek J, Adams RP (2014) Freeze-thaw bayesian optimization.",
CoRR abs/1406.3896.,
https://arxiv.org/abs/1406.3896 Tomanek K (2010) Resource-aware annotation through active learning.,
"PhD thesis, Dortmund University of Technology Vallet F, Cailton JG, Refregier P (1989) Linear and nonlinear extension of the pseudo-inverse solution for learning boolean functions.",
"EPL (Europhysics Letters) 9(4):315 Viering TJ, Loog M (2023) The shape of learning curves: A review.",
"IEEE Transactions on Pattern Analysis and Machine Intelligence 45(6):7799–7819 Viering TJ, Mey A, Loog M (2020) Making learners (more) monotone.",
In: Advances in Intelligent Data Analysis XVIII.,
"pp 535–547 Waltz M, Fu K (1965) A heuristic approach to reinforcement learning control systems.",
"IEEE Transactions on Automatic Control 10(4):390–398 Wang X, Chen Y, Zhu W (2022) A survey on curriculum learning.",
"IEEE Transactions on Pattern Analysis and Machine Intelligence 44(9):4555–4576 Weiss GM, Provost FJ (2003) Learning when training data are costly: The effect of class distribution on tree induction.",
"Journal of Artificial Intelligence Research 19:315–354 Weiss GM, Tian Y (2006) Maximizing classifier utility when training data is costly.",
"SIGKDD Explorations 8(2):31–38 Springer Nature 2021 LATEX template Learning Curves for Decision Making 65 Weiss GM, Tian Y (2008) Maximizing classifier utility when there are data acquisition and modeling costs.",
"Data Mining and Knowledge Discovery 17(2):253–282 White C, Safari M, Sukthanker R, et al (2023) Neural architecture search: Insights from 1000 papers.",
CoRR abs/2301.08727.,
"https://arxiv.org/abs/ 2301.08727 Wistuba M, Pedapati T (2019) Inductive transfer for neural architecture optimization.",
CoRR abs/1903.03536.,
"https://arxiv.org/abs/1903.03536 Zeng X, Luo G (2017) Progressive sampling-based bayesian optimization for efficient and automatic machine learning model selection.",
Health Informa- tion Science and Systems 5(1):2,
"Active learning for data streams: a survey Davide Cacciarelli1,2* and Murat Kulahci1,3 1Department of Applied Mathematics and Computer Science, Technical University of Denmark, Kgs.",
"Lyngby, Denmark.",
"2Department of Mathematical Sciences, Norwegian University of Science and Technology, Trondheim, Norway.",
"3Department of Business Administration, Technology and Social Sciences, Lule˚a University of Technology, Lule˚a, Sweden.",
*Corresponding author(s).,
E-mail(s): dcac@dtu.dk; Abstract Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream.,
"The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form.",
"Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data.",
"To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models.",
These approaches can be broadly divided into two categories: static pool-based and stream-based active learning.,
"Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews.",
"However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream.",
This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in real time.,
"We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research.",
Keywords: stream-based active learning; online active learning; data streams; online learning; unlabeled data; query strategies; selective sampling; concept drift; experimental design; bandits.,
1 Introduction The deployment of machine learning models in real-world applications is often reliant on the availability of significant amounts of annotated data.,
"While recent advancements in sensor technology have facilitated the collection of larger amounts of data, this data is not always labeled and ready for use in training models.",
"Indeed, the process of obtaining labeled observations for supervised learning models can be cost-prohibitive and time- consuming, as it often requires quality inspections or manual annotation.",
"In such cases, active learning proves to be a valuable strategy to identify the most informative data points for use in training, thereby reducing the overall cost of labeling and improving the performance of the model.",
"Over the years, a plethora of active learning approaches have been proposed in the literature, each with its own benefits and limitations.",
These approaches seek to strike a balance between the cost of labeling and the quality of the model by selectively choosing the most informative observations for querying.,
"By carefully selecting the most informative observations, active learning helps to minimize the amount of labeled data required and streamlines the learning process, contributing to its overall efficiency.",
Published in Machine Learning by Springer (2023).,
"https://doi.org/10.1007/s10994-023-06454-2 arXiv:2302.08893v4 [stat.ML] 29 Nov 2023 While several surveys have been published on pool-based active learning (Aggarwal et al, 2014; Settles, 2009; Fu et al, 2013; Kumar and Gupta, 2020), which involves selecting a fixed set of observations from a pool of unlabeled data, the dynamic and sequential nature of many real-world problems often renders these approaches impractical.",
"This has led to growing interest in the online variant of active learning, also referred to as stream- based active learning, which involves continuously selecting and labeling observations as they arrive in a stream, allowing for real-time adaptation to changing data distributions.",
Lughofer (2017) provided a review of online active learning approaches with a focus on fuzzy models.,
"However, since its publication, numerous other online active learning approaches have been proposed, and to the best of our knowledge, no other surveys have been published to synthesize these developments.",
"Moreover, surveys purely focusing on online learning from data streams (Lu et al, 2018; Tieppo et al, 2022; Lima et al, 2022; Hoi et al, 2021) discuss methods that assume a complete availability of labels, which is not the case in many real-world applications.",
The aim of this review is to fill this gap by providing a comprehensive overview 1 of the most recently developed query strategies for online active learning.,
"It is worth noting that in certain cases, stream-based active learning is narrowly defined as the act of selecting the most informative observations from a data stream to fit a predictive model.",
"Instead, the act of determining which observations to query while making predictions is referred to as online selective sampling (Hanneke and Yang, 2021).",
"In this work, we cover and examine all the methods that address the crucial problem of selecting the most informative data points to label from a data stream in an online fashion.",
"We will present the techniques that have been proposed so far, discussing their strengths and limitations, as well as the challenges and opportunities that exist in this field.",
"In addition, we will provide an overview of evaluation strategies for online active learning algorithms and highlight some real-world applications.",
"Finally, we will identify potential future research directions in this area.",
"This survey comprehensively explores various facets of active learning, encompassing both theoretical founda- tions and practical challenges.",
"By delving into this review, we aim to shed light on pertinent research questions, including: 1.",
Query strategy.,
What sampling strategy should be used to maximize learning efficiency in a streaming context?,
Timing of queries.,
When and how often should data points be queried to balance learning and resource constraints?,
Model updates.,
When should predictive models be updated and how can they adapt to changing data distributions and concept drift?,
Scalability.,
How can active learning methods be made scalable and efficient for high-velocity data streams?,
Evaluation.,
What are appropriate evaluation metrics for assessing the performance of stream-based active learning algorithms?,
The structure of this paper is as follows.,
"In Section 2, we provide an overview of active learning, including the main instance selection criteria, an overview of the main active learning scenarios, and the connection between active learning and semi-supervised learning.",
"Section 3 represents the core of the review, with a brief overview of how online active learning approaches have been classified, followed by a detailed description of the state-of-the-art approaches.",
"In Section 4, we examine evaluation strategies for online active learning algorithms.",
Section 5 highlights real-world applications and challenges.,
Section 6 provides a summary of the most common online active learning methods and highlights potential directions for future research.,
"Finally, Section 7 provides conclusions and summarizes the key contributions of the review.",
"2 Preliminaries on active learning In supervised learning, we seek to learn a function that can predict the output variable, also known as response, given a set of input variables, also known as covariates.",
This function is often learned by training a model on a labeled dataset that consists of a large number of input-output pairs.,
"However, obtaining labeled examples is not always straightforward, and it may not be possible or practical to label all the available data.",
"In these cases, active learning can be used to select a subset of the data for labeling in order to improve the performance of the model, when there is a budget constraint on the number of unlabeled observations that can be queried.",
"Indeed, there are many examples of how a classification or regression model can achieve a performance that is similar to what can be achieved when all the labels are available, using only a small fraction of the available observations.",
"1We conducted a search on SCOPUS and Google Scholar using the following keywords: ”on-line active learning”, ”online active learning”, ”stream-based active learning”, ”single pass active learning”, ”online selective sampling”, ”sequential selective sampling”, and ”active learning” combined with ”data stream”.",
Each paper was reviewed individually to determine its relevance to online active learning.,
We eliminated irrelevant papers and manually added some papers that did not contain these keywords but used online active learning methods or were relevant to our discussion.,
"Additionally, we included related papers that were necessary to understand the bigger picture from the references of the reviewed strategies.",
2 2.1 Instance selection criteria The main challenge in active learning is deciding which data points to label.,
"There are many strategies for selecting data points in active learning, and most of them can be associated with one of these groups: • Uncertainty-based query strategies.",
"These approaches focus on selecting data points that the model is least confident about, in order to reduce its uncertainty (Lu et al, 2016; Tong and Koller, 2002).",
"When using classification models, the most widely used is the margin-based query strategy, where data points close to the decision boundary are selected (Roth and Small, 2006; Balcan et al, 2007).",
• Expected error or variance minimization.,
"These strategies estimate the future error or variance, when a newly labeled example is made available, and try to minimize it directly (Cohn et al, 1996; Roy and Mccallum, 2001).",
• Expected model change maximization.,
"This strategy involves selecting data points that would have the greatest impact on the estimate of the current model parameters if they were labeled and added to the training set (Cai et al, 2013).",
• Disagreement-based query strategies.,
"These approaches focus on selecting data points where there is disagree- ment among multiple models or experts (Hanneke, 2014; Wang, 2011; Steve and Liu, 2014; Sheng et al, 2008).",
"One of the most common approaches that use an ensemble of models is query by committee (Seung et al, 1992; Freund et al, 1997; Burbidge et al, 2007), which uses an ensemble of models to identify instances where the models have conflicting predictions.",
• Diversity- and density-based approaches.,
These methods exploit the structural information of the instances and try to select data points that are diverse and representative of the overall distribution of the data.,
"One example of this approach is the use of Mahalanobis distance to seek observations that are far from the currently labeled data points (Ge, 2014; Cacciarelli et al, 2022a).",
"Clustering may be applied to label representative data points (Nguyen and Smeulders, 2004; Min et al, 2020; Ienco et al, 2013), and graph-based methods can be employed to explore the structure information of labeled and unlabeled data points (Zhang et al, 2020b) or to build upon the semi-supervised label propagation strategy (Long et al, 2008).",
• Hybrid strategies.,
"These are active learning algorithms that combine multiple instance selection criteria (Don- mez et al, 2007; Huang et al, 2014).",
"For example, by combining margin-based sampling with clustering the learner can select the most uncertain observations within different areas of the input space.",
"By considering these different strategies, one can select the most appropriate approach for a given problem based on the characteristics of the data and the specific requirements of the application.",
"2.2 Active learning scenarios Active learning can be broadly categorized into three macro scenarios, based on how the unlabeled instances are supplied to the learner and then selected to be labeled by an oracle.",
"Regardless of the particular query strategy being employed, these macro scenarios provide a framework for understanding the flow of information and the decision-making steps involved in active learning.",
"These scenarios serve as a high-level categorization of different methods for approaching the active learning problem, each with its own set of advantages and disadvantages depending on the specific use case.",
Understanding these macro scenarios is crucial for selecting the appropriate active learning technique for a particular problem and for comparing different active learning algorithms.,
"In the next subsections, each of the three macro scenarios will be discussed.",
2.2.1 Membership query synthesis active learning This scenario represents the case when the learner is given complete freedom to ask for the label of any data point belonging to the input space or for a synthetically generated one.,
"Some examples of membership query synthesis active learning include image classification, where the learner can generate modified versions of existing images to be labeled, or object detection, where the learner can generate new instances by combining and transforming existing instances.",
"In natural language processing (NLP) tasks such as text classification or sentiment analysis, the learner might generate synthetic examples in the form of sentences or paragraphs that cover a wider range of variations in the language.",
"Also, in speech recognition, the learner might generate synthetic speech samples in different accents, pronunciations, or speaking styles in order to improve the recognition accuracy.",
"However, as highlighted by Baum and Lang (1992) and Settles (2009), the main drawback of this strategy is that it could generate unlabeled examples for which no labels can be associated by a human annotator (e.g., a mixture between a number and a letter).",
"A general flowchart for this scenario is reported in Figure 1, where the scheme is repeated until a budget constraint on the requested labels is met, or a stopping criterion on the achieved performance is satisfied.",
3 Labeled data Train/update model Synthetically generate instance(s) Ask for the label(s) Model Fig.,
1 Membership query synthesis active learning.,
"In the context of deep active learning (Ren et al, 2022), the membership query synthesis scenario can be addressed by using generative models.",
"For instance, generative adversarial networks (GANs) have been used to generate additional instances from the input space that may provide more informative labels for the learner (Goodfellow et al, 2014).",
"This can be done by using GANs for data augmentation, as GANs are capable of generating diverse and high-quality instances (Zhu and Bento, 2017).",
"Another approach is to combine the use of variational autoencoders (VAEs) (Kingma and Welling, 2013) and Bayesian data augmentation, as demonstrated by Tran et al.",
"(Tran et al, 2019, 2017).",
"The authors used VAEs to generate instances from the disagreement regions between multiple models, and Bayesian data augmentation to incorporate the uncertainty of the generated instances in the learning process.",
2.2.2 Pool-based active learning Pool-based active learning is one of the most widely studied scenarios in the machine learning literature.,
"The goal is to select the most informative subset of observations from a closed, static set of unlabeled data points.",
"The majority of the proposed pool-based active learning approaches have been developed for classification tasks (Cai et al, 2013), with image classification being a common application in computer vision (Li and Guo, 2013), as manually labeling large image datasets can be a challenging task.",
Train/update model Rank observations Ask for the label(s) Labeled data Unlabeled data Select top 𝑘 instance(s) Model Fig.,
2 Pool-based active learning.,
"The flowchart in Figure 2 provides an overview of pool-based active learning sampling schemes, where k represents the number of unlabeled instances whose label is queried at each round.",
"Traditional machine learning models that do not require substantial computational resources to train are typically associated with a choice of k equal to one (Vahdat et al, 2019).",
"This allows a timely update of the instance selection criteria, avoiding the redundant labeling of similar data points.",
"However, larger values of k have also been used in practice, such as the analysis performed by Ge (2014) for values ranging from 5 to 30 or the approach used by Cai et al (2013) to add 3% of the total number of observations to the training set each time.",
"Using a higher k value may be more practical when working with large models, as repeated training can be computationally expensive and challenging.",
"To this extent, batch mode active learning is generally considered to be a more efficient and effective option for image classification or detection tasks compared to the one-by-one query strategy, as the latter can be resource-intensive and time-consuming when working with large neural networks (Ren et al, 2022).",
"This is because re-training the model with just one new data point with high input dimensionality may not result in significant improvement (Ren et al, 2022).",
"In general, the choice of k may be problem- or model-specific, as it represents a trade-off between computational efficiency and the risk of querying redundant labels.",
"To enhance pool-based active learning, many approaches combine uncertainty-based instance selection criteria with acquisition functions such as entropy (Shannon, 1948; Wu et al, 2022), mutual information (Haussmann 4 et al, 2020), or variation ratio (Schmidt et al, 2020).",
Entropy is commonly used as an acquisition function in active learning because it provides a way to measure the uncertainty of the model predictions for a given data point.,
The entropy of a probability distribution is a measure of the amount of disorder or randomness in the distribution.,
"In the context of active learning, the entropy of a model’s predicted class probabilities for a data point can be used as a measure of the model’s uncertainty about the correct class label for that data point.",
"Acquiring examples with the highest uncertainty is one way to select data points for annotation, but it is not the only way.",
"Mutual information and variation ratio can also be used on the predictions obtained with the current model, in order to seek a diverse set of data points for which the predictions are the most uncertain.",
"For a more comprehensive discussion on pool-based active learning, readers are referred to the surveys (Aggarwal et al, 2014; Settles, 2009; Fu et al, 2013; Kumar and Gupta, 2020).",
"2.2.3 Online active learning In this type of active learning, we cannot greedily select the most informative observations from a static pool, as the instances are generated in a continuous stream and cannot be stored in their entirety before a decision is made.",
"This is similar to the famous statistical puzzle known as the secretary problem (Freeman, 1983), where a hiring manager must make a hiring decision for each applicant as they are interviewed, without the benefit of seeing all applicants first.",
"In general, online active learning is a crucial scenario for various real-world applications where the ability to make a sampling decision in real-time is of utmost importance.",
A few examples are: • Chemical or manufacturing processes.,
"In these applications, a learner is tasked with predicting the quality of the final product but may only have a short timeframe to make the sampling decision, to avoid traceability issues, particularly in high-volume production (Schmitt et al, 2013; Lieber et al, 2012).",
"Also, tasks like predic- tive maintenance and visual inspection might benefit from a real-time selection of new examples to be labeled and included in the training set (Roˇzanec et al, 2022).",
• Video streaming and clinical trials.,
"In these cases, a decision must be made on the fly, as users arrive or volunteers appear sequentially, and there may not be enough time to accumulate a pool of potential users or patients (Fowler et al, 2023; Riquelme, 2017).",
"• Text classification: In NLP, online active learning can be used for tasks such as sentiment analysis and spam detection, where the learner continuously learns from new incoming data points which need to be labeled to update the model in real-time and improve accuracy (Kranjc et al, 2015).",
• Fraud detection.,
"To effectively detect fraudulent activities, the learner must continuously select new examples to label so that it can continuously update its decision-making process (Carcillo et al, 2018, 2017).",
• Online customer service.,
Online customer service agents can use online active learning to improve their per- formance by continuously learning from customer interactions.,
"To do this, the learner must continuously select new examples to label or customer information to obtain, so that it can predict the best response based on past interactions and improve its accuracy over time (Zheng and Padmanabhan, 2006).",
• Marketing.,
"Online active learning could also be applied in the field of marketing to select informative examples in real-time and continuously optimize customer targeting and personalization (Carnein and Trautmann, 2019; Jamil and Khan, 2016).",
Train/update model Observe an unlabeled data point Ask for the label Labeled data Data stream Is it useful?,
Discard the observation Yes No Model Fig.,
3 Single-pass online active learning.,
One of the defining features of online active learning strategies is their data processing capabilities.,
Figure 3 and Figure 4 provide a visual representation of the two main approaches; single-pass and window-based.,
"Single- pass algorithms observe and evaluate each incoming data point on the fly, whereas window-based algorithms, also referred to as batch-based methods, observe a fixed-size chunk of data at a time.",
"In this approach, the learner evaluates the entire batch of data and selects the top k observations as the most informative ones to be labeled.",
5 This approach is referred to as best-out-of-window sampling.,
The specific value of k and the dimensionality of the buffer can vary based on the storage capabilities of the system and the computational time required to update the model.,
Window-based methods are useful in situations where data is generated in large quantities and the algorithm does not have a tight constraint on the time available for decision-making.,
"In contrast, single- pass methods are necessary when the algorithm needs to make a decision immediately after observing a specific data point.",
Train/update model Observe an unlabeled data point Ask for the label(s) Labeled data Data stream Is it full?,
Yes No Model Buffer Select top 𝑘 instance(s) Fig.,
4 Window- or batch-based online active learning.,
Another critical property in the design of an effective online active learning strategy is the assumption made about the data stream distribution.,
One important difference to consider is whether the data stream is stationary or drifting.,
A stationary data stream is characterized by a stable data generating process where the statistical properties of the data distribution that remain constant over time.,
"Conversely, a drifting data stream is marked by changing statistical properties of the data distribution over time, potentially due to alterations in the underlying data generating process.",
The distinction between stationary and drifting data streams is significant because it affects the performance of the active learning strategies.,
Online active learning strategies that have been developed for stationary data streams may lead to suboptimal performance when applied to drifting data streams.,
"This is because concept drift can alter the scale of the informativeness measure of unlabeled data points or even urge a complete change of the model, with the acquisition of more observations to accommodate the new concept.",
"Therefore, it is important to accurately assess the nature of the data stream distribution in the design of an active learning strategy.",
A failure to do so can result in a suboptimal performance and a reduced ability to effectively leverage the strengths of active learning.,
Another important property to consider when designing an active learning strategy is the label delay or verification latency.,
This refers to the time needed by the oracle to provide the label when it is requested by the learner.,
"In some cases, there may be a delay L in the oracle providing the label after it has been requested.",
This property must be taken into account when designing a sampling strategy as there may be redundant label requests for similar instances if this issue is not properly addressed.,
"Label delay can be classified into null latency, intermediate latency, or extreme latency (Souza et al, 2018).",
"The case with null latency, or immediate availability of the label upon request, is commonly used in the stream mining community, but may not be realistic for many practical applications.",
"Extreme latency, where labels are never made available to the learner, is closer to an unsupervised learning task.",
Intermediate latency assumes a delay 0 < L < ∞in the availability of the labels from the oracle.,
"Finally, the training efficiency of the online active learning algorithms should also be taken into considera- tion.",
There are two main training approaches in active learning; incremental training and complete re-training.,
"Incremental training involves updating model parameters with a small batch of new data, without starting the training process from scratch (Polikar et al, 2001; Wu et al, 2019; Shilton et al, 2005; Istrate et al, 2018).",
This approach allows the model to learn from new data while preserving its existing knowledge.,
"This can be achieved through fine-tuning the model parameters with the new data, or by using techniques such as elastic weight consolidation, which prevent previous knowledge from being erased.",
"Complete re-training, on the other hand, involves training a new model from scratch using the entire labeled data collected so far.",
"This approach discards the previous knowledge of the model and starts anew, which may result in the loss of knowledge learned from previous data.",
"Complete re-training is typically used when the amount of new data is substantial, the previous model is no longer relevant, or when the model architecture needs to be altered.",
It is important to note that the choice of training approach in online active learning algorithms can have a significant impact on the overall performance and effectiveness of the model.,
"6 2.3 Connection between active learning and semi-supervised learning Semi-supervised learning is a field of research that is closely related to active learning, as both methods are developed to deal with limited labeled data.",
"While active learning aims to minimize the amount of labeled data required to train a model, semi-supervised learning is a technique that trains a model using a combination of labeled and unlabeled data.",
"Active learning can be considered a special case of semi-supervised learning, as it allows the model to actively select which data points it wants to be labeled, rather than relying on a fixed set of labeled data.",
"In the context of online learning, Kulkarni et al (2016) conducted a study that provided an overview of semi-supervised learning techniques for classifying data streams.",
"These techniques do not address the primary question of active learning, which is when to query, but they are useful in exploiting the information contained in the unlabeled data points and in addressing issues related to model update and retraining in limited labeled data environments.",
It is also worth noting that semi-supervised learning can be used in combination with active learning to improve the data selection strategy.,
"By leveraging the strengths of both methods, it is possible to achieve better performance and more efficient learning compared to using either method alone.",
"Semi-supervised learning approaches can be distinguished into three categories, unsupervised preprocessing, wrapper methods, and graph-based methods.",
"Unsupervised preprocessing refers to the use of unsupervised learning techniques, such as dimensionality reduction (Cacciarelli and Kulahci, 2023), clustering, or feature extraction, to preprocess the entire dataset, labeled and unlabeled, before it is fed to the supervised model (Frumosu and Kulahci, 2018).",
The goal is to transform the data into a more useful representation that can be learned more easily by a supervised model and can support the sampling of more informative data points.,
"This strategy can also help reduce the dimensionality of the learning problem, thus improving the model parameter estimation when only a few queries can be made.",
"Related to the online active learning problem, Roˇzanec et al (2022) used a pre-trained network to extract salient features from unlabeled images before starting the sampling routine.",
"Similarly, Cacciarelli et al (2022a) used an autoencoder trained on all the available unlabeled data points to improve the performance of online active learning for linear regression models.",
"Wrapper methods, on the other hand, use one or more supervised learners that are trained on labeled data and pseudo-labeled unlabeled data.",
"There are two main variants of wrapper methods, self-training and co- training.",
"Self-training uses a single supervised model that is trained on labeled data, and pseudo-labels are used for the data points with confident predictions.",
"Co-training, on the other hand, extends self-training to multiple supervised models, where two or more models exchange the most confident predictions to obtain pseudo-labels.",
"Pseudo-labels can be very beneficial in label-scarce environments, but one must be mindful of the confirmation bias issue, where the model might rely on incorrect self-created labels.",
"This problem has been extensively analyzed by Baykal et al (2022) in the active distillation scenario, which is a strategy where a smaller model, known as the student model, is trained to mimic the behavior of a larger pre-trained model, known as the teacher model (Hoang et al, 2021; Kwak et al, 2022).",
"In this context, confirmatory bias refers to the student model tendency to reproduce the predictions of the teacher model, even when the teacher predictions are incorrect.",
"This can happen when the student model is trained to mimic the teacher model output too closely, without considering the underlying errors.",
"To mitigate this, active distillation techniques use sample selection methods that encourage the student model to learn from data points where the teacher model makes errors, rather than just reproducing the teacher model predictions.",
"In the more general active learning framework, confirmation bias might also refer to the tendency of an active learning algorithm to select examples that confirm its current hypothesis, rather than selecting examples that would challenge or improve it.",
"Finally, graph-based methods construct a graph on all available data and fit a supervised model, where the loss comprises a supervised loss and a regularization term that penalizes the difference between the labels predicted for connected data points.",
"In the online active learning scenario, the graph structure can be used to model the similarity between data points, and the active learning algorithm can select the examples to label based on their position on the graph, such as selecting examples that are in low-density regions or are distant from other labeled examples.",
"3 Online active learning approaches In this review, we present a taxonomy of online active learning strategies into four categories: 1.",
Stationary data stream classification approaches.,
"These methods are designed to tackle online classification tasks, where the model is updated on the fly using newly labeled examples selected from a stream of data that does not change significantly over time.",
"These methods are particularly useful in scenarios where the data distribution is relatively stable, such as quality control in industrial processes, where stationarity is often ensured by control actions taken at regular intervals and continuous maintenance of the components of the 7 system (Bisgaard and Kulahci, 2011).",
"Another example is represented by human activity recognition using wearable devices, where data is collected over time from wearable devices such as fitness trackers to identify patterns of activity like walking, running, or sleeping.",
"This scenario would fall into this category because the data stream is relatively stable, and the model can be updated in real-time as new labeled examples become available (Miu et al, 2015).",
Drifting data stream classification approaches.,
These online active learning strategies are specifically designed to handle classification tasks in dynamic environments where the data distribution constantly changes.,
These approaches are designed to adapt to changes in the data distribution in order to maintain high classification accuracy.,
Some real-world applications might be fraud detection or intrusion detection.,
"In financial fraud detection, fraudsters often change their methods to evade detection, so a classification model used for fraud detection must be able to adapt to new patterns of fraud as they emerge or to new customer habits (Zhang et al, 2022).",
"In real-time intrusion detection, computer networks detection systems must be able to detect new forms of cyberattacks as they appear, so the classification models used must be able to adapt to changes in the data distribution over time (Nixon et al, 2021).",
"This scenario would fall into this category because the data stream is constantly changing, and the model must be able to adapt to changes in the data distribution over time to maintain high accuracy.",
Evolving fuzzy system approaches.,
"These approaches are based on a type of fuzzy system that can adapt and change over time, in response to new data or changes in the environment (Gu et al, 2023).",
"In traditional fuzzy systems, the rules and membership functions that define the system are fixed and do not change over time.",
"Evolving fuzzy systems, on the other hand, are able to adapt their rules and membership functions based on new data or changes in the environment.",
"This is particularly useful in applications where the data or the environment is non-stationary and evolves over time, such as in control systems for autonomous vehicles, where we must be able to adapt to changes in the environment, such as traffic patterns, road conditions, and weather (Naranjo et al, 2007; Wang et al, 2015).",
Experimental design and bandit approaches.,
"These methods, mostly related to regression models, actively select the most informative data points to improve model predictions.",
This category includes online active linear regression and sequential decision-making strategies like bandit algorithms or reinforcement learning.,
These methods adaptively select the most promising options in a given situation.,
"An example is given by online advertising, where a model is used to select the most promising advertisements to display to users based on their browsing history and other factors (Avadhanula et al, 2021).",
This scenario would fall into this category because the model must adaptively select the most promising options in real-time based on the information available at that time.,
"Also, in clinical trials, a model is used to select the most promising patients to enroll in a clinical trial based on their medical history and other personal information.",
"Finally, in drug development studies (R´eda et al, 2020), online active learning can be used to select the most promising compounds for further testing and development, based on their potential efficacy and safety.",
This categorization provides a comprehensive overview of the different types of online active learning strategies and how they can be applied in various scenarios.,
"While the simplest active learning strategy, random sampling, is available and involves selecting data points randomly from the stream for annotation, we will primarily focus on more specialized strategies designed to address scenarios where informed decisions are crucial due to resource constraints or where the data distribution is non-stationary.",
Figure 5 depicts a general framework illustrating the essential components shared by the various categories of online active learning algorithms.,
The accompanying callouts highlight key options utilized by these methods.,
The following sections will provide an in-depth analysis of these strategies.,
"For a more detailed flowchart regarding the drift detection and adaptation process, please refer to Lu et al (2018); Lima et al (2022).",
"3.1 Stationary data stream classification approaches In online active learning, a commonly employed strategy is to request labels for data points that are considered to be informative enough based on a pre-determined threshold.",
"This threshold can be established through a variety of techniques, depending on the instance selection criterion used to evaluate the informativeness of the unlabeled observations.",
"Another method, sometimes referred to as b-sampling, is to calculate the probability that a data point will be queried by adjusting the parameter of a Bernoulli random variable, as proposed by Cesa-Bianchi et al.",
"in one of the pioneering studies on online active learning (Cesa-Bianchi et al, 2004, 2006).",
"They used a linear predictor characterized by the weight vector w ∈Rd and, at each time step t, after observing the current data point xt, the binary output y ∈{−1, +1} is predicted using byt = SGN  w⊤ t−1xt  (1) 8 Instance evaluation Data stream Drifting data?",
Model update Yes No Model Instance selection Sampling strategy - Single pass - Batch - Uncertainty - Diversity - … - Thresholding - 𝑏-sampling - Single model - Ensemble Drift detection - Re-training - Incremental - Batch-mode - Adjust sampling rate - Replace model - Change ensemble weights - … - Classification - Regression - … Drift adaptation - DDM - ADWIN - … Fig.,
5 Online active learning: general framework.,
"where wt−1 is the weight vector estimated with the previously seen labeled examples (x1, y1) , .",
", (xt−1, yt−1).",
"The value w⊤ t−1xt is the margin, bpt, of wt−1 on the instance xt.",
"If the learner queries the label yt, a new weight vector is estimated using the newly added labeled example (xt, yt) with the regular perceptron update rule (Rosenblatt, 1958) as in wt = wt−1 + Mtytxt (2) where Mt represents the indicator function of the event byt ̸= yt.",
"If the label is not requested, the model remains unchanged, and we have wt = wt−1.",
"At each time step t, the learner decides whether to query the label of a data point xt by drawing a Bernoulli random variable Zt ∈{0, 1}, whose parameter is given by Pt = b b + |bpt| (3) where b > 0 is a positive smoothing constant that can be tuned to adjust the labeling rate.",
"In general, as bpt approaches 0, the sampling probability Pt converges to 1, suggesting that the labels are requested for highly uncertain observations.",
"The sampling scheme introduced by Cesa-Bianchi et al (2004) is referred to as selective sampling perceptron, and it is reported in Algorithm 1.",
"A similar approach to the one proposed by Cesa-Bianchi et al (2004) was investigated by Dasgupta et al (2005), who presented one of the first thresholding techniques for online active learning.",
"They suggested setting a threshold on the margin, with the idea of sampling data points xt with a value of |bpt| lower than a given threshold Γ.",
The threshold is initially set at a high value and iteratively divided by two until enough misclassifications occur among the queried points.,
The linear classifier is updated using the reflection concept [60] to give more focus to recent data points.,
Sculley (2007) built on the works of Cesa-Bianchi and Dasgupta to analyze the online active learning scenarios for real-time spam filtering.,
"The author compares two models, a perceptron and a support vector machine (SVM), and tries three different instance selection criteria, the fixed thresholding approach by Dasgupta et al (2005), the Bernoulli-based approach by Cesa-Bianchi et al (2004), and a newly developed logistic margin sampling.",
"The perceptron is updated as per Dasgupta et al (2005), while the SVM is retrained on all available labeled observations each time a new data point is added.",
"According to the logistic margin sampling strategy, the sampling decision is taken by drawing a Bernoulli random variable Zt ∈{0, 1} with a parameter given by Pt = e−γ|bpt| (4) As in the traditional b-sampling approach introduced by Cesa-Bianchi et al (2004), this sampling strategy depends on the uncertainty, meant as the distance from the prediction hyperplane.",
"The main difference between the two strategies is the shape of the resulting sampling distribution, which can be observed in Figure 6.",
"9 Algorithm 1 Selective sampling perceptron Require: a data stream S, an initial model w0 = (0, .",
", 0)⊤, a time horizon T, a sampling budget B, a parameter b. t ←1 ▷Timestamp c ←0 ▷Labeling cost while c ≤B, t ≤T do Observe an incoming data point xt ∈S and set bpt = w⊤ t−1xt Predict the label byt = SGN (bpt) Draw a Bernoulli random variable Zt of parameter Pt = b/ (b + |bpt|) if Zt = 1 then ▷Sampling decision Ask for the true label yt and update the model c ←c + 1 ▷Pay for the label else Discard xt end if t ←t + 1 end while Fig.",
"6 Shape of the sampling distributions for b-sampling (a) and logistic sampling (b), for different values of b and γ.",
"The selective sampling perceptron approach has also been investigated by Lu et al (2016), who proposed an online passive-aggressive active learning variant of the algorithm.",
"Similarly to the b-sampling approach, at each time step t, a Bernoulli random variable Zt ∈{0, 1} is drawn to decide whether to query the label of the current data point xt or not.",
"In this case, the parameter of Zt is given by Pt = δ δ + |bpt| (5) where δ ≥1 is a smoothing parameter.",
"Besides not allowing the smoothing parameter to assume a value lower than 1, the sampling distribution is the same as the one governed by the parameter in Equation 3.",
The main difference lies in the passive-aggressive approach used for updating the weight vector.,
"Indeed, while the traditional perceptron update, shown in Equation 2, only uses misclassified examples to update the model, the passive- aggressive approach updates the weight vector w ∈Rd whenever the current loss ℓt (wt−1; (xt, yt)) is nonzero (Crammer et al, 2006).",
"The new parameter wt is found using wt = wt−1 + τtytxt (6) where τt represents the step size, and can be computed according to three different policies 10 τt =        ℓt (wt−1; (xt, yt)) / ∥xt∥2 min  κ, ℓt (wt−1; (xt, yt)) / ∥xt∥2 ℓt (wt−1; (xt, yt)) /  ∥xt∥2 + 1/2κ  (7) where κ is a penalty cost parameter.",
"Passive-aggressive algorithms are known for their aggressive approach in updating the model, which is motivated by the fact that traditional perceptron updates might waste data points that have been correctly classified but with low prediction confidence.",
"A related issue to the update of the weight vector wt was emphasized by Bordes et al (2005), who noted that always picking the most misclassified example is a reasonable sampling strategy only when the training examples are highly confident.",
"When dealing with noisy labels, this strategy could lead to the selection of misclassified examples or examples lying on the wrong side of the optimal decision boundary.",
"To address this, they suggested a more conservative approach that selects examples for updating wt based on a minimax gradient strategy.",
"In addition to confidence in the labels of the training examples, confidence in the model itself must be considered when the sampling strategy is based solely on model predictions.",
"Hao et al (2018b) pointed out that a margin-based sampling strategy may be suboptimal when the classifier is not precise, especially in the early rounds of active learning when the model performance may be poor due to limited training feedback, leading to misleading sampling decisions.",
"This issue is also referred to as cold-start active learning (Houlsby et al, 2014; Yuan et al, 2020; Jin et al, 2022).",
"To address this, Hao et al (2018b) propose considering second-order information in addition to margin value when deciding whether or not to query the label of a data point xt.",
"In general, first-order online active learning strategies only consider the margin value, while second-order methods also take into account the confidence associated with it.",
"To do this, they assume that the weight vector of the classifier w ∈Rd is distributed as w ∼N(µ, Σ) (8) where the values µi and Σi,i encode the model knowledge and confidence in the weight vector for the ith feature wi.",
"The covariance between the ith and jth features is captured by the term Σi,j.",
"The smaller the variance associated with the coefficient wi, the more confident the learner is about its mean value µi.",
The objective of the proposed method is to take into account the confidence of the model when updating the model and making the sampling decision.,
"With regards to the model update, when the true label yt of xt is queried, the Gaussian distribution in Equation 8 is updated by minimizing an objective function based on the Kullback- Leibler divergence (Joyce, 2011) to ensure the updated model is not too different from the previous one.",
"The sampling decision uses an additional parameter to the margin bpt, which is defined as ct = −η 2  1 νt + 1 γ  (9) where η, γ > 0 are two fixed hyper-parameters and νt represents the variance of the margin related to the data point xt.",
"The intuition is that, when the variance νt is high, the model has not been sufficiently trained on instances similar to xt, and querying its label would lead to a model improvement.",
"Then, a soft margin-based approach is employed by computing ρt = |bpt| + ct (10) If ρt ≤0, the label is always queried as the model is extremely uncertain about the margin.",
"Instead, when ρt > 0, the model is more confident, and the labeling decision is taken by drawing a Bernoulli random variable of parameter Pt = δ δ + ρt (11) where δ > 0 is a smoothing parameter.",
"Finally, Hao et al (2018b) also introduced a cost-sensitive variant of the loss function, for dealing with class-imbalanced applications.",
"For a comprehensive discussion on imbalanced data stream analysis, please see Aguiar et al (2023).",
"The cold-start issue related to the application of active learning to imbalanced datasets has also been high- lighted by Qin et al (2021), who used extreme learning machines (Huang et al, 2006) and extended the active learning framework initially proposed by Yu et al (2015) to the multiclass classification scenario.",
"They highlighted the challenge of the lack of instances for certain classes in imbalanced datasets, which can seriously impact the predictive ability of the model for those classes.",
"To address this issue, they propose a sampling strategy that 11 considers both diversity and uncertainty.",
The diversity is calculated by computing pairwise Manhattan distance between the unlabeled observations.,
The uncertainty of a data point xt is computed by taking the difference between the largest two posterior probabilities as in margin (xt) = p (y = cb | xt) −p (y = csb | xt) (12) where cb and csb are the classes with the highest posterior probabilities.,
"This approach is also referred to as best-versus-second-best margin and, as highlighted by Joshi et al (2009), is a good indicator of uncertainty when a large number of classes are present in the data.",
It should be noted that the sampling strategy introduced by Qin et al (2021) is not suited for single-pass active learning as it requires computing similarity and uncertainty measures for all the unlabeled observations in the current batch.,
"Another approach to deal with class imbalance in active learning was proposed by Ferdowsi et al (2013), who used linear SVMs and a sampling strategy that switches between multiple instance selection criteria online.",
"This approach, however, is limited to a pool-based setting and requires predicting an unsupervised evaluation score for all available unlabeled instances.",
"The impact of the last queried observations on the scores associated with the unlabeled data points is evaluated, and a greedy approach is used to decide which instance selection criterion to trust.",
"SVMs have also been used by Ghassemi et al (2016), who proposed a differentially private approach to online active learning.",
"The privacy concerns are tackled both during the instance selection and the training phase, by randomizing the strategy introduced by Tong and Koller (2002).",
"The informativeness of a data point xt is measured by its closeness to the current hyperplane wt as in c(t) = exp (−d (xt, wt)) ∈[0, 1] (13) where the distance function d (xt, wt) is defined as d (xt, wt) ≜|⟨wt, xt⟩| ∥wt∥ (14) In the traditional framework, the label yt is queried if we have c(t) > Γ, where Γ is a pre-defined threshold.",
"It should be noted that c(t) > Γ is equivalent to d (xt, wt) ≤log 1/Γ, which means that the observation xt is in a sampling region of width 2 log 1/Γ around wt.",
"However, to avoid a deterministic decision process on the labeling and ensure privacy, some randomness needs to be introduced.",
This can be done in two ways.,
"First, the labeling decision can be modeled as a Bernoulli random variable of parameter p if c(t) < Γ or (1 −p) if c(t) ≥Γ, where p < 1/2.",
Another approach is based on the exponential mechanism introduced by McSherry and Talwar (2007).,
"According to this strategy, the algorithm sets a constant probability of labeling data points within a sampling region defined by α, and a decaying probability for points outside of it.",
"The selection strategy is represented by a Bernoulli of parameter q(t) = ( e−αϵ/∆ d (xt, wt) ≤α e−d(xt,wt)ϵ/∆ d (xt, wt) > α (15) where ϵ > 0 and ∆= (1−α/M)M. The authors assumed all data points belonging to the stream to be bounded in norm by M, ∥xt∥≤M for t = 1, .",
", T. To tackle the privacy concerns while training, the authors propose two mini-batch strategies, to avoid the problem of slow convergence that may result from introducing noise according to the private stochastic gradient descent scheme (Bassily et al, 2014; Song et al, 2013; Duchi et al, 2013).",
Two different approaches have been proposed by Ma et al (2016) and Shah and Manwani (2020).,
Ma et al (2016) proposed a query-while-learning strategy for decision tree classifiers.,
"They used entropy intervals extracted from the evidential likelihood to determine the dominant attributes, which are ordered based on the information gain ratio.",
"When a new data point xt is observed, its label is queried only if there does not exist a dominant attribute.",
This will help to identify one and narrow the entropy interval.,
"However, it should be noted that the authors consider a query while learning framework that only partially relates to to online active learning.",
Shah and Manwani (2020) investigated the online active learning problem for reject option classifiers.,
"Given the high cost that is sometimes associated with a misclassification error, these models are given the option of not predicting anything, for example when dealing with a highly ambiguous instance.",
"A typical application of reject option classifiers is in the medical field, when making a diagnosis with ambiguous symptoms might be particularly difficult.",
"In this case, it could be more beneficial not to provide a prediction but suggest further tests instead.",
"They proposed an approach based on a non-convex double ramp loss function ℓdr (Manwani et al, 2013), where the label of the current example xt is queried only if it falls in the linear region of the loss given by |ft (xt)| ∈[ρt −1, ρt + 1], which is the region where the parameter would be updated.",
"Here, ρ refers to the bandwidth parameter of the reject option classifier that determines the rejection region.",
12 Fujii and Kashima (2016) investigated the problem of Bayesian online active learning.,
They provided a general framework based on policy-adaptive submodularity to handle data streams in an online setting.,
"The authors distinguish between the stream setting, where the labeling decision can be made within a given timeframe, and the secretary setting, introduced in Section 2, where the labeling decision must be made immediately.",
"The proposed framework can be applied in a variety of active learning scenarios, such as active classification, active clustering, and active feature selection.",
"The framework is based on the concept of adaptive submodular maximization, which extends the idea of submodular maximization.",
"A set function is considered to be submodular if it satisfies the property of diminishing returns, meaning that adding an element to a smaller set has a greater impact on the function value than adding the same element to a larger set.",
"Adaptive submodular maximization allows the model to adapt to the changing distribution of data over time, by adjusting the set function to reflect the current state of knowledge.",
This leads to more efficient use of available data and improved performance.,
"So far, we discussed several single model approaches to active learning, which have shown promising results in various applications.",
"However, it is important to note that single models have their limitations and can sometimes struggle to capture complex patterns and diverse representations present in the data.",
"To address these limitations, researchers have proposed the use of ensembles or committees as an alternative (Krawczyk et al, 2017).",
An ensemble or committee refers to a group of multiple models that collaborate to produce a more robust and accurate prediction by combining their individual predictions.,
"The models in an ensemble or committee can be trained on different subsets of the data or with varying hyperparameters, and the final prediction is typically made through either voting or weighted averaging.",
"Ensembles or committees can also be regarded as a collection of models that work together to make a prediction, either by exchanging information or learning from one another.",
"Among this class of methods, a common sampling strategy is represented by disagreement-based active learning.",
A framework to perform disagreement-based active learning in online settings was recently introduced by Huang et al (2022).,
"They characterized the learner by a hypothesis space H of Vapnik-Chervonenkis (VC) dimension d, which is composed of all the classifiers currently under consideration, and a Tsybakov noise model (Mammen and Tsybakov, 1999; Tsybakov, 2004).",
"Each classifier h ∈H is a measurable function mapping the observation xt to binary output yt = {0, 1}.",
"The disagreement among two classifiers is given by d (h1, h2) = P [h1(x) ̸= h2(x)] and the disagreement region is defined as D (h1, h2) = {x ∈X : h1(x) ̸= h2(x)} (16) The online active learning strategy is represented by the policy π = ({vt} , {λt}), where {vt}t≥1 is the map of the queried data points, and {λt}t≥1 is the sequence of prediction rules.",
"Over the time horizon T, the performance of the policy π is evaluated using the label complexity and the regret.",
"The label complexity measures the expected number of labels queried, with respect to the stochastic process induced by π, and it is given by E[Q(T)] = E "" T X t=1 1 [vt = 1] # (17) The regret, on the other hand, represents the expected number of excess classification errors with respect to h∗, and it is obtained as E[R(T)] = E   X t≤T :vt=0 1 [λt ̸= yt] −1 [h∗(xt) ̸= yt]   (18) The objective of the algorithm is to minimize the label complexity with a constraint on the regret.",
"At the first round, the initial version space is the entire hypothesis space H, while the initial region of disagreement is the whole input space X.",
"Then, at time step t, the learner updates the version space Ht using the M collected labels, and computes a new region of disagreement as D (Ht) = {x ∈X : ∃h1, h2 ∈Ht, h1(x) ̸= h2(x)} (19) If xt ∈D (Ht), then the label of the current data point is queried, otherwise a prediction is produced using an arbitrary classifier in Ht.",
"At the end of the iteration t, the set Zt of M collected labeled examples is used to estimate the empirical error ϵZt(h) of the classifiers in H and identify the best currently available classifier.",
"Then, the version space is updated by removing all the suboptimal hypotheses whose empirical error exceeds the one obtained with h∗ t by a threshold ∆Zt (h, h∗ t ).",
The threshold regulates the trade-off between reducing label complexity by narrowing the region of disagreement and increasing the regret by eliminating good classifiers.,
"The disagreement concept was also used by Desalvo et al (2021), while proposing an approach to online active learning for binary classification tasks based on surrogate losses.",
"The overall framework is similar to 13 the disagreement-based one used by Huang et al (2022), with the main difference being the use of weak-labels to optimize the sampling strategy.",
"At each time step t, the learner observes the unlabeled data point xt and either decides to request its label or assigns a pseudo-label byt.",
"Then, the pseudo labels byt and the true labels yt processed so far are used together to obtain an estimate of the empirical risk ϵSt(h), where St is obtained by combining the collected labeled examples Zt with the pseudo-labeled ones b Zt.",
"This represents an example of combining active learning and semi-supervised learning, as highlighted in Section 2.3.",
Loy et al (2012) presented a Bayesian framework that leverages the principle of committee consensus to bal- ance exploration and exploitation in online active learning.,
"The aim of exploration is to discover new, previously unknown classes, while exploitation focuses on refining the decision boundary for known classes.",
"To address the issue of unknown classes, the framework uses a Pitman-Yor Processes (PYP) prior model (Pitman and Yor, 1997) with a Dirichlet process mixture model (DPMM).",
A DPMM is a non-parametric clustering and classification model that models the data generating process using a mixture of probability distributions.,
"Each data point is assigned to a cluster, which is associated with a probability distribution over the classes.",
"The number of clusters is modeled using a Dirichlet process, which is a distribution over distributions that allows for an infinite number of clusters but ensures that the number of actual clusters is always finite.",
"At each time step t, the learner samples two random hypotheses h1 and h2 from the model.",
"Then, it computes the posterior probability of the current class c corresponding to k, p (c = k | xt), for each of the two hypotheses.",
"Finally, hi (xt) = arg max p (c | xt) is calculated for i = 1, 2.",
"The label of the current data point is queried in two cases: first, if h1 (xt) ̸= h2 (xt), meaning the two hypotheses disagree, and second, if hi (xt) = K + 1∀i, where K is the number of currently known classes, meaning the current data point belongs to a new class.",
"The DPMM has also been used by Mohamad et al (2020), who proposed a semi-supervised strategy for performing active learning in online human activity recognition with sensory data.",
"To account for the possibility of dealing with different sensor network layouts, the authors proposed pre-training a conditional restricted Boltzmann machine (Taylor and Hinton, 2009; Taylor et al, 2006) and used it to extract generic features from the sensory input.",
"The instance selection strategy follows a Bayesian approach, in trying to minimize the uncertainty about the model parameters.",
"To assess the usefulness of labeling the data point xt, they measure the discrepancy between the model uncertainty computed from the data observed until the time step t and the expected risk associated with yt.",
This gives a hint of how the current label would impact the current model uncertainty.,
A dynamically adaptive threshold Γ is finally used to the determine whether the current expected risk is greater than the current risk.,
A different kind of committee has been considered by Hao et al (2018a).,
"They proposed a framework for minimizing the number of queries made by an online learner that is trying to make the best possible forecast, given the advice received from a pool of experts.",
"To do so, they adapted the exponentially weighted average forecaster (EWAF) and the greedy forecaster (GF) to the online active learning scenario.",
A comprehensive analysis of forecasters to perform prediction with expert advice can be found in the book by Cesa-Bianchi and Lugosi (2006).,
"In general, at each time step t, the learner or forecaster has access to the predictions for the data point xt made by the N experts, fi,t (xt) : Rd →[0, 1] with i = 1, .",
", N. Based on these predictions, it outputs its own prediction pt for the outcome yt.",
"Then, if the label is revealed, the predictions made by the forecaster and the experts are scored using a nonnegative loss function ℓ.",
"The objective of the learner is to minimize the cumulative regret over the time horizon T, which can be seen as the difference between its loss and the one obtained with each expert i as in Ri,T = T X t=1 (ℓ(pt, yt) −ℓ(fi,t (xt) , yt)) = bLT −Li,T (20) The most simple approach to obtain a prediction pt from the learner is to compute a weighted average of the experts predictions as in pt = PN i=1 ωi,tfi,t (xt) PN i=1 ωi,t (21) where ωi,t ≥0 is the weight assigned at time t to the ith expert.",
"With the EWAF, the weight for the ith expert are obtained using ωi,t = eηRi,t−1 PN i=1 eηRi,t−1 (22) where η is a positive decay factor and Ri,t−1 is the cumulative loss of expert i observed until step t. The exponential decay factor η determines the weight given to the past losses, with more recent losses having a higher 14 weight and older losses having a lower weight.",
"Instead, the GF works by minimizing, at each time step, the largest possible increase of the potential function for all the possible outcomes of yt.",
"The potential function is the function that assigns a potential value to each expert, which captures the quality of an expert advice based on its past performance.",
Hao et al (2018a) extended the EWAF and GF by proposing the active EWAF (AEWAF) and active GF (AGF).,
"The key idea is that, while the standard EWAF and GF assume the availability of the true label yt after each prediction, in the online active learning framework the loss ℓcan only be measured a limited number of times.",
"To factor this in, a binary variable Zt ∈{0, 1} is introduced to decide whether or not at round t the label is requested.",
"Consequently, the cumulative loss suffered by the ith expert on the instances queried by the active forecaster is given by bLi,T = T X t=1 ℓ(fi,t (xt) , yt) · Zt (23) The sampling strategy is based on the determination of a confidence condition on the difference between the prediction pt of the fully supervised forecaster and the prediction bpt made by the active forecaster.",
"For the active forecaster we have that bpt = π[0,1] (pt), where pt depends on the chosen model.",
"The AEWAF is based upon the observation that if we have max 1≤i,j≤N |fi,t (xt) −fj,t (xt)| ≤δ (24) then |pt −bpt| ≤δ, where δ is a tolerance threshold.",
This means that the prediction of the forecaster is close to the one obtained in the fully supervised setting if the maximum difference of advice between any two experts is not too large.,
"This assumption might not hold in the presence of noisy or bad experts and, to tackle this problem, the authors proposed a robust variant of the AEWAF.",
"The AGF uses instead a confidence condition based on the fact that if max 1≤i,j≤N |fi,t (xt) −pt| ≤δ (25) then |pt −bpt| ≤δ.",
The general scheme for performing online active learning with expert advice is reported in Algorithm 2.,
"Algorithm 2 Online active learning with expert advice Require: a data stream S, a loss function ℓ, a time horizon T, a set of N experts, a tolerance threshold δ, a sampling budget B. t ←1 ▷Timestamp c ←0 ▷Labeling cost while c ≤B, t ≤T do Observe an incoming data point xt ∈S Receive advide by experts {fi,t (xt) : i = 1, .",
", N} Generate prediction pt for the label yt and set bpt = π[0,1] (pt) Draw a Bernoulli random variable Zt of parameter Pt = b/ (b + |bpt|) if Equation 24 or 25 is satisfied then ▷Sampling decision Discard xt else Ask for the true label yt c ←c + 1 ▷Pay for the label end if t ←t + 1 end while A similar framework, in conjunction with multiple kernel learning (MKL), has been investigated by Chae and Hong (2021).",
They propose an active MKL (AMKL) algorithm based on random feature approximation.,
"In general, online MKL based on random feature approximation is a method for online learning and prediction that combines multiple kernel functions to improve the performance of a learning algorithm (Jin et al, 2010; Hoi et al, 2013).",
"In MKL, multiple kernel functions are used to capture different aspects of the data, and the optimal combination of kernels is learned from the data.",
"The online version of MKL based on random feature approximation is designed to handle data that arrives sequentially, and the learning algorithm is updated after 15 each new data point.",
"In kernel-based learning, the target function f(x) is assumed to belong to a reproducing Hilbert kernel space (RKHS).",
In the proposed AMKL the learner uses an ensemble of N kernel functions.,
"At each time step t, two main steps are implemented.",
"First, each kernel function ˆfi,t (xt) , with i = 1, .",
", n, is optimized independently of the other kernel functions.",
This is referred to as local step.,
"Then, in the global step, the learner seeks the best function approximation bft (xt) by combining the N kernel functions as in bft (xt) = N X bvi,t ˆfi,t (xt) (26) where bvi,t refers to the weight for the ith kernel function at round t. Similarly to the case with expert advice, the weights are determined by minimizing the regret over the time horizon T, which is defined as the difference between the loss of the learner and the one obtained with the best kernel function f ∗ i,t.",
"To do so, the weights are computed based on the past losses ℓas bωi,t = exp  −ηg X τ∈At−1 ℓ  ˆfi,τ (xτ) , yτ    (27) where ηg > 0 is a tunable parameter and At−1 is an index of time stamps t indicating the instances for which has label has been requested, thus permitting to measure the loss.",
"Then, the weights bvi,t are obtained from bωi,t as follows bvi,t = bωi,t PN i=1 bωi,t (28) Finally, the instance selection criterion is based on a confidence condition, denoted by with δ > 0, on the similarity of the learned kernel function, which is a similar to the condition used by Hao et al (2018a) in the formulation of the AEWAF max 1≤j≤N N X i=1 bvi,tℓ  bfi,t (xt) , bfj,t (xt)  ≤δ (29) 3.2 Drifting data stream classification approaches Active learning strategies belonging to this category aim to tackle online classification tasks in time-varying data streams affected by distribution shifts.",
"We can classify distribution shifts into three main categories, depending on whether they concern the feature space x or the output dimension y.",
"A shift that only affects the input distribution p(x), and not the conditional distribution p(y | x), is referred to as covariate shift (Zhou et al, 2021; Wu et al, 2021; Li et al, 2021) or virtual drift (Baier et al, 2021).",
"In these circumstances, for two different time steps, ti and ti+∆, we have that pti(x) ̸= pti+∆(x) and pti(y | x) = pti+∆(y | x), meaning that the underlying model is not being altered by phenomena like class swaps or coefficient changes.",
"Conversely, in the presence of a real concept drift (Baier et al, 2021; Su´arez-Cetrulo et al, 2023), the conditional distribution changes, and we have pti(y | x) ̸= pti+∆(y | x).",
"In this scenario, the predictive performance of the fitted model dramatically deteriorates, and a model update or replacement becomes necessary.",
"An example of this kind of distribution shift can be identified in the changes of the consumer behaviors over time, or following a major event as the COVID-19 pandemic (Zwanka and Buff, 2021).",
"However, it should be noted that virtual drifts and real concept drifts often occur together (Tsymbal et al, 2008), leading to a situation where we have both pti(x) ̸= pti+∆(x) and pti(y | x) ̸= pti+∆(y | x) (Lu et al, 2018).",
"Lastly, we can incur in a label distribution shift (Wu et al, 2021) when the shift only affects p(y), leading to pti(y) ̸= pti+∆(y).",
This situation can be observed in many real-world scenarios where the target distribution changes over time.,
"A typical example is the prediction of diseases like influenza, whose distribution can dramatically change depending on the season, or in the presence of sudden outbreaks.",
"Another key characteristic of distribution shifts is represented by the change rate, namely how fast the new concept or distribution is introduced into the data stream.",
"To this extent, we can identify four kinds of drifts (Lu et al, 2018; Lima et al, 2022), which are illustrated in Figure 7.",
"A sudden or abrupt drift is a drift that can be immediately detected from two consecutive time steps, ti and ti+1.",
It refers to a sudden and clearly identifiable change in the data distribution.,
"An example of this would be a sudden change in the weather, which would affect the behavior of customers at a retail store.",
"The change is noticeable, and the model needs to be updated immediately.",
"A gradual drift exhibits a transition phase, where a mixture or overlap between the two distributions pti and pti+∆exists.",
"In this case, the change is slower and more difficult to detect, making it 16 challenging to update the model.",
"An example would be a change in consumer behavior over time, which is hard to detect but can have a significant impact on a business.",
"Another type of drift is the incremental drift, which has an extremely low transition rate, which makes it very difficult to detect changes between the data points observed in the transition period.",
"This type of drift is often caused by changes in the data generating process that happen gradually over time, in small steps rather than all at once.",
"An example would be changes in the types of products that are popular among customers, which happen gradually and are hard to detect.",
"Finally, a data stream can also be affected by recurring concepts, which sequentially alternate over time.",
"An example would be a retail store where the same types of products are popular at different times of the year, such as winter coats and summer dresses.",
The model needs to be able to detect and adapt to these recurring concepts in order to maintain good performance.,
(a) (b) (c) (d) 𝑡 𝑡 𝑡 𝑡 C1 C2 C1 C1 C1 C2 C2 C2 Fig.,
"7 Different types of drifts that can affect the data stream: abrupt drift (a), gradual drift (b), incremental drift (c), recurring concepts (d).",
C1 and C2 indicate the two concepts that might characterize the data distribution.,
"In online active learning for drifting data streams, some approaches address the presence of concept drifts by combining active learning strategies with drift detectors (Zhang et al, 2020a; Krawczyk et al, 2018).",
Drift detectors are algorithms that try to detect distribution shifts and identify when the context is changing.,
"They can be divided into three macro-categories (Lu et al, 2018).",
"The first group of methods is represented by the error-based drift detectors, which try to detect online changes in the error rate of a base classifier.",
"Among these, one of the most commonly employed strategies is the drift detection method (DDM) proposed by Gama et al (2004).",
Another popular approach is the adaptive window (ADWIN) strategy proposed by Bifet and Gavald`a (2007).,
"The second class of drift detectors is called data distribution-based drift detection, and the third class is represented by multiple hypothesis testing strategies.",
"While the first class contains the majority of the proposed approaches, it assumes that we are able to observe the labels of all the incoming data points to assess the error rate.",
"Instead, the last two classes could be implemented even in an unsupervised manner.",
An exhaustive overview on unsupervised drift detection methods has been proposed by Gemaque et al (2020).,
"While the unsupervised nature of the data distribution-based and multiple hypothesis testing strategies make them ideal for the active learning scenario, it should be noted that real concept drifts can hardly be detected in a completely unsupervised fashion.",
"Indeed, in a circumstance when the input distribution p(x) remains unaltered while the underlying model relating the input variables x to the label y changes, it would not be possible to detect the change of concept without collecting labels.",
This is why Krawczyk et al (2018) propose to apply an error-based drift detector to the few labels collected during the online active learning routine.,
"To this extent, they use the ADWIN (Bifet and Gavald`a, 2007) method to detect drifts and decide when the current model needs to be updated or replaced.",
The proposed general framework for dealing with online active learning with drifting data streams is reported in Algorithm 3.,
"Moreover, the authors proposed the use of a time-variable threshold to balance the budget use over time.",
"Their approach is based on the intuition that, when a new concept is introduced, more labeling effort will be required to quickly collect representative observations belonging to the new concept and replace the outdated model.",
This is obtained by adjusting a time-variable threshold to balance the budget use over time.,
"Given a threshold Γ on the uncertainty of the classifier and a labeling rate adjustment r ∈[0, 1], the threshold is reduced to Γ −r when ADWIN raises a warning and to Γ −2r when a real drift is detected.",
"Thus, when allocating the labeling budget, the key requirement is that the labeling rate employed when a drift is detected should be strictly larger than the one used in static conditions.",
"A similar thresholding idea has also been used by Castellani et al (2022), who proposed an active learning strategy for non-stationary data streams in the presence of verification latency.",
"They used a piece-wise constant budget function, where the labeling rate α is increased to αhigh when 17 Algorithm 3 Online active learning with drifting data streams Require: a data stream S, a classifier Θ, a drift detector Θ, a sampling strategy Υ, a labeling rate α, a sampling budget B. t ←1 ▷Timestamp c ←0 ▷Labeling cost while c ≤B and t ≤|S| do Observe incoming data point xt ∈S if Υ(xt) = True then ▷Sampling decision Ask for the true label yt c ←c + 1 ▷Pay for the label Update classifier Ψ with the labeled example (xt, yt) Update drift detector Θ with the labeled example (xt, yt) if drift warning = True then Start to train a new classifier Ψnew Increase labeling rate α else if drift detected = True then ▷A detection is always preceded by a warning Replace C with Cnew Further increase α else Return to initial labeling rate α end if end if if Cnew exists then ▷Keeps being updated in the background until replacement Update classifier Cnew with the labeled example (xt, yt) end if end if t ←t + 1 end while a drift is detected and, after a while, reduced to αlow.",
"Finally, the labeling rate is restored to its nominal value α.",
A visual representation of the labeling approach is shown in Figure 8.,
"The length of the time segments where the labeling rate is altered depends on the desired values for αhigh and αlow, constraining the overall labeling rate to be equal to α. 𝑡 𝛼 𝑡!",
"𝑡"""" 𝛼&'( 𝛼)#*) Fig.",
8 Piece-wise constant budget function introduced by Castellani et al (2022).,
"The sampling rate α is increased to αhigh when a drift is detected (tdrift), then reduced to αlow between tr1 and tr2, before being restored to its nominal value.",
The authors also tackled the verification latency issue by considering the spatial information of a queried point for which the label has not been made available yet by the oracle.,
"In this way, it is possible to avoid oversampling from regions where many close points have a high utility, namely a low classification confidence.",
"While assessing the utility of the incoming data points the authors use real and pseudo-labels by propagating the information contained in the already labeled observations, as suggested by Pham et al (2022).",
"The idea is to 18 use the spatial information of the queried labels by estimating the still missing labels with a weighted majority vote of the label of its k-nearest neighbors labels, where the weight for each nearest neighbor depends on the arrival time of the labels.",
The verification latency issue in online active learning with drifting data streams was also extensively analyzed by Pham et al (2022).,
"Consider the general case where at time tx i we draw an instance xi, and find it interesting enough to send it to the oracle, which will send back the label yi only at time ty i , where ty i > tx i .",
"Before the requested label arrives, we might encounter another instance similar to xi and ask again for its label, since the learner could not update its utility function or threshold.",
"Similarly, we might use outdated information when updating the policy in a future window.",
"To tackle these issues, the authors propose a forgetting and simulating strategy to avoid using soon-to-be outdated observations and prevent redundant labeling.",
The instance selection is based upon the variable uncertainty strategy proposed by Zliobaite et al (2014) and the balanced incremental quantile filter by Kottke et al (2015).,
"If we denote the current sliding window at time tx n as Wn = [tx n −∆, tx n) and use windows of fixed size ∆, we know that the sliding window that would be used for training when the label yn related to xn arrives will be given by Dn = [ty n −∆, ty n).",
The forgetting step is then implemented by discarding outdated labeled examples that are included in Wn but will not be included in Dn.,
"If ai is a Boolean variable indicating whether the ith observation has been labeled, the set of instances selected to be forgotten is given by On = [(xi, yi) ∀i < n : ai = 1 ∧tx i , ty i ∈Wn\Dn] .",
"(30) Similarly, there is a second set of observations, with time stamps D+ n = Dn\Wn = [tx n, ty n), where there might be instances that have been queried but whose label is not currently available.",
"To avoid losing such information and redundantly asking for the label of similar instances, the algorithm simulates incoming labels with a bagging approach by averaging across multiple utility estimations.",
They also consider an alternative simulation approach based on fuzzy labeling.,
"Similarly to Krawczyk et al (2018), the ADWIN drift detector has also been used by Zhang et al (2020a) while proposing a method for dealing with online active learning in environments characterized by concept drifts and class imbalance.",
"The instance selection criterion is based on the predictive uncertainty, which they estimate using the best-versus-second-best margin value (Equation 12), as they tackle a multi-class classification problem.",
An initial pool of n observations is passively collected from the stream to initialize the active learning strategy.,
"Then, a threshold Γi is estimated for each class as in Γi = ( nm niL n niL ≥1 m n niL < 1 (31) where i = 1, .",
", L is the number of classes and m is a pre-defined constant used to control the size of the threshold.",
"The model is represented by an ensemble of N classifiers and, when ADWIN detects a concept drift, the classifier with the higher error is replaced with a newly trained one.",
"Finally, the class imbalance issue is also taken into account in two ways, during the training of the ensemble with the use of class-specific weights, and during the active learning routine, by dynamically adjusting the threshold to select more observations belonging to the minority class.",
"Recently, Cheng et al (2023) presented another approach to combine online active learning with drift detec- tion.",
Their method involves segmenting the data stream S into fixed-length chunks and then detecting drifts by comparing the distributions of adjacent chunks.,
"After a drift is detected, a multi-objective optimization problem is formulated in order to identify the most relevant and diverse data points within the current batch.",
"For a data point xt, relevance is defined as its contribution to the new concept, and diversity as the Pearson correlation coefficient with other instances in the same region.",
"Instead, Martins et al (2023) proposed to sample the most uncertain data points from each chunk, using a meta-learning framework to fine-tune the threshold used for each window.",
This allows to reduce the need for labels while maintaining a steady adaptation to the new concepts.,
Another window-based approach to perform active learning from data streams has been proposed by Zhu et al (2007).,
The authors developed an ensemble E by partitioning the data stream S into chunks and then training each of the k models composing the ensemble E on a different chunk of data.,
"In this way, even if the previous observations become unavailable, the models can be used when taking the sampling decision in order to take into account a global uncertainty measure, which is a more robust approach than treating each chunk as a static dataset.",
"At time step t, the learner receives a data chunk St, which is used to build the current classifier Ct. At this point, the ensemble is composed by Ct, together with the most recent k −1 classifiers, Ct−k+1, .",
", Ct−1, trained on the labeled examples sampled from the previously observed data chunks, Lt−k+1, .",
"At each iteration, the objective is to predict the remaining unlabeled data points from the current chunk, Ut.",
The ensemble-based active learning framework is depicted in Figure 9.,
"The instances selected to be queried are the ones with the 19 largest ensemble variance, and the predictions are obtained by combining the predictions of the single classifiers using the weights ωt−k+1, .",
"Finally, a weight updating rule is used to adapt to dynamic data streams.",
Ensemble 𝐸 Classifier 𝐶!,
"""#""$ Classifier 𝐶!",
"""# Classifier 𝐶!",
Predict … 𝜔!,
9 Ensemble-based active learning framework for data streams proposed by Zhu et al (2007).,
Shan et al (2019) and Zhang et al (2018) developed online active learning strategies by building upon the pairwise classifiers strategy introduced by Xu et al (2016).,
"The pairwise strategy makes use of two models, a stable classifier Cs and a dynamic classifier Cd, and divides the data stream into batches as in (Zhu et al, 2007).",
"The prediction for an incoming data point xt is obtained with a weighted average of the predictions obtained from the two classifiers as in fE (xt) = ωsfCs (xt) + ωdfCd (xt) (32) where ωs and ωd are the weights associated with the stable and the dynamic classifier, respectively.",
"At time t, the stable classifier Cs is trained on the labeled portions of all the batches processed so far, L1, .",
"Conversely, Cd is trained exclusively on Lt−1.",
"The key idea is that whenever the reactive classifier starts to outperform the stable classifier, the stable classifier is replaced by the reactive one, which is eventually reset.",
"This replacement allows the learner to adapt to the drift and focus on the most recent instances, forgetting the seemingly obsolete data points.",
The main drawback of this approach is that it cannot effectively address gradual drifts as the replacement with the classifier trained on the most recent observations makes the learner forget about observations away from the current window.,
"Hence, similarly to the approach of Zhu et al (2007), Shan et al (2019) proposed an extension of this approach, based on an ensemble of classifiers in trying to contemporarily address gradual drifts and abrupt drifts.",
"In their strategy, the stable classifier learns from all the labeled instances and the reactive classifier is replaced by an ensemble of dynamic classifiers, trained on multilevel sliding windows to capture changes in the data stream at different time intervals.",
"The instance selection approach combines random sampling and uncertainty sampling, where the latter is based on the margin value of the predictions obtained by the ensemble.",
It should be noted that the prediction fE for the data point xt is obtained as a weighted combination of the predictions obtained with the stable and dynamic classifiers as in fE (xt) = ωsfCs (xt) + D X d=1 ωdfCd (xt) (33) The stable classifier has a constant weight ωs = 0.5 and plays a crucial role in trying to learn the overall trend and direction of concept drift.,
"Conversely, the dynamic classifiers have gradually decaying weights, according to a damped sliding winding approach where each weight is initialized at 1 D and then reduced according to its creation time ωd =  ωd  1 −1 D  d = 1, .",
", D −1 1 D d = D (34) The most recent classifiers are useful in detecting sudden concept drifts and have highest weights while the old dynamic classifiers have lower weights and can help to identify gradual drifts.",
The same pairwise strategy based on an ensemble composed by a stable classifier and D dynamic classifiers was used by Zhang et al (2022).,
They modified the original strategy by introducing a reinforcement mechanism to adjust the weights ωd according to the prediction performance and the class imbalance issue.,
The weights adjustment strategy is described by 20 Algorithm 4.,
It should be noted that this procedure is only implemented after the true label yt has been revealed by the oracle.,
The damped class imbalance ratio (DCIR) value is obtained by taking into account the number of observations for each class collected so far.,
This is expected to be useful when dealing with imbalanced classes.,
"With regards to the instance selection criterion, the authors consider a hybrid strategy combining uncertainty sampling and random sampling, since approaches solely based on uncertainty could ignore a concept change that is not close to the boundary.",
Wo´zniak et al (2023) recently proposed another ensemble-based active learning strategy where the data points to be labeled are selected from the current chunk using the budget labeling active learning strategy introduced by Zyblewski et al (2020).,
"According to this approach, the learner selects both random and informative data points, where the informativeness is determined using the support function threshold, which in the case of binary classification problems can be interpreted as a distance from the decision boundary.",
"Algorithm 4 Weight adjustment for dynamic classifiers Require: a labeled observation (xt, yt), number of classes K, number of dynamic classifiers D, current weights ωd with d = 1, .",
", D, DCIR for each class DCIRκ for κ ∈K.",
"if DCIR [yt] < 1 K then ▷Check if it belongs to the minority class for d in (1, D) do if Cd(xt) = yt then ▷Check if the prediction made by Cd is correct ωd ←ωd  1 + 1 D  ▷Increase weight of classifier Cd else ωd ←ωd  1 −1 D  ▷Decrease weight of classifier Cd end if end for end if Another way to perform online active learning in time-varying data streams is to use clustering-based approaches.",
Halder et al (2023) extended the framework based on stable and dynamic classifiers by introduc- ing a clustering step that aims to train the new stable classifier Cs on the most informative and representative instances from each data block.,
"Similarly, Ienco et al (2013) investigated a clustering-based approach in a batch- based scenario, where only a fraction of the incoming block of observations can be labeled.",
"They extend the pre-clustering approach (Nguyen and Smeulders, 2004), which had been previously studied in the pool-based scenario, to the stream-based case.",
"The sampling strategy takes into account an extra-cluster metric, to sort the clusters, and an intra-cluster one, to sort the observations within each cluster.",
"When a new batch arrives, observations are clustered, and clusters are sorted based on the homogeneity of the clusters, which is measured taking into account the number of (predicted) classes within each cluster.",
"If a cluster is balanced in the number of expected classes, it is regarded to as an uncertain cluster that covers a more difficult area of the input space.",
"Within each cluster, the certainty of an observations is determined by its representativeness, namely the distance from the centroid, and the uncertainty, meant as the maximum a posterior probability among all the predicted classes for xt.",
"When the clusters and observations are ranked, the learner starts to iteratively ask the observa- tions label in an alternate fashion.",
"To sample the most representative data points from each batch, Zhang et al (2023) suggested the use of density-peak clustering and recognize the incomplete clusters in the dynamic feature space through the altitude of these data points.",
This allows to query the observations belonging to those regions in the following iterations.,
"Recently, Yin et al (2023) proposed an adaptive data stream classification method based on microclustering.",
"After initializing micro-clusters from the initial training data, they collected new labels using a mixed strategy that combines random sampling with a class-weighted margin score.",
"Then, the micro-cluster learning model is dynamically updated to adapt to the presence of concept drifts.",
"Another approach that tries to exploit the clustering nature of the incoming observations has been proposed by Mohamad et al (2018), with the use of bi-criteria active learning algorithm that considers both density in the input space and label uncertainty.",
"The density-based criterion makes use of the growing Gaussian mixture model proposed (GGMM) by Bouchachia and Vanaret (2014), which is used to find clusters in the data and estimate its density.",
"This model creates a new cluster when a new data point xt has a Mahalanobis distance greater than a given closeness threshold from the nearest cluster, among the currently available ones.",
A flowchart describing the main steps of the GGMM is depicted in Figure 10.,
A Bayesian logistic regression model is used for addressing the label uncertainty criterion and the concept drift.,
"As the classifier parameters wt are assumed to evolve over time, the model is incrementally updated using 21 Observe an unlabeled data point Gaussian matches?",
Yes No Update parameter of the Gaussian Compute the probability of match with each Gaussian Decay the weight of all Gaussians Remove less contributing Gaussian Initialize new Gaussian with the current data point Fig.,
10 Main steps of the growing Gaussian mixture model used by Mohamad et al (2018).,
"a discrepancy measure, which is computed as the difference between the uncertainty of the model in xt before and after the true label yt is added to the training set.",
"The query strategy follows the b-sampling approach, in trying to sample, with high probability, the observations that contribute the most to the current error.",
"The combination of density and uncertainty is also employed by Liu et al (2021), who proposed a cognitive dual query strategy for online active learning in the presence of concept drifts and noise.",
The local density measure is used to obtain representative instances and the uncertainty criterion aim to select data points where the classifier is less confident.,
"The cognitive aspect takes into account Ebbinghaus’s law of human memory (Ebbinghaus, 2013) to determine an optimal replacement policy.",
The proposed strategy tries to tackle both gradual and abrupt drifts.,
"The drift is generally considered as a change in the underlying joint probability distribution from one time step t to another, namely pt(x, y) ̸= pt+1(x, y).",
"The local density of an observation xt is defined by the number of times that xt is the nearest neighbor of other instances (Ienco et al, 2014).",
"Since we are in an online framework, the authors proposed to measure the local density using a sliding window model, referred to as a cognition window.",
"Based on the concept of memory strength, the model determines when the current window is full and needs to be updated.",
"Finally, the labeling decision is taken by using two thresholds, one for the local density and one for the classifier uncertainty.",
A different sliding window-based online active learning strategy is the one proposed by Kurlej and Wo´zniak (2011).,
The authors proposed a sliding window approach based on a nearest neighbors classifier.,
"The reference set for the k-nearest neighbors model is a window, and it is updated in two ways: in a first-in-first-out manner or using the examples selected by the active learning strategy.",
"Since the reference set is updated over time, this method can effectively deal with concept drift and time-varying data streams.",
The sampling strategy is also based on two criteria.,
"The first one is similar to the margin-based approaches, an instance is queried if it has a low distance from two observations belonging to different classes.",
"The second criterion, similar to the greedy sampling strategy, seeks observations that have a large minimum distance from the observations in the current reference set.",
Both criteria are implemented by setting a threshold on the distances.,
A simpler approach for taking into account the time-varying aspect of evolving data stream is to force the model to focus on the most recent observations.,
"Along these lines, Chu et al (2011) propose a framework based on a Bayesian probit model and a time-decay variant.",
"Online Bayesian learning is used to maintain a posterior distribution of the weight vector of a linear classifier over time wt, and the time-decay strategies are employed to tackle the concept drift and give more importance to recent observations.",
"They also propose an online approximation technique that can handle weighted examples, which is based upon Minka (2001).",
"They tested different sampling strategies, built upon an online probit classifier.",
"The instance selection criteria are based on entropy, function-value, and random sampling.",
"3.3 Evolving fuzzy systems approaches An alternative way to take into account the time-varying nature of evolving data streams is the use of evolving fuzzy systems (EFS) (Lughofer, 2011), which are soft computing techniques that can efficiently deal with novelty and knowledge expansion.",
"EFS are self-developing, self-learning fuzzy rule-based or neuro-fuzzy systems that self-adapt both their parameters and their structure on-the-fly.",
They try to mimic human-like reasoning by modeling it with a dynamically developing fuzzy rule-based structure and implementing it utilizing data streams using a formal learning process.,
The basic rule structure of a fuzzy model is given by 22 Rulei : if (x1 is Xi1) and .,
"and (xn is Xin) then (yi = ai0 + ai1x1 + · · · + ainxn) (35) where Rule i with i = 1, 2, .",
", R is one of several fuzzy rules in the current rule base; xj(j = 1, 2, .",
", n) are input variables; yi denotes the output of the ith fuzzy rule; Xij denotes the jth prototype (focal point) of the ith fuzzy rule; aij denotes the jth parameter of the ith fuzzy rule.",
"For a more thorough discussion on EFS and their use in online learning, please see (Lughofer, 2017, 2011; Ge and Zeng, 2020; Gu et al, 2022).",
The main components of an EFS are shown in Figure 11.,
"The two key components of an EFS are the structure evolving scheme, which contains the rule generation and simplification modules, and the parameters updating scheme.",
The rule generation module defines when a new rule needs to be added to the current model.,
The rule merging and pruning steps simplify the models by removing redundant rules and combining two rules when their similarity is larger than a given threshold.,
The parameter updating modules are used to keep track of the model evolution.,
"These learning modules are used to update the EFS every time a new labeled example (xt, yt) is made available.",
Structure evolving Rule generation Rule merging Rule pruning Parameters updating Antecedent Consequent Fig.,
"11 Learning modules of an EFS (Ge and Zeng, 2020).",
The first single-pass active learning approach based on the use of evolving classification models has been proposed by Lughofer (2012).,
"The proposed algorithm is based on two key concepts, conflict and ignorance.",
"The former is related to an incoming data point lying close to the boundary between any two classes; the latter considers the distance of the incoming observation from the currently labeled training set, in the feature space.",
This suggests that the data point falls within a region that has not been thoroughly explored by the learner.,
"Later on, Lughofer and Pratama (2018) also proposed the first online active learning approach for evolving regression models.",
"Similarly to their previous work (Lughofer, 2012), the authors consider the ignorance about the input space in the instance selection criterion.",
"Moreover, they also consider the uncertainty in the model outputs and in the model parameters.",
The predictive uncertainty is assessed in terms of confidence intervals using locally adaptive error bars.,
"The error bars are inspired by (ˇSkrjanc, 2009) and the authors propose a new merging approach for dealing with the case of overlapping fuzzy rules.",
"The uncertainty in the model parameters is instead evaluated using the A-optimality criterion, which will be discussed in Section 3.4 together with other alphabetic optimality criteria.",
"Instead of leveraging the uncertainty about the output, Pratama et al (2015) set a dynamic threshold based on the variable uncertainty strategy introduced by Zliobaite et al (2014) while trying to address the what-to-learn question in the training of a recurrent fuzzy classifier.",
"The key idea is that the model is iteratively retrained using data points that fall within rules with low support, which were formed using the smallest amount of observations.",
"Recently, Lughofer and ˇSkrjanc (2023) proposed an online active learning strategy for fuzzy models based on three criteria.",
"• D-optimality in the consequent space to reduce parameter uncertainty, as in Cacciarelli et al (2022b).",
• Overlap degree in the antecedent space to reduce the number of data points lying in the overlap regions of two different rules.,
"• Novelty content in the antecedent space, indicating the required knowledge expansion through rule evolution.",
"A different kind of threshold, based on the spherical potential theory, has been suggested by Subramanian et al (2014), with the proposal of a meta-cognitive component that evaluates the novelty content of incoming data points.",
"This is done using a knowledge measure represented by the spherical potential, which has been thoroughly investigated in kernel-based approaches (Hoffmann, 2007).",
The spherical potential is used to set a threshold and decide whether to add a new rule to capture the knowledge in the current sample.,
"It should be 23 noted that the authors also used a threshold based on the prediction error, which could not be used with scarcity of labels.",
"The prediction error is assessed using the hinge loss error function (Suresh et al, 2008; Zhang, 2004).",
Fuzzy models have also been used to solve computer vision tasks.,
"Weigl et al (2016) analyze the visual inspection quality control case, which is also considered by Roˇzanec et al (2022).",
"They assess the usefulness of the images in a single-pass manner, but the instances that are selected to be queried are accumulated in a buffer, which is later on assigned to an oracle for labeling.",
Choosing the size of the buffer represents a trade-off problem between timely updating the classifier and requiring continuous interventions from a human annotator.,
The active learning strategy works by setting a threshold on the certainty of the model with regards to the incoming data points.,
"The authors take into account two model classes, a random forest classifier and an evolving fuzzy classifier.",
"When using random forest, certainty is computed using the best-versus-second-best margin score.",
"Instead, when using evolving fuzzy classifiers, the sample selection criterion takes into account the conflict and ignorance concepts as in Lughofer (2012).",
"Finally, Cernuda et al (2014) combine the use of fuzzy models with a sampling approach inspired by the multivariate statistical process control literature.",
"Indeed, using a latent structure model, they propose a query strategy based on the Hotelling T 2 and the squared prediction error (SPE) statistics, which have been extensively used in anomaly detection problems (Cacciarelli and Kulahci, 2022; Gajjar et al, 2018; Vanhatalo and Kulahci, 2016; Vanhatalo et al, 2017).",
Ge (2014) used these statistics for pool-based active learning in conjunction with a principal component regression model.,
The key idea is to use the Hotelling T 2 and the SPE statistics to measure the distance between the currently labeled training set and a new unlabeled data point.,
"A high value in one of the two statistics would most likely suggest that the new observation is violating the current model, and thus its inclusion in the training set could bring some valuable information.",
"Similarly, Cernuda et al (2014) use the Hotelling T 2 and the SPE statistics with a partial least squares model.",
"Then, when a new observation is added to the training set, they retrain a TS fuzzy model using a sliding window approach.",
"3.4 Experimental design and bandit approaches Optimal experimental design (Karlin and Studden, 1966) is a research field that is closely related to active learning.",
"It deals with the design of experiments that allow for efficient estimation of model parameters or improved prediction performance while minimizing the number of required labeled examples, also referred to as the number of runs N. Many optimality criteria have been developed in thriving to strike a balance between efficient use of resources and ensuring good performance of the model.",
"The traditional framework of optimal experimental designs focuses on linear regression models of the form y = Xβ + ε (36) where, given d input variables, y is a N × 1 vector of response variables, X is a N × d model matrix, β is a d × 1 vector of regression coefficients, and ε is a N × 1 vector representing the noise, with covariance matrix σ2I.",
"If the matrix X⊤X is of full rank, an ordinary least square (OLS) estimator for β can be obtained using bβ =  X⊤X −1 X⊤y (37) In general, design optimality criteria leverage the information contained in the moment matrix, which is defined as M = X⊤X/N.",
"The matrix X⊤X plays a crucial role in the estimation of the model coefficients β, and it is important to perceive information about the design geometry.",
"Indeed, with Gaussian noise characterized by ε ∼N  0, σ2I  , we know that bβ | X ∼N  β,  X⊤X −1 σ2 (38) and we can define a 100(1 −α)% confidence ellipsoid related to the solutions of β using (b −bβ)⊤ X⊤X  (b −bβ) ds2 ≤Fα,d,N−d (39) where s2 represents the residual mean square, Fα,d,N−d is the 100(1 −α) percentile derived from the Fisher distribution, and b indicates all the possible vectors that could be the true model parameter β.",
"The ellipsoid can also be expressed as (b −bβ)⊤ X⊤X  (b −bβ) ≤C, where C = ds2Fα,d,N−d.",
"The volume of this ellipsoid is inversely proportional to the square root of the determinant of X⊤X, and the length of its axes is proportional to 1/λi, where λi represents the ith eigenvalue of X⊤X, with i = 1, .",
", d. The so-called alphabetic optimality criteria pursuit efficient designs by exploiting these properties (Kiefer, 1959).",
"The most commonly employed optimality criteria for good parameter estimation are A-, D- and E-optimality: 24 • A-optimality.",
This criterion pursues good model parameter estimation by minimizing the sum of the variances of the regression coefficients.,
"Knowing that the coefficients variances appear on the diagonal of the matrix  X⊤X −1, it can be shown that an A-optimal design is given by a design D∗that satisfies minD tr[M(D)]−1 = tr [M (D∗)]−1.",
• D-optimality.,
"This criterion takes into account both the variance and covariance of the regression coefficients, directly minimizing the total volume of the confidence ellipsoid (Myers et al, 2016).",
"A D-optimal design is given by a design D∗that satisfies maxD |M(D)| = |M (D∗)| (John and Draper, 1975).",
• E-optimality.,
This strategy tries to shrink the ellipsoid by minimizing the maximum eigenvalue of the covariance matrix.,
"The geometrical intuition behind these criteria is illustrated, in the two-dimensional case, in Figure 12.",
"𝑏"" ""𝛃 (b) 𝑏!",
"𝑏"" ""𝛃 (c) 𝑏!",
"12 Confidence ellipsoid around the model parameters and optimality criteria: A-optimality (a) shrinks the hyperrectangular enclosing the confidence ellipsoid (Asprey and Macchietto, 2002; Galvanin, 2010), D-optimality (b) aim to shrink the total volume of the ellipsoid, and E-optimality (c) tries to reduce the length of the longest axis (Jamieson, 2018).",
"Finally, there are also optimality criteria that focus on developing models with good predictive properties.",
"Within this class, G-optimality represents a criterion that is used to seek protection against the worst-case prediction variance in a region of interest R. This is achieved by solving min D  max x∈R v(x)  (40) where v(x) represents the scaled prediction variance of the current model in the data point x, which can be computed as v(x) = Nx(m)T  X⊤X −1 x(m) (41) where x(m) represents the data point where the variance is being estimated, expanded to the model form.",
"It should be noted that G-optimality can be highly influenced by anomalous observations, as it protects against the highest possible variance over all the region R. This issue can be tackled by using I- or V-optimality, which estimate the overall prediction variance over R by integrating or averaging, respectively.",
"For a more extensive discussion on optimal designs, please see Montgomery (2012) or Myers et al (2016).",
"The use of optimality criteria has proven to be highly beneficial in offline experimental design, allowing practitioners to pre-determine the location of each design point with ease.",
"However, these methods require modification to be applied in a stream-based scenario where data points arrive sequentially.",
A common approach for obtaining a near-optimal design with streaming observational data is represented by thresholding.,
"Riquelme (2017) proposed a thresholding algorithm for online active linear regression, which is related to the A-optimality criterion.",
"Their approach uses a norm-thresholding algorithm, where only observations with large, scaled norms are selected.",
The design is augmented with the observations x whose norm exceeds a threshold Γ given by P(∥x∥≥Γ) = α (42) where α is the ratio of observations we are willing to label out of the incoming data stream.,
"Another approach related to the A-optimality criterion was proposed by Fontaine et al (2021), who studied online optimal design under heteroskedasticity assumptions, with the objective of optimally allocating the total labeling budget between 25 covariates in order to balance the variance of each estimated coefficient.",
Cacciarelli et al (2022b) further extended the thresholding approach introduced by Riquelme (2017) by proposing a conditional D-optimality (CDO) algo- rithm.,
"The terms conditional refers to the fact the design is marginally optimal, given an initial set of labeled observations to be augmented.",
The main steps of the CDO approach are reported in Algorithm 5.,
The authors exploited the connection between D-optimality and prediction variance previously highlighted by Myers et al (2016).,
The sampling strategy selects observations by setting a threshold Γ given by P  x⊤ t  X⊤X −1 xt ≥Γ  = α (43) where X is the current set of labeled observations and xt is the data point that is currently under evaluation.,
"The threshold is estimated using kernel density estimation (KDE) on a set of j unlabeled observations, which are taken passively from the data stream without querying any label.",
"This provides an initial set of data, referred to as warm-up set, that can be used to estimate the covariance matrix and the threshold.",
"Algorithm 5 Online active learning using CDO Require: an initial random design X, a data stream S, a warm-up length j, a sampling rate α, a budget B t ←1 ▷Timestamp c ←0 ▷Labeling cost Set W = ∅ ▷Warm-up set to estimate Σ and Γ while t ≤j do Observe incoming data point xt ∈S Select xt : W = W ∪xt t ←t + 1 end while Estimate the covariance matrix Σ of W and perform eigendecomposition Σ = UΛU⊤ Whiten the initial design by computing Z = Λ−1/2U⊤X Whiten the warm-up observations by computing V = Λ−1/2U⊤W Estimate Γ using KDE on V with the desired sampling rate α using Equation 43 with Z and V while c ≤B and t ≤|S| do Observe incoming data point xt ∈S Whiten xt by computing zt = Λ−1/2U⊤xt if z⊤ t (Z⊤Z)−1zt ≥Γ then Ask for the label yi and augment the labeled dataset: Z ←Z ∪zt c ←c + 1 ▷Pay for the label Update threshold Γ to measure the prediction variance of the enlarged design else Discard xt end if t ←t + 1 end while Cacciarelli et al (2023) also investigated how the presence of outliers affect the performance of online active linear regression strategies.",
"They showed how the design optimality-based sampling strategies might be attracted to outliers, whose inclusion in the design eventually degrades the predictive performance of the model.",
"This issue can be tackled by bounding the search area of the learner with two thresholds, as in P  Γ1 ≤x⊤ t  X⊤X −1 xt ≤Γ2  = α (44) where the choice of Γ2 represents a trade-off between seeking protection against outliers and exploring uncertain regions of the input space.",
"The norm-thresholding approach was also extended by Riquelme et al (2017a) to the case where the learner tries to estimate uniformly well a set of models, given a shared budget.",
"This scenario is similar to a multi-armed bandit (MAB) problem where the learner wants to estimate the mean of a finite set of arms by setting a budget on the number of allowed pulls (Ruan et al, 2020; Audibert and Munos, 2010; Jamieson and Nowak, 2014; Soare et al, 2013).",
The authors propose a trace upper confidence bound (UCB) algorithm to simultaneously estimate the difficulty of each model and allocate the shared labeling budget proportionally to these estimates.,
"UCB is a common algorithm used in MAB problems to balance exploration and exploitation (Carpentier et al, 2015; 26 Garivier and Moulines, 2008), which takes into account the predicted mean value and the predicted standard deviation, weighted by an adjustable parameter (Thompson et al, 2022).",
This allows to balance the exploitation of data points with a high predicted value and the exploration of areas with high uncertainty.,
"In general, MAB problems can be seen as a special case of sequential experimental design, where the goal is to sequentially choose experiments to perform with the aim of maximizing some outcome.",
"The typical framework of a MAB problem can be regarded as an optimization problem where the learner must identify the option or arm with the highest reward, among a set of available arms characterized by different reward distributions.",
"Both MAB and active learning paradigms involve a sequential decision-making process where the learner aims to maximize a reward or improve model accuracy by selecting an arm to pull or a data point to label, respectively, and receiving feedback (in the form of a reward or label request) for each selection.",
There are two main approaches to tackle MAB problems: • Regret minimization.,
This approach is coherent with the objective of maximizing the cumulative reward observed over many trials.,
"In this case, the learner must balance exploration, namely trying out different arms to learn more about the reward distributions, with exploitation, i.e., using current knowledge to choose the most promising arm.",
These kinds of algorithms strike a balance between learning a good model and obtaining high rewards.,
"A few examples might be treatment design, online advertising and recommender systems.",
• Pure exploration.,
"In this case, we are interested in finding the most promising arm, with a certain confidence or given a fixed budget on the number of pulls.",
"To do so, the objective is to learn a good model while minimizing the number of measurements or labels required.",
"This scenario is suggested in circumstances where, due to safety constraints, we are not given complete freedom to change the variable levels and we are mostly interested in understanding the underlying model governing the system.",
"Possible examples include drug discovery or soft sensor development (Fortuna et al, 2007; Shi and Xiong, 2018; Chan et al, 2018; Tang et al, 2018).",
"The pure exploration approach is particularly useful when coupled with the study of linear bandits, which are a type of contextual bandit algorithms that assume a linear relationship between the features of the context and the expected reward of each arm.",
"In this type of problem, when an arm x ∈X is pulled, the learner observes a reward r(x) that depends on an unknown parameter θ∗∈Rd according to the linear model r(x) = x⊤θ∗+ ε (45) where ε is a zero-mean i.i.d.",
"This is similar to active linear regression in that, in both cases, the learner aims to select the most informative data points to learn about the underlying model or system (Audibert and Munos, 2010; Jamieson and Nowak, 2014).",
"Soare et al (2014), investigated this problem, in the offline setting, using the G-optimality criterion and a newly proposed XY-allocation algorithm.",
"Jedra and Proutiere (2020) proposed a fixed-confidence algorithm for the same problem, while Azizi et al (2022) analyzed the fixed-budget case, extending the framework to the case where the underlying model is represented by a generalized linear model (Filippi et al, 2010).",
An interesting variant of this problem is presented in the study of transductive experimental designs.,
"A transductive design is a problem where we can pull arms from a set X ∈Rd, with the objective of identifying the best arm or improve the predictions over a separate set of observations Z ∈Rd, which is given, in an unlabeled form, beforehand.",
"A practical example of this case is when we are trying to infer the user preferences over a set of products, but we can only do that by pulling arms from a limited set of free trials.",
"Alternatively, we might be interested in estimating the efficacy of a drug over a certain population, while doing experiments on a population with different characteristics.",
"This problem has been tackled with an active learning approach by Yu et al (2006), with the idea of exploiting unlabeled data points in Z while evaluating the informativeness of the data points in X.",
"The transductive case of sequential experimental design has been explored by Fiez et al (2019), but instead of performing active learning, they were interested in inferring the best reward over Z, only pulling the arms in X.",
"Finally, this has been extended to the online scenario by Camilleri et al (2021), balancing the trade-off between time complexity and label complexity, namely between the number of unlabeled observations spanned and the number of labels queried in order to stop the learning procedure and declare the best-arm.",
"In addition to MAB, reinforcement learning-based approaches can also be applied to active learning in order to optimize a decision-making policy that balances the exploration of uncertain data with the exploitation of information learned from previous observations.",
"This can be particularly useful in applications where the goal is to maximize the expected cumulative reward over time, such as in robotics or game playing.",
"Compared to MAB, reinforcement learning-based approaches offer a more general and flexible framework for active learning, allowing for a wider range of problem formulations and feedback signals (Menard et al, 2021; Fang et al, 2017; Rudovic et al, 2019).",
"One approach to combining active learning and reinforcement learning is through modeling the sampling routine as a contextual-bandit problem, as proposed by Wassermann et al (2019).",
"In this approach, 27 the rewards are based on the usefulness of the query behavior of the learner.",
"The key intuition behind the use of reinforcement learning in online active learning is that the learner gets feedback after the requested label, based on how useful the request actually was.",
"In contrast to the traditional active learning view, where most of the effort is dedicated to the instance selection phase, the learner is penalized ex-post for querying useless instances.",
"The learner gets a positive reward ρ+ if it asks for the label when it would have otherwise predicted the wrong class, and a negative reward ρ−when querying was unnecessary as the model would have predicted the right label.",
"The contextual bandit problem is implemented by building an ensemble of different models, with each expert suggesting whether to query or not based on whether its prediction certainty exceeds a threshold Γ.",
The models are assigned a decision power based on how past suggestions were rewarded and how coherent they were with the other experts’ suggestions.,
"When an observation is sent to the oracle for labeling, the reward is computed, and the objective function of the learner is to maximize the total reward over a time horizon T. Another reinforcement learning-based approach has been proposed by Woodward and Finn (2017).",
They considered the case where at each time step t the learner needs to decide whether to predict the label of the unlabeled data point xt or pay to request its label yt.,
"The reinforcement learning framework is used to find an optimal policy π∗(st) that takes into account the cost of asking for a label and the cost of making an incorrect prediction, where st represents the state that is given in input at the timet to a policy π (st) that outputs the suggested action at.",
The authors approximate the action-value function using a long short-term memory (LSTM) neural network with a linear output layer.,
"The optimal policy is determined by maximizing the long-term reward, after assigning a reward to a label request Rreq, a correct prediction Rcorr, and an incorrect prediction Rinc.",
"It should be noted that Rcorr and Rinc should be negative rewards, as they are associated with costly actions.",
"4 Evaluation strategies The use of active learning approaches is becoming increasingly common in machine learning, allowing models to be trained more efficiently by selecting the most informative examples for labeling.",
"To evaluate the performance of these approaches, it is typical to compare them to a passive random sampling strategy by generating learning curves that plot the model performance (e.g., accuracy, F1 score, or root mean square error) on a holdout test set over the number of labeled examples used for training.",
"Learning curves are a useful tool for comparing the asymptotic performance of different strategies and their sample efficiency, with the slope of the curve reflecting the rate at which the model performance improves with additional labeled examples.",
A steeper slope indicates a more sample-efficient strategy.,
"When multiple sampling strategies are being compared, a visual inspection of the learning curves may not be sufficient, and more rigorous statistical tests may be necessary.",
Reyes et al (2018) recommend the use of non-parametric statistical tests to analyze the effectiveness of active learning strategies for classification tasks.,
"The sign test (Steel, 1959) or the Wilkinson signed-ranks test (Wilcoxon, 1945) can be used to compare two strategies, while the Friedman test (Friedman, 1940), the Friedman aligned-ranks test (Hodges and Lehmann, 1962), the Friedman test with Iman-Davenport correction (Iman and Davenport, 1980), or the Quade test (Quade, 1979) can be used when evaluating more than two strategies.",
These statistical tests can provide insight into whether the difference in performance between the active learning and passive random sampling strategies is statistically significant.,
"Algorithm 6 Prequential evaluation for online active learning Require: an initial model w0, a data stream S, a budget B, an active learning strategy Q. t ←1 ▷Timestamp P ←∅ ▷Storing predictions while c ≤B and i ≤|S| do Observe the data point xt ∈S Predict the label byt and store it: P ←P ∪byt if Q(xt) = True then ▷Sampling decision Ask for the true label yt and update the model c ←c + 1 ▷Pay for the label else Discard xt end if t ←t + 1 end while 28 Overall, the use of learning curves and statistical tests can provide valuable insights into the effectiveness and efficiency of different active learning strategies.",
"By understanding the statistical significance of differences in performance between these strategies, researchers can make informed decisions about which approaches are more effective for a particular task or dataset.",
"Furthermore, the choice of the evaluation scheme is crucial when assessing the performance of active learning approaches.",
"If we use an evaluation scheme based on a holdout test set, at each learning step t the performance of the model is assessed using the same test set.",
"This can be a reasonable approach if we are dealing with a stationary data stream, which does not evolve over time.",
"Under these assumptions, using the same test set we might be able to better assess the prediction improvement as more labeled examples are included in the design.",
"However, this approach might not be ideal when dealing with drifting data streams.",
"In these circumstances, a prequential evaluation scheme can be more useful to monitor the evolution of the prediction error over time (Su´arez-Cetrulo et al, 2021; Cerqueira et al, 2020; Tieppo et al, 2022; Cacciarelli and Boresta, 2021).",
"In online learning, prequential evaluation is also referred to as test-then- train approach, and it involves using each incoming instance first to measure the prediction error, and then to be included in the training set (Su´arez-Cetrulo et al, 2023).",
The main steps of the test-then-train approach are reported in Algorithm 6.,
"The key idea is that at each time step t, we first test the model by making a prediction, then we decide whether to query the true labels and finally we update our model.",
"An in-depth analysis and discussion between the use of a holdout test set and the prequential evaluation scheme for streaming data has been provided by Gama et al (2009, 2013), who suggested the use of a prequen- tial evaluation scheme with forgetting mechanisms.",
"For scenarios with imbalanced data streams, a specialized prequential variant of the area under the curve metric has been proposed by Brzezinski and Stefanowski (2015, 2017).",
"From an implementation perspective, Bifet et al (2010) developed an open source software suite called MOA for data stream mining, which includes both the holdout and prequential strategies.",
"This framework has found widespread application in the evaluation of online active learning strategies, as evidenced by the studies conducted by Liu et al (2021); Shan et al (2019); Weigl et al (2016); Zhang et al (2020a); Alabdulrahman et al (2016).",
"Evaluation Strategy Works Holdout test set Desalvo et al (2021); Wassermann et al (2019); Roˇzanec et al (2022); Narr et al (2016); Ferdowsi et al (2013); Bordes et al (2005); Suzuki et al (2021); Ghassemi et al (2016); Qin et al (2021); Woodward and Finn (2017); Riquelme et al (2017b); Cacciarelli et al (2022b, 2023); Manjah et al (2023) Prequential/Test-then-train Zhang et al (2022); Pham et al (2022); Castellani et al (2022); Chu et al (2011); Zhang et al (2018); Krawczyk et al (2018); Xu et al (2016); Mohamad et al (2020); Weigl et al (2016); Ienco et al (2013); Zhang et al (2020a) Table 1 Evaluation strategies.",
"In Table 1, we categorize the studies based on the experimental protocols they employed to evaluate the sam- pling strategies.",
The table exclusively includes approaches where the evaluation strategy was explicitly defined.,
"In most cases, when assessing active learning methods in the context of drifting data streams, a prequential approach is favored.",
"Conversely, for scenarios where the methods are ill-suited to handle concept drifts, hold- out test sets tend to be the preferred choice.",
"In approaches not featured in the table, the evaluation strategies exhibited some variations or lacked explicit specification.",
"For instance, in the work by Fujii and Kashima (2016), their evaluation strategy involved training models on the queried data and subsequently testing them with the entire dataset.",
"This approach differs from the conventional test-then-train paradigm since, in this case, models are tested on data they encountered during training, at least in part.",
"Another example is found in Zhu et al (2007), who utilized a window-based approach, assessing prediction accuracy across all observations in the cur- rent batch.",
"On a different note, Hao et al (2018a) employed the per-round regret metric, which quantifies the loss difference between the forecaster and the best expert at each iteration of the active learning process.",
"In some instances, none of the previously mentioned methods were employed, as the analysis took a more theoretical per- spective.",
This is exemplified by the works of Dasgupta et al (2005); Chae and Hong (2021); Huang et al (2022).,
"Lastly, bandit algorithms employed a distinct evaluation approach, often aiming to identify the most promising arm with a fixed confidence or budget.",
"In the fixed confidence setting, performance typically hinges on compar- ing label complexity to problem dimensionality or the number of arms pulled, as observed in Fiez et al (2019).",
"Alternatively, regret or error metrics were evaluated against the required number of trials, as demonstrated in the studies by Riquelme et al (2017a); Sudarsanam and Ravindran (2018); Fontaine et al (2021).",
"29 5 Real-world applications and challenges 5.1 Applications Online active learning has been recognized as a powerful technique in scenarios where data is arriving at a high velocity, labeling data is expensive, and it is infeasible to store all the unlabeled data before making a decision about which observations to query to update the model.",
"In particular, these techniques have proven particularly useful in dynamic and ever-evolving environments, where models need to adapt to new data in real-time, by selectively querying the most informative instances.",
"One of the first real-world applications of online active learning has been presented by Sculley (2007), who investigated the scenario of low-cost active spam filtering (Figure 13) where a filter is updated online by selecting the most informative emails in real time.",
Another application of online active learning in the field of IT has been recently presented by Zhang et al (2020a).,
They analyzed the scenario of network protocol identification and proposed a method (presented in Section 3.2) to select the most representative instances on the fly and adapt the model to dynamic data distributions.,
Stream of emails Stream of emails Receive an email Query label?,
Filter Classify email Update filter Yes No Fig.,
"13 Low-cost active spam filtering (Sculley, 2007).",
Computer vision is another interesting area where online active learning can be applied.,
"Deep learning models require a large amount of annotated data, making manual annotation of thousands of images one of the most challenging aspects of model development.",
"However, it is important to note that the most effective deep active learning methods proposed so far are not easily adaptable to a stream-based setting.",
"Many of these methods involve clustering or measuring pairwise similarity among image embeddings (Sener and Savarese, 2017; Agarwal et al, 2020; Ash et al, 2019; Citovsky et al, 2021; Prabhu et al, 2020), which cannot be easily done in a single- pass manner.",
"As a result, most online applications of active learning in computer vision rely on the use of traditional models with uncertainty-based sampling.",
Narr et al (2016) analyze the stream-based active learning problem for the classification of 3D objects.,
"They used a mondrian forest classifier (Lakshminarayanan et al, 2014), which is an efficient alternative of random forest for the online learning scenario, and selected images with high classification uncertainty to be labeled.",
Roˇzanec et al (2022) used online active learning to reduce the data labeling effort while performing vision-based process monitoring.,
"Initially, features are extracted from the images using a pre-trained ResNet-18 model (He et al, 2015) and then, using the mutual information criterion (Kraskov et al, 2004), only √n features (Hua et al, 2005) are retained to fit an online classifier, where n is the total number of observations in the training set.",
"The authors combine a simple active learning strategy based on model uncertainty with five streaming classification algorithms, including Hoeffding tree (Hulten et al, 2001), Hoeffding adaptive tree (Bifet and Gavald`a, 2009), stochastic gradient tree (Gouk et al, 2019), streaming logistic regression, and streaming k-nearest neighbors.",
"Recently, Saran et al (2023) proposed a novel approach to streaming active learning with deep neural networks.",
"Given a neural network with f with parameters θ, last- layer parameters θL, and the cross-entropy function ℓ, they compute the gradient representation of the data point xt, which is given by g(xt) = ∂ ∂θL ℓ(f(xt; θ), byt) (46) where byt = argmax f(xt; θ).",
"Then, the data points to be included in the batch for training the model are chosen by using a probability pt proportional to the contribution of the current example to the covariance matrix of the examples collected so far, as in pt ∝det  bΣt + g(xt)g(xt)⊤ (47) where bΣt is the covariance matrix of the data points that have been selected to be included int he current batch, up to the time step t. 30 Online active learning has also been explored for object detection tasks.",
Manjah et al (2023) proposed a stream-based active distillation (SBAD) framework by combining the concepts of active learning and self- supervision as described in Section 2.3.,
The SBAD framework enables the deployment of scalable deep-learning models as it does not rely on human annotators and takes into account the imperfection of the oracle when distilling knowledge from a large teacher model to a lightweight student.,
"Indeed, the authors suggest setting a threshold on the confidence of the images and only querying images with high confidence in trying to avoid confirmation bias.",
"The threshold is determined using a warm-up phase, similarly to the approach proposed by Cacciarelli et al (2022b) presented in Algorithm 5.",
The SBAD pipeline for model development and evaluation is reported in Figure 14.,
"14 SBAD framework (Manjah et al, 2023): sampling, fine-tuning and evaluation.",
The sampling is performed in a single-pass manner via thresholding.,
The problem of performing active learning for object detection with streaming data has also been explored by Beck et al (2023).,
"In the case of a camera placed on an autonomous vehicle, the collected data encompasses various scenarios, including clear weather, foggy conditions, and rainy weather, all of which require the model to perform effectively.",
"However, the frequency of these scenarios can vary significantly.",
"In situations where one scenario is prevalent, a passive sampling strategy could tend to sample very few examples from the most rare slices.",
"Instead, the proposed streamline approach by attempts to smartly allocate the budget to obtain more observations from the slices where the model is under-performing.",
"The case of autonomous cars was also considered by Yan et al (2023), who used a diversity-based online active learning strategy to reduce false alarm rate and learn unseen faults.",
Another interesting industrial application has been recently presented by Ghiasi et al (2023).,
They proposed a deployable framework that combines a thermodynamics-based compressor model and a Gaussian Process-based surrogate model with an online active learning module.,
The objective of the study was to minimize the power absorbed by the machine during the boil off process of centrifugal compressor.,
"In the proposed framework, the simulator, the surrogate model, and the optimizer interact in real time to determine the new experimental points.",
"5.2 Challenges When applying online active learning strategies to real-world problems, there are several potential issues to consider, including: • Algorithm scalability.",
Online active learning algorithms need to be efficient and scalable to handle large datasets and high-velocity data streams.,
"As the amount of data grows, the computational demands of active learning can become prohibitive, making it difficult to deploy in practice.",
The time required to make the sampling decision needs to be lower than the feed rate of the process being analyzed.,
"If the algorithm is too slow, it may require a buffer, which reduces the benefits of online active learning.",
• Labeling quality.,
"Most online active learning strategies rely heavily on the quality of labeled data, which can be challenging to ensure in real-world scenarios.",
"Human annotators may make errors, introduce biases, or interpret labeling instructions differently.",
"For this reason, in real-life situations, it may be necessary to consider oracle imperfections like in the knowledge distillation case (Baykal et al, 2022).",
"Another difficult aspect related to labeling quality is the delay or latency, which has been described in Section 2.2.3.",
31 • Data drift.,
"In real-world settings, data distributions may shift over time, making it challenging for models to adapt and continue providing accurate predictions.",
"Changes in the data distribution may also affect the quality of the labeled data, as the criteria for selecting informative instances may become less effective.",
Methods from Sections 3.2 and 3.3 should be used when dynamic and ever-changing behaviors are expected.,
• Model interpretability.,
"Besides simply asking for the most informative instances from a modeling perspective, it might be useful to provide additional information on why a particular instance is beneficial for improving the performance of the current model.",
In fields like healthcare and manufacturing this might help practitioners to improve their understanding of the underlying problem.,
• Evaluation.,
"When developing active learning methods from a research perspective, the different query strate- gies are evaluated assuming the ground-truth labels to be available for a held-out test set, or for the data stream being analyzed.",
"However, in real life, the key motivation behind active learning is label scarcity and thus it might be difficult to thoroughly assess the effectiveness of the deployed sampling strategy.",
• Human-computer interaction.,
"In the context of active learning for data streams, the synergy between human labelers and computer systems plays a pivotal role in the labeling process.",
"While the majority of online active learning methods focus on querying the most informative data points in real-time, we can distinguish between two distinct labeling scenarios: 1.",
Real-time annotation.,
"In most of the presented works, it is assumed that labels are immediately available when a data point is queried from the stream.",
"This immediate access to true labels enables an optimized active learning routine, as the model can be promptly updated and can recommend exploration of new regions based on up-to-date information.",
"However, this approach poses some implementation challenges that need to be addressed with the use of advanced data annotation tools (Feuz and Cook, 2013).",
Postponed annotation.,
There are cases where we must allow for a delay between data querying and labeling.,
"For instance, methods that consider verification latency (Castellani et al, 2022; Pham et al, 2022) take into account the possibility of delayed labels.",
This is particularly relevant in situations where a physical quality inspection or medical treatment must occur before the label is revealed.,
"Another example is in the training of deep neural networks, where real-time sampling from a data stream is necessary due to memory constraints (Manjah et al, 2023), but the labeling and model update phase may occur when a batch is collected, following a batch-mode active learning strategy (Ren et al, 2022).",
6 Summary and future directions This survey outlines the challenge of conducting active learning with data streams and investigates different approaches for selecting the most informative data points in real-time.,
"Table 2 provides a summary of the relevant state-of-the-art approaches, highlighting their main properties and settings.",
"Our examination reveals that existing research has predominantly concentrated on creating online classification models, which can operate with both stationary and drifting data streams.",
"However, there has been comparatively limited effort devoted to online active linear regression or dedicated to constructing online regression models in general.",
We believe that there are several promising directions for future research in this field.,
"First, we recommend further investigation into online active learning strategies specifically designed for regression models.",
"Given the limited work in this area, there is a need for more advanced methods that can be applied to nonlinear models, beyond linear models or linear bandits.",
"For example, there has been a recent spark of interest toward the use of Bayesian optimization for active learning in nonlinear regression problems (Mohamadi and Amindavar, 2020; Riis et al, 2022).",
"Additionally, model-agnostic methods that can be applied to a variety of regression models could be valuable as they would provide a more general solution to the problem.",
"Second, we believe that there is potential for research into single-pass online sampling strategies for dynamic data streams.",
"Ensemble models and batch- based approaches have been the dominant methods in online classification, but some of their assumptions or requirements may not hold in many real-world applications.",
"For instance, in some applications, data may arrive in a continuous stream, and it may not be possible to divide it into batches due to time or memory constraints.",
"In such cases, single-pass online sampling strategies that do not require the use or update of multiple models would be more practical.",
"Moreover, it could be beneficial to develop online active learning strategies that are able to tackle all the types of distribution shifts introduced in Section 3.2.",
"Finally, the combination of reinforcement learning and active learning in pool-based scenarios is an area of ongoing research.",
We believe that the study of online reinforcement learning to optimize sampling strategies could provide valuable insights into how to best perform active learning in dynamic environments.,
"32 Data processing Data stream Task Model Work(s) Single-pass Stationary Classification Single Model Cesa-Bianchi et al (2004, 2006); Dasgupta et al (2005); Sculley (2007); Lu et al (2016); Hao et al (2018b); Ghassemi et al (2016); Shah and Man- wani (2020); Mohamad et al (2020); Saran et al (2023); Roˇzanec et al (2022); Wood- ward and Finn (2017) Ensemble Huang et al (2022); Desalvo et al (2021); Loy et al (2012); Hao et al (2018a); Chae and Hong (2021) Regression Single Model Riquelme (2017); Fontaine et al (2021); Cacciarelli et al (2022b, 2023, 2022a) Object detection Single Model Manjah et al (2023) Drifting Classification Single Model Krawczyk et al (2018); Castel- lani et al (2022); Pham et al (2022); Yin et al (2023); Mohamad et al (2018); Liu et al (2021); Kurlej and Wo´zniak (2011); Chu et al (2011) Ensemble Zhang et al (2020a); Shan et al (2019); Zhang et al (2018, 2022) Evolving Classification Single Model Lughofer (2012); Pratama et al (2015) Regression Single Model Lughofer and Pratama (2018); Lughofer and ˇSkrjanc (2023) Batch Stationary Classification Single Model Bordes et al (2005); Qin et al (2021); Fujii and Kashima (2016) Object detection Single Model Beck et al (2023) Drifting Classification Single Model Cheng et al (2023); Martins et al (2023); Ienco et al (2013); Zhang et al (2023); Yan et al (2023) Ensemble Zhu et al (2007); Wo´zniak et al (2023); Halder et al (2023) Evolving Classification Single Model Subramanian et al (2014); Weigl et al (2016); Cernuda et al (2014) Table 2 Online active learning strategies: summary based on data processing capabilities, assumptions about the data stream, task of the model and model characteristics.",
7 Conclusion The field of online active learning with data streams is a rapidly evolving and highly relevant area of research in machine learning.,
"The ability to effectively learn from data streams in real-time is becoming increasingly important, as the amount of data generated by modern applications continues to grow at an exponential rate.",
"However, obtaining annotated data to train complex prediction and decision-making models presents a major roadblock.",
"This hinders the proper integration of artificial intelligence models with real-world applications such as healthcare, autonomous driving and industrial production.",
Our survey provides a comprehensive overview of the current state of the art in this field and highlights the challenges and opportunities that researchers face when developing methods for online active learning.,
"We reviewed a wide range of strategies for selecting the most 33 informative data points in online active learning, including methods based on uncertainty sampling, diversity sampling, query by committee, and reinforcement learning, among others.",
"Our analysis has shown that these strategies have been applied in a variety of contexts, including online classification, online regression, and online semi-supervised learning.",
We hope that this survey will inspire further research in the field of online active learning with data streams and encourage the development of new and advanced methods for handling this type of data.,
"In particular, we believe that there is significant potential for the development of model-agnostic and single-pass online active learning strategies that can be applied in practical settings.",
"Acknowledgments The authors gratefully acknowledge the support of the DTU Strategic Alliances Fund, which made this research possible.",
We would also like to extend our sincere thanks to John Sølve Tyssedal for his invaluable help and support throughout the project.,
"References Agarwal S, Arora H, Anand S, et al (2020) Contextual diversity for active learning.",
"European Conference on Computer Vision 2020 https://doi.org/https://doi.org/10.1007/978-3-030-58517-4 9, URL http://arxiv.org/ abs/2008.05723 Aggarwal CC, Kong X, Gu Q, et al (2014) Data Classification (Chapter: ”Active Learning: A Survey”).",
"Taylor & Francis, URL http://charuaggarwal.net/active-survey.pdf Aguiar G, Krawczyk B, Cano A (2023) A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework.",
"Machine Learning pp 1–79 Alabdulrahman R, Viktor H, Paquet E (2016) An active learning approach for ensemble-based data stream mining.",
"In: International Conference on Knowledge Discovery and Information Retrieval, SCITEPRESS, pp 275–282 Ash JT, Zhang C, Krishnamurthy A, et al (2019) Deep batch active learning by diverse, uncertain gradient lower bounds.",
"2020 International Conference on Learning Representations URL http://arxiv.org/abs/1906.03671 Asprey S, Macchietto S (2002) Designing robust optimal dynamic experiments.",
Journal of Process Control 12:545–556.,
"https://doi.org/10.1016/S0959-1524(01)00020-8 Audibert JY, Munos R (2010) Best arm identification in multi-armed bandits.",
"COLT - 23th Conference on Learning Theory URL http://certis.enpc.fr/∼audibert/Mes%20articles/COLT10.pdf Avadhanula V, Colini Baldeschi R, Leonardi S, et al (2021) Stochastic bandits for multi-platform budget optimization in online advertising.",
"In: Proceedings of the Web Conference 2021, pp 2805–2817 Azizi MJ, Kveton B, Ghavamzadeh M (2022) Fixed-budget best-arm identification in structured bandits.",
"Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22) URL https://www.ijcai.org/proceedings/2022/0388.pdf Baier L, Schl¨or T, Sch¨offer J, et al (2021) Detecting concept drift with neural network model uncertainty.",
"Hawaii International Conference on System Sciences (HICSS) 2023 URL http://arxiv.org/abs/2107.01873 Balcan MF, Broder A, Zhang T (2007) Margin based active learning.",
"COLT - 23th Conference on Learning Theory 4739. https://doi.org/https://doi.org/10.1007/978-3-540-72927-3 5 Bassily R, Smith A, Thakurta A (2014) Private empirical risk minimization: Efficient algorithms and tight error bounds.",
2014 IEEE 55th Annual Symposium on Foundations of Computer Science pp 464–473.,
"https: //doi.org/10.1109/FOCS.2014.56 Baum E, Lang K (1992) Query learning can work poorly when a human oracle is used.",
"Proceedings of the IEEE International Joint Conference on Neural Networks 34 Baykal C, Trinh K, Iliopoulos F, et al (2022) Robust active distillation.",
"URL http://arxiv.org/abs/2210.01213 Beck N, Kothawade S, Shenoy P, et al (2023) Streamline: Streaming active learning for realistic multi- distributional settings.",
"arXiv preprint arXiv:230510643 Bifet A, Gavald`a R (2007) Learning from time-changing data with adaptive windowing.",
Proceedings of the 2007 SIAM International Conference on Data Mining pp 443–448.,
"https://doi.org/10.1137/1.9781611972771.42 Bifet A, Gavald`a R (2009) Adaptive learning from evolving data streams.",
IDA 2009: Advances in Intelligent Data Analysis VIII pp 249–260.,
"https://doi.org/10.1007/978-3-642-03915-7 22 Bifet A, Holmes G, Pfahringer B, et al (2010) Moa: Massive online analysis, a framework for stream classification and clustering.",
"In: Proceedings of the first workshop on applications of pattern analysis, PMLR, pp 44–50 Bisgaard S, Kulahci M (2011) Time series analysis and forecasting by example.",
"John Wiley & Sons Bordes A, Ertekin S, Weston J, et al (2005) Fast kernel classifiers with online and active learning.",
The Journal of Machine Learning Research 6.,
"URL https://jmlr.csail.mit.edu/papers/v6/bordes05a.html Bouchachia A, Vanaret C (2014) Gt2fc: An online growing interval type-2 self-learning fuzzy classifier.",
IEEE Transactions on Fuzzy Systems 22:999–1018.,
"https://doi.org/10.1109/TFUZZ.2013.2279554 Brzezinski D, Stefanowski J (2015) Prequential auc for classifier evaluation and drift detection in evolving data streams.",
"3rd International Workshop on New Frontiers in Mining Complex Patterns, (NFMCP 2014) pp 87–101.",
"https://doi.org/10.1007/978-3-319-17876-9 6 Brzezinski D, Stefanowski J (2017) Prequential auc: properties of the area under the roc curve for data streams with concept drift.",
Knowledge and Information Systems 52:531–562.,
"https://doi.org/10.1007/ s10115-017-1022-8 Burbidge R, Rowland JJ, King RD (2007) Active learning for regression based on query by committee.",
"8th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2007 https://doi.",
"org/10.1007/978-3-540-77226-2 22 Cacciarelli D, Boresta M (2021) What drives a donor?",
a machine learning-based approach for predicting responses of nonprofit direct marketing campaigns.,
"International Journal of Nonprofit and Voluntary Sector Marketing https://doi.org/10.1002/nvsm.1724 Cacciarelli D, Kulahci M (2022) A novel fault detection and diagnosis approach based on orthogonal autoen- coders.",
"Computers & Chemical Engineering 163:107853. https://doi.org/10.1016/j.compchemeng.2022.107853 Cacciarelli D, Kulahci M (2023) Hidden dimensions of the data: Pca vs autoencoders.",
"Quality Engineering pp 1–10 Cacciarelli D, Kulahci M, Tyssedal J (2022a) Online active learning for soft sensor development using semi- supervised autoencoders.",
"ICML 2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World URL https://arxiv.org/abs/2212.13067 Cacciarelli D, Kulahci M, Tyssedal JS (2022b) Stream-based active learning with linear models.",
"Knowledge-Based Systems 254:109664. https://doi.org/10.1016/j.knosys.2022.109664 Cacciarelli D, Kulahci M, Tyssedal JS (2023) Robust online active learning.",
"Quality and Reliability Engineer- ing International https://doi.org/https://doi.org/10.1002/qre.3392, URL https://onlinelibrary.wiley.com/doi/ abs/10.1002/qre.3392, https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.3392 Cai W, Zhang Y, Zhou J (2013) Maximizing expected model change for active learning in regression.",
"Proceedings - IEEE International Conference on Data Mining, ICDM pp 51–60.",
"https://doi.org/10.1109/ICDM.2013.104 Camilleri R, Xiong Z, Fazel M, et al (2021) Selective sampling for online best-arm identification.",
"35th Conference on Neural Information Processing Systems (NeurIPS 2021) URL http://arxiv.org/abs/2110.14864 35 Carcillo F, Le Borgne YA, Caelen O, et al (2017) An assessment of streaming active learning strategies for real- life credit card fraud detection.",
"In: 2017 ieee international conference on data science and advanced analytics (dsaa), IEEE, pp 631–639 Carcillo F, Le Borgne YA, Caelen O, et al (2018) Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization.",
"International Journal of Data Science and Analytics 5:285–300 Carnein M, Trautmann H (2019) Customer segmentation based on transactional data using stream clustering.",
"In: Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, Part I 23, Springer, pp 280–292 Carpentier A, Lazaric A, Ghavamzadeh M, et al (2015) Upper-confidence-bound algorithms for active learning in multi-armed bandits Castellani A, Schmitt S, Hammer B (2022) Stream-based active learning with verification latency in non- stationary environments.",
"https://doi.org/10.1007/978-3-031-15937-4 22, URL http://arxiv.org/abs/2204.",
"06822http://dx.doi.org/10.1007/978-3-031-15937-4 22 Cernuda C, Lughofer E, Mayr G, et al (2014) Incremental and decremental active learning for optimized self- adaptive calibration in viscose production.",
Chemometrics and Intelligent Laboratory Systems 138:14–29.,
"https: //doi.org/10.1016/j.chemolab.2014.07.008 Cerqueira V, Torgo L, Mozetiˇc I (2020) Evaluating time series forecasting models: an empirical study on perfor- mance estimation methods.",
Machine Learning 109:1997–2028.,
"https://doi.org/10.1007/s10994-020-05910-7 Cesa-Bianchi N, Lugosi G (2006) Prediction, Learning, and Games.",
"Cambridge University Press, https://doi.",
"org/10.1017/CBO9780511546921 Cesa-Bianchi N, Gentile C, Zaniboni L (2004) Worst-case analysis of selective sampling for linear-threshold algorithms.",
"Advances in Neural Information Processing Systems URL https://proceedings.neurips.cc/paper files/paper/2004/hash/92426b262d11b0ade77387cf8416e153-Abstract.html Cesa-Bianchi N, Gentile C, Zaniboni L (2006) Worst-case analysis of selective sampling for linear classification.",
The Journal of Machine Learning Research 7.,
"URL https://www.jmlr.org/papers/volume7/cesa-bianchi06b/ cesa-bianchi06b.pdf Chae J, Hong S (2021) Stream-based active learning with multiple kernels.",
2021 International Conference on Information Networking (ICOIN) pp 718–722.,
"https://doi.org/10.1109/ICOIN50884.2021.9333940 Chan LLT, Wu QY, Chen J (2018) Dynamic soft sensors with active forward-update learning for selection of useful data from historical big database.",
Chemometrics and Intelligent Laboratory Systems 175:87–103.,
"https://doi.org/10.1016/j.chemolab.2018.01.015 Cheng J, Zheng Z, Guo Y, et al (2023) Active broad learning with multi-objective evolution for data stream classification.",
"Complex & Intelligent Systems pp 1–18 Chu W, Zinkevich M, Li L, et al (2011) Unbiased online active learning in data streams.",
"Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’11 p 195. https://doi.org/10.1145/2020408.2020444 Citovsky G, DeSalvo G, Gentile C, et al (2021) Batch active learning at scale.",
"35th Conference on Neural Information Processing Systems, NeurIPS 2021 URL http://arxiv.org/abs/2107.14263 Cohn DA, Ghahramani Z, Jordan MI (1996) Active learning with statistical models.",
Journal of Artiicial Intelligence Research 4:129–145.,
"https://doi.org/10.1613/jair.295 Crammer K, Dekel O, Keshet J, et al (2006) Online passive-aggressive algorithms.",
"The Journal of Machine Learning Research URL https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf 36 Dasgupta S, Kalai AT, Monteleoni C (2005) Analysis of perceptron-based active learning.",
COLT ’05 - Interna- tional Conference on Computational Learning Theory pp 249–263.,
"https://doi.org/10.1007/11503415 17 Desalvo G, Gentile C, Thune TS (2021) Online active learning with surrogate loss functions.",
"Advances in Neural Information Processing Systems 34 (NeurIPS 2021) URL https://proceedings.neurips.cc/paper/2021/hash/ c1619d2ad66f7629c12c87fe21d32a58-Abstract.html Donmez P, Carbonell J, Bennet P (2007) Dual strategy active learning.",
"18th European Conference on Machine Learning, ECML 2007 4701. https://doi.org/10.1007/978-3-540-74958-5 14 Duchi JC, Jordan MI, Wainwright MJ (2013) Local privacy and statistical minimax rates.",
2013 IEEE 54th Annual Symposium on Foundations of Computer Science pp 429–438.,
https://doi.org/10.1109/FOCS.2013.53 Ebbinghaus H (2013) Memory: A contribution to experimental psychology.,
"Annals of Neurosciences 20. https: //doi.org/10.5214/ans.0972.7531.200408 Fang M, Li Y, Cohn T (2017) Learning how to active learn: A deep reinforcement learning approach.",
"URL https://arxiv.org/abs/1708.02383 Ferdowsi Z, Ghani R, Settimi R (2013) Online active learning with imbalanced classes.",
2013 IEEE 13th International Conference on Data Mining pp 1043–1048.,
"https://doi.org/10.1109/ICDM.2013.12 Feuz KD, Cook DJ (2013) Real-time annotation tool (rat).",
"In: Workshops at the Twenty-Seventh AAAI Conference on Artificial Intelligence Fiez T, Jain L, Jamieson K, et al (2019) Sequential experimental design for transductive linear bandits.",
"33rd Conference on Neural Information Processing Systems (NeurIPS 2019) URL https://proceedings.neurips.cc/ paper files/paper/2019/file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf Filippi S, Cappe O, Garivier A, et al (2010) Parametric bandits: The generalized linear case.",
"Advances in Neural Information Processing Systems 23 (NIPS 2010) URL https://papers.nips.cc/paper files/paper/2010/hash/ c2626d850c80ea07e7511bbae4c76f4b-Abstract.html Fontaine X, Perrault P, Valko M, et al (2021) Online a-optimal design and active linear regression.",
"URL http: //proceedings.mlr.press/v139/fontaine21a/fontaine21a.pdf Fortuna L, Graziani S, Rizzo A, et al (2007) Soft sensors for monitoring and control of industrial processes, vol 22.",
"Springer, URL https://link.springer.com/book/10.1007/978-1-84628-480-9 Fowler K, Kokilepersaud K, Prabhushankar M, et al (2023) Clinical trial active learning.",
"In: The 14th ACM Conference on Bioinformatics, Computational Biology and Health Informatics (ACM-BCB) Freeman PR (1983) The secretary problem and its extensions: A review.",
International Statistical Review 51:189– 206.,
"URL https://www.jstor.org/stable/1402748 Freund Y, Seung HS, Shamir E, et al (1997) Selective sampling using the query by committee algorithm.",
Machine Learning 28:133–168.,
https://doi.org/10.1023/a:1007330508534 Friedman M (1940) A comparison of alternative tests of significance for the problem of m rankings.,
The Annals of Mathematical Statistics 11:86–92.,
"https://doi.org/10.1214/aoms/1177731944 Frumosu FD, Kulahci M (2018) Big data analytics using semi-supervised learning methods.",
Quality and Reliability Engineering International 34:1413–1423.,
"https://doi.org/10.1002/qre.2338 Fu Y, Zhu X, Li B (2013) A survey on instance selection for active learning.",
Knowledge and Information Systems 35:249–283.,
"https://doi.org/10.1007/s10115-012-0507-8 Fujii K, Kashima H (2016) Budgeted stream-based active learning via adaptive submodular maximization.",
"30th Annual Conference on Neural Information Processing Systems, NIPS 2016 URL https://proceedings.neurips.",
"cc/paper/2016/hash/07cdfd23373b17c6b337251c22b7ea57-Abstract.html 37 Gajjar S, Kulahci M, Palazoglu A (2018) Real-time fault detection and diagnosis using sparse principal component analysis.",
Journal of Process Control 67:112–128.,
https://doi.org/10.1016/j.jprocont.2017.03.005 Galvanin F (2010) Optimal model-based design of experiments in dynamic systems: novel techniques and unconventional applications.,
"Thesis URL https://hdl.handle.net/11577/3427095 Gama J, Medas P, Castillo G, et al (2004) Learning with drift detection.",
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 3171:286–295.,
"https://doi.org/10.1007/978-3-540-28645-5 29 Gama J, Sebastiao R, Rodrigues PP (2009) Issues in evaluation of stream learning algorithms.",
"In: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp 329–338 Gama J, Sebastiao R, Rodrigues PP (2013) On evaluating stream learning algorithms.",
"Machine learning 90:317– 346 Garivier A, Moulines E (2008) On upper-confidence bound policies for non-stationary bandit problems.",
"URL https://arxiv.org/abs/0805.3415 Ge D, Zeng XJ (2020) Learning data streams online — an evolving fuzzy system approach with self- learning/adaptive thresholds.",
Information Sciences 507:172–184.,
https://doi.org/10.1016/j.ins.2019.08.036 Ge Z (2014) Active learning strategy for smart soft sensor development under a small number of labeled data samples.,
Journal of Process Control 24:1454–1461.,
"https://doi.org/10.1016/j.jprocont.2014.06.015 Gemaque RN, Costa AFJ, Giusti R, et al (2020) An overview of unsupervised drift detection methods.",
"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10. https://doi.org/10.1002/widm.1381 Ghassemi M, Sarwate AD, Wright RN (2016) Differentially private online active learning with applications to anomaly detection.",
"AISec 2016 - Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security, co-located with CCS 2016 pp 117–128.",
"https://doi.org/10.1145/2996758.2996766 Ghiasi S, Pazzi G, Del Grosso C, et al (2023) Combining thermodynamics-based model of the centrifugal com- pressors and active machine learning for enhanced industrial design optimization.",
"In: 1st Workshop on the Synergy of Scientific and Machine Learning Modeling@ ICML2023 Goodfellow IJ, Pouget-Abadie J, Mirza M, et al (2014) Generative adversarial networks.",
"URL https://arxiv.org/ abs/1406.2661 Gouk H, Pfahringer B, Frank E (2019) Stochastic gradient trees.",
"URL http://proceedings.mlr.press/v101/ gouk19a/gouk19a.pdf Gu X, Han J, Shen Q, et al (2022) Autonomous learning for fuzzy systems: a review.",
"Artificial Intelligence Review https://doi.org/10.1007/s10462-022-10355-6 Gu X, Han J, Shen Q, et al (2023) Autonomous learning for fuzzy systems: a review.",
"Artificial Intelligence Review 56(8):7549–7595 Halder B, Hasan KA, Amagasa T, et al (2023) Autonomic active learning strategy using cluster-based ensemble classifier for concept drifts in imbalanced data stream.",
Expert Systems with Applications p 120578 Hanneke S (2014) Theory of disagreement-based active learning.,
Foundations and Trends in Machine Learning 7:131–309.,
"https://doi.org/10.1561/2200000037 Hanneke S, Yang L (2021) Toward a general theory of online selective sampling: Trading off mistakes and queries.",
"Proceedings of The 24th International Conference on Artificial Intelligence and Statistics URL https: //proceedings.mlr.press/v130/hanneke21a.html Hao S, Hu P, Zhao P, et al (2018a) Online active learning with expert advice.",
"ACM Transactions on Knowledge Discovery from Data 12. https://doi.org/10.1145/3201604 38 Hao S, Lu J, Zhao P, et al (2018b) Second-order online active learning and its applications.",
IEEE Transactions on Knowledge and Data Engineering 30:1338–1351.,
"https://doi.org/10.1109/TKDE.2017.2778097 Haussmann E, Fenzi M, Chitta K, et al (2020) Scalable active learning for object detection.",
"Proceedings 31st IEEE Intelligent Vehicles Symposium (IV) https://doi.org/https://doi.org/10.1109/IV47402.2020.9304793 He K, Zhang X, Ren S, et al (2015) Deep residual learning for image recognition.",
Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition https://doi.org/10.1109/CVPR.,
"2016.90 Hoang TN, Hong S, Xiao C, et al (2021) Aid: Active distillation machine to leverage pre-trained black-box models in private data settings.",
Proceedings of the Web Conference 2021 pp 3569–3581.,
https://doi.org/10.,
"1145/3442381.3449944 Hodges J, Lehmann E (1962) Rank methods for combination of independent experiments in analysis of variance.",
The Annals of Mathematical Statistics Hoffmann H (2007) Kernel pca for novelty detection.,
Pattern Recognition 40:863–874.,
"https://doi.org/10.1016/ j.patcog.2006.07.009 Hoi SC, Sahoo D, Lu J, et al (2021) Online learning: A comprehensive survey.",
Neurocomputing 459:249–289.,
"https://doi.org/10.1016/j.neucom.2021.04.112 Hoi SCH, Jin R, Zhao P, et al (2013) Online multiple kernel classification.",
Machine Learning 90:289–316.,
"https: //doi.org/10.1007/s10994-012-5319-2 Houlsby N, Hernandez-Lobato JM, Ghahramani Z (2014) Cold-start active learning with robust ordinal matrix factorization.",
"31st International Conference on Machine Learning URL https://proceedings.mlr.press/v32/ houlsby14.html Hua J, Xiong Z, Lowey J, et al (2005) Optimal number of features as a function of sample size for various classification rules.",
Bioinformatics 21:1509–1515.,
"https://doi.org/10.1093/bioinformatics/bti171 Huang B, Salgia S, Zhao Q (2022) Disagreement-based active learning in online settings.",
IEEE Transactions on Signal Processing 70:1947–1958.,
"https://doi.org/10.1109/TSP.2022.3159388 Huang GB, Zhu QY, Siew CK (2006) Extreme learning machine: Theory and applications.",
Neurocomputing 70:489–501.,
"https://doi.org/10.1016/j.neucom.2005.12.126 Huang SJ, Jin R, Zhou ZH (2014) Active learning by querying informative and representative examples.",
IEEE Transactions on Pattern Analysis and Machine Intelligence 36:1936–1949.,
https://doi.org/10.1109/TPAMI.,
"2014.2307881 Hulten G, Spencer L, Domingos P (2001) Mining time-changing data streams.",
Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’01 pp 97–106.,
"https: //doi.org/10.1145/502512.502529 Ienco D, Bifet A, Zliobaite, et al (2013) Clustering based active learning for evolving data streams.",
"16th International Conference on Discovery Science https://doi.org/10.1007/978-3-642-40897-7 6 Ienco D, Pfahringer B, ˇZliobait˙e I (2014) High density-focused uncertainty sampling for active learning over evolving stream data.",
"BIGMINE’14: Proceedings of the 3rd International Conference on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications URL https: //proceedings.mlr.press/v36/ienco14.html Iman RL, Davenport JM (1980) Approximations of the critical region of the fbietkan statistic.",
Communications in Statistics - Theory and Methods 9:571–595.,
"https://doi.org/10.1080/03610928008827904 Istrate R, Malossi ACI, Bekas C, et al (2018) Incremental training of deep convolutional neural networks.",
URL https://arxiv.org/abs/1803.10232 39 Jamieson K (2018) Online and adaptive machine learning.,
regression (part 7).,
URL https://courses.cs.,
"washington.edu/courses/cse599i/18wi/ Jamieson K, Nowak R (2014) Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting.",
2014 48th Annual Conference on Information Sciences and Systems (CISS) pp 1–6.,
"https://doi.org/ 10.1109/CISS.2014.6814096 Jamil S, Khan A (2016) Churn comprehension analysis for telecommunication industry using alba.",
"In: 2016 International Conference on Emerging Technologies (ICET), IEEE, pp 1–5 Jedra Y, Proutiere A (2020) Optimal best-arm identification in linear bandits.",
"34th Conference on Neu- ral Information Processing Systems (NeurIPS 2020) URL https://proceedings.neurips.cc/paper/2020/file/ 7212a6567c8a6c513f33b858d868ff80-Paper.pdf Jin Q, Yuan M, Li S, et al (2022) Cold-start active learning for image classification.",
Information Sciences 616:16–36.,
"https://doi.org/10.1016/j.ins.2022.10.066 Jin R, Hoi S, Yang T (2010) Online multiple kernel learning: Algorithms and mistake bounds.",
"Proceedings of the 21st International Conference on Algorithmic Learning Theory https://doi.org/10.1007/978-3-642-16108-7 31 John RCS, Draper NR (1975) D-optimality for regression designs: A review.",
Technometrics 17:15–23.,
"https: //doi.org/10.1080/00401706.1975.10489266 Joshi AJ, Porikli F, Papanikolopoulos N (2009) Multi-class active learning for image classification.",
2009 IEEE Conference on Computer Vision and Pattern Recognition pp 2372–2379.,
https://doi.org/10.1109/CVPR.2009.,
5206627 Joyce JM (2011) Kullback-leibler divergence.,
"https://doi.org/10.1007/978-3-642-04898-2 327 Karlin S, Studden WJ (1966) Optimal experimental designs.",
The Annals of Mathematical Statistics 37:783–815.,
URL https://www.jstor.org/stable/2238570 Kiefer J (1959) Optimum experimental designs.,
"Journal of the Royal Statistical Society Series B (Methodological) URL https://www.jstor.org/stable/2983802 Kingma DP, Welling M (2013) Auto-encoding variational bayes.",
"2nd International Conference on Learning Representations, ICLR URL https://arxiv.org/abs/1312.6114 Kottke D, Krempl G, Spiliopoulou M (2015) Probabilistic active learning in datastreams.",
https://doi.org/10.,
"1007/978-3-319-24465-5 13 Kranjc J, Smailovi´c J, Podpeˇcan V, et al (2015) Active learning for sentiment analysis on data streams: Methodology and workflow implementation in the clowdflows platform.",
"Information Processing & Management 51(2):187–203 Kraskov A, St¨ogbauer H, Grassberger P (2004) Estimating mutual information.",
"Physical Review E 69:066138. https://doi.org/10.1103/PhysRevE.69.066138 Krawczyk B, Minku LL, Gama J, et al (2017) Ensemble learning for data stream analysis: A survey.",
"Information Fusion 37:132–156 Krawczyk B, Pfahringer B, Wozniak M (2018) Combining active learning with concept drift detection for data stream mining.",
2018 IEEE International Conference on Big Data (Big Data) pp 2239–2244.,
"https://doi.org/ 10.1109/BigData.2018.8622549 Kulkarni RV, Patil SH, Subhashini R (2016) An overview of learning in data streams with label scarcity.",
"Proceedings of the International Conference on Inventive Computation Technologies, ICICT 2016 2. https: //doi.org/10.1109/INVENTIVE.2016.7824874 40 Kumar P, Gupta A (2020) Active learning query strategies for classification, regression, and clustering: A survey.",
Journal of Computer Science and Technology 35:913–945.,
"https://doi.org/10.1007/s11390-020-9487-4 Kurlej B, Wo´zniak M (2011) Learning curve in concept drift while using active learning paradigm.",
https://doi.,
"org/10.1007/978-3-642-23857-4 13 Kwak B, Kim Y, Kim YJ, et al (2022) Trustal: Trustworthy active learning using knowledge distillation.",
"The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22) URL https://arxiv.org/abs/2201.11661 Lakshminarayanan B, Roy D, Teh YW (2014) Mondrian forests: Efficient online random forests.",
"Advances in Neural Information Processing Systems (NIPS) URL https://proceedings.neurips.cc/paper files/paper/2014/ file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf Li A, Boyd A, Smyth P, et al (2021) Detecting and adapting to irregular distribution shifts in bayesian online learning.",
35th Conference on Neural Information Processing Systems (NeurIPS 2021) URL https://papers.,
"nips.cc/paper/2021/file/362387494f6be6613daea643a7706a42-Paper.pdf Li X, Guo Y (2013) Adaptive active learning for image classification.",
2013 IEEE Conference on Computer Vision and Pattern Recognition pp 859–866.,
"https://doi.org/10.1109/CVPR.2013.116 Lieber D, Konrad B, Deuse J, et al (2012) Sustainable interlinked manufacturing processes through real-time quality prediction.",
"In: Leveraging Technology for a Sustainable World: Proceedings of the 19th CIRP Confer- ence on Life Cycle Engineering, University of California at Berkeley, Berkeley, USA, May 23-25, 2012, Springer, pp 393–398 Lima M, Neto M, Filho TS, et al (2022) Learning under concept drift for regression—a systematic literature review.",
IEEE Access 10:45410–45429.,
"https://doi.org/10.1109/ACCESS.2022.3169785 Liu S, Xue S, Wu J, et al (2021) Online active learning for drifting data streams.",
"IEEE Transactions on Neural Networks and Learning Systems https://doi.org/10.1109/TNNLS.2021.3091681 Long J, Yin J, Zhao W, et al (2008) Graph-based active learning based on label propagation.",
MDAI 2008: Modeling Decisions for Artificial Intelligence pp 179–190.,
"https://doi.org/10.1007/978-3-540-88269-5 17 Loy CC, Hospedales TM, Xiang T, et al (2012) Stream-based joint exploration-exploitation active learning.",
Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition pp 1560–1567.,
"https://doi.org/10.1109/CVPR.2012.6247847 Lu J, Zhao P, Hoi SCH (2016) Online passive-aggressive active learning.",
Machine Learning 103:141–183.,
"https: //doi.org/10.1007/s10994-016-5555-y Lu J, Liu A, Dong F, et al (2018) Learning under concept drift: A review.",
IEEE Transactions on Knowledge and Data Engineering pp 1–1.,
"https://doi.org/10.1109/TKDE.2018.2876857 Lughofer E (2011) Evolving Fuzzy Systems – Methodologies, Advanced Concepts and Applications, vol 266.",
"Springer Berlin Heidelberg, https://doi.org/10.1007/978-3-642-18087-3 Lughofer E (2012) Single-pass active learning with conflict and ignorance.",
Evolving Systems 3:251–271.,
https: //doi.org/10.1007/s12530-012-9060-7 Lughofer E (2017) On-line active learning: A new paradigm to improve practical useability of data stream modeling methods.,
Information Sciences 415-416:356–376.,
"https://doi.org/10.1016/j.ins.2017.06.038 Lughofer E, Pratama M (2018) Online active learning in data stream regression using uncertainty sampling based on evolving generalized fuzzy models.",
IEEE Transactions on Fuzzy Systems 26:292–309.,
https://doi.,
"org/10.1109/TFUZZ.2017.2654504 Lughofer E, ˇSkrjanc I (2023) Online active learning for evolving error feedback fuzzy models within a multi- innovation context.",
"IEEE Transactions on Fuzzy Systems 41 Ma L, Destercke S, Wang Y (2016) Online active learning of decision trees with evidential data.",
Pattern Recognition 52:33–45.,
"https://doi.org/10.1016/j.patcog.2015.10.014 Mammen E, Tsybakov AB (1999) Smooth discrimination analysis.",
"The Annals of Statistics 27. https://doi.org/ 10.1214/aos/1017939240 Manjah D, Cacciarelli D, Standaert B, et al (2023) Stream-based active distillation for scalable model deployment.",
"Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition (CVPR) Workshops Manwani N, Desai K, Sasidharan S, et al (2013) Double ramp loss based reject option classifier.",
"19th Pacific- Asia Conference on Advances in Knowledge Discovery and Data Mining (PAKDD) https://doi.org/10.1007/ 978-3-319-57454-7 53 Martins VE, Cano A, Junior SB (2023) Meta-learning for dynamic tuning of active learning on stream classification.",
"Pattern Recognition 138:109359 McSherry F, Talwar K (2007) Mechanism design via differential privacy.",
48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07) pp 94–103.,
"https://doi.org/10.1109/FOCS.2007.41 Menard P, Domingues OD, Jonsson A, et al (2021) Fast active learning for pure exploration in reinforcement learning.",
Proceedings of the 38th International Conference on Machine Learning URL http://proceedings.mlr.,
"press/v139/menard21a/menard21a-supp.pdf Min F, Zhang SM, Ciucci D, et al (2020) Three-way active learning through clustering selection.",
International Journal of Machine Learning and Cybernetics 11:1033–1046.,
https://doi.org/10.1007/s13042-020-01099-2 Minka TP (2001) A family of algorithms for approximate bayesian inference.,
Thesis URL https://hd.media.mit.,
"edu/tech-reports/TR-533.pdf Miu T, Missier P, Pl¨otz T (2015) Bootstrapping personalised human activity recognition models using online active learning.",
"In: 2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing, IEEE, pp 1138–1147 Mohamad S, Bouchachia A, Sayed-Mouchaweh M (2018) A bi-criteria active learning algorithm for dynamic data streams.",
IEEE Transactions on Neural Networks and Learning Systems 29:74–86.,
https://doi.org/10.,
"1109/TNNLS.2016.2614393 Mohamad S, Sayed-Mouchaweh M, Bouchachia A (2020) Online active learning for human activity recognition from sensory data streams.",
Neurocomputing 390:341–358.,
"https://doi.org/10.1016/j.neucom.2019.08.092 Mohamadi S, Amindavar H (2020) Deep bayesian active learning, a brief survey on recent advances.",
URL https://arxiv.org/abs/2012.08044 Montgomery DC (2012) Design and Analysis of Experiments.,
"John Wiley & Sons, Inc., https://doi.org/10.1002/ 9781118147634 Myers RH, Montgomery D, Anderson-Cook CM (2016) Response surface methodology: process and prod- uct optimization using designed experiments.",
"Wiley, URL https://www.wiley.com/en-au/Response+ Surface+Methodology:+Process+and+Product+Optimization+Using+Designed+Experiments,+4th+ Edition-p-9781118916018 Naranjo JE, Sotelo MA, Gonzalez C, et al (2007) Using fuzzy logic in automated vehicle control.",
"IEEE intelligent systems 22(1):36–45 Narr A, Triebel R, Cremers D (2016) Stream-based active learning for efficient and adaptive classification of 3d objects.",
Proceedings - IEEE International Conference on Robotics and Automation 2016-June:227–233.,
"https://doi.org/10.1109/ICRA.2016.7487138 42 Nguyen HT, Smeulders A (2004) Active learning using pre-clustering.",
"Proceedings of the twenty-first interna- tional conference on Machine learning https://doi.org/https://doi.org/10.1145/1015330.1015349 Nixon C, Sedky M, Hassan M (2021) Reviews in online data stream and active learning for cyber intrusion detection-a systematic literature review.",
"In: 2021 Sixth International Conference on Fog and Mobile Edge Computing (FMEC), IEEE, pp 1–6 Pham T, Kottke D, Krempl G, et al (2022) Stream-based active learning for sliding windows under the influence of verification latency.",
Machine Learning 111:2011–2036.,
"https://doi.org/10.1007/s10994-021-06099-z Pitman J, Yor M (1997) The two-parameter poisson-dirichlet distribution derived from a stable subordinator.",
The Annals of Probability 25.,
"URL https://www.jstor.org/stable/20680193 Polikar R, Upda L, Upda S, et al (2001) Learn++: an incremental learning algorithm for supervised neural networks.",
"IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews) 31:497– 508. https://doi.org/10.1109/5326.983933 Prabhu V, Chandrasekaran A, Saenko K, et al (2020) Active domain adaptation via clustering uncertainty- weighted embeddings.",
URL https://github.com/virajprabhu/CLUE.,
"Pratama M, Anavatti SG, Lu J (2015) Recurrent classifier based on an incremental metacognitive-based scaf- folding algorithm.",
IEEE Transactions on Fuzzy Systems 23:2048–2066.,
https://doi.org/10.1109/TFUZZ.2015.,
"2402683 Qin J, Wang C, Zou Q, et al (2021) Active learning with extreme learning machine for online imbalanced multiclass classification.",
Knowledge-Based Systems 231:107385. https://doi.org/10.1016/j.knosys.2021.107385 Quade D (1979) Using weighted rankings in the analysis of complete blocks with additive block effects.,
"Journal of the American Statistical Association 74:680. https://doi.org/10.2307/2286991 R´eda C, Kaufmann E, Delahaye-Duriez A (2020) Machine learning applications in drug development.",
"Compu- tational and structural biotechnology journal 18:241–252 Ren P, Xiao Y, Chang X, et al (2022) A survey of deep active learning.",
ACM Computing Surveys 54:1–40.,
"https://doi.org/10.1145/3472291 Reyes O, Altalhi AH, Ventura S (2018) Statistical comparisons of active learning strategies over multiple datasets.",
Knowledge-Based Systems 145:274–288.,
"https://doi.org/10.1016/j.knosys.2018.01.033 Riis C, Antunes F, H¨uttel FB, et al (2022) Bayesian active learning with fully bayesian gaussian processes.",
In Proceedings of Advances in Neural Information Processing Systems 35 (NeurIPS 2022) URL https:// proceedings.neurips.cc/paper files/paper/2022/file/4f1fba885f266d87653900fd3045e8af-Paper-Conference.pdf Riquelme C (2017) Online active learning with linear models.,
"Thesis URL http://purl.stanford.edu/rp382fv8012 Riquelme C, Ghavamzadeh M, Lazaric A (2017a) Active learning for accurate estimation of linear models.",
"Proceedings of the 34th International Conference on Machine Learning URL http://proceedings.mlr.press/ v70/riquelme17a/riquelme17a.pdf Riquelme C, Johari R, Zhang B (2017b) Online active linear regression via thresholding.",
Thirty-First AAAI Conference on Artificial Intelligence URL www.aaai.org Rosenblatt F (1958) The perceptron: A probabilistic model for information storage and organization in the brain.,
Psychological Review 65:386–408.,
"https://doi.org/10.1037/h0042519 Roth D, Small K (2006) Margin-based active learning for structured output spaces.",
Machine Learning: ECML 2006 pp 413–424.,
"https://doi.org/10.1007/11871842 40 Roy N, Mccallum A (2001) Toward optimal active learning through sampling estimation of error reduction.",
"Proceedings of the Eighteenth International Conference on Machine Learning URL https://dl.acm.org/doi/ 43 10.5555/645530.655646 Roˇzanec JM, Trajkova E, Dam P, et al (2022) Streaming machine learning and online active learning for automated visual inspection.",
IFAC-PapersOnLine 55:277–282.,
"https://doi.org/10.1016/j.ifacol.2022.04.206 Ruan Y, Yang J, Zhou Y (2020) Linear bandits with limited adaptivity and learning distributional optimal design.",
"STOC 2021: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing https://doi.org/https://doi.org/10.1145/3406325.3451004 Rudovic O, Zhang M, Schuller B, et al (2019) Multi-modal active learning from human data: A deep reinforcement learning approach.",
2019 International Conference on Multimodal Interaction pp 6–15.,
https://doi.org/10.,
"1145/3340555.3353742 Saran A, Yousefi S, Krishnamurthy A, et al (2023) Streaming active learning with deep neural networks.",
"In: Krause A, Brunskill E, Cho K, et al (eds) Proceedings of the 40th International Conference on Machine Learn- ing, Proceedings of Machine Learning Research, vol 202.",
"PMLR, pp 30005–30021, URL https://proceedings.",
"mlr.press/v202/saran23a.html Schmidt S, Rao Q, Tatsch J, et al (2020) Advanced active learning strategies for object detection.",
2020 IEEE Intelligent Vehicles Symposium (IV) pp 871–876.,
"https://doi.org/10.1109/IV47402.2020.9304565 Schmitt R, Jatzkowski P, Peterek M (2013) Traceable measurements using machine tools.",
"In: Laser metrology and machine performance X: 10th International Conference and Exhibition on Laser Metrology, Machine Tool, CMM & Robotic Performance, Lamdamap, pp 20–21 Sculley D (2007) Online active learning methods for fast label efficient spam filtering.",
"Proceedings of the Fourth Conference on Email and AntiSpam Sener O, Savarese S (2017) Active learning for convolutional neural networks: A core-set approach.",
ICLR Settles B (2009) Active learning literature survey.,
"Technical Report 1648, University of Wisconsin-Madison Department of Computer Sciences URL https://burrsettles.com/pub/settles.activelearning.pdf Seung HS, Opper M, Sompolinsky H (1992) Query by committee.",
Proceedings of the fifth annual workshop on Computational learning theory - COLT ’92 pp 287–294.,
"https://doi.org/10.1145/130385.130417 Shah K, Manwani N (2020) Online active learning of reject option classifiers.",
Proceedings of the AAAI Conference on Artificial Intelligence 34:5652–5659.,
"https://doi.org/10.1609/aaai.v34i04.6019 Shan J, Zhang H, Liu W, et al (2019) Online active learning ensemble framework for drifted data streams.",
IEEE Transactions on Neural Networks and Learning Systems 30:486–498.,
https://doi.org/10.1109/TNNLS.2018.,
2844332 Shannon E (1948) A mathematical theory of communication.,
"The Bell System Technical Journal Sheng VS, Provost F, Ipeirotis PG (2008) Get another label?",
"improving data quality and data mining using mul- tiple, noisy labelers.",
"Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08 p 614. https://doi.org/10.1145/1401890.1401965 Shi X, Xiong W (2018) Approximate linear dependence criteria with active learning for smart soft sensor design.",
Chemometrics and Intelligent Laboratory Systems 180:88–95.,
"https://doi.org/10.1016/j.chemolab.2018.07.009 Shilton A, Palaniswami M, Ralph D, et al (2005) Incremental training of support vector machines.",
IEEE Transactions on Neural Networks 16:114–131.,
"https://doi.org/10.1109/TNN.2004.836201 Soare M, Lazaric A, Munos R (2013) Active learning in linear stochastic bandits.",
Bayesian Optimization in The- ory and Practice URL https://www.univ-orleans.fr/lifo/Members/soare/files/active learning linear bandit.,
"pdf 44 Soare M, Lazaric A, Munos R (2014) Best-arm identification in linear bandits.",
"27th Conference on Neural Information Processing Systems (NeurIPS 2014) Song S, Chaudhuri K, Sarwate AD (2013) Stochastic gradient descent with differentially private updates.",
2013 IEEE Global Conference on Signal and Information Processing pp 245–248.,
"https://doi.org/10.1109/ GlobalSIP.2013.6736861 Souza V, Pinho T, Batista G (2018) Evaluating stream classifiers with delayed labels information.",
2018 7th Brazilian Conference on Intelligent Systems (BRACIS) pp 408–413.,
https://doi.org/10.1109/BRACIS.2018.,
00077 Steel RGD (1959) A multiple comparison sign test: Treatments versus control.,
"Journal of the American Statistical Association 54:767. https://doi.org/10.2307/2282500 Steve H, Liu Y (2014) Minimax analysis of active learning.",
"Journal of Machine Learning Research URL https: //www.jmlr.org/papers/volume16/hanneke15a/hanneke15a.pdf Subramanian K, Das AK, Sundaram S, et al (2014) A meta-cognitive interval type-2 fuzzy inference sys- tem and its projection based learning algorithm.",
Evolving Systems 5:219–230.,
"https://doi.org/10.1007/ s12530-013-9102-9 Sudarsanam N, Ravindran B (2018) Using linear stochastic bandits to extend traditional offline designed experiments to online settings.",
"Computers & Industrial Engineering 115:471–485 Suresh S, Sundararajan N, Saratchandran P (2008) Risk-sensitive loss functions for sparse multi-category classification problems.",
Information Sciences 178:2621–2638.,
"https://doi.org/10.1016/j.ins.2008.02.009 Suzuki K, Sunagawa T, Sasaki T, et al (2021) Annotation cost reduction of stream-based active learning by automated weak labeling using a robot arm.",
2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) pp 9000–9007.,
"https://doi.org/10.1109/IROS51168.2021.9636355 Su´arez-Cetrulo AL, Kumar A, Miralles-Pechu´an L (2021) Modelling the covid-19 virus evolution with incremental machine learning.",
"29th Irish Conference on Artificial Intelligence and Cognitive Science, AICS 2021 URL https://ceur-ws.org/Vol-3105/paper1.pdf Su´arez-Cetrulo AL, Quintana D, Cervantes A (2023) A survey on machine learning for recurring concept drifting data streams.",
"Expert Systems with Applications 213:118934. https://doi.org/10.1016/j.eswa.2022.118934 Tang Q, Li D, Xi Y (2018) A new active learning strategy for soft sensor modeling based on feature reconstruction and uncertainty evaluation.",
Chemometrics and Intelligent Laboratory Systems 172:43–51.,
https://doi.org/10.,
"1016/j.chemolab.2017.11.001 Taylor G, Hinton G (2009) Factored conditional restricted boltzmann machines for modeling motion style.",
"Proceedings of the 26th International Conference on Machine Learning, Montreal, Canada, 2009 https://doi.",
"org/https://doi.org/10.1145/1553374.1553505 Taylor G, Hinton G, Roweis S (2006) Modeling human motion using binary latent variables.",
"Advances in Neural Information Processing Systems 19 (NIPS 2006) URL https://papers.nips.cc/paper files/paper/2006/hash/ 1091660f3dff84fd648efe31391c5524-Abstract.html Thompson J, Walters WP, Feng JA, et al (2022) Optimizing active learning for free energy calculations.",
"Artificial Intelligence in the Life Sciences 2:100050. https://doi.org/10.1016/j.ailsci.2022.100050 Tieppo E, dos Santos RR, Barddal JP, et al (2022) Hierarchical classification of data streams: a systematic literature review.",
Artificial Intelligence Review 55:3243–3282.,
"https://doi.org/10.1007/s10462-021-10087-z Tong S, Koller D (2002) Support vector machine active learning with applications to text classification.",
"The Journal of Machine Learning Research 2. https://doi.org/10.1162/153244302760185243 45 Tran T, Pham T, Carneiro G, et al (2017) A bayesian data augmentation approach for learning deep models.",
"31st Conference on Neural Information Processing Systems (NIPS 2017) URL https://proceedings.neurips.cc/ paper files/paper/2017/file/076023edc9187cf1ac1f1163470e479a-Paper.pdf Tran T, Do TT, Reid I, et al (2019) Bayesian generative active deep learning.",
Proceedings of the 36th International Conference on Machine Learning URL https://arxiv.org/abs/1904.11643 Tsybakov AB (2004) Optimal aggregation of classifiers in statistical learning.,
"The Annals of Statistics https: //doi.org/10.1214/aos/1079120131 Tsymbal A, Pechenizkiy M, Cunningham P, et al (2008) Dynamic integration of classifiers for handling concept drift.",
Information Fusion 9:56–68.,
"https://doi.org/10.1016/j.inffus.2006.11.002 Vahdat A, Belbahri M, Nia VP (2019) Active learning for high-dimensional binary features.",
"15th Interna- tional Conference on Network and Service Management (CNSM) URL https://www.computer.org/csdl/ proceedings-article/cnsm/2019/09012676/1hQr3hscsJG Vanhatalo E, Kulahci M (2016) Impact of autocorrelation on principal components and their use in statistical process control.",
Quality and Reliability Engineering International 32:1483–1500.,
https://doi.org/10.1002/qre.,
"1858 Vanhatalo E, Kulahci M, Bergquist B (2017) On the structure of dynamic principal component analysis used in statistical process monitoring.",
Chemometrics and Intelligent Laboratory Systems 167:1–11.,
"https://doi.org/ 10.1016/j.chemolab.2017.05.016 Wang L (2011) Smoothness, disagreement coefficient, and the label complexity of agnostic active learning.",
"The Journal of Machine Learning Research URL https://www.jmlr.org/papers/volume12/wang11b/wang11b.pdf Wang X, Fu M, Ma H, et al (2015) Lateral control of autonomous vehicles based on fuzzy logic.",
"Control Engineering Practice 34:1–17 Wassermann S, Cuvelier T, Casas P (2019) Ral-improving stream-based active learning by reinforcement learning.",
"URL https://hal.archives-ouvertes.fr/hal-02265426 Weigl E, Heidl W, Lughofer E, et al (2016) On improving performance of surface inspection systems by online active learning and flexible classifier updates.",
Machine Vision and Applications 27:103–127.,
https://doi.org/ 10.1007/s00138-015-0731-9 Wilcoxon F (1945) Individual comparisons by ranking methods.,
Biometrics Bulletin 1:80. https://doi.org/10.,
"2307/3001968 Woodward M, Finn C (2017) Active one-shot learning.",
"NIPS 2016, Deep Reinforcement Learning Workshop URL http://arxiv.org/abs/1702.06559 Wo´zniak M, Zyblewski P, Ksieniewicz P (2023) Active weighted aging ensemble for drifted data stream classification.",
"Information Sciences 630:286–304 Wu J, Chen J, Huang D (2022) Entropy-based active learning for object detection with progressive diversity constraint.",
2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.,
"org/10.1109/CVPR52688.2022.00918 Wu R, Guo C, Su Y, et al (2021) Online adaptation to label distribution shift.",
"35th Conference on Neural Information Processing Systems (NeurIPS 2021) URL https://www.kaggle.com/Cornell-University/arxiv Wu Y, Chen Y, Wang L, et al (2019) Large scale incremental learning.",
"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/CVPR.2019.00046 Xu W, Zhao F, Lu Z (2016) Active learning over evolving data streams using paired ensemble framework.",
2016 Eighth International Conference on Advanced Computational Intelligence (ICACI) pp 180–185.,
"https: //doi.org/10.1109/ICACI.2016.7449823 46 Yan X, Sarkar M, Lartey B, et al (2023) An online learning framework for sensor fault diagnosis analysis in autonomous cars.",
"IEEE Transactions on Intelligent Transportation Systems Yin C, Chen S, Yin Z (2023) Clustering-based active learning classification towards data stream.",
"ACM Transactions on Intelligent Systems and Technology 14(2):1–18 Yu H, Sun C, Yang W, et al (2015) Al-elm: One uncertainty-based active learning algorithm using extreme learning machine.",
Neurocomputing 166:140–150.,
"https://doi.org/10.1016/j.neucom.2015.04.019 Yu K, Bi J, Tresp V (2006) Active learning via transductive experimental design.",
"Proceedings of the 23rd International Conference on Machine Learning https://doi.org/https://doi.org/10.1145/1143844.1143980 Yuan M, Lin HT, Boyd-Graber J (2020) Cold-start active learning through self-supervised language modeling.",
"roceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) https: //doi.org/10.18653/v1/2020.emnlp-main.637 Zhang H, Liu W, Shan J, et al (2018) Online active learning paired ensemble for concept drift and class imbalance.",
IEEE Access 6:73815–73828.,
"https://doi.org/10.1109/ACCESS.2018.2882872 Zhang H, Liu W, Sun L, et al (2020a) Analyzing network traffic for protocol identification: An ensemble online active learning method.",
"Proceedings - 2020 6th International Conference on Big Data and Information Analytics, BigDIA 2020 pp 167–172.",
"https://doi.org/10.1109/BigDIA51454.2020.00035 Zhang H, Ravi SS, Davidson I (2020b) A graph-based approach for active learning in regression.",
Proceedings of the 2020 SIAM International Conference on Data Mining (SDM) https://doi.org/https://doi.org/10.1137/1.,
"9781611976236.32 Zhang H, Liu W, Liu Q (2022) Reinforcement online active learning ensemble for drifting imbalanced data streams.",
IEEE Transactions on Knowledge and Data Engineering 34:3971–3983.,
"https://doi.org/10.1109/ TKDE.2020.3026196 Zhang K, Liu S, Chen Y (2023) Online active learning framework for data stream classification with density-peaks recognition.",
IEEE Access 11:27853–27864 Zhang T (2004) Statistical behavior and consistency of classification methods based on convex risk minimization.,
"The Annals of Statistics 32. https://doi.org/10.1214/aos/1079120130 Zheng Z, Padmanabhan B (2006) Selectively acquiring customer information: A new data acquisition problem and an active learning-based solution.",
"Management Science 52(5):697–712 Zhou C, Ma X, Michel P, et al (2021) Examining and combating spurious features under distribution shift.",
"Proceedings of the 38 th International Conference on Machine Learning URL https://github.com/violet-zct/ Zhu JJ, Bento J (2017) Generative adversarial active learning.",
"URL https://arxiv.org/abs/1702.07956 Zhu X, Zhang P, Lin X, et al (2007) Active learning from data streams.",
"Proceedings - IEEE International Conference on Data Mining, ICDM pp 757–762.",
"https://doi.org/10.1109/ICDM.2007.101 Zliobaite I, Bifet A, Pfahringer B, et al (2014) Active learning with drifting streaming data.",
IEEE Transactions on Neural Networks and Learning Systems 25:27–39.,
"https://doi.org/10.1109/TNNLS.2012.2236570 Zwanka RJ, Buff C (2021) Covid-19 generation: A conceptual framework of the consumer behavioral shifts to be caused by the covid-19 pandemic.",
Journal of International Consumer Marketing 33:58–67.,
"https://doi.org/ 10.1080/08961530.2020.1771646 Zyblewski P, Ksieniewicz P, Wo´zniak M (2020) Combination of active and random labeling strategy in the non-stationary data stream classification.",
"In: International Conference on Artificial Intelligence and Soft Computing, Springer, pp 576–585 47 ˇSkrjanc I (2009) Confidence interval of fuzzy models: An example using a waste-water treatment plant.",
Chemometrics and Intelligent Laboratory Systems 96:182–187.,
https://doi.org/10.1016/j.chemolab.2009.01.,
"Accepted at the ICLR 2023 Workshop on Physics for Machine Learning PHYSICS-INSPIRED INTERPRETABILITY OF MACHINE LEARNING MODELS Maximilian P. Niroomand, David J. Wales Department of Chemistry University of Cambridge {mpn26,djw34}@cam.ac.uk ABSTRACT The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving.",
Great interest exists in understanding which features of the input data prompt model decision making.,
"In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences.",
"By identifying conserved weights within groups of min- ima of the loss landscapes, we can identify the drivers of model decision making.",
"Analogues to this idea exist in the molecular sciences, where coordinate invari- ants or order parameters are employed to identify critical features of a molecule.",
"However, no such approach exists for machine learning loss landscapes.",
"We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both synthetic and from the real world, for how these methods can help to make models more interpretable.",
1 INTRODUCTION Machine learning methods have achieved impressive results in recent years.,
"Besides famous appli- cations in areas like chess (Silver et al., 2017a) and Go (Silver et al., 2017b), AI plays a critical role in advances to autonomous driving (Grigorescu et al., 2020), protein structure prediction (Jumper et al., 2021), cancer identification (Sammut et al., 2022) and in cybersecurity (Dasgupta et al., 2022).",
"However, in order for AI methods to take the next step and be commonly employed for critical ap- plications without any humans in the loop, we want to be able to understand the decision making process.",
A critical component towards explainable AI is understanding which parts of the input data are utilised by the model in its decision making.,
"In neural networks, the most popular approach is to study the outgoing weights and gradients from an individual input node.",
"Larger weights are reason- ably assumed to indicate a greater significance of the particular input, and indeed, an entire class of interpretability metrics, namely gradient-based methods, are founded on this idea (Simonyan et al., 2013; Linardatos et al., 2020).",
"Yet, given the immense complexity of overparameterised, deep neu- ral networks, current methods are in practice often insufficient to appropriately explain a model.",
"Using methods from the physical sciences, we propose a novel approach as a next step towards interpretable neural networks.",
"1.1 ENERGY LANDSCAPES In the physical sciences, energy landscapes (ELs) are employed to explore molecular configuration space (Wales et al., 1998; 2003).",
"Each molecular configuration is associated with an energy value, and local minima of the energy landscape represent stable isomers.",
"The analogy to machine learning loss landscapes (ML-LLs) is straightforward, the main difference perhaps being that non-minima are valid configurations for sets of weights.",
"Due to this similarity between ELs and ML-LLs, various, well-established methods from the field of energy landscapes can be employed to study ML-LLs.",
One key area of interest here is interpretability.,
"Employing well-understood methods from a mature field, with a solid mathematical basis in the physical world, to move away from black-box machine learning models may be a helpful step towards interpretable machine learning models.",
1 arXiv:2304.02381v2 [cs.LG] 15 Dec 2024 Accepted at the ICLR 2023 Workshop on Physics for Machine Learning 1.2 RELATED WORK Various approaches to interpretability in deep learning for neural networks exist.,
"Below, we are mostly interested in gradient-based methods due to their applicability to non-image data.",
"Vari- ous other methods to interpret the output of CNNs on images exist, as for example summarised in Linardatos et al.",
"(2020), but will not be reviewed below.",
Gradient-based methods: All gradient-based methods are concerned with changes in the predic- tion as the input data is slightly perturbed.,
"For a vector-valued input x ⊂X ∈Rd and some loss function, L, a gradient-based method computes some expression of the form ∂L/∂x, usually for each input node individually.",
Gradient-based methods were first introduced for images by Si- monyan et al.,
"(2013), who used them to compute how changes in the input affect predictions in the neighbourhood of the input, allowing the computation of a salience map (K¨ummerer et al., 2014; Zhao et al., 2015).",
"More recently, integrated gradient methods (Sundararajan et al., 2017) consider the derivative of the output (loss) with respect to individual input nodes.",
"If the change in loss is large with respect to some input feature, that feature is more likely to be relevant to the decision making.",
"Various other gradient and perturbation based methods exist (Alvarez-Melis & Jaakkola, 2018), yet their usefulness and accuracy is debated, and is generally agreed to be insufficient (Srini- vas & Fleuret, 2020).",
"Energy landscapes in machine learning: Energy landscapes methods have been employed to study machine learning in previous contributions (Ballard et al., 2017; Chitturi et al., 2020).",
Niroomand et al.,
"(2022) used energy landscapes to characterise new loss functions, and the landscapes view has been used more broadly to gain insights into machine learning models (Segura et al., 2022; Verpoort et al., 2020).",
"Lastly, other applications of energy landscape methods have employed various con- cepts from physical sciences in machine learning, including the heat capacity (Bradley et al., 2022; Niroomand et al., 2022), both for characterisation and model improvement.",
"Interpreting energy landscapes: Due to the associated physical meaning, energy landscapes are usually more easily interpretable.",
"Only minima represent equilibrium configurations, and each min- imum is associated with a unique structure.",
"However, for larger, complex molecules, many minima may exist, and enumerating them may be infeasible.",
"Instead, common features between sets of min- ima, grouped by their energetic properties, may be identified.",
"For example, in (R¨oder et al., 2020) and (R¨oder & Wales, 2022) a multi-funnelled landscape is analysed to understand which structural differences of a molecule characterise solutions in a specific funnel.",
"2 ENERGY LANDSCAPES METHODS The study of energy landscapes is a well-established field (Wales et al., 1998).",
"Various approaches exist for constructing a faithful representation of the landscape by optimising the non-convex energy function, and visualising this landscape.",
"Visualisation is commonly performed using disconnectivity graphs (Becker & Karplus, 1997; Wales et al., 1998) as described below.",
"2.1 LANDSCAPE VISUALISATION A disconnectivity graph is a low-dimensional representation of a complex function landscape, which reduces the function to key characteristic stationary points, namely minima and transition states.",
A transition state is an index-1 saddle point of the funciton.,
"The vertical axis of a disconnectivity graph represents the energy or loss value, and ordering along the horizontal axis is arbitrary.",
"To identify distinct groups of minima, usually called funnels, we introduce the notion of levels and nodes.",
"Levels are cross-sections of the energy at some evenly spaced, discrete heights in the disconnectivity graph.",
"The highest energy level in the disconnectivity graph is level 1, and the lowest corresponds to the global minimum.",
"Thus, each minimum belongs to one of evenly spaced intervals.",
"Within each level, minima are grouped by a shared parent node, located higher up.",
"In the disconnectivity graphs below, levels and nodes are represented as level node.",
"In the molecular sciences, a transition state between two minima describes the energy barrier to be overcome for a molecule to change configuration from one state to the other.",
This particular notion does not have a direct meaning in machine learning.,
"However, given the optimisation procedure required for model training, the concept of a transition state is highly relevant, since it may determine which minimum basin the optimiser will fall into.",
"Thus, disconnectivity graphs can be employed as a faithful coarse-grained representation of the loss landscape.",
"In particular, it will be relevant 2 Accepted at the ICLR 2023 Workshop on Physics for Machine Learning below to understand that any group of minima close together, perhaps separated from other groups of minima via high-lying transition states, may share commonalities.",
"This effect has been observed in (R¨oder et al., 2020; R¨oder & Wales, 2022) for molecular systems, and we find that the same argument holds for ML-LLs.",
3 EXPERIMENTS We report results for two separate experiments on two datasets.,
We believe that the underlying idea applies without loss of generality to any neural network architecture.,
"However, further work will be required to validate this suggestion.",
"Figures 1and 2 show disconnectivity graphs for (1) a 2-dimensional synthetic checkerboard dataset (Kluger et al., 2003) and (2) an anonymised, 29- dimensional credit card fraud detection dataset (Dal Pozzolo et al., 2015), which are binary classi- fication problems.",
The lowest lying node in each graph is the global minimum.,
"To identify groups of minima with conserved weights, we follow a two-step procedure.",
"Firstly, we identify groups of minima, that are separated from other groups by a higher-lying transition state.",
This segregation leads to the notion of nodes and levels described above.,
"Secondly, we identify groups of minima that share a subset of conserved weights by computing the standard deviation of each weight across each node in each level.",
"A subset of weights ˜w ⊂W is conserved if σ(w) < n for any w ∈W, where W denotes the weights of all minima in one node of one level.",
8_1 9_2 10_6 11_5 12_6 13_9 14_12 15_12 16_8 17_5 18_7 19_4 19_8 20_13 21_8 22_6 22_11 23_323_8 23_9 24_11 25_7 Figure 1: Disconnectivity graph for the checkerboard dataset.,
The conserved weights for a specific local minimum are highlighted in the respective colour for the chosen examples.,
"For visualisation purposes, we employ single-layer neural networks, which is sufficient for our anal- ysis, with only a few nodes.",
The AUC of the best solutions is > 0.95 for both problems.,
"Hence, these networks provide a realistic solutions to the set problems.",
"In both figures, we visualised the conserved weights for a group of minima in the corresponding colour.",
"In Figure 1, various weights across the network are conserved, highlighting how this approach identifies relevant weights for the model.",
"In Figure 2, the funnel containing the global minimum (red) conserves 3 weights, all related 3 Accepted at the ICLR 2023 Workshop on Physics for Machine Learning to one specific input node.",
Randomly permuting the 3 identified weights for the group of minima around the global minimum in figure 2 reduces the best AUC from ≈0.95 to 0.76.,
"In contrast, permuting any random set of 3 weights by the same magnitude on average only decreases the best AUC by 0.05 to an average best AUC of ≈0.9.",
"In 2, for group 25 7 (red), weights for only a sin- gle minimum are conserved, in group 22 6 (blue), weights outgoing from different input nodes are conserved.",
Figure 2: Disconnectivity graph for credit card data.,
Group 25 7 in red includes the global mini- mum.,
"Coloured edges indicate that for all minima in the specific group, these particular weights are conserved, i.e.",
have a standard deviation < n which has been set to n = 0.01 here.,
3.1 PERMUTATIONAL INVARIANCE GROUPS As discussed in Niroomand et al.,
"(2022), the magnitude of individual weights must always be viewed with caution due to permutational isomers.",
"For a given neural network of H hidden layers, with nl nodes in hidden layer l, there exist at least |G| = QH l=1 (nl!",
× 2nl) sets of weights that are invariant with respect to the model prediction.,
"This effect must be considered when identifying conserved weights; for example, a negative inverse could still be valid and conserved (Niroomand et al., 2022).",
We account for this effect by identifying permutationally invariant sets of weights and only considering a single minimum m ∈G for each G. 4 DISCUSSION AND CONCLUSIONS Well-established methods from computational chemical physics can be employed to enhance our un- derstanding of machine learning systems.,
"In this work, we have shown how both concepts and asso- ciated tools from the study of energy landscapes can be employed for ML-LLs to guide interpretabil- ity.",
"We have shown that groups of minima share conserved weights and importantly, that these weights are critical to model performance.",
"Randomly permuting the conserved weights strongly decreases model performance, much more so than permuting any other random set of weights S of equivalent cardinality |S|.",
Figure 2 indicates that all the conserved weights are associated with the particular input node 6.,
"Since the credit card dataset is anonymised and PCA-reduced (Dal Pozzolo et al., 2015), we are unable to say which specific feature it is that helps the model in making a deci- sion, but we can say where it can be found.",
"In Figure 1, we know that both input nodes are relevant, which is confirmed by studying the conserved weights for the three given examples.",
"Importantly, different weights are conserved across different examples, highlighting the importance of studying the loss landscape.",
"Studying the applicability of our method to larger and more complex architec- tures, and perhaps also to different types of machine leaning models, will provide valuable insights, and is an interesting direction for future work.",
"4 Accepted at the ICLR 2023 Workshop on Physics for Machine Learning 5 ACKNOWLEDGEMENTS DJW gratefully acknowledges an International Chair at the Interdisciplinary Institute for Artificial Intelligence at 3iA Cote d’Azur, supported by the French government, reference ANR-19-P3IA- 0002, which has provided interactions that furthered the present research project.",
"MPN acknowl- edges funding from Downing College, Cambridge.",
REFERENCES David Alvarez-Melis and Tommi S Jaakkola.,
On the robustness of interpretability methods.,
"arXiv preprint arXiv:1806.08049, 2018.",
"Andrew J Ballard, Ritankar Das, Stefano Martiniani, Dhagash Mehta, Levent Sagun, Jacob D Stevenson, and David J Wales.",
Energy landscapes for machine learning.,
"Physical Chemistry Chemical Physics, 19(20):12585–12603, 2017.",
Oren M Becker and Martin Karplus.,
The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics.,
"The Journal of chemical physics, 106 (4):1495–1517, 1997.",
"Arwen V Bradley, Carlos A Gomez-Uribe, and Manish Reddy Vuyyuru.",
"Shift-curvature, sgd, and generalization.",
"Machine Learning: Science and Technology, 3(4):045002, 2022.",
"Sathya R Chitturi, Philipp C Verpoort, David J Wales, et al.",
Perspective: new insights from loss func- tion landscapes of neural networks.,
"Machine Learning: Science and Technology, 1(2):023002, 2020.",
"Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi.",
Calibrating prob- ability with undersampling for unbalanced classification.,
"In 2015 IEEE symposium series on computational intelligence, pp.",
"IEEE, 2015.",
"Dipankar Dasgupta, Zahid Akhtar, and Sajib Sen. Machine learning in cybersecurity: a comprehen- sive survey.",
"The Journal of Defense Modeling and Simulation, 19(1):57–106, 2022.",
"Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.",
A survey of deep learning techniques for autonomous driving.,
"Journal of Field Robotics, 37(3):362–386, 2020.",
"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al.",
Highly accurate protein structure prediction with alphafold.,
"Nature, 596(7873):583–589, 2021.",
"Yuval Kluger, Ronen Basri, Joseph T Chang, and Mark Gerstein.",
Spectral biclustering of microarray data: coclustering genes and conditions.,
"Genome research, 13(4):703–716, 2003.",
"Matthias K¨ummerer, Lucas Theis, and Matthias Bethge.",
Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet.,
"arXiv preprint arXiv:1411.1045, 2014.",
"Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis.",
Explainable ai: A review of machine learning interpretability methods.,
"Entropy, 23(1):18, 2020.",
"Maximilian P Niroomand, John WR Morgan, Conor T Cafolla, and David J Wales.",
On the capacity and superposition of minima in neural network loss function landscapes.,
"Machine Learning: Science and Technology, 3(2):025004, 2022.",
Konstantin R¨oder and David J Wales.,
The energy landscape perspective: Encoding structure and function for biomolecules.,
"Frontiers in Molecular Biosciences, 9, 2022.",
"Konstantin R¨oder, Guillaume Stirnemann, Anne-Catherine Dock-Bregeon, David J Wales, and Samuela Pasquali.",
Structural transitions in the rna 7sk 5 hairpin and their effect on hexim binding.,
"Nucleic acids research, 48(1):373–389, 2020.",
"Stephen-John Sammut, Mireia Crispin-Ortuzar, Suet-Feung Chin, Elena Provenzano, Helen A Bard- well, Wenxin Ma, Wei Cope, Ali Dariush, Sarah-Jane Dawson, Jean E Abraham, et al.",
Multi-omic machine learning predictor of breast cancer therapy response.,
"Nature, 601(7894):623–629, 2022.",
"5 Accepted at the ICLR 2023 Workshop on Physics for Machine Learning Carolina Herrera Segura, Edison Montoya, and Diego Tapias.",
Subaging in underparametrized deep neural networks.,
"Machine Learning: Science and Technology, 3(3):035013, 2022.",
"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al.",
Mastering chess and shogi by self-play with a general reinforcement learning algorithm.,
"arXiv preprint arXiv:1712.01815, 2017a.",
"David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.",
Mastering the game of go without human knowledge.,
"nature, 550(7676):354–359, 2017b.",
"Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.",
Deep inside convolutional networks: Vi- sualising image classification models and saliency maps.,
"arXiv preprint arXiv:1312.6034, 2013.",
Suraj Srinivas and Franc¸ois Fleuret.,
Rethinking the role of gradient-based attribution methods for model interpretability.,
"arXiv preprint arXiv:2006.09128, 2020.",
"Mukund Sundararajan, Ankur Taly, and Qiqi Yan.",
Axiomatic attribution for deep networks.,
"In International conference on machine learning, pp.",
"PMLR, 2017.",
"Philipp C Verpoort, Alpha A Lee, and David J Wales.",
Archetypal landscapes for deep neural net- works.,
"Proceedings of the National Academy of Sciences, 117(36):21857–21864, 2020.",
"David J Wales, Mark A Miller, and Tiffany R Walsh.",
Archetypal energy landscapes.,
"Nature, 394 (6695):758–760, 1998.",
David J Wales et al.,
"Energy landscapes: Applications to clusters, biomolecules and glasses.",
"Cam- bridge University Press, 2003.",
"Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang.",
Saliency detection by multi-context deep learning.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.",
"1265–1274, 2015.",
"arXiv:2303.15563v1 [cs.LG] 27 Mar 2023 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare PRIVACY-PRESERVING MACHINE LEARNING FOR HEALTHCARE: OPEN CHALLENGES AND FUTURE PER- SPECTIVES Alejandro Guerra-Manzanares∗, L. Julian Lechuga Lopez∗, Michail Maniatakos and Farah E. Shamout Department of Computer Engineering, New York University Abu Dhabi {ag9454, ljl5178, mm6446, fs999}@nyu.edu ABSTRACT Machine Learning (ML) has recently shown tremendous success in modeling var- ious healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment.",
"Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference.",
"In this paper, we conduct a review of recent literature concerning Privacy-Preserving Ma- chine Learning (PPML) for healthcare.",
"We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of ex- isting trends, identify challenges, and discuss opportunities for future research directions.",
"The aim of this review is to guide the development of private and ef- ﬁcient ML models in healthcare, with the prospects of translating research efforts into real-world settings.",
"1 INTRODUCTION Machine Learning (ML) and Deep Learning (DL) have shown great promise in many domains, lever- aging the use of large datasets.",
"Some notable contributions include AlphaFold (Jumper et al., 2021) for the prediction of protein structures and Transformers (Vaswani et al., 2017) for natural language processing.",
"Healthcare is one of the domains in which ML is expected to provide substantial im- provements in the delivery of patient care worldwide (WHO, 2021).",
"Given the rapid growth in the number of models over the last couple of years (Rav`ı et al., 2016; Miotto et al., 2018; Kaul et al., 2022; Javaid et al., 2022), healthcare applications deserve special consideration considering the sen- sitive nature of the data that is required to train the models and the safety-critical nature of medical decision-making.",
"In this regard, real-world implementation of such models is still hampered by ethical and legal con- straints.",
"Legal frameworks have been developed and enforced to guarantee the transparency and privacy of ML-based healthcare solutions, such as the Health Insurance Portability and Account- ability Act (HIPAA) in the United States (Gostin et al., 2009) and the General Data Protection Reg- ulation (GDPR) in Europe (Voigt & Von dem Bussche, 2017).",
"Therefore, there is a crucial need for Privacy-Preserving Machine Learning (PPML) in healthcare to enable the implementation of trust- worthy systems in the future.",
The main goal of this review is to provide a comprehensive overview of state-of-the-art PPML in healthcare and encourage the development of new methodologies that tackle speciﬁc challenges relevant to the nature of the domain.,
Motivation.,
There exist several related literature reviews that focus on a speciﬁc subset of PPML for healthcare.,
"Several highlight recent advancements in federated learning (Xu et al., 2021; Ali et al., 2022; Joshi et al., 2022; Nguyen et al., 2022), cryptographic techniques (Zalonis et al., 2022), or se- curity aspects of ML models, such as adversarial attacks (Liu et al., 2021).",
"Existing review articles cover a wide range of applications related to health and input data modalities, ranging from IoT sensors to medical images (Qayyum et al., 2020).",
"Compared to existing work, our review has three main contributions with the intent of bridging between research pertaining to ML for healthcare and cybersecurity.",
"First, we distinguish between PPML for training and inference, i.e., ML-as-a-service.",
∗Equal contributions.,
"1 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Second, we focus on state-of-the-art (SOTA) literature published in the last three years, considering the high proliferation of ML in healthcare and recent methodological advancements in ML and DL (e.g., network architectures, model pre-training, etc.).",
"Third, we consider studies that develop or apply methodologies using two popular modalities based on publicly available datasets and state- of-the-art in ML for healthcare, namely medical images and data extracted from Electronic Health Records (EHR) (Kaul et al., 2022).Despite the use of other input modalities in medical applications, such as video (Ouyang et al., 2020) or text (Srivastava et al., 2019), our review exclusively focuses on medical images and EHR as they are the most prevalent input modalities in diagnostic and prog- nostic settings (Shehab et al., 2022).",
"Lastly, although we acknowledge the importance of security for ML models, it is out of the scope of this paper since we primarily focus on privacy.",
"To this end, we review papers that meet the following inclusion criteria: 1.",
"We include recently published work i.e., publication year ≥2020.",
"We include articles that focus on the application or development of PPML either for model training and/or inference, including but not restricted to homomorphic encryption, differ- ential privacy, federated learning, and multi-party secure computation.",
We include articles that consider clinical tasks involving medical images and/or EHR data.,
"In Section 2, we provide background knowledge about concepts and terminology concerning PPML.",
"In Section 3, we provide an overview of the state-of-the-art pertaining to PPML for training (Section 3.1) and for inference (Section 3.2).",
"Later in Section 4, we discuss open challenges and derive future directions.",
"Finally, we provide concluding remarks in Section 5.",
"2 PRIVACY-PRESERVING MACHINE LEARNING: BACKGROUND & TERMINOLOGY 2.1 FEDERATED LEARNING Since medical data is highly sensitive, data sharing is difﬁcult, and subject to ethical restrictions and legal constraints if at all possible.",
"Federated learning (FL) (McMahan et al., 2017) aims to overcome the challenges of data sharing by enabling collaborative training, which does not require that the involved parties share their training data.",
"Therefore, the data remains private to each local node within the FL network, such that only the model updates are shared and integrated in a centralized model.",
"Federated averaging (McMahan et al., 2017) is the most common form of FL.",
"In this setting, a centralized server is connected to N entities, which have their own training data.",
"The central server orchestrates the collaborative training process as follows: (1) the initial model is distributed amongst all entities, (2) each entity performs a training iteration on their local model using their own training data, typically one epoch, and shares its resulting model parameters with the central server, (3) the server averages the model parameters shared by all entities and distributes the resulting (averaged) model amongst all entities, and (4) steps (2) and (3) are repeated sequentially until a performance threshold or a speciﬁc number of training iterations is achieved.",
"FL has proven to be very efﬁcient in training models with strong performance, while avoiding the need for data sharing (McMahan et al., 2017).",
"However, FL might be vulnerable to privacy issues such as reconstruction attacks (Liu et al., 2022), thus requiring that it is combined with other privacy-preserving methods to ensure robust privacy guarantees (Nguyen et al., 2022).",
2.2 DIFFERENTIAL PRIVACY Differential Privacy (DP) has its origins in statistical analysis of databases.,
"Its main aim is to address the paradox of learning nothing about speciﬁc individuals, while learning useful information about the general population (Dwork et al., 2014).",
"In the FL context, it is usually incorporated in the form of additive noise to model updates, either artiﬁcially or using a differentiable private optimizer, prior to transferring the updates from the entities to the central server (Abadi et al., 2016).",
"The amount of artiﬁcial noise added is directly proportional to the degree of privacy desired (i.e., privacy budget) (Zhang et al., 2021c).",
"DP can successfully make privacy attacks fail, such as reconstruction attacks, 2 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare as the added noise hinders the inference of actual knowledge about the training data by the attacker.",
"However, adding too much noise (i.e., high privacy budget) can hamper learning and negatively impact the model accuracy (Chilukoti et al., 2022).",
"2.3 HOMOMORPHIC ENCRYPTION In mathematics, the term homomorphic refers to the transformation of a given set into another while preserving the relation between the elements in both sets.",
"Thus, Homomorphic Encryption (HE) refers to the conversion of plaintext into ciphertext while preserving the structure of the data.",
"Con- sequently, speciﬁc operations applied to the ciphertext will provide the same results as if they were applied to the plaintext but without compromising the encryption (Acar et al., 2018).",
"That is, the plaintext data is never accessed nor decrypted as the operations are directly applied to the encrypted data.",
The result of the transformations on the ciphertext can only be decrypted back to plaintext by the encryption key owner.,
"Despite the beneﬁt of provable privacy guarantees, the range of operations available in HE is re- stricted to addition and multiplication i.e., fully homomorphic encryption.",
"This limits the set and number of transformations applicable to the data and requires the use of approximations for more complex operations (e.g., HE-ReLU is the polynomial approximation of the ReLU function (Yue et al., 2021b)).",
"This also signiﬁcantly increases the computational time needed to process en- crypted text compared to plaintext by several orders of magnitude (Popescu et al., 2021).",
"2.4 SECURE MULTI-PARTY COMPUTATION Secure Multi-Party Computation (SMPC) (Goldreich, 1998) provides a framework in which two or more parties jointly compute a public function with their data while keeping the inputs private and hidden from other parties using cryptographic protocols.",
Most protocols used for SMPC with more than two parties are based on Secret Sharing (SS).,
"In SS, a portion of the secret input is shared among a number of other parties.",
"Most ML methods use Shamir’s SS and additive SS (Singh & Shukla, 2021b).",
"Although these methods are considered information-theoretic secure cryptosystems, recent studies show that leakage of global data properties can occur in some scenarios (Zhang et al., 2021a).",
"While both FL and SMPC rely on collaborative training via knowledge sharing and keep the end- point data private, their implementation differs signiﬁcantly.",
"SMPC involves cryptography and can be used for training and inference, whereas FL does not involve cryptography nor provides strong privacy guarantees, and is only used for model training.",
"3 OVERVIEW OF STATE-OF-THE-ART Following the inclusion criteria described in Section 1, we summarize existing work on PPML for healthcare based on whether the work focuses on model training (Table 1) or model inference (Table 2).",
For each study (row) we describe several attributes.,
Use case provides a succinct summary of the objective of the study.,
Model reports the ML or DL architecture that was employed to model the task.,
Medical datasets summarizes the datasets that were used for model training and evaluation.,
"Additionally, we use the * symbol to indicate the use of a private dataset.",
"ML task describes the nature of the prediction task (e.g., binary or multi-class classiﬁcation).",
"Input modality reports the nature of the model’s input data, which could either be I for medical images or E for EHR data.",
"In the Validation column, we report whether the trained model was internally and/or externally evaluated, with Ëindicating the use of internal validation i.e., test set from the same distribution of the training data, and ËËindicating the assessment of the generalization of the model on an external test dataset.",
"Lastly, Metrics lists the evaluation metrics used to describe the performance of the proposed model.",
"3.1 PRIVACY-PRESERVING TRAINING FOR HEALTHCARE As observed in Table 1, the most commonly used privacy-preserving approach for model training is FL, either independently or in combination with DP.",
"DP is added to increase the privacy of the FL training updates i.e., adding noise to the shared weights, thus making the system more robust to 3 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Table 1: Summary of PPML in healthcare for model training.",
We summarize studies that focus on de- veloping PPML in the context of model training.,
"We group them based on the methodology considered, i.e.",
"federated learning, homomorphic encryption, and differential privacy.",
Reference Use case Model Medical dataset/s ML task Input modality Validation Metrics FEDERATED LEARNING Dou et al.,
"(2021) COVID-19 Computed Tomography (CT) analysis RetinaNet Multi-institution lung CT data* Object detection I ËË mAP, Speciﬁty, Recall, AUROC Field et al.",
"(2022) Cardiovascular admission after lung cancer treatment Logistic regression Multi-institution lung CT data* Risk prediction I+E Ë AUROC, C-index Lee & Shin (2020) FL benchmarking and reliability in healthcare Neural Network, LSTM, CNN MIMIC-III, PhysioNet ECG Mortality prediction, Multi-class classiﬁcation I+E Ë AUROC, AUPRC, F1-score Yang et al.",
"(2022) FL benchmarking and monetary cost in healthcare Transformer, EfﬁcientNet-B0, ResNet-NC-SE eICU, ISIC19, HAM10000, PhysioNet ECG Mortality prediction, Length of stay, Discharge time, Acuity prediction I+E Ë AUROC, AUPRC Sadilek et al.",
"(2021) FL benchmarking vs. centralized learning in healthcare Logistic regression, Neural Network, Generalized linear model UCI Heart failure, MIMIC-III, Malignancy in SARS- CoV-2 infection Risk prediction E Ë AUROC Loftus et al.",
"(2022) COVID-19 detection DenseNet Multi-institution COVID-19 X-ray* Binary classiﬁcation I ËË AUROC, AUPRC Wolff et al.",
"(2022) Coronary artery calciﬁcation (CAC) forecast Random Forest CAC risk factors* Risk prediction E Ë Recall, Speciﬁcity Wang & Zhou (2022) Cancer inference via gene expression Gradient Boosting Decision Tree iDASH 2020 Multi-class classiﬁcation E Ë Accuracy, AUC, Recall, Precision, F1-score Islam et al.",
"(2022a) Diabetic kidney risk prediction Logistic regression, MLP CERNER Health Facts Risk prediction E Ë F1-score Deist et al.",
"(2020) Lung cancer post- treatment 2-year survival Logistic regression Multi-institution lung cancer EHR* Mortality prediction E Ë RMSE, Accuracy, AUROC Park et al.",
"(2021) COVID-19 detection Transformer with DenseNet, TransUNet and RetinaNet Multi-institution COVID-19 X-ray (public and private datasets) Multi-task: classiﬁcation, segmentation, object detection I ËË AUC, mAP, Dice coefﬁcient Yan et al.",
"(2023) Multiple medical prediction tasks Self-supervised vision transformer COVID-19 X-ray, Kaggle Diabetic Retinopathy, Dermatology ISIC Binary/multi-class classiﬁcation, Object detection I ËË Accuracy, F1-score HOMOMORPHIC ENCRYPTION Boulila et al.",
"(2022) COVID-19 detection MobileNet-V2 COVID-19 X-ray Multi-class classiﬁcation I Ë Accuracy, Recall, Precision, F1-score Ma et al.",
"(2020) Heart and thyroid disease classiﬁcation XGBoost UCI Heart Disease, Kaggle Hypothyroid Binary classiﬁcation E Ë Accuracy Paul et al.",
"(2021) Intensive Care Unit patient outcome LSTM MIMIC-III Binary classiﬁcation E Ë Recall, AUROC, Precision Chen et al.",
(2022) Dermatology diagnostics SVM UCI Dermatology Multi-class classiﬁcation E Ë Accuracy Baruch et al.,
"(2022) COVID-19 detection AlexNet, SqueezeNet COVID-19 X-ray, COVID-19 CT Multi-class classiﬁcation I Ë Accuracy, F1-score DIFFERENTIAL PRIVACY Zhang et al.",
"(2021b) Thoracic pathology detection DenseNet-121 CheXpert Multi-class classiﬁcation I+E Ë AUROC, Accuracy Chilukoti et al.",
(2022) COVID-19 detection EfﬁcientNet-B2 COVID-19 X-ray Binary classiﬁcation I Ë Accuracy Suriyakumar et al.,
"(2021) Multiple medical prediction tasks CNN, DenseNet-121, Logistic regression, GRU-D MNIST NIH Chest X-ray, MIMIC-III Binary, Multi-class classiﬁcation I+E Ë AUROC privacy threats, such as reconstruction attacks by an external actor intercepting the communication channel or an honest-but-curious central server (Nguyen et al., 2022).",
"The second most commonly investigated approach for private training is HE, which leverages en- cryption schemes to provide privacy with provable mathematical guarantees.",
"However, as described in the previous section, training ML models on encrypted data signiﬁcantly increases the computa- tional complexity and the processing overhead by several orders of magnitude (Wibawa et al., 2022; Zhang et al., 2022).",
"It also adds noise to the training process due to the approximations of activation functions, especially in large models.",
"The third most common approach is standalone DP, which is less computationally demanding and provides strong privacy guarantees.",
"However, the increase in privacy guarantees is negatively cor- related with model accuracy, as it is associated with an increase in the quantity of noise applied.",
"Therefore, the trade-off between privacy (i.e., privacy budget) and model accuracy is a relevant factor to take into account for the inclusion of DP in any ML solution.",
There are other PPML ap- 4 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Table 1 Continued: Continued summary of PPML in healthcare for model training.,
"We summarize here studies that use a combination of federated learning and other privacy-preserving techniques, blockchain, Secure Multi-Party Computation (SMPC), image encryption, and image modiﬁcation.",
Reference Use case Model Medical dataset/s ML task Input modality Validation Metrics FEDERATED LEARNING + DIFFERENTIAL PRIVACY Islam et al.,
"(2022b) Cardiomyopathy risk prediction Random Forest, Naive Bayes iDASH 2021, Breast Cancer TCGA Risk prediction E Ë AUROC Kerkouche et al.",
"(2021) In-hospital mortality prediction CNN Premier Healthcare Database* Mortality prediction E Ë AUROC, Overhead Dayan et al.",
"(2021) COVID-19 patient triage ResNet-34 DeepCrossNet Multi-institution chest x-ray and EHR* Risk prediction I+E ËË AUROC, Recall, Speciﬁcity BLOCKCHAIN Zerka et al.",
(2020) Distributed training ResNet-18 NSCLC-Radiomics Binary classiﬁcation I ËË AUROC Warnat-Herresthal et al.,
(2020) Disease classiﬁcation Neural Network Blood transcriptomes* Binary classiﬁcation E Ë Accuracy FEDERATED LEARNING+HOMOMORPHIC ENCRYPTION Wibawa et al.,
"(2022) COVID-19 detection CNN COVID-19 X-ray Binary classiﬁcation I Ë Accuracy, Recall Precision, F1 score, Execution time FEDERATED LEARNING+HOMOMORPHIC ENCRYPTION+SMPC Zhang et al.",
"(2022) Skin cancer classiﬁcation CNN HAM10000 Multi-class classiﬁcation I Ë Accuracy, Overhead SMPC Hong et al.",
"(2020) Tumor detection Logistic regression iDASH 2019 Binary classiﬁcation E Ë Accuracy, Overhead IMAGE ENCRYPTION Huang et al.",
"(2022) Brain tumor, COVID-19 DenseNet-121, XceptionNet MRI Brain Tumor, COVID-19 X-ray Multi-class classiﬁcation I Ë F1-score IMAGE MODIFICATION Montenegro et al.",
"(2021) Glaucoma recognition VGAN-based CNN Warsaw-BioBase Disease-Iris v2.1 Binary classiﬁcation I Ë F1-score, Accuracy proaches for model training that have been evaluated in related work, including the addition of a blockchain ledger to avoid the centralization of training (i.e., fully distributed learning), image mod- iﬁcation to increase data privacy in the context of model explainability, and SMPC as an alternative encryption scheme to HE.",
"Most of the reviewed studies use a single source of input data i.e., image or EHR and only one medical dataset.",
"Although some studies train their models on several datasets, including popular computer vision benchmarks, the vast majority restrict their evaluation to one input modality from the same dataset.",
"This limits the generalization of the results and neglects the potential improvement in predictive performance that could result from combining different data sources in multi-modal learning set- tings (Ramachandram & Taylor, 2017).",
"Furthermore, most studies perform internal validation, such that the test sets are from the same distribution as the training dataset.",
"This is generally a com- mon challenge in healthcare applications considering distribution shifts across different hospitals, for example due to differences in patient demographics.",
"Finally, most existing work focuses on convolutional neural networks to handle computer vision tasks.",
"However, validation schemes and metrics reported are not consistent, making the comparison among them very difﬁcult.",
"Due to these reasons and the lack of medical benchmark datasets, a fair comparison of the approaches is difﬁcult, and therefore we do not assess performance metrics results in this review and defer it to future work.",
"3.2 PRIVACY-PRESERVING INFERENCE FOR HEALTHCARE We now focus on the literature employing PPML methods for inference, as summarized in Ta- ble 2.",
"We frame PPML for inference as providing private machine-learning-as-a-service (MLaaS) or inference-as-a-service (IaaS) (Lins et al., 2021).",
"In this scenario, a model with strong perfor- mance is controlled by a single party (i.e., model owner), and other external parties (i.e., clients) would like the model to perform inference on their own data.",
The external parties can share data samples with the model owner and their predictions are sent back.,
Due to legal and/or ethical con- 5 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Table 2: Summary of PPML in healthcare for model inference.,
We summarize studies that focus on developing PPML in the context of model inference.,
"We group them based on the methodology considered, i.e.",
"homomorphic encryption, combination of federated learning and Secure Multi-Party Computation (SMPC), differential privacy and SMPC, federated learning with blockchain and SMPC, and ﬁnally federated learning with differential privacy and homomorphic encryption.",
Reference Use case Model Medical dataset/s ML task Input modality Validation Metrics HOMOMORPHIC ENCRYPTION Yue et al.,
"(2021a) Breast and cervical cancer classiﬁcation Convolutional LSTM Cervigram Image, BreaKHis Binary, Multi-class classiﬁcation I Ë AUROC T’Jonck et al.",
"(2022) Breast cancer classiﬁcation Neural Network, SVM UCI IRIS, UCI Breast Cancer Binary, Multi-class classiﬁcation E Ë Accuracy, Privacy budget, Overhead Sarkar et al.",
"(2022) Cancer inference via gene expression SVM, Logistic regression, Neural Network iDASH 2020 Multi-class classiﬁcation E ËË Accuracy, AUROC Vizitiu et al.",
"(2020) Coronary angiography view classiﬁcation CNN X-ray coronary angiography* Binary, Multi-class classiﬁcation I Ë Accuracy FEDERATED LEARNING + SMPC Ziller et al.",
"(2020), Kaissis et al.",
"(2021)+ Paediatric chest X-ray classiﬁcation ResNet-18 Chest X-ray* Multi-class classiﬁcation I ËË AUROC, Latency DIFFERENTIAL PRIVACY + SMPC Singh & Shukla (2021a) Pneumonia detection CNN, VGG-16 Kaggle X-ray Pneumonia Binary classiﬁcation I Ë Accuracy Jarin & Eshete (2021) Accuracy-privacy trade-off analysis Neural Network Kaggle IDC, MIMIC-III Binary, Multi-class classiﬁcation I Ë Accuracy, Recall Precision, Privacy FEDERATED LEARNING + BLOCKCHAIN + SMPC Kasyap & Tripathy (2021) Multiple medical image datasets classiﬁcation CNN MedMNIST (CXR, Breast, Hand, ChestCT, Abdomen, HeadCT) Multi-class classiﬁcation I Ë Accuracy FEDERATED LEARNING + DIFFERENTIAL PRIVACY + HOMOMORPHIC ENCRYPTION Gopalakrishnan et al.",
"(2021) Multiple medical image datasets classiﬁcation CNN MedMNIST (Pneumonia Breast, Retina, Blood) Multi-class classiﬁcation I Ë Accuracy, Execution time, Bandwidth + Kaissis et al.",
(2021) is an extension of Ziller et al.,
"straints related to privacy, clients cannot disclose their data with the model owner, thus requiring the use of PPML to maintain the privacy of the data they wish to share.",
"Compared to the number of studies addressing PPML for training, a relatively fewer number have explored PPML for inference.",
"Most studies within the theme of PPML for inference, focus on the deployment of the trained model as a service and its use by third parties.",
"The most common approach for delivering PPML IaaS is HE, which ensures with provable mathematical guarantees that neither the model owner nor any intermediate party are able to inspect the original data nor the detection result i.e., both are encrypted and can only be decrypted by the data owner.",
"Another common approach is SMPC, which also leverages encryption schemes, being used in combination with other privacy-preserving collaborative approaches such as FL, DP and blockchain.",
"Similar to PPML for training, most studies here use a single source of input data (i.e., images in most cases), neglecting many other diverse medical modalities of varying characteristics.",
The lack of use of benchmark medical datasets and inconsistent validation schemes and metrics hinders the generalization of the proposed approaches.,
3.3 OPEN CHALLENGES There is no one-size-ﬁts-all PPML approach for model training or inference by design.,
We observe that previous work pick and choose PPML approaches based on the intended clinical use case.,
"Currently, there is no consensus on what different “privacy models” look like in healthcare.",
"Since the methodology depends on the use case, we also observe a clear trade-off between privacy and accuracy, based on the availability of computational resources.",
"For instance, standalone FL is computationally faster than HE, but it does not provide strong privacy guarantees.",
"On the other hand, HE and DP can provide strong privacy guarantees but they add noise to the model both for private training and private inference resulting in less accurate solutions.",
"For HE, this is especially critical 6 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare for model training where successive layers of approximations are needed to perform operations that are not supported, such as softmax, or that are computationally inefﬁcient, such as max pooling.",
"In general, encryption-based options are provably secure but computationally inefﬁcient, since they increase the processing overhead of training using cyphertext compared to plaintext data.",
"Additionally, the availability of computational resources is a decisive factor in choosing a partic- ular PPML methodology.",
"For instance, in the HE scenario, sending data over a communication channel does not require infrastructure for model training but still requires handling the encryp- tion/decryption process appropriately.",
"In the FL context, it requires that the entity has allocated resources for model training.",
The centralization of model training in FL poses an additional security threat.,
Relying on a single central server entails a single point of failure that is highly susceptible to security attacks such as Denial-of-Service.,
"Although blockchain has been proposed to achieve fully distributed training and mitigate this threat, it increases the complexity of the information technology infrastructure signiﬁcantly, requiring dedicated resources for the implementation of the distributed ledger and modeling framework.",
"Most existing work use a single dataset and do not conduct external validation, thus aris- ing concerns about the generalization of the results.",
We observe that existing work focus on a limited set of medical datasets.,
"Additionally, some work only evaluate their solutions on com- puter vision benchmark datasets (e.g., MNIST or CIFAR-10) inferring that good performance on these datasets will provide similar results on medical image data (Festag & Spreckelsen, 2020; Onesimu & Karthikeyan, 2020).",
"However, this assumption is not empirically supported by work that uses both medical and non-medical datasets (Suriyakumar et al., 2021; Zhang et al., 2021b; Gopalakrishnan et al., 2021; Jarin & Eshete, 2021; Vizitiu et al., 2020) and must, therefore, be avoided.",
MLaaS for healthcare has not been explored thoroughly.,
"As demonstrated by the limited lit- erature on this topic, we observe that the literature is highly skewed towards PPML for training.",
"Considering disparities in technical capabilities and expertise, information technology resources, and availability of data across medical institutions, the case in which an entity does not have enough resources to perform model training independently is highly likely.",
"Thus, the usage of third-party models as inference systems that can run on proprietary data is a prominent scenario that has not been thoroughly explored and should be considered in future research.",
"MLaaS can provide access to models with strong performance, enabling full preservation of data privacy using PPML methods.",
This makes it a more efﬁcient solution for small-scale or low-resource medical entities to access and leverage third-party knowledge.,
"4 FUTURE RESEARCH DIRECTIONS 4.1 COMPREHENSIVE EVALUATION ON DIVERSE MEDICAL DATASETS For the sake of comparison and generalization of results, studies should complement their internal dataset evaluation with additional extensive evaluation on benchmark medical datasets.",
This is due to the fact that most of the existing work use a single dataset and do not perform external validation.,
The number of studies that use external datasets for validation is marginal.,
Only 9 out of the 40 studies considered validated their results with an independent test set.,
This hampers model general- ization and hinders performance comparison among approaches built for the same medical task.,
"For benchmarking, we suggest MedMNIST (Yang et al., 2023), which contains curated datasets for dif- ferent medical tasks and modalities.",
"Therefore, similar to MNIST or CIFAR-10 for computer vision models, this medical dataset could be employed as a common benchmark for medical applications.",
"4.2 MULTI-MODAL MODELS Current advances in ML for healthcare are moving towards multi-modal learning, where several sources of information are combined to improve performance (Ramachandram & Taylor, 2017; Soenksen et al., 2022).",
This approach not only tends to provide better performance but also en- sures a comprehensive understanding of the different physiological variables involved in studying and modeling the development of human biology and pathology.,
"As observed in Sections 3.1 and 7 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare 3.2, most work is restricted to a single modality.",
"To develop robust and strong ML models, the use of different data sources to develop multi-modal systems is paramount.",
"Notwithstanding that, the use of more clinical data entails more privacy concerns (e.g., individuals may be identiﬁed using correlated data) and requires more training resources due to increased model complexity.",
"There- fore, additional privacy and computational constraints must be considered in the design of these algorithms.",
4.3 MACHINE LEARNING AS A SERVICE (MLAAS) The deployment of PPML within MLaaS is a very promising opportunity to access strong proprietary models by less resourceful institutions.,
"Indeed, one of the main objectives of ML in healthcare is to develop efﬁcient and scalable solutions that improve healthcare delivery.",
"In addition to lack of resources, the deployment of these systems in medical settings can also be highly challenging (Kreuzberger et al., 2022; Wiesenfeld et al., 2022).",
The development of MLaaS is signiﬁcantly less investigated than PPML for model training.,
"Therefore, further research on this topic is required to provide secure, private and efﬁcient data sharing between third-party model providers and client institutions.",
"Reducing obstacles for clinical institutions to access powerful inference systems could lead to a major improvement in healthcare delivery across regions, bypassing physical barriers.",
It can also lead to an increase in the conﬁdence and widespread adoption of ML in healthcare.,
It is important to note that the success of MLaaS is dependent on improvements in model generalizability and fairness in external datasets.,
"4.4 INTEGRATION OF SOTA AND ADVANCES IN DEEP LEARNING Future work should also investigate the integration of recent advances in DL and ML models in healthcare, considering that most of the current PPML work focuses on convolutional neural net- works.",
"For instance, the Transformer architecture and its variants (Dosovitskiy et al., 2020), which are considered the current SOTA for many computer vision or natural language processing tasks, are only adopted by Park et al.",
"(2021), Yang et al.",
"(2022), and Yan et al.",
(2023) in the current related literature.,
"Adopting SOTA architectures can take advantage of the latest advances in research, both in terms of optimizing hardware and software, to maintain performance improvements in clinical prediction tasks.",
"4.5 GLOBAL AND LOCAL EXPLAINABILITY Transparency and model explainability are essential for trustworthy artiﬁcial intelligence (OECD, 2023b).",
"However, PPML methods, such as data encryption or noise addition, hinder global model and local prediction explainability.",
"The collision between two key principles for trustworthy ar- tiﬁcial intelligence, secure and PPML (OECD, 2023a) and explainability, highlights an important research problem that is currently under-investigated.",
Only Montenegro et al.,
"(2021) attempt to address this problem, which should encourage future work in this research direction.",
"5 CONCLUSION In this paper, we introduce and summarize recent literature concerning PPML for model training and inference in the healthcare domain.",
"We highlight trends, challenges and promising future research directions.",
"In conclusion, we recognize the lack of consensus when it comes to deﬁning the require- ments of privacy-preserving frameworks in healthcare.",
"This requires collaboration between machine learning scientists, healthcare practitioners, and privacy and security experts.",
"From the perspective of advancing ML approaches, we encourage researchers to perform comprehensive evaluation of proposed algorithms on diverse medical datasets to increase generalization, to investigate the con- straints of PPML in multi-modal learning settings, to further consider the promise of MLaaS in healthcare as a catalyst for improved healthcare delivery, and to adopt state-of-the-art advances in deep learning architectures to enhance model performance.",
Our suggestions aim to address research gaps and guide future research in PPML to facilitate the future adoption of trustworthy and private ML for healthcare.,
"8 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare REFERENCES Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.",
Deep learning with differential privacy.,
"In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp.",
"308–318, 2016.",
"Abbas Acar, Hidayet Aksu, A Selcuk Uluagac, and Mauro Conti.",
A survey on homomorphic en- cryption schemes: Theory and implementation.,
"ACM Computing Surveys (Csur), 51(4):1–35, 2018.",
"Mansoor Ali, Faisal Naeem, Muhammad Tariq, and Geroges Kaddoum.",
Federated learning for privacy preservation in smart healthcare systems: A comprehensive survey.,
"IEEE journal of biomedical and health informatics, 2022.",
"Moran Baruch, Nir Drucker, Lev Greenberg, and Guy Moshkowich.",
A methodology for training ho- momorphic encryption friendly neural networks.,
"In Applied Cryptography and Network Security Workshops: ACNS 2022 Satellite Workshops, AIBlock, AIHWS, AIoTS, CIMSS, Cloud S&P, SCI, SecMT, SiMLA, Rome, Italy, June 20–23, 2022, Proceedings, pp.",
"Springer, 2022.",
"Wadii Boulila, Adel Ammar, Bilel Benjdira, and Anis Koubaa.",
Securing the classiﬁcation of covid- 19 in chest x-ray images: a privacy-preserving deep learning approach.,
"In 2022 2nd International Conference of Smart Systems and Emerging Technologies (SMARTTECH), pp.",
"IEEE, 2022.",
"Yange Chen, Qinyu Mao, Baocang Wang, Pu Duan, Benyu Zhang, and Zhiyong Hong.",
Privacy- preserving multi-class support vector machine model on medical diagnosis.,
"IEEE Journal of Biomedical and Health Informatics, 26(7):3342–3353, 2022.",
"Vijay Srinivas Tida Sai Venkatesh Chilukoti, Sonya Hsu, and Xiali Hei.",
Privacy-preserving deep learning model for covid-19 disease detection.,
"arXiv preprint arXiv:2209.04445, 2022.",
"Ittai Dayan, Holger R Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, Anas Z Abidin, Andrew Liu, Anthony Beardsworth Costa, Bradford J Wood, Chien-Sung Tsai, et al.",
Federated learning for predicting clinical outcomes in patients with covid-19.,
"Nature medicine, 27(10): 1735–1743, 2021.",
"Timo M Deist, Frank JWM Dankers, Priyanka Ojha, M Scott Marshall, Tomas Janssen, Corinne Faivre-Finn, Carlotta Masciocchi, Vincenzo Valentini, Jiazhou Wang, Jiayan Chen, et al.",
Dis- tributed learning on 20 000+ lung cancer patients–the personal health train.,
"Radiotherapy and Oncology, 144:189–200, 2020.",
"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.",
An image is worth 16x16 words: Transformers for image recognition at scale.,
"arXiv preprint arXiv:2010.11929, 2020.",
"Qi Dou, Tiffany Y So, Meirui Jiang, Quande Liu, Varut Vardhanabhuti, Georgios Kaissis, Zeju Li, Weixin Si, Heather HC Lee, Kevin Yu, et al.",
Federated deep learning for detecting covid-19 lung abnormalities in ct: a privacy-preserving multinational validation study.,
"NPJ digital medicine, 4 (1):60, 2021.",
"Cynthia Dwork, Aaron Roth, et al.",
The algorithmic foundations of differential privacy.,
"Foundations and Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014.",
Sven Festag and Cord Spreckelsen.,
Privacy-preserving deep learning for the detection of protected health information in real-world data: Comparative evaluation.,
"JMIR Formative Research, 4(5): e14064, 2020.",
"Matthew Field, David I Thwaites, Martin Carolan, Geoff P Delaney, Joerg Lehmann, Jonathan Sykes, Shalini Vinod, and Lois Holloway.",
Infrastructure platform for privacy-preserving dis- tributed machine learning development of computer-assisted theragnostics in cancer.,
"Journal of Biomedical Informatics, 134:104181, 2022.",
Oded Goldreich.,
Secure multi-party computation.,
Manuscript.,
"Preliminary version, 78(110), 1998.",
"9 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Aparna Gopalakrishnan, Narayan P Kulkarni, Chethan Raghavendra, Raghavendra Manjappa, Prasad B Honnavalli, and Sivaraman Eswaran.",
Primed: Private federated training and encrypted inference on medical images in healthcare.,
"Available at SSRN 4196696, 2021.",
"Lawrence O Gostin, Laura A Levit, Sharyl J Nass, et al.",
"Beyond the hipaa privacy rule: enhancing privacy, improving health through research.",
"Cheng Hong, Zhicong Huang, Wen-jie Lu, Hunter Qu, Li Ma, Morten Dahl, and Jason Mancuso.",
Privacy-preserving collaborative machine learning on genomic data using tensorﬂow.,
"In Proceed- ings of the ACM Turing Celebration Conference-China, pp.",
"39–44, 2020.",
"Qi-Xian Huang, Wai Leong Yap, Min-Yi Chiu, and Hung-Min Sun.",
Privacy-preserving deep learn- ing with learnable image encryption on medical images.,
"IEEE Access, 10:66345–66355, 2022.",
"Humayera Islam, Khuder Alaboud, Tanmoy Paul, Md Kamruz Zaman Rana, and Abu Mosa.",
A privacy-preserved transfer learning concept to predict diabetic kidney disease at out-of-network siloed sites using an in-network federated model on real-world data.,
"In AMIA Annual Symposium Proceedings, volume 2022, pp.",
"American Medical Informatics Association, 2022a.",
"Tanzir Ul Islam, Reza Ghasemi, and Noman Mohammed.",
Privacy-preserving federated learning model for healthcare data.,
"In 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC), pp.",
"IEEE, 2022b.",
Ismat Jarin and Birhanu Eshete.,
Pricure: privacy-preserving collaborative inference in a multi-party setting.,
"In Proceedings of the 2021 ACM Workshop on Security and Privacy Analytics, pp.",
"25–35, 2021.",
"Mohd Javaid, Abid Haleem, Ravi Pratap Singh, Rajiv Suman, and Shanay Rab.",
"Signiﬁcance of machine learning in healthcare: Features, pillars and applications.",
"International Journal of Intel- ligent Networks, 3:58–73, 2022.",
"Madhura Joshi, Ankit Pal, and Malaikannan Sankarasubbu.",
"Federated learning for healthcare domain-pipeline, applications and challenges.",
"ACM Transactions on Computing for Healthcare, 3(4):1–36, 2022.",
"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al.",
Highly accurate protein structure prediction with alphafold.,
"Nature, 596(7873):583–589, 2021.",
"Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Th´eo Ryffel, Dmitrii Usynin, An- drew Trask, Ion´esio Lima Jr, Jason Mancuso, Friederike Jungmann, Marc-Matthias Steinborn, et al.",
End-to-end privacy preserving deep learning on multi-institutional medical imaging.,
"Nature Machine Intelligence, 3(6):473–484, 2021.",
Harsh Kasyap and Somanath Tripathy.,
Privacy-preserving decentralized learning framework for healthcare system.,
"ACM Transactions on Multimedia Computing, Communications, and Appli- cations (TOMM), 17(2s):1–24, 2021.",
"Deeksha Kaul, Harika Raju, and BK Tripathy.",
Deep learning in healthcare.,
"Deep Learning in Data Analytics: Recent Techniques, Practices and Applications, pp.",
"97–115, 2022.",
"Raouf Kerkouche, Gergely Acs, Claude Castelluccia, and Pierre Genev`es.",
Privacy-preserving and bandwidth-efﬁcient federated learning: An application to in-hospital mortality prediction.,
"In Proceedings of the Conference on Health, Inference, and Learning, pp.",
"25–35, 2021.",
"Dominik Kreuzberger, Niklas K¨uhl, and Sebastian Hirschl.",
"Machine learning operations (mlops): Overview, deﬁnition, and architecture.",
"arXiv preprint arXiv:2205.02302, 2022.",
Geun Hyeong Lee and Soo-Yong Shin.,
Federated learning on clinical benchmark data: performance assessment.,
"Journal of medical Internet research, 22(10):e20891, 2020.",
"Sebastian Lins, Konstantin D Pandl, Heiner Teigeler, Scott Thiebes, Calvin Bayer, and Ali Sunyaev.",
Artiﬁcial intelligence as a service: Classiﬁcation and research directions.,
"Business & Information Systems Engineering, 63:441–456, 2021.",
"10 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, and Zihuai Lin.",
When machine learning meets privacy: A survey and outlook.,
"ACM Computing Surveys (CSUR), 54(2):1–36, 2021.",
"Pengrui Liu, Xiangrui Xu, and Wei Wang.",
"Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives.",
"Cybersecurity, 5(1):1–19, 2022.",
"Tyler J Loftus, Matthew M Ruppert, Benjamin Shickel, Tezcan Ozrazgat-Baslanti, Jeremy A Balch, Philip A Efron, Gilbert R Upchurch Jr, Parisa Rashidi, Christopher Tignanelli, Jiang Bian, et al.",
Federated learning for preserving data privacy in collaborative healthcare research.,
"Digital Health, 8:20552076221134455, 2022.",
"Zhuoran Ma, Jianfeng Ma, Yinbin Miao, Ximeng Liu, Kim-Kwang Raymond Choo, Ruikang Yang, and Xiangyu Wang.",
Lightweight privacy-preserving medical diagnosis in edge computing.,
"IEEE Transactions on Services Computing, 15(3):1606–1618, 2020.",
"Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.",
Communication-efﬁcient learning of deep networks from decentralized data.,
"In Artiﬁcial intelli- gence and statistics, pp.",
"PMLR, 2017.",
"Riccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley.",
"Deep learning for healthcare: review, opportunities and challenges.",
"Brieﬁngs in bioinformatics, 19(6):1236–1246, 2018.",
"Helena Montenegro, Wilson Silva, and Jaime S Cardoso.",
Privacy-preserving generative adversarial network for case-based explainability in medical image analysis.,
"IEEE Access, 9:148037–148047, 2021.",
"Dinh C Nguyen, Quoc-Viet Pham, Pubudu N Pathirana, Ming Ding, Aruna Seneviratne, Zihuai Lin, Octavia Dobre, and Won-Joo Hwang.",
Federated learning for smart healthcare: A survey.,
"ACM Computing Surveys (CSUR), 55(3):1–37, 2022.",
"Robustness, security and safety (principle 1.4), 2023a.",
URL https://oecd.ai/en/dashboards/ai-principles/P8.,
"Transparency and explainability (principle 1.3), 2023b.",
URL https://oecd.ai/en/dashboards/ai-principles/P7.,
J A Onesimu and J Karthikeyan.,
An efﬁcient privacy-preserving deep learning scheme for medical image analysis.,
"Journal of Information Technology Management, 12(Special Issue: The Impor- tance of Human Computer Interaction: Challenges, Methods and Applications.",
"):50–67, 2020.",
"David Ouyang, Bryan He, Amirata Ghorbani, Neal Yuan, Joseph Ebinger, Curtis P Langlotz, Paul A Heidenreich, Robert A Harrington, David H Liang, Euan A Ashley, et al.",
Video-based ai for beat-to-beat assessment of cardiac function.,
"Nature, 580(7802):252–256, 2020.",
"Sangjoon Park, Gwanghyun Kim, Jeongsol Kim, Boah Kim, and Jong Chul Ye.",
Federated split vision transformer for covid-19 cxr diagnosis using task-agnostic training.,
"arXiv preprint arXiv:2111.01338, 2021.",
"Jestine Paul, Meenatchi Sundaram Muthu Selva Annamalai, William Ming, Ahmad Al Badawi, Bharadwaj Veeravalli, and Khin Mi Mi Aung.",
Privacy-preserving collective learning with homo- morphic encryption.,
"IEEE Access, 9:132084–132096, 2021.",
"Andreea Bianca Popescu, Ioana Antonia Taca, Cosmin Ioan Nita, Anamaria Vizitiu, Robert Deme- ter, Constantin Suciu, and Lucian Mihai Itu.",
Privacy preserving classiﬁcation of eeg data using machine learning and homomorphic encryption.,
"Applied Sciences, 11(16):7360, 2021.",
"Adnan Qayyum, Junaid Qadir, Muhammad Bilal, and Ala Al-Fuqaha.",
Secure and robust machine learning for healthcare: A survey.,
"IEEE Reviews in Biomedical Engineering, 14:156–180, 2020.",
Dhanesh Ramachandram and Graham W Taylor.,
Deep multimodal learning: A survey on recent advances and trends.,
"IEEE signal processing magazine, 34(6):96–108, 2017.",
"11 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Daniele Rav`ı, Charence Wong, Fani Deligianni, Melissa Berthelot, Javier Andreu-Perez, Benny Lo, and Guang-Zhong Yang.",
Deep learning for health informatics.,
"IEEE journal of biomedical and health informatics, 21(1):4–21, 2016.",
"Adam Sadilek, Luyang Liu, Dung Nguyen, Methun Kamruzzaman, Stylianos Serghiou, Benjamin Rader, Alex Ingerman, Stefan Mellem, Peter Kairouz, Elaine O Nsoesie, et al.",
Privacy-ﬁrst health research with federated learning.,
"NPJ digital medicine, 4(1):132, 2021.",
"Esha Sarkar, Eduardo Chielle, Gamze Gursoy, Leo Chen, Mark Gerstein, and Michail Maniatakos.",
Scalable privacy-preserving cancer type prediction with homomorphic encryption.,
"arXiv preprint arXiv:2204.05496, 2022.",
"Mohammad Shehab, Laith Abualigah, Qusai Shambour, Muhannad A Abu-Hashem, Mohd Khaled Yousef Shambour, Ahmed Izzat Alsalibi, and Amir H Gandomi.",
Machine learning in medical applications: A review of state-of-the-art methods.,
"Computers in Biology and Medicine, 145:105458, 2022.",
Shreyansh Singh and KK Shukla.,
Privacy-preserving machine learning for medical image classiﬁ- cation.,
"arXiv preprint arXiv:2108.12816, 2021a.",
Shreyansh Singh and KK Shukla.,
Privacy-preserving machine learning for medical image classiﬁ- cation.,
"arXiv preprint arXiv:2108.12816, 2021b.",
"Luis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalobos Carballo, Liangyuan Na, Holly M Wiberg, Michael L Li, Ignacio Fuentes, and Dimitris Bertsimas.",
In- tegrated multimodal artiﬁcial intelligence framework for healthcare applications.,
"NPJ Digital Medicine, 5(1):149, 2022.",
"Saurabh Kumar Srivastava, Sandeep Kumar Singh, and Jasjit S Suri.",
Effect of incremental feature enrichment on healthcare text classiﬁcation system: A machine learning paradigm.,
"Computer methods and programs in biomedicine, 172:35–51, 2019.",
"Vinith M Suriyakumar, Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi.",
Chasing your long tails: Differentially private prediction in health care settings.,
"In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp.",
"723–734, 2021.",
"Kristof T’Jonck, Chandrakanth R Kancharla, Bozheng Pang, Hans Hallez, and Jeroen Boydens.",
Pri- vacy preserving classiﬁcation via machine learning model inference on homomorphic encrypted medical data.,
"In 2022 XXXI International Scientiﬁc Conference Electronics (ET), pp.",
"IEEE, 2022.",
"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.",
Attention is all you need.,
"Advances in neural informa- tion processing systems, 30, 2017.",
"Anamaria Vizitiu, Cosmin Ioan Nit¸˘a, Andrei Puiu, Constantin Suciu, and Lucian Mihai Itu.",
Towards privacy-preservingdeep learning based medical imaging applications.,
"In 2019 IEEE international symposium on medical measurements and applications (MeMeA), pp.",
"IEEE, 2020.",
Paul Voigt and Axel Von dem Bussche.,
The eu general data protection regulation (gdpr).,
"A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10–5555, 2017.",
Qingyong Wang and Yun Zhou.,
Fedspl: federated self-paced learning for privacy-preserving disease diagnosis.,
"Brieﬁngs in Bioinformatics, 23(1):bbab498, 2022.",
"Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathya- narayanan Manamohan, Saikat Mukherjee, Vishesh Garg, Ravi Sarveswara, Kristian H¨andler, Peter Pickkers, N Ahmad Aziz, et al.",
Swarm learning as a privacy-preserving machine learning approach for disease classiﬁcation.,
"BioRxiv, pp.",
"2020–06, 2020. WHO.",
"Who issues ﬁrst global report on artiﬁcial intelligence (ai) in health and six guiding principles for its design and use, 2021.",
"URL https://www.who.int/news/item/28-06-2021-who-issues-first-global-report-on-ai-in-h 12 ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare Febrianti Wibawa, Ferhat Ozgur Catak, Murat Kuzlu, Salih Sarp, and Umit Cali.",
Homomorphic encryption and federated learning based privacy-preserving cnn training: Covid-19 detection use- case.,
"In Proceedings of the 2022 European Interdisciplinary Cybersecurity Conference, pp.",
"85– 90, 2022.",
"Batia Mishan Wiesenfeld, Yin Aphinyanaphongs, and Oded Nov. Ai model transferability in health- care: a sociotechnical perspective.",
"Nature Machine Intelligence, 4(10):807–809, 2022.",
"Justus Wolff, Julian Matschinske, Dietrich Baumgart, Anne Pytlik, Andreas Keck, Arunakiry Natarajan, Claudio E von Schacky, Josch K Pauling, and Jan Baumbach.",
Federated machine learning for a facilitated implementation of artiﬁcial intelligence in healthcare–a proof of concept study for the prediction of coronary artery calciﬁcation scores.,
"Journal of Integrative Bioinfor- matics, 19(4), 2022.",
"Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei Wang.",
Federated learning for healthcare informatics.,
"Journal of Healthcare Informatics Research, 5:1–19, 2021.",
"Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Lei Xing, and Yuyin Zhou.",
Label-efﬁcient self-supervised federated learning for tackling data heterogeneity in medical imaging.,
"IEEE Transactions on Medical Imaging, 2023.",
"Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pﬁster, and Bingbing Ni.",
Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classiﬁcation.,
"Scientiﬁc Data, 10(1):41, 2023.",
"Seongjun Yang, Hyeonji Hwang, Daeyoung Kim, Radhika Dua, Jong-Yeup Kim, Eunho Yang, and Edward Choi.",
Towards the practical utility of federated learning in the medical domain.,
"arXiv preprint arXiv:2207.03075, 2022.",
"Zijie Yue, Shuai Ding, Lei Zhao, Youtao Zhang, Zehong Cao, Mohammad Tanveer, Alireza Jol- faei, and Xi Zheng.",
Privacy-preserving time-series medical images analysis using a hybrid deep learning framework.,
"ACM Transactions on Internet Technology (TOIT), 21(3):1–21, 2021a.",
"Zijie Yue, Shuai Ding, Lei Zhao, Youtao Zhang, Zehong Cao, Mohammad Tanveer, Alireza Jol- faei, and Xi Zheng.",
Privacy-preserving time-series medical images analysis using a hybrid deep learning framework.,
"ACM Transactions on Internet Technology (TOIT), 21(3):1–21, 2021b.",
"Jasmin Zalonis, Frederik Armknecht, Bj¨orn Grohmann, and Manuel Koch.",
Report: State of the art solutions for privacy preserving machine learning in the medical context.,
"arXiv preprint arXiv:2201.11406, 2022.",
"Fadila Zerka, Visara Urovi, Akshayaa Vaidyanathan, Samir Barakat, Ralph TH Leijenaar, Sean Walsh, Hanif Gabrani-Juma, Benjamin Miraglio, Henry C Woodruff, Michel Dumontier, et al.",
Blockchain for privacy preserving and trustworthy distributed machine learning in multicentric medical imaging (c-distrim).,
"Ieee Access, 8:183939–183951, 2020.",
"Li Zhang, Jianbo Xu, Pandi Vijayakumar, Pradip Kumar Sharma, and Uttam Ghosh.",
Homomorphic encryption-based privacy-preserving federated learning in iot-enabled healthcare system.,
"IEEE Transactions on Network Science and Engineering, 2022.",
"Wanrong Zhang, Shruti Tople, and Olga Ohrimenko.",
Leakage of dataset properties in multi-party machine learning.,
"In USENIX Security Symposium, pp.",
"2687–2704, 2021a.",
"Xinyue Zhang, Jiahao Ding, Maoqiang Wu, Stephen TC Wong, Hien Van Nguyen, and Miao Pan.",
Adaptive privacy preserving deep learning algorithms for medical data.,
"In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.",
"1169–1178, 2021b.",
"Xinyue Zhang, Jiahao Ding, Maoqiang Wu, Stephen TC Wong, Hien Van Nguyen, and Miao Pan.",
Adaptive privacy preserving deep learning algorithms for medical data.,
"In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.",
"1169–1178, 2021c.",
"Alexander Ziller, Jonathan Passerat-Palmbach, Th´eo Ryffel, Dmitrii Usynin, Andrew Trask, Ion´esio Da Lima Costa Junior, Jason Mancuso, Marcus Makowski, Daniel Rueckert, Rickmer Braren, et al.",
Privacy-preserving medical image analysis.,
"arXiv preprint arXiv:2012.06354, 2020.",
"A Benchmark Study of Machine Learning Models for Online Fake News Detection Junaed Younus Khan*1, Md.",
"Tawkat Islam Khondaker*1, Sadia Afroz2, Gias Uddin3 and Anindya Iqbal1 1Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology 2International Computer Science Institute 3Department of Electrical and Computer Engineering, University of Calgary 1405051.jyk@ugrad.cse.buet.ac.bd, 1405036.mtik@ugrad.cse.buet.ac.bd, sadia@icsi.berkeley.edu, gias.uddin@ucalgary.ca, anindya@cse.buet.ac.bd March 29, 2021 Abstract The proliferation of fake news and its propagation on social media has become a major concern due to its ability to create devastating impacts.",
Different machine learning approaches have been suggested to detect fake news.,
"However, most of those focused on a speciﬁc type of news (such as political) which leads us to the question of dataset-bias of the models used.",
"In this research, we conducted a benchmark study to assess the performance of different applicable machine learning approaches on three different datasets where we accumulated the largest and most diversiﬁed one.",
We explored a number of advanced pre-trained language models for fake news detection along with the traditional and deep learning ones and compared their performances from different aspects for the ﬁrst time to the best of our knowledge.,
"We ﬁnd that BERT and similar pre-trained models perform the best for fake news detection, especially with very small dataset.",
"Hence, these models are signiﬁcantly better option for languages with limited electronic contents, i.e., training data.",
"We also carried out several analysis based on the models’ performance, article’s topic, article’s length, and discussed different lessons learned from them.",
We believe that this benchmark study will help the research community to explore further and news sites/blogs to select the most appropriate fake news detection method.,
1 Introduction Fake news can be deﬁned as a type of yellow journalism or propaganda that consists of deliberate misinformation or hoaxes spread via traditional print and broadcast news media or online social media [35].,
"With the growth of online news portals, social-networking sites, and other online media, online fake news has become a major concern nowadays.",
But people are often unable to spend enough time to cross-check references and be sure of the credibility of news.,
"Hence, considering the scale of the users and contributors to the online media, automated detection of fake news is probably the only way to take remedial measures, and therefore currently receiving huge attention from the research community.",
"Several research works have been carried out on automated fake news detection using both traditional machine learning and deep learning methods over the years [14, 30, 52, 56, 60, 63, 67].",
"However, most of them focused on de- tecting news of particular types (such as political).",
"Accordingly, they developed their models and designed features for speciﬁc datasets that match their topic of interest.",
These approaches might suffer from dataset bias and perform poorly on news of another topic.,
"Hence, it is important to study if these are sufﬁcient for different types of news pub- lished in online media by evaluating various models on different diverse datasets and comparing their performances.",
"However, the existing comparative studies on fake news detection methods also focused on a speciﬁc type of dataset or explored a limited number of models.",
"For example, Wang built a benchmark dataset namely, Liar, and experimented *The authors contribute equally to this paper.",
Names are sorted in alphabetical order.,
1 arXiv:1905.04749v2 [cs.CL] 26 Mar 2021 some existing models on it [63].,
"However, the length of this dataset is not sufﬁcient for neural network based advanced models, and some models were found to suffer from overﬁtting.",
Gilda explored a few machine learning approaches but did not evaluate any neural network-based model [21].,
"Recently, Gravanis et al.",
evaluated a number of machine learning models on different datasets to address the issue of dataset-bias [24].,
"However, they also did not explore any deep learning based models in their study.",
"Moreover, very few works have been done to explore advanced pre-trained language models (e.g., BERT, ELECTRA, ELMo) for fake news detection [29, 32] in spite of their state-of-the-art performances in various natural language processing and text classiﬁcation tasks [1,22,36,38,42,44,61].",
"Our study ﬁlls this gap by evaluating a wide range of machine learning approaches that include both traditional (e.g., SVM, LR, Decision Tree, Naive Bayes, k-NN) and deep learning (e.g., CNN, LSTM, Bi-LSTM, C-LSTM, HAN, Conv-HAN) models on three different datasets.",
"We have prepared a new combined dataset containing 80k news of a great variety of topics (e.g., politics, economy, investigation, health-care, sports, entertainment) collecting from various sources.",
"To the best of our knowledge, this is the largest dataset used for fake news detection study.",
"We also explored a variety of pre-trained models, e.g., BERT [16], RoBERTa [39], DistilBERT [55], ELECTRA [11], ELMo [46] in our comparative analysis.",
"To the best of our knowledge, no previous study has incorporated such advanced pre-trained models to compare their performance with other machine learning models on fake news detection task.",
"In particular, we answer the following research questions.",
RQ1: How accurate are the traditional machine learning vs deep learning models to detect fake news?,
"We ﬁnd that deep learning models generally outperform the traditional machine learning models, Among the traditional learning models, Na¨ıve Bayes which achieves 93% accuracy on combined corpus.",
"Among the deep learning models, Bi-LSTM and C-LSTM show great promise with 95% accuracy on combined corpus.",
RQ2: Can the advanced pre-trained language models outperform the traditional and deep learning models?,
"We investigated pre-trained models like BERT, DistilBERT, RoBERTa, ELECTRA, and ELMo.",
"Overall, these models outperform traditional and deep learning ones.",
"For example, the pre-trained RoBERTa shows 96% accuracy on combined corpus, which is more than the traditional and deep learning models.",
"We also ﬁnd that BERT and similar transformer-based models (BERT, DistilBERT, RoBERTa, ELECTRA) perform better than ELMo.",
RQ3: Which model performs best with small training data?,
The superior performance of deep learning and pre-trained models we observed in our datasets could be due to large dataset sizes.,
"However, the construction of a large dataset may not always be possible.",
"We, therefore, attempted to understand whether smaller datasets can still be used to train the models without a considerable reduction in accuracy.",
We see that the pre-trained models can achieve high performance with very small training dataset compared to tradi- tional or deep learning models.,
"For example, RoBERTa achieved over 90% accuracy with only 500 training data used for ﬁne-tuning while traditional and deep learning models fail to achieve even 80% accuracy with such small dataset (see Figure 3).",
"In contrast, the best performing traditional learning model Na¨ıve Bayes only achieved 65% accuracy with a sample size of 500 training set.",
"Therefore, our ﬁnding can be useful for electronic-resource-limited languages where fake news dataset collections are likely to be small in size.",
"In such cases, based on our observations, pre-trained models are the best option to achieve quality performance for these languages.",
"Note that different languages such as Dutch, Italian, Arabic, Bangla, etc.",
"have pre-trained BERT models [4, 15, 47] that can be ﬁne-tuned with small fake news dataset to develop detection tool.",
Replication Package with code and data is shared online at https://github.com/JunaedYounusKhan51/ FakeNewsDetection.,
Paper Organizations.,
The rest of this paper is structured as follows.,
"In Section 2, we compare related research works.",
"In Section 3, we describe our study setup by introducing the datasets, the features, and the models we used in our experiments.",
Section 4 presents the performance of different models on three datasets and answer three research questions.,
Section 5 compares the performance and analyzes the misclassiﬁed cases.,
We conclude in Section 6.,
"2 Related Work Related work can broadly be divided into the following categories: (1) Exploratory analysis of the characteristics of fake news, (2) Traditional machine learning based detection, (3) Deep learning based detection, (4) Advanced language model based detection, and (5) Benchmark studies.",
2 Exploratory analysis of the characteristics of fake news Several research works have been done over the years on the characteristics of fake news and its’ detection.,
Conroy et al.,
"mentioned three types of fake news: Serious Fabrications, Large-Scale Hoaxes, and Humorous Fakes [53].",
They have termed fake news as a news article that is intentionally and veriﬁably false and could mislead readers [3].,
"This narrow deﬁnition is useful in the sense that it can eliminate the ambiguity between fake news and other related concepts, e.g., hoaxes, and satires.",
Traditional machine learning based detection Different traditional machine learning based approaches have been proposed for the automatic detection of fake news.,
"In [57], the authors proposed to use linguistic-based features such as total words, characters per word, frequencies of large words, frequencies of phrases, i.e., “n-grams” and bag-of-words approaches [20], parts-of-speech (POS) tagging for fake news detection.",
Conroy et al.,
argued that simple content-related n-grams and part-of-speech (POS) tagging had been proven insuf- ﬁcient for the classiﬁcation task [13].,
"Rather, they suggested Deep Syntax analysis using Probabilistic Context-Free Grammars (PCFG) following another work by Feng et al.",
"[19] to distinguish rule categories (i.e., lexicalized, non- lexicalized, parent nodes, etc.)",
for deception detection with 85-91% accuracy.,
"However, Shlok Gilda reported that while bi-gram TF-IDF yielded highly effective models for detecting fake news, the PCFG features had little to add to the models’ efﬁcacy [21].",
Many research works also suggested the use of sentiment analysis for deception detection as some correlation might be found between the sentiment of the news article and its type.,
"Reference [52] proposed expanding the possi- bilities of word-level analysis by measuring the utility of features like part of speech frequency, and semantic categories such as generalizing terms, positive and negative polarity (sentiment analysis).",
"Cliche described the detection of sarcasm on twitter using n-grams, words learned from tweets speciﬁcally tagged as sarcastic [12].",
His work also included the use of sentiment analysis as well as identiﬁcation of topics (words that are often grouped together in tweets) to improve prediction accuracy.,
Deep learning based detection Several research works used deep learning models to detect fake news.,
Wang et al.,
built a hybrid convolutional neural network model that outperforms other traditional machine learning models [63].,
Rashkin et al.,
performed an extensive analysis of linguistic features and showed promising result with LSTM [50].,
Singhania et al.,
"proposed a three-level hierarchical attention network, one each for words, sentences, and the headline of a news article [58].",
Ruchansky et al.,
"created the CSI model where they have captured text, the response of an article, and the source characteristics based on users’ behaviour [54].",
"Among the recent works, Shu et al.",
argued that a critical aspect of fake news detection is the explainability of such detection in [56].,
The authors developed a sentence-comment co-attention sub-network to exploit both news contents and user comments.,
"In this way, the authors focused on jointly capturing explainable check-worthy sentences and user comments for fake news detection.",
"In the work [30], the authors developed a multimodal variational auto-encoder by using a bi-modal variational auto-encoder coupled with a binary classiﬁer for the task of fake news detection.",
The authors claimed that this end-to-end network utilizes the multimodal representations obtained from the bi-modal variational auto-encoder to classify posts as fake or not.,
Zhou et al.,
"focused on studying the patterns of spreading of fake news in social networks, and the relationships among the spreaders [67].",
Hamdi et al.,
proposed a hybrid approach to detect misinformation in Twitter [25].,
The authors extracted user characteristics using node2vec to verify the credibility of the contents.,
"Advanced language model based detection Currently, Advanced pre-trained language models (i.e., BERT, ELECTRA, ELMo) are receiving great attention for several natural language tasks including text classiﬁcation [1,22,36,38,42,44,61].",
"However, only a few studies have explored them for fake news detection.",
"For example, Jwa et al.",
detected fake news by analyzing the relationship between the headline and the body text of news [29].,
The authors claimed that the deep-contextualizing nature of BERT improves F-score by 0.14 over the previous state-of-the-art models.,
Kula et al.,
presented a hybrid architecture 3 Table 1: Comparison between our benchmark study and prior benchmark studies Theme Prior Benchmark Study Limitations Our Benchmark Study Experiment and Result Bondielli et al.,
surveyed the different approaches to automatic detection of fake news in the recent literature [6].,
They did not run any experiments and did not report any results.,
We experimented with all the models and analyzed their performances.,
Dwivedi et al.,
presented a literature survey on various fake news detection methods [17].,
Zhang et al.,
presented an overview of the existing datasets and fake news detection approaches [66].,
"Dataset Length and Diversity Wang experimented with some existing models on their benchmark dataset namely, Liar [63].",
They evaluated the models only on one dataset.,
"Moreover, the length of the dataset was not sufﬁcient, and some models were found to suffer from overﬁtting.",
We evaluated all the methods on three different and diverse datasets.,
Range of Models Explored Gilda explored a few machine learning approaches for fake news detection [21].,
They did not evaluate any deep learning based model.,
We explored deep learning and advanced pre-trained language models along with traditional ones.,
Gravanis et al.,
evaluated a number of machine learning models on different datasets [24].,
Oshikawa et al.,
compared various existing methods for fake news detection on different datasets [43].,
"They did not explore advanced language models such as BERT, ELECTRA, ELMo, etc 4 Table 2: Properties of Datasets Dataset #Total Data #Fake News #Real News Avg.",
"Length of News Articles (in words) Topic(s) LIAR 12791 5657 7134 18 Politics Fake or Real News 6335 3164 3171 765 Politics (2016 USA election) Combined Corpus 79548 38859 40689 644 Politics, Economy, Investigation, Health, Sports, Entertainment connecting BERT with RNN to tackle the impact of fake news [32].",
worked on hyperpartisan dataset and leveraged BERT on semi-supervised pseudo-label dataset [34].,
"Benchmark studies While most of the existing researches have focused on deﬁning the types of fake news and suggesting different ap- proaches to detect them, very few studies are carried out to compare such approaches independently on different datasets.",
"Among the categories, the benchmark-based studies are the most similar to our study.",
"Table 1 compares our work with the previous benchmark-based studies along three themes: (1) experimental setup and results, (2) dataset length and diversity, and (3) range of models explored.",
We discuss the related work below.,
Wang et al.,
"compared the performance of SVM, LR, Bi-LSTM, and CNN models on their proposed dataset “LIAR” [63].",
Oshikawa et al.,
"compared various machine learning models (e.g., SVM, CNN, LSTM) for fake news detection on different datasets [43].",
Gravanis et al.,
"compared several traditional machine learning models (i.e., k-NN, Decision Tree, Naive Bayes, SVM, AdaBoost, Bagging) for fake news detection on different datasets [24].",
Dwivedi et al.,
presented a literature survey on various fake news detection methods [17].,
Zhang et al.,
presented a comprehensive overview of the existing datasets and approaches proposed for fake news detection in previous literature [66].,
"In summary, these few existing comparative studies lack in terms of the range of evaluated models and the diversity of the used datasets.",
"Moreover, a complete exploration of the advanced pre-trained language models for fake news detection and comparison among them and with other models (i.e., traditional and deep learning) were missing in previous works.",
The benchmark study presented in this paper is focused on dealing with the above issues.,
"We extend the state-of-the-art research in fake news detection by offering a comprehensive an in-depth study of 19 models (eight traditional shallow learning models, six traditional deep learning models, and ﬁve advanced pre-trained language models).",
"3 Study Setup In this section, we ﬁrst introduce the datasets used in our study and discuss how we preprocess those (Section 3).",
Then we discuss different features that we used in our models in Section 3.,
"Finally, we discuss the traditional learning, deep learning and pre-trained models that we investigated in our study (Section 3).",
"Finally, we discuss the performance metrics we used to evaluate the models and the train and test data settings in Section 3.",
"Studied Datasets In this comparative study, we make use of three following datasets.",
Table 2 shows the detailed statistics of them.,
We describe the datasets below.,
5 Liar Liar1 is a publicly available dataset that has been used in [63].,
It includes 12.8K human-labeled short statements from POLITIFACT.COM.,
"It comprises six labels of truthfulness ratings: pants-ﬁre, false, barely-true, half-true, mostly-true, and true.",
"In our work, we try to differentiate real news from all types of hoax, propaganda, satire, and misleading news.",
"Hence, we mainly focus on classifying news as real and fake.",
"For the binary classiﬁcation of news, we transform these labels into two labels.",
"Pants-ﬁre, false, barely-true are contemplated as fake and half-true, mostly-true, and true are as true.",
Our converted dataset contains 56% true and 44% fake statements.,
"This dataset mostly deals with political issues that include statements of democrats and republicans, as well as a signiﬁcant amount of posts from online social media.",
"The dataset provides some additional meta-data like the subject, speaker, job, state, party, context, history.",
"However, in the real-life scenario, we may not have this meta-data always available.",
"Therefore, we experiment on the texts of the dataset using textual features.",
Fake or Real News Fake or real news dataset is developed by George McIntire.,
The fake news portion of this dataset was collected from Kaggle fake news dataset2 comprising news of the 2016 USA election cycle.,
"The real news portion was collected from media organizations such as the New York Times, WSJ, Bloomberg, NPR, and the Guardian for the duration of 2015 or 2016.",
"The GitHub repository of the dataset includes around 6.3k news with an equal allocation of fake and real news, and half of the corpus comes from political news.",
"Combined Corpus Apart from the other two datasets, we have built a combined corpus that contains around 80k news among which 51% are real, and 49% are fake.",
"One important property of this corpus is that it incorporates a wide range of topics including national and international politics, economy, investigation, health-care, sports, entertainment, and others.",
"To demonstrate the topic diversity, we show the inter-topic distances3 of our combined corpus using LDA-based (Latent Dirichlet Allocation) topic modeling [5] in Figure 1.",
"Based on the empirical analysis of inter-topic distances, we divided the dataset into ten clusters (circles) where each cluster represents a topic.",
The coordinates of each topic cluster (circle) were measured following the MDS (Multidimensional Scaling) algorithm [9].,
X-axis (PC1) and Y-axis (PC2) maintained an aspect ratio to 1 to preserve the MDS distances.,
We used Jensen-Shannon divergence [37] to compute distances between topics.,
The area of a cluster was calculated by the portion of tokens that respective topic generated compared to the total tokens in the corpus.,
We named the topic of a cluster based on the most relevant terms representing that cluster.,
The most relevant terms were determined on the basis of frequency.,
"For example, the most relevant (i.e., most frequent) terms for cluster-7 are ‘Trump’, ‘Clinton’, ‘Election’, ‘Campaign’, etc (Figure 1).",
"Hence, the news of this cluster represents the 2016 US election.",
"On the other hand, the most relevant terms for cluster-3 are ’Bank’, ’Job’, ’Financial’, ’Tax’, ’Market’, etc.",
"Thus, this cluster is related to the Economy.",
"Additionally, overlapping of clusters (e.g., Economy and Politics) indicates shared relevant words (e.g., ‘Government’, ‘People’) between them.",
"We have collected news from several sources of the same time domain mostly from 2015 to 2017 4,5,6.",
"Multiple types of fake news such as hoax, satire, and propaganda have come from The Onion, Borowitz Report, Clickhole, American News, DC Gazette, Natural News, and Activist Report.",
"We have collected the real news from the trusted sources like the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Gigaword News, Reuters, Vox, and the Washington Post.",
"Data Preprocessing Before feeding into the models, raw texts of news required some preprocessing.",
We ﬁrst eliminated unnecessary IP and URL addresses from our texts.,
The next step was to remove stop words.,
"After that, we cleaned our corpus by correcting the spelling of words.",
We split every text by white-space and remove sufﬁces from words by stemming 1https://www.cs.ucsb.edu/˜william/data/liar_dataset.zip 2https://www.kaggle.com/mrisdal/fake-news 3generated using pyLDAvis: https://pyldavis.readthedocs.io/ 4https://homes.cs.washington.edu/˜hrashkin/factcheck.html 5https://github.com/suryattheja/Fake-news-detection 6https://www.kaggle.com/snapcrack/all-the-news 6 Figure 1: Inter-topic distance map of Combined Corpus.,
"Finally, we rejoined the word tokens by white-space to present our clean text corpus which had been tokenized later for feeding into the models.",
"Studied Features We used lexical and sentiment features, n-gram, and Empath generated features for traditional machine learning mod- els, and pre-trained word embedding for deep learning models.",
"Lexical and Sentiment Features Several studies have proposed to use lexical and sentiment features for fake news detection [50, 52, 57].",
"For lexical features, we used word count, average word length, article length, count of numbers, count of parts of speech, and count of exclamation mark.",
"We calculated the sentiment (i.e., positive and negative polarity) of every article and used them as sentiment features.",
"n-gram Feature Word-based n-gram was used to represent the context of the document and generate features to classify the document as fake and real [2,7,23,50,62,65].",
We used both uni-gram and bi-gram features in this benchmark and evaluated their effectiveness.,
Empath Generated Features Empath is a tool that can generate lexical categories from a given text using a small set of seed terms [18].,
"Using Empath, we calculated these categories (e.g., violence, crime, pride, sympathy, deception, war) for every news data and used them as features to identify key information in a news article.",
"Since it has been used in literature for understanding deception in review systems [18], we feel motivated to investigate their contribution in this context.",
"Pre-trained Word Embedding For neural network models, word embeddings were initialized with 100-dimensional pre-trained embeddings from GloVe [45].",
GloVe is an unsupervised learning algorithm for obtaining vector representations for words.,
It was trained on a dataset of one billion tokens (words) with a vocabulary of 400 thousand words.,
"7 Studied Models We experimented various traditional, deep learning and pre-trained language models in this work.",
"Here, we describe all the models that we studied.",
"Traditional Machine Learning Models We built our ﬁrst three models using SVM (Support Vector Machine), LR (Logistic Regression), and Decision Tree with the lexical and sentiment features.",
"Among the four main variants of the SVM kernel, we used the linear one.",
We also evaluated ensemble learning method like AdaBoost combining 30 decision trees with lexical and sentiment features.,
"Next, we explored the Multinomial Naive Bayes classiﬁer with the n-gram features.",
We used the Empath generated features with k-NN (k-Nearest Neighbors) classiﬁer.,
We use the square-root of the total training data size as k as suggested by Lall and Sharma [33].,
"Hence, the value of k was chosen to be 70, 90, and 250 for Liar, Fake or Real, and Combined Corpus respectively.",
"Deep Learning Models In this study, we have evaluated six deep learning models for fake news detection including CNN, LSTM, Bi-LSTM, C-LSTM, HAN, and Convolutional HAN.",
The models are described below with their experimental setups.,
(1) CNN: One dimensional convolutional neural network can extract features and classify texts after transforming words in the sentence corpus into vectors [31].The one-dimensional convolutional model was initialized with 100- dimensional pre-trained GloVe embeddings.,
It contained 128 ﬁlters of ﬁlter size 3 and a max pooling layer of pool size 2 is selected.,
A dropout probability of 0.8 was preserved which was expunged for Combined Corpus.,
The model was compiled with ADAM optimizer with a learning rate of 0.001 to minimize binary cross-entropy loss.,
A sigmoid activation function was used for the ﬁnal output layer.,
A batch size of 64 and 512 was used for training the datasets over 10 epochs.,
(2) LSTM: Our LSTM model was pre-trained with 100-dimensional GloVe embeddings.,
The output dimension and time steps were set to 300.,
ADAM optimizer with learning rate 0.001 was applied to minimize binary cross-entropy loss.,
Sigmoid was the activation function for the ﬁnal output layer.,
The model was trained over 10 epochs with batch size 64 and 512.,
"(3) Bi-LSTM: Usually, news that is deemed as fake is not fully comprised of false information, rather it is blended with true information.",
"To detect the anomaly in a certain part of the news, we need to examine it both with previous and next events of action.",
We constructed a Bi-LSTM model to perform this task.,
Bi-LSTM was initialized with 100- dimensional pre-trained GloVe embeddings.,
The output dimension of 100 and time steps of 300 was applied.,
ADAM optimizer with a learning rate of 0.001 was used to minimize binary cross-entropy loss.,
The training batch size was set to 128 and loss over each epoch was observed with a callback.,
The learning rate was reduced by a factor of 0.1.,
We also used an early stop to monitor validation accuracy to check whether the accuracy was deteriorating for 5 epochs.,
The loss of the binary cross-entropy of the model was minimized by ADAM with a learning rate of 0.0001.,
(4) C-LSTM: The C-LSTM based model contained one convolutional layer and one LSTM layer.,
We used 128 ﬁlters with ﬁlter size 3 on top of which a max pooling layer of pool size 2 was set.,
We fed it to our LSTM architecture with 100 output dimensions and dropout 0.2.,
"Finally, we used sigmoid as the activation function of our output layer.",
(5) HAN: We used a hierarchical attention network consisting of two attention mechanisms for word-level and sentence-level encoding.,
"Before training, we set the maximum number of sentences in a news article as 20 and the maximum number of words in a sentence as 100.",
"In both level encoding, a bidirectional GRU with output dimension 100 was fed to our customized attention layer.",
We used word encoder as input to our sentence encoder time-distributed layer.,
We optimized our model with ADAM that learned at a rate of 0.001.,
"(6) Convolutional HAN: In order to extract high-level features of the input, we incorporated a one-dimensional convolutional layer before each bidirectional GRU layer in HAN.",
This layer selected features of each tri-gram from the news article before feeding it to the attention layer.,
8 Figure 2: Fine-tuning of pre-trained language models.,
"Advanced Language Models Here, we ﬁrst discuss the advanced language models that we used in this study and then describe their experimental setup.",
(1) BERT: BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model which was designed to learn contextual word representations of unlabeled texts [16].,
"Among the two versions of BERT (i.e., BERT-Base and BERT-Large) proposed originally, we used BERT-Base for this study considering the huge time and memory requirements of the BERT-Large model.",
The BERT-Base model has 12 layers (transformer blocks) with 12 attention heads and 110 million parameters.,
"(2) RoBERTa: RoBERTa (Robustly optimized BERT approach), originally suggested in [39], is the second pre- trained model that we experimented.",
It achieves better performance than original BERT models by using larger mini- batch sizes to train the model for a longer time over more data.,
It also removes the NSP loss in BERT and trains on longer sequences.,
"Moreover, it dynamically changes the masking pattern applied to the training data.",
"(3) DistilBERT: DistilBERT [55] is a smaller, faster, cheaper, and lighter version of original BERT which has 40% fewer parameters than the BERT-Base model.",
"Though original BERT models perform better, DistilBERT is more appropriate for production-level usage due to its’ low resource requirements.",
"Considering potential users of non-proﬁt blogs and online media, we think low-resource models have a good appeal.",
"Hence, this is worth investigating.",
(4) ELECTRA: ELECTRA (Efﬁciently Learning an Encoder that Classiﬁes Token Replacements Accurately) [11] is a transformer model for self-supervised language representation learning.,
This model pre-trained with the use of another (small) masked language model.,
"First, a language model takes an input text and randomly masked the text with generated input token.",
"Then, ELECTRA models are trained to distinguish ”real” input tokens vs ”fake” input tokens generated by the former language model.",
"At small scale, ELECTRA can achieve strong results even when trained on a single GPU.",
(5) ELMo: ELMo (Embeddings from Language Models) is a contextualized word representation learned from a deep bidirectional language model that is trained on a large text corpus [46].,
We used the original pre-trained ELMo model proposed by the authors that has 2 bi-LSTM layers and 93.6 million parameters.,
Experimental Setup of Advanced Language Models: We appended a classiﬁcation head composed of a single linear layer on the top of the pre-trained advanced language models.,
The architecture of the classiﬁer head is kept simple to focus on what information can readily be extracted from these pre-trained models.,
"We used the respective pre-trained embeddings of the corresponding models (e.g., BERT embeddings, ELECTRA embeddings, ELMo embeddings) as the input of the classiﬁcation heads and ﬁne-tuned them for the fake news detection task (Figure 2).",
We trained them on all the datasets for 10 epochs with a mini-batch size of 32.,
We applied early stop to prevent our models from overﬁtting [49].,
Validation loss was considered as the metric of the early stopping while delta is set to zero [48].,
We set the maximum sequence length of the input data to 300.,
"For the Combined Corpus dataset, we conﬁgured the gradient accumulation steps as 2 due to the large dataset size.",
"We used AdamW optimizer [40] with the learning rate set to 4e-5, ß1 to 0.9, ß2 to 0.999, and epsilon to 1e-8 [16,59].",
"Finally, we used binary cross-entropy to calculate the loss [51].",
We performed the experiments on NVIDIA Tesla T4 GPU provided by Google Colab.,
Evaluation Metrics We created a standard training and test set for each of the three datasets by splitting it in an 80:20 ratio so that different models can be evaluated on the same ground.,
"For the ﬁrst two datasets (i.e., Liar, Fake or Real), we did the split 9 randomly as they only contain one type of news.",
"On the other hand, as the Combined Corpus covers a wide variety of topics, we took 80% (20%) data from each topic and include them in train (test) set to maintain a balanced distribution of every topic in training and test data.",
"We report the performance of each model in terms of accuracy, precision, recall, and F1-score.",
"For precision, recall, and F1-score, we considered the macro-average of both class.",
"In our experiment, we considered real news as ‘positive class’, and fake news as ‘negative class’.",
"Hence, True Positive (TP) means the news is actually real, and also predicted as real while False Positive (FP) indicates that the news is actually false, but predicted as real.",
True Negative (TN) and False Negative (FN) imply accordingly.,
Accuracy is the number of correctly predicted instances out of all instances.,
Accuracy(A) = TP + TN TP + FN + TN + FP (1) Precision is the ratio between the number of correctly predicted instances and all the predicted instances for a given class.,
"For real and fake classes, we presented this metric as P(R) and P(F) respectively.",
"Hence, the macro-average precision, P will be the average of P(R) and P(F).",
"P(R) = TP TP + FP , P(F) = TN TN + FN , P = P(R) + P(F) 2 (2) Recall represents the ratio of the number of correctly predicted instances and all instances belonging to a given class.",
"For real and fake classes, we presented this metric as R(R) and R(F) respectively.",
"Hence, the macro-average recall, R will be the average of R(R) and R(F).",
"R(R) = TP TP + FN , R(F) = TN TN + FP , R = R(R) + R(F) 2 (3) F1-score is the harmonic mean of the precision and recall.",
"F1 = 2 · P · R P + R (4) 4 Study Results In this section, we answer three research questions: RQ1.",
How accurate are the traditional and deep learning models to detect fake news in our datasets?,
Can the advanced pre-trained language models outperform the traditional and deep learning models?,
Which model performs best with small training data?,
Previous studies on fake news detection mainly focused on traditional machine learning models.,
"Therefore, it is im- portant to compare their performance with the deep learning models.",
We address this concern in RQ1.,
"In particular, the goal of RQ1 is to compare the performance of different traditional machine learning models (e.g., SVM, Naive Bayes, Decision Tree) and deep learning models (e.g., CNN, LSTM, Bi-LSTM) on fake news detection.",
"Consider- ing the great success of pre-trained advanced language models on various text classiﬁcation tasks, it is important to investigate how these models perform on fake news detection compared to the traditional and deep learning models.",
The answers to RQ2 will offer insights into whether and how the pre-trained advanced language models are useful to detect fake news.,
A common issue for any supervised learning problem is the limitation of labeled data.,
"Intuitively, the more performance we can get with less amount of labeled data, the easier it would be to investigate and develop machine learning models to facilitate fake news detection.",
"Therefore, as part of RQ3, we investigate the performance of the models we used in our study on smaller samples of our datasets.",
10 Table 3: Performance of Traditional Machine Learning Models Datasets Liar Fake or Real News Combined Corpus Model Feature A P R F1 A P R F1 A P R F1 SVM Lexical .56 .56 .56 .48 .67 .67 .67 .67 .71 .78 .71 .72 SVM Lexical +Sentiment .56 .57 .56 .48 .66 .66 .66 .66 .71 .77 .71 .72 LR Lexical +Sentiment 0.56 .56 .56 .51 .67 .67 .67 .67 .76 .79 .76 .77 Decision Tree Lexical +Sentiment .51 .51 .51 .51 .65 .65 .65 .65 .67 .71 .69 .7 AdaBoost Lexical +Sentiment .56 .56 .56 .54 .72 .72 .72 .72 .73 .74 .73 .74 Naive Bayes Unigram (TF-IDF) .60 .60 .60 .57 .82 .82 .82 .82 .91 .91 .91 .91 Naive Bayes Bigram (TF-IDF) .60 .59 .60 .59 .86 .86 .86 .86 .93 .93 .93 .93 k-NN Empath Features .54 .54 .54 .54 .71 .72 .71 .71 .71 .70 .70 .70 How accurate are the traditional and deep learning models to detect fake news in our datasets?,
"(RQ1) In Table 3, we report the performances of various traditional machine learning models in detecting fake news.",
"We observe that among the traditional machine learning models, Naive Bayes with n-gram features performs the best with 93% accuracy on our Combined Corpus.",
We also ﬁnd that the addition of sentiment features with lexical features does not improve the performance considerably.,
"For lexical and sentiment features, SVM and LR models perform better than other traditional machine learning models as suggested by most of the prior studies [10, 52, 60, 63, 64].",
"On the other hand, Empath generated features do not show promising performance for fake news detection, although they had been used earlier for understanding deception in review systems [18].",
"In Table 4, we report the performances of different deep learning models.",
"The baseline CNN model is considered as the best model for Liar in [63], but we ﬁnd it to be the second-best among all the models.",
LSTM-based models are most vulnerable to overﬁtting for this dataset which is reﬂected by its performance.,
"Although Bi-LSTM is also a vic- tim of overﬁtting on the Liar dataset as mentioned in [63], we ﬁnd it to be the third-best neural network-based model according to its performance on the dataset.",
"The models successfully used for text classiﬁcation like C-LSTM, HAN hardly surmount the overﬁtting problem for the Liar dataset.",
Our hybrid Conv-HAN model exhibits the best perfor- mance among the neural models for the Liar dataset with 0.59 accuracy and 0.59 F1-score.,
LSTM-based models show an improvement on the Fake or Real dataset whereas CNN and Conv-HAN continue their impressive performance.,
LSTM-based models exhibit their best performance on our Combined Corpus where both Bi-LSTM and C-LSTM achieve 0.95 accuracy and 0.95 F1-score.,
CNN and all hierarchical attention models including Conv-HAN maintain a decent performance on this dataset with more than 0.90 accuracy and F1-score.,
"This result indicates that, although neural network-based models may suffer from overﬁtting for a small dataset (LIAR), they show high accuracy and F1-score on a moderately large dataset (Combined Corpus).",
"We ﬁnd that the traditional machine learning models are generally outperformed by the deep learning models in fake news detection, i.e., the overall accuracy of the traditional models is much lower than the deep learning ones (Table 11 Table 4: Performance of Deep Learning Models (using Glove word embedding as feature) Datasets Liar Fake or Real News Combined Corpus Model A P R F1 A P R F1 A P R F1 CNN .58 .58 .58 .58 .86 .86 .86 .86 .93 .93 .93 .93 LSTM .54 .29 .54 .38 .76 .78 .76 .76 .93 .94 .93 .93 Bi-LSTM .58 .58 .58 .57 .85 .86 .85 .85 .95 .95 .95 .95 C-LSTM .54 .29 .54 .38 .86 .87 .86 .86 .95 .95 .95 .95 HAN .57 .57 .57 .56 .87 .87 .87 .87 .92 .92 .92 .92 Conv-HAN .59 .59 .59 .59 .86 .86 .86 .86 .92 .92 .92 .92 3, 4).",
"The difference is more prominent on large dataset, i.e., Combined Corpus which highlights the fact that deep learning models are prone to overﬁtting on small dataset.",
"However, despite being a traditional model, Naive Bayes (with n-gram) shows great promise in fake news detection which almost reaches the performance of deep learning models and achieves 93% accuracy on Combined Corpus.",
"However, further analysis indicates that the performance of Naive Bayes reaches saturation at some point (2.5K training data) and after that improves very slowly with the increase of sample size, while the performance of the deep learning model, i.e., Bi-LSTM has a greater rate of improvement with the increase of training data (see Figure 3).",
"So it can be deduced that with enough training samples, deep learning models might be able to outperform Naive Bayes.",
Summary of RQ1.,
How accurate are the traditional vs deep learning models to detect fake news?,
The deep learning models generally outperform the traditional learning models.,
The difference of performance between deep learning and traditional models depends on the dataset length.,
"While deep learning models are vulnerable to overﬁtting on a small dataset, traditional models like Naive Bayes can show impressive performance on this type of dataset.",
"As the dataset length increases, the performance of the deep learning models also improves, and as a result, the deep learning models outperform the traditional models on a large dataset.",
Can the advanced pre-trained language models outperform the traditional and deep learning models?,
(RQ2) Table 5 shows the performances of different pre-trained language models on three datasets.,
"While these models incor- porate more complex architectures, they do not suffer from overﬁtting on a smaller dataset as much as the deep learning models do as previously discussed.",
This is because these models use pre-trained weights in all the layers except the ﬁnal classiﬁcation layers.,
"As a result, they do not need a large dataset for ﬁne-tuning their complex architecture.",
"Therefore, all the pre-trained models we evaluated outperform the other traditional ML and deep learning-based mod- els having F1-score no less than 0.62 on the Liar dataset and no less than 0.95 on the Fake or Real News dataset.",
"Given the large dataset (i.e., Combined Corpus), these pre-trained models achieve better performance in the fake news detection task.",
"We observe that among the pre-trained language models, the BERT and transformer-based models (i.e., BERT, RoBERTa, DistilBERT, ELECTRA) are generally better than the other one (i.e., ELMo).",
"For example, Dis- tillBERT (66M parameters), BERT (110M parameters), Electra (110M parameters), RoBERTa (125M parameters), achieve 0.93, 0.95, 0.95, and 0.96 accuracy, respectively on the Combined Corpus dataset while ELMo (93.6M pa- rameters) achieves 0.91.",
We also notice that the performance of the transformer-based models is proportionate to their number of pre-trained parameters.,
"This relative performance can be justiﬁed by their state-of-the-art results on the text classiﬁcation task [39,55].",
12 Table 5: Performance of Advanced Pre-trained Language Models Datasets Liar Fake or Real News Combined Corpus Model A P R F1 A P R F1 A P R F1 BERT .62 .62 .62 .62 .96 .96 .96 .96 .95 .95 .95 .95 RoBERTa .62 .63 .62 .62 .98 .98 .98 .98 .96 .96 .96 .96 DistilBERT .60 .60 .60 .60 .95 .95 .95 .95 .93 .93 .93 .93 ELECTRA .61 .61 .61 .61 .96 .96 .96 .95 .95 .95 .95 .95 ELMo .61 .61 .61 .61 .93 .93 .93 .93 .91 .91 .91 .91 Summary of RQ2.,
Can the advanced pre-trained language models outperform the traditional and deep learning models?,
"In our experiment, the pre-trained models perform signiﬁcantly better than the traditional and deep learning models on all datasets (Table 3, 4, 5).",
"Since these models are pre-trained to learn contextual text representations on much larger quantities of text corpus and they have produced new state-of-the-art in several text classiﬁcation tasks [41], their commanding performance over the traditional and deep learning models in the fake news detection task is quite expected.",
Which model performs best with small training data?,
"(RQ3) Figure 3: Comparison of Naive Bayes, Bi-LSTM, and RoBERTa with different training dataset size (from Fake or Real News dataset).",
We ﬁnd that pre-trained BERT-based models can perform very well with small datasets.,
We can realize that from their superior performance on small datasets like Liar and Fake or Real News which is signiﬁcantly better than other models.,
"To further verify this, we take the best model from each of three types, i.e., Naive Bayes with n-gram (tradi- 13 tional), Bi-LSTM (deep learning), RoBERTa (BERT-based) and compare their performances.",
"As their performances differ on the Fake or Real News dataset by very clear margins, we choose this dataset for this analysis.",
"We report their accuracy on small sets of training data (i.e., 500, 2500, and 5000) chosen from Fake or Real News dataset.",
We show that RoBERTa achieves notably better performance than the other two (Figure 3).,
RoBERTa reaches more than 90% accuracy with just 500 training data and continues to improve with the increase of sample size.,
It hits 98% accuracy with 5000 sample size.,
"On the other hand, both Naive Bayes and Bi-LSTM perform poorly when the size of training dataset is very small, i.e., 500 (Figure 3).",
"Though their performances improve with the increase of dataset size, they fail to achieve 90% accuracy when the sample size is below 5000.",
Figure 4: Comparison of RoBERTa’s performance on different training dataset size (from Fake or Real News dataset).,
We further analyze the performance of RoBERTa on smaller datasets (Figure 4).,
We ﬁnd that the model continues to exhibit impressive accuracy (84%) even when the dataset size is 300.,
This is because pre-trained weights of RoBERTa have already learned the semantic representation from large text corpora.,
Fine-tuning on the labeled news articles help to learn the model to distinguish between the real and fake news.,
We observe that the performance of the model starts to drop quickly after the dataset length has been reduced to less than 300.,
Reducing the data makes it more difﬁcult for the model to differentiate the news articles.,
"Therefore, the performance decreases quickly.",
Summary of RQ3.,
Which model performs best with small training data?,
"Pre-trained models (i.e., RoBERTa) show quality performance even with very small training data in our experiment.",
We ﬁnd that RoBERTa achieves over 90% accuracy with a training set of 500 samples only (see Figure 3).,
"5 Discussion In this section, we compare the performance of the 19 models we studied along several dimensions like features used, resource requirements, etc.",
(see Section 5).,
"We then analyze our models’ misclassiﬁcation, which is discussed in Section 5.",
"Analysis of Performance of Different Models In Table 6 we summarize the models we studied in our study based on their accuracy across the three datasets, i.e., Liar, Fake or Real, Combined Corpus.",
"Among the eight types of models we studied under traditional learning approach, Na¨ıve Bayes shows the best accuracy on all the three datasets: combined corpus (0.93), Fake or Real (0.86), and Liar (0.60).",
"Among the six traditional deep learning models we studied, there are three different winners in the three datasets: C-LSTM shows the best performance on the combined corpus (Accuracy = 0.95), HAN shows the best performance (Accuracy = 0.87) on Fake or Real and HAN shows the best performance on the Liar dataset (Accuracy 14 Table 6: Summary of all models and performances Model Type Model Rationale for Picking Feature Used Summary of Result (Acc.)",
Liar˜ Fake Or Real Combined Corpus Traditional Machine Learning Models SVM These traditional models are used in different classiﬁcation tasks including text classiﬁcation.,
Different existing studies used them for fake news detection as well.,
Lexical 0.56 0.67 0.71 SVM Lexical + Sentiment 0.56 0.66 0.71 LR Lexical + Sentiment 0.56 0.,
76 Decision Tree Lexical + Sentiment 0.51 0.,
67 AdaBoost Lexical + Sentiment 0.56 0.,
74 Na¨ıve Bayes Unigram 0.60 0.,
91 Na¨ıve Bayes Bigram 0.60 0.,
93 k-NN Empath 0.54 0.,
71 Deep Learning Models CNN CNN extracts features and classify texts by transforming words into vectors.,
GloVe embedding 0.58 0.,
93 LSTM LSTM remembers information for long sentences.,
93 Bi-LSTM Bi-LSTM analyzes a certain part from both previous and next events.,
95 C-LSTM Convolutional layer with max- pooling combines the local features into a global vector to help LSTM remembering important information.,
95 HAN HAN applies attention mechanism for both word-level and sentence-level representation.,
92 Conv-HAN Convolutional layer encodes embedding into feature for word-level and senetence- level attention.,
92 Advanced Pre-trained Language Models BERT These language models are˜ pre-trained on large text corpus˜ and can be ﬁne-tuned for˜ text classiﬁcation.,
BERT embeddings 0.62 0.,
95 RoBERTa RoBERTa embeddings 0.62 0.,
96 DistilBERT DistilBERT embeddings 0.60 0.,
93 ELECTRA ELECTRA embeddings 0.61 0.,
95 ELMo ELMo embeddings 0.61 0.,
91 15 Table 7: Comparison of training time and GPU usage (in testing) for BERT-based models Model #Parameters Avg.,
training time per epoch (sec) GPU used in testing (GB) DistilBERT 66 M 2175 2.48 BERT 110 M 3149 2.95 RoBERTa 125 M 4020 3.07 = 0.75).,
"Among the ﬁve pre-trained advanced natural language deep learning models we studied, RoBERTa shows the best performance across the three datasets: combined corpus (Accuracy = 0.96), Fake or Real dataset (Accuracy = 0.98), and Liar (0.62).",
"Overall, RoBERTa is the best performing model for two datasets (Combined corpus and Fake or Real) across all the models we studied, while HAN is the best performer for the Liar dataset.",
The performance of Na¨ıve Bayes (with n-gram) is only slightly less than the deep learning and pre-trained language models.,
"As such, Na¨ıve Bayes can be a good choice for fake news detection on a sufﬁciently large dataset with hardware constraint.",
Naive Bayes (with n-gram) has also been reported to show good performance in spam detection in earlier studies [27].,
We ﬁnd that the performance of Naive Bayes (with n-gram) is almost equivalent to the performances of deep learning models on Combined Corpus (see Table 3).,
"Hence, in the absence of hardware resource requirement of deep learning and advanced pre-trained models (a possible case for non-proﬁt blogs/websites), Naive Bayes with n-gram can be a suitable option with a sufﬁciently large dataset.",
"Note that the required size of the dataset may vary with its nature, i.e., the number of topics included.",
"However, Naive Bayes fails to achieve considerable accuracy when trained on a minimal sample set (see Figure 3).",
"Among the diverse features, we studied for the traditional learning models (lexical, sentiment, n-grams), bigram-based models (e.g., Na¨ıve Bayes) show better performance than other features.",
"Overall, the incorporation of sentiment indicators into the models did not improve their performance.",
"For example, for SVM the performance is the same (0.71) for both settings: lexical and lexical + sentiment.",
"Therefore, Sentiment features are not observed as useful for fake news detection in our study.",
"The classiﬁcation of news (as real or fake) has very little to do with the polarity (i.e., sentiment), as fake news can be made up in both directions (positive or negative).",
"While two LSTM-based models (Bi-LSTM, C-LSTM) are the best performer among all the traditional deep learning models, their performance degrades signiﬁcantly when the dataset sizes are smaller (see RQ3).",
We observe that LSTM based models show gradual improvement when the dataset length increases from LIAR to Combined Corpus.,
"The more an article contains information, the less these models will be vulnerable to overﬁtting, and the better they will perform.",
"Hence, neural network-based models may show high performance on a larger dataset over 100k samples [28].",
The pre-trained BERT-based models outperform the other models not only on the overall datasets but also on smaller samples of the datasets (see RQ3).,
"We see that the BERT-based model (i.e., RoBERTa) is capable of achieving high accuracy (over 90%) even with a limited sample size of 500 data (see Figure 3).",
"Hence, these models can be utilized for fake news detection in different languages where a large collection of labeled data is not feasible.",
"Different pre-trained BERT models are already available for different languages, e.g., ALBERTO for Italian [47], AraBERT for Arabic [4], BanglaBERT 7.",
We measured the average training time (per epoch) and GPU usage (during testing) for each BERT-based model on Combined Corpus.,
"We ﬁnd that the training time needed by DistilBERT is almost half of BERT and RoBERTa, and it requires less GPU for testing (i.e., prediction) as well (see Table 7).",
"Therefore, while DistilBERT shows 0.93 accu- racy on the combined corpus which is only slightly behind BERT (0.95) or RoBERTa (0.96), DistilBERT can be useful for production-level usage with hardware constraint and less response time.",
"This is because DistilBERT is developed using the concept of knowledge distillation [8, 26].",
"Hence, it is suitable for production-level usage considering its’ high performance and low resource requirement.",
"Misclassiﬁcation Analysis Among the three datasets in our study, the best models (pre-trained language models) show more than 96% accuracy for two datasets (Combined corpus and Fake or Real).",
"For the other dataset (Liar), the best performing model was HAN 7https://github.com/sagorbrur/bangla-bert 16 Figure 5: Relation between models’ performance and article length.",
with 75% accuracy.,
"Compared to the other two datasets, the Liar dataset has signiﬁcantly smaller articles (18 words on average) compared to the other two datasets (average 644 words for Combined Corpus and 765 words for Fake or Real news).",
"Indeed, we have observed that when the number of training data is constant, the accuracy of this model is proportional to the average article length of news (see Figure 5).",
We conﬁrmed this by analyzing the performance of the Naive Bayes model on 5000 randomly selected records from each of our three datasets.,
This observation is also consistent with other models.,
"Thus, with the increase of news article length, the models can become more accurate, because those can extract more information to classify the news correctly.",
"Among the three datasets, two datasets are related to politics (Fake or Real news, Liar), while the other dataset (Combined Corpus) has fake news about diverse topics like health and research, politics, economy, and so on (see Figure 1 in Section 3).",
"To understand whether the topic of the news has any effect on the classiﬁcation, we apply topic-based analysis on the fake news articles from the Combined corpus, which our model misclassiﬁes as real.",
We then map each misclassiﬁed case to the ten topics that we found in Figure 1 of the combined corpus.,
"Overall, quotes are greatly misused to design fake news.",
"We ﬁnd that the most frequent words in these articles are ‘said’, ‘study’, and ‘research’.",
The profuse use of the word ‘said’ indicates how fake news sources misconstrue quotes to make these as believable as possible and carry out their own agendas.,
Table 8: Topic-wise percentage of false positive news in the Combined Corpus Topic False Positive News (%) Health and Research 49.6 Politics 27.6 Miscellaneous 22.8 The topic-wise analysis of misclassiﬁcation in the combined corpus shows that 49.6% of the false positive news (that are mispredicted as fake in our study) are related to health and research-based topics (Table 8).,
"On the other 17 hand, a tiny portion (27.6%) of the false positive news are related to politics.",
This high false positive rate of health and research-related news bears evidence that clickbait news on health and research can be produced more convincingly.,
"A slight change in the actual research article will still keep the fake news in the close proximity of the actual article, which makes it difﬁcult to identify them as fake news.",
"In this way, it is quite easy for clickbait news sources to attract people by publishing news claiming the invention of a vaccine for incurable diseases like terminal cancer.",
"Hence, although in recent times the media has focused mostly on combating unauthentic political news, it should also pay attention to stop the proliferation of false health and research-related news for public safety.",
We can realize this lesson even better if we think of the impact of fake news during the current COVID-19 pandemic.,
Corona related fake news has caused serious troubles and confusion among the people.,
"Several fake news such as “Alcohol cures COVID-19”,“5G spreads coronavirus”, etc have affected people both physically and mentally8.",
"Considering the threats associated with it, corona related fake news has been compared to a second pandemic or infodemic9.",
"6 Conclusions In this study, we present an overall performance analysis of 19 different machine learning approaches on three different datasets.",
"Eight out of the 19 models are traditional learning models, six models are traditional deep learning models, and ﬁve models are advanced pre-trained language models like BERT.",
We ﬁnd that BERT-based models have achieved better performance than all other models on all datasets.,
"More importantly, we ﬁnd that pre-trained BERT-based models are robust to the size of the dataset and can perform signiﬁcantly better on very small sample size.",
We also ﬁnd that Naive Bayes with n-gram can attain similar results to neural network-based models on a dataset when the dataset size is sufﬁcient.,
The performance of LSTM-based models greatly depends on the length of the dataset as well as the information given in a news article.,
"With adequate information provided in a news article, LSTM-based models have a higher probability of overcoming overﬁtting.",
"The results and ﬁndings based on our comparative analysis can facilitate future researches in this direction and also help the organizations (e.g., online news portals and social media) to choose the most suitable model who are interested in detecting fake news.",
Our future work in this direction will focus on designing models that can detect misinformation and health-related fake news that are prevalent in social media during the COVID-19 pandemic.,
"References [1] Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and Jimmy Lin.",
Docbert: Bert for document classiﬁcation.,
"arXiv preprint arXiv:1904.08398, 2019.",
"[2] Hadeer Ahmed, Issa Traore, and Sherif Saad.",
Detection of online fake news using n-gram analysis and machine learning techniques.,
"In International Conference on Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments, pages 127–138.",
"Springer, 2017.",
[3] Hunt Allcott and Matthew Gentzkow.,
Social media and fake news in the 2016 election.,
"Journal of Economic Perspectives, 31(2):211–36, 2017.",
"[4] Wissam Antoun, Fady Baly, and Hazem Hajj.",
Arabert: Transformer-based model for arabic language under- standing.,
"arXiv preprint arXiv:2003.00104, 2020.",
"[5] David M Blei, Andrew Y Ng, and Michael I Jordan.",
Latent dirichlet allocation.,
"Journal of machine Learning research, 3(Jan):993–1022, 2003.",
[6] Alessandro Bondielli and Francesco Marcelloni.,
A survey on fake news and rumour detection techniques.,
"Infor- mation Sciences, 497:38–55, 2019.",
"[7] Peter Bourgonje, Julian Moreno Schneider, and Georg Rehm.",
From clickbait to fake news detection: an approach based on detecting the stance of headlines to articles.,
"In Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism, pages 84–89, 2017.",
"8https://www.bbc.com/news/stories-52731624, Accessed on: Oct 05, 2020.",
"9https://www.nature.com/articles/d41586-020-01409-2, Accessed on: Oct 05, 2020.",
"18 [8] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil.",
Model compression.,
"In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541, 2006.",
[9] J Douglas Carroll and Phipps Arabie.,
Multidimensional scaling.,
"In Measurement, judgment and decision making, pages 179–250.",
"Elsevier, 1998.",
"[10] Yimin Chen, Niall J Conroy, and Victoria L Rubin.",
Misleading online content: Recognizing clickbait as false news.,
"In Proceedings of the 2015 ACM on Workshop on Multimodal Deception Detection, pages 15–19.",
"[11] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning.",
Electra: Pre-training text encoders as discriminators rather than generators.,
"arXiv preprint arXiv:2003.10555, 2020.",
[12] Mathieu Cliche.,
"The sarcasm detector, 2014.",
"[13] Niall J Conroy, Victoria L Rubin, and Yimin Chen.",
Automatic deception detection: Methods for ﬁnding fake news.,
"In Proceedings of the 78th ASIS&T Annual Meeting: Information Science with Impact: Research in and for the Community, page 82.",
"American Society for Information Science, 2015.",
"[14] Enyan Dai, Yiwei Sun, and Suhang Wang.",
Ginger cannot cure cancer: Battling fake health news with a com- prehensive data repository.,
"In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 853–862, 2020.",
"[15] Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim.",
Bertje: A dutch bert model.,
"arXiv preprint arXiv:1912.09582, 2019.",
"[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
Bert: Pre-training of deep bidirectional transformers for language understanding.,
"arXiv preprint arXiv:1810.04805, 2018.",
[17] Sanjeev M Dwivedi and Sunil B Wankhade.,
Survey on fake news detection techniques.,
"In International Confer- ence on Image Processing and Capsule Networks, pages 342–348.",
"Springer, 2020.",
"[18] Ethan Fast, Binbin Chen, and Michael S Bernstein.",
Empath: Understanding topic signals in large-scale text.,
"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pages 4647–4657.",
"[19] Song Feng, Ritwik Banerjee, and Yejin Choi.",
Syntactic stylometry for deception detection.,
"In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 171–175.",
"Association for Computational Linguistics, 2012.",
[20] Johannes F¨urnkranz.,
A study using n-gram features for text categorization.,
"Austrian Research Institute for Artiﬁcal Intelligence, 3(1998):1–10, 1998.",
[21] Shlok Gilda.,
Evaluating machine learning algorithms for fake news detection.,
"In Research and Development (SCOReD), 2017 IEEE 15th Student Conference on, pages 110–115.",
"IEEE, 2017.",
[22] Santiago Gonz´alez-Carvajal and Eduardo C Garrido-Merch´an.,
Comparing bert against traditional machine learn- ing text classiﬁcation.,
"arXiv preprint arXiv:2005.13012, 2020.",
[23] Mykhailo Granik and Volodymyr Mesyura.,
Fake news detection using naive bayes classiﬁer.,
"In Electrical and Computer Engineering (UKRCON), 2017 IEEE First Ukraine Conference on, pages 900–903.",
"IEEE, 2017.",
"[24] Georgios Gravanis, Athena Vakali, Konstantinos Diamantaras, and Panagiotis Karadais.",
Behind the cues: A benchmarking study for fake news detection.,
"Expert Systems with Applications, 128:201–213, 2019.",
"[25] Tarek Hamdi, Hamda Slimi, Ibrahim Bounhas, and Yahya Slimani.",
A hybrid approach for fake news detection in twitter based on user features and graph embedding.,
"In International Conference on Distributed Computing and Internet Technology, pages 266–280.",
"Springer, 2020.",
"[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.",
Distilling the knowledge in a neural network.,
"arXiv preprint arXiv:1503.02531, 2015.",
19 [27] Johan Hovold.,
Naive bayes spam ﬁltering using word-position-based attributes.,
"In CEAS, pages 41–48, 2005.",
"[28] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.",
Bag of tricks for efﬁcient text classiﬁ- cation.,
"arXiv preprint arXiv:1607.01759, 2016.",
"[29] Heejung Jwa, Dongsuk Oh, Kinam Park, Jang Mook Kang, and Heuiseok Lim.",
exbake: Automatic fake news detection model based on bidirectional encoder representations from transformers (bert).,
"Applied Sciences, 9(19):4062, 2019.",
"[30] Dhruv Khattar, Jaipal Singh Goud, Manish Gupta, and Vasudeva Varma.",
Mvae: Multimodal variational autoen- coder for fake news detection.,
"In The World Wide Web Conference, pages 2915–2921, 2019.",
[31] Yoon Kim.,
Convolutional neural networks for sentence classiﬁcation.,
"arXiv preprint arXiv:1408.5882, 2014.",
"[32] Sebastian Kula, Michał Chora´s, and Rafał Kozik.",
Application of the bert-based architecture in fake news detec- tion.,
"In Conference on Complex, Intelligent, and Software Intensive Systems, pages 239–249.",
"Springer, 2020.",
[33] Upmanu Lall and Ashish Sharma.,
A nearest neighbor bootstrap for resampling hydrologic time series.,
"Water Resources Research, 32(3):679–693, 1996.",
"[34] Nayeon Lee, Zihan Liu, and Pascale Fung.",
Team yeon-zi at semeval-2019 task 4: Hyperpartisan news detection by de-noising weakly-labeled data.,
"In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 1052–1056, 2019.",
[35] David Leonhardt and Stuart A Thompson.,
Trump’s lies.,
"New York Times, 21, 2017.",
"[36] Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam.",
Exploiting bert for end-to-end aspect-based sentiment analysis.,
"arXiv preprint arXiv:1910.00883, 2019.",
[37] Jianhua Lin.,
Divergence measures based on the shannon entropy.,
"IEEE Transactions on Information theory, 37(1):145–151, 1991.",
[38] Yang Liu.,
Fine-tune bert for extractive summarization.,
"arXiv preprint arXiv:1903.10318, 2019.",
"[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.",
Roberta: A robustly optimized bert pretraining approach.,
"arXiv preprint arXiv:1907.11692, 2019.",
[40] Ilya Loshchilov and Frank Hutter.,
Decoupled weight decay regularization.,
"arXiv preprint arXiv:1711.05101, 2017.",
"[41] Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.",
Deep learning based text classiﬁcation: A comprehensive review.,
"arXiv preprint arXiv:2004.03705, 2020.",
"[42] Manish Munikar, Sushil Shakya, and Aakash Shrestha.",
Fine-grained sentiment classiﬁcation using bert.,
"In 2019 Artiﬁcial Intelligence for Transforming Business and Society (AITB), volume 1, pages 1–5.",
"IEEE, 2019.",
"[43] Ray Oshikawa, Jing Qian, and William Yang Wang.",
A survey on natural language processing for fake news detection.,
"arXiv preprint arXiv:1811.00770, 2018.",
"[44] Yifan Peng, Shankai Yan, and Zhiyong Lu.",
Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets.,
"arXiv preprint arXiv:1906.05474, 2019.",
"[45] Jeffrey Pennington, Richard Socher, and Christopher Manning.",
Glove: Global vectors for word representation.,
"In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.",
"[46] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle- moyer.",
Deep contextualized word representations.,
"arXiv preprint arXiv:1802.05365, 2018.",
"[47] Marco Polignano, Pierpaolo Basile, Marco de Gemmis, Giovanni Semeraro, and Valerio Basile.",
Alberto: Italian bert language understanding model for nlp challenging tasks based on tweets.,
"In CLiC-it, 2019.",
20 [48] Lutz Prechelt.,
Automatic early stopping using cross validation: quantifying the criteria.,
"Neural Networks, 11(4):761–767, 1998.",
[49] Lutz Prechelt.,
Early stopping-but when?,
"In Neural Networks: Tricks of the trade, pages 55–69.",
"Springer, 1998.",
"[50] Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi.",
Truth of varying shades: Ana- lyzing language in fake news and political fact-checking.,
"In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931–2937, 2017.",
"[51] Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri.",
Are loss functions all the same?,
"Neural Computation, 16(5):1063–1076, 2004.",
"[52] Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell.",
Fake news or truth?,
using satirical cues to detect potentially misleading news.,
"In Proceedings of the Second Workshop on Computational Approaches to Deception Detection, pages 7–17, 2016.",
"[53] Victoria L Rubin, Yimin Chen, and Niall J Conroy.",
Deception detection for news: three types of fakes.,
"In Proceedings of the 78th ASIS&T Annual Meeting: Information Science with Impact: Research in and for the Community, page 83.",
"American Society for Information Science, 2015.",
"[54] Natali Ruchansky, Sungyong Seo, and Yan Liu.",
Csi: A hybrid deep model for fake news detection.,
"In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 797–806.",
"[55] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.",
"Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.",
"arXiv preprint arXiv:1910.01108, 2019.",
"[56] Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.",
defend: Explainable fake news detection.,
"In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 395–405, 2019.",
"[57] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu.",
Fake news detection on social media: A data mining perspective.,
"ACM SIGKDD Explorations Newsletter, 19(1):22–36, 2017.",
"[58] Sneha Singhania, Nigel Fernandez, and Shrisha Rao.",
3han: A deep neural network for fake news detection.,
"In International Conference on Neural Information Processing, pages 572–581.",
"Springer, 2017.",
"[59] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.",
How to ﬁne-tune bert for text classiﬁcation?,
"In China National Conference on Chinese Computational Linguistics, pages 194–206.",
"Springer, 2019.",
"[60] Eugenio Tacchini, Gabriele Ballarin, Marco L Della Vedova, Stefano Moret, and Luca de Alfaro.",
Some like it hoax: Automated fake news detection in social networks.,
"arXiv preprint arXiv:1704.07506, 2017.",
"[61] Ian Tenney, Dipanjan Das, and Ellie Pavlick.",
Bert rediscovers the classical nlp pipeline.,
"arXiv preprint arXiv:1905.05950, 2019.",
"[62] James Thorne, Mingjie Chen, Giorgos Myrianthous, Jiashu Pu, Xiaoxuan Wang, and Andreas Vlachos.",
Fake news stance detection using stacked ensemble of classiﬁers.,
"In Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism, pages 80–83, 2017.",
[63] William Yang Wang.,
"” liar, liar pants on ﬁre”: A new benchmark dataset for fake news detection.",
"arXiv preprint arXiv:1705.00648, 2017.",
"[64] Liang Wu, Jundong Li, Xia Hu, and Huan Liu.",
Gleaning wisdom from the past: Early detection of emerging rumors in social media.,
"In Proceedings of the 2017 SIAM International Conference on Data Mining, pages 99–107.",
"SIAM, 2017.",
[65] Liang Wu and Huan Liu.,
Tracing fake-news footprints: Characterizing social media messages by how they propagate.,
"In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 637–645.",
21 [66] Xichen Zhang and Ali A Ghorbani.,
"An overview of online fake news: Characterization, detection, and discussion.",
"Information Processing & Management, 57(2):102025, 2020.",
[67] Xinyi Zhou and Reza Zafarani.,
Network-based fake news detection: A pattern-driven approach.,
"ACM SIGKDD Explorations Newsletter, 21(2):48–60, 2019.",
”Machine Learning” manuscript No.,
(will be inserted by the editor) Emotion in Reinforcement Learning Agents and Robots: A Survey.,
Thomas M. Moerland · Joost Broekens · Catholijn M. Jonker Received: date / Accepted: date Abstract This article provides the ﬁrst survey of computational models of emo- tion in reinforcement learning (RL) agents.,
"The survey focuses on agent/robot emotions, and mostly ignores human user emotions.",
Emotions are recognized as functional in decision-making by inﬂuencing motivation and action selection.,
"Therefore, computational emotion models are usually grounded in the agent’s deci- sion making architecture, of which RL is an important subclass.",
Studying emotions in RL-based agents is useful for three research ﬁelds.,
"For machine learning (ML) researchers, emotion models may improve learning eﬃciency.",
"For the interactive ML and human-robot interaction (HRI) community, emotions can communicate state and enhance user investment.",
"Lastly, it allows aﬀective modelling (AM) re- searchers to investigate their emotion theories in a successful AI agent class.",
This survey provides background on emotion theory and RL.,
"It systematically addresses 1) from what underlying dimensions (e.g., homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, 2) what types of emotions have been derived from these dimensions, and 3) how these emotions may either inﬂuence the learning eﬃciency of the agent or be useful as social signals.",
"We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL.",
"In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identiﬁes challenges and directions for future emotion-RL research.",
Keywords Reinforcement learning · Emotion · Motivation · Agent · Robot 1 Introduction This survey systematically covers the literature on computational models of emo- tion in reinforcement learning (RL) agents.,
Computational models of emotions are T.M.,
"Moerland, J. Broekens, C.M.",
"Jonker Delft University of Technology Mekelweg 4, 2628CD, Delft, The Netherlands E-mail: {T.M.Moerland, D.J.Broekens, C.M.Jonker}@tudelft.nl arXiv:1705.05172v1 [cs.LG] 15 May 2017 2 Moerland et al.",
usually grounded in the agent decision-making architecture.,
"In this work we focus on emotion models in a successful learning architecture: reinforcement learning, i.e.",
agents optimizing some reward function in a Markov Decision Process (MDP) formulation.,
"To directly avoid confusion: the topic does not imply the agent should ‘learn its emotions’, i.e.",
"emotions are rather hooked on characteristics of the MDP (like value and transition functions), and may for example well persist after learn- ing has converged.",
One may question why it is useful to study emotions in machines at all.,
The computational study of emotions is an example of bio-inspiration in computational science.,
"Many important advancements in machine learning and optimization were based on biological principles, such as neural networks, evolutionary algorithms and swarm-based optimization (Russell et al, 1995).",
"An example encountered in this survey is homeostasis, a concept closely related to emotions, and a biological principle that led researchers to implement goal switching in RL agents.",
The study of emotions in learning agents is useful for three research ﬁelds.,
"First, for the machine learning (ML) community, emotions may beneﬁt learning eﬃciency.",
"For example, there are important connections to the work on (intrinsi- cally) motivated RL.",
"Second, researchers working on interactive machine learning and human-robot interaction (HRI) may beneﬁt from emotions to enhance both transparency (i.e.",
communicate agent internal state) and user empathy.,
"Finally, from an aﬀective modelling (AM) perspective, where emotions are mostly studied in cognitive agents, RL agents provide the general beneﬁts of the MDP formula- tion: these agents require few assumptions, can be applied to a variety of tasks without much prior knowledge, and, allow for learning.",
"This also gives AM re- searchers access to complex, high-dimensional test domains to evaluate emotion theories.",
"Emotion is an important part of human intelligence (Johnson-Laird and Oat- ley, 1992; Damasio, 1994; Baumeister et al, 2007).",
"On the one hand, emotion has been deﬁned as a response to a signiﬁcant stimulus - characterized by brain and body arousal and a subjective feeling - that elicits a tendency towards moti- vated action (Calvo et al, 2014; Frijda et al, 1989).",
This emphasizes the relation of emotions with motivation and action.,
"On the other hand, emotions have also been identiﬁed as complex feedback signals used to shape behaviour (Baumeister et al, 2007; Broekens et al, 2013).",
This view emphasizes the feedback function of emotion.,
The common ground in both: 1) emotions are related to action selection mechanisms and 2) emotion processing is in principle beneﬁcial to the viability of the individual.,
"As an illustration, Damasio (1994) showed that people with impaired emotional processing (due to brain damage) show failures in work and social life.",
"These observations connecting emotions to action selection and adap- tive decision-making sparked interest in the computer science community as well, mainly following the initial work by Ca˜namero (1997b) and Gadanho and Hallam (1998).",
We wrote this survey for two reasons.,
"First, while the topic of emotion in RL agents has received attention for nearly 20 years, it appears to fall in between the machine learning and aﬀective modelling communities.",
"In particular, there is no framework connecting the variety of models and implementations.",
"Although Rumbell et al (2012) compared emotion models in twelve diﬀerent agents, their work does not provide a full survey of the topic, nor does it focus on agents with a learning architecture.",
"Our main aim is to establish such a framework, Emotion in Reinforcement Learning Agents and Robots: A Survey.",
3 hoping to bridge the communities and potentially align research agendas.,
"As a second motivation, this survey is also useful to engineers working on social agents and robots.",
"Emotion has an important functional role in social interaction and social robotics (Fong et al, 2003).",
Our survey is also a practical guideline for engineers who wish to implement emotional functionality in their RL-based agents and robots.,
"As a ﬁnal note, the term ‘reinforcement learning’ may be misleading to readers from a cognitive AI or psychological background.",
"RL may reminisce of ‘instru- mental conditioning’, with stimulus-response experiments on short time-scales.",
"Although indeed related, RL here refers to the computational term for a successful class of algorithms solving Markov Decision Processes by sampling and learning from data.",
MDPs (introduced in Section 2.4) provide a generic speciﬁcation for short-term and long-term sequential decision-making problems with minimal as- sumptions.,
"Note that many cognitive AI approaches, that usually employ a notion of ‘goal’, are also expressible in MDP formulation by deﬁning a sparse reward function with positive reward at the goal state.",
The structure of this review is as follows.,
"First, Section 2 provides the necessary background on emotion and reinforcement learning from psychology, neuroscience and computer science.",
Section 3 discusses the survey’s methodology and proposed taxonomy.,
"Subsequently, Sections 4-6 contain the main results of this survey by systematically categorizing approaches to emotion elicitation, emotion types and emotion functionality.",
"Additionally, the comparison of evaluation criteria is pre- sented in (Section 7).",
"The survey ends with a general discussion of our ﬁndings, highlights some important problems and indicates future directions in this ﬁeld (Section 8).",
"2 Background As many papers included in this survey build upon psychological (2.1) and neu- roscientiﬁc (2.2) theories of emotion, this section provides a high-level overview of these ﬁelds.",
"Subsequently, we position our work in the computer science and machine learning community (2.3).",
We conclude these preliminaries by formally introducing computational reinforcement learning (2.4).,
"2.1 Psychology We discuss three dominant psychological emotion theories: categorical, dimen- sional, and componential theories (see also Lisetti and Hudlicka (2014)).",
Categorical emotion theory assumes there is a set of discrete emotions forming the ‘basic’ emotions.,
"These ideas are frequently inspired by the work by Ekman et al (1987), who identiﬁed the cross-cultural recognition of anger, fear, joy, sad- ness, surprise and disgust on facial expressions.",
"In an evolutionary perspective, each basic emotion can be considered as an elementary response pattern, or ac- tion tendency (Frijda et al, 1989).",
"For example, fear has the associated action tendency of avoidance, which helps the organism to survive a dangerous situation, accompanied by a negative feeling and prototypical facial expression.",
"However, the concept of ‘basic’ emotions remains controversial within psychology, as is reﬂected 4 Moerland et al.",
in the ongoing debate about which emotions should be included.,
"The number of emotions to be included ranges from 2 to 18, see Calvo et al (2014).",
"Dimensional emotion theory (Russell, 1978) assumes an underlying aﬀective space.",
This space involves at least two dimensions; usually valence (i.e.,
posi- tive/negative evaluation) and arousal (i.e.,
"activation level) (Russell and Barrett, 1999).",
"For example, fear is a highly arousing and negative aﬀective state.",
"The theory was originally developed as a ‘Core aﬀect’ model, i.e.",
"describing a more long-term, underlying emotional state.",
"Osgood et al (1964) orginally added dom- inance as a third dimension, resulting in the PAD (pleasure, arousal, dominance) model.",
"Dimensional models have diﬃculty separating emotion categories such as anger and disgust, which is a common critique on the theory.",
"Finally, componential emotion theory, best known as cognitive appraisal the- ory (Lazarus, 1991), considers emotions as the results of evaluations (appraisals) of incoming stimuli according to personal relevance.",
"Some examples of frequently oc- curring appraisal dimensions are valence, novelty, goal relevance, goal congruence and coping potential.",
Distinct emotions relate to speciﬁc patterns of appraisal activation.,
"For example, anger is a result of evaluating a situation as harmful to one’s own goals with the emotion attributed to the responsible actor and at least some feeling of power.",
"Some well-known appraisal theories that have been a basis for computational models are the OCC model (named after the authors Ortony, Clore and Collins) (Ortony et al, 1990), the component process theory of emotions (CPT) (Scherer et al, 2001), and the belief-desire theory of emotions (BDTE) (Reisenzein, 2009).",
"Although cognitive appraisal theories describe the structure of emotion well, they are limited with respect to explaining where ap- praisals themselves come from, what the function of emotion is in cognition and intelligence, and how they are related to evolution.",
Note that the presented theories focus on diﬀerent aspects of emotions.,
"For example, appraisal theory focuses on how emotions are elicited, while categorical emotion models focus on action tendencies, i.e.",
the immediate function of emo- tions.,
"Some consider emotions to precede action selection, while others focus on emotions as feedback signals (Baumeister et al, 2007).",
"In this survey emotions are considered in a reward-based feedback loop, which involves both emotion elicita- tion and function.",
"2.2 Neuroscience Aﬀective responses and their relation to behaviour and learning have also been ex- tensively studied in neuroscience; for a survey see (Rolls and Grabenhorst, 2008).",
"We discuss theories by LeDoux, Damasio and Rolls.",
The work by LeDoux (2003) mainly focussed on the role of the amygdala in fear conditioning.,
"LeDoux iden- tiﬁed that incoming sensory stimuli can directly move from thalamus to amyg- dala, thereby bypassing the previously assumed intermediate step through the neo-cortex.",
"As such, the work showed that emotional responses may also be elicited without neo-cortical reasoning.",
Damasio (1994) took a diﬀerent perspective on rational emotions through the ‘somatic marker hypothesis’.,
"He proposes that emotions are the result of bodily sensations, which tell the organism that current sensations (i.e.",
events) are beneﬁ- cial (e.g.,
pleasure) or harmful (e.g.,
The somatic marker is therefore a signal Emotion in Reinforcement Learning Agents and Robots: A Survey.,
5 that can be interpreted as feedback about the desirability of current and imagined situations.,
"The somatic marker hypothesis has been interpreted in terms of RL as well (Dunn et al, 2006).",
"Later work by Rolls shifted the attention from the amygdala to the orbito- frontal cortex (OFC) (Rolls and Grabenhorst, 2008) Imaging studies have impli- cated the OFC in both reinforcement and aﬀect, with direct input connections of most sensory channels (taste, olfactory, visual, touch), while projecting to sev- eral brain areas involving motor behaviour (striatum) and autonomic responses (hypothalamus) (Rolls and Grabenhorst, 2008).",
"Also, single neuron studies have shown that visual and taste signals (the latter being a well-known primary rein- forcer) converge on the same neurons (Rolls and Baylis, 1994), coined ’conditional reward neurons’.",
"Earlier work already identiﬁed ’error neurons’, which mainly re- spond when an expected reward is not received (Thorpe et al, 1983).",
"Together, these theories suggest that emotions are closely linked to reward pro- cessing.",
These ideas are implicitly reﬂected in part of the reinforcement learning- based implementations in this survey.,
"These ideas are also reﬂected in Rolls’ evolu- tionary theory of emotion (Rolls and Grabenhorst, 2008), which identiﬁes emotions as the results of primary reinforcers (like taste, aﬃliative touch, pain) which spec- ify generic goals for survival and reproductive success (like food, company and body integrity).",
"According to Rolls, emotions exclusively emerge from these goal- related events.",
This view is also compatible with the cognitive appraisal view that emotions are the result of stimuli being evaluated according to their goal/need rel- evance.,
"However, in cognitive appraisal theory the ’goal’ is deﬁned at a diﬀerent level of abstraction.",
"2.3 Computer Science Aﬀective modelling is a vibrant ﬁeld in computer science with active subﬁelds (Calvo et al, 2014), including work on aﬀect detection and social signal processing (Vinciarelli et al, 2012; Calvo and D’Mello, 2010), computational modelling of aﬀect in robots and virtual agents (Marsella et al, 2010), and expression of emotion in robots and virtual agents (Ochs et al, 2015; Paiva et al, 2015; Lhommet and Marsella, 2015).",
"Since this survey focusses on aﬀective modelling, in particular in RL-based agents, we provide some context by discussing emotions in diﬀerent agent architectures, in particular symbolic and (non-RL) machine learning-based.",
"One of the earliest symbolic/cognitive architectures was Velasquez’ Cathexis model (Velasquez, 1998).",
"It incorporated Ekman’s six emotions in the pet robot Yuppy, which later also formed the basis for the well-known social robot Kismet (Breazeal, 2003).",
"Several well-known symbolic architectures have also incorporated emotions, either based on categorical emotions (Murphy et al, 2002), somatic marker hypothesis (Laird, 2008), or appraisal theories (EMIB (Michaud, 2002), EMA (Marsella and Gratch, 2009) and LIDA (Franklin et al, 2014)).",
"Although symbolic/cognitive architecture approaches are capable of solving a variety of AI tasks, they are limited with respect to learning from exploration and feedback in unstructured tasks.",
"In contrast, machine learning implementations focus on learning, as the agent should gradually adapt to its environment and task.",
"The dominant research direc- tion in this ﬁeld is reinforcement learning (RL) (Sutton and Barto, 1998), which we 6 Moerland et al.",
formally introduce in the next section.,
There are however other machine learning implementations that incorporate emotions.,
"Some examples include agents based on evolutionary neural networks (Parisi and Petrosino, 2010), the free-energy prin- ciple (Joﬃly and Coricelli, 2013), Bayesian models (Antos and Pfeﬀer, 2011) or entropy (Belavkin, 2004).",
"Finally, we want to stress that the focus of this review is on agent emotion, i.e.",
how it is elicited and may inﬂuence the agent’s learning loop.,
A related but clearly distinct topic is how human emotion may act as a teaching signal for this loop.,
Broekens (2007) showed human emotional feedback speeds up agent learning in a grid-world task compared to a baseline agent.,
"There are a few other examples in this direction (Hasson et al, 2011; Moussa and Magnenat-Thalmann, 2013), but in general the literature of emotion as a teaching signal is limited.",
"Although the way in which humans actually tend to provide feedback is an active research topic (Thomaz and Breazeal, 2008; Knox et al, 2012, 2013), it remains a question whether emotions would be a viable channel for human feedback.",
"We do not further pursue this discussion here, and put our focus on agent emotions in RL agents.",
"2.4 Computational Reinforcement Learning Computational reinforcement learning (RL) (Sutton and Barto, 1998; Wiering and Van Otterlo, 2012) is a successful approach that enables autonomous agents to learn from interaction with their environment.",
"We adopt a Markov Decision Process (MDP) speciﬁed by the tuple: {S, A, T, r, γ}, where S denotes a set of states, A a set of actions, T : S × A →P(S) denotes the transition function, r : S × A × S →R denotes the reward function and γ ∈(0, 1] denotes a discount parameter.",
"The goal of the agent is to ﬁnd a policy π : S →P(A) that maximizes the expected (inﬁnite-horizon) discounted return: Qπ(s, a) = Eπ,T n ∞ X t=0 γtr(st, at, st+1)|s0 = s, a0 = a o = X s′∈S T(s′|s, a) h r(s, a, s′) + γ X a′∈A π(s′, a′)Qπ(s′, a′) i (1) where we explicitly write out the expectation over the (possibly) stochastic policy and transition function.",
"The optimal value function is deﬁned as Q⋆(s, a) = max π Qπ(s, a) (2) from which we can derive the optimal policy π⋆(s) = argmax a∈A Q⋆(s, a) (3) There are several approaches to learning the optimal policy.",
"When the envi- ronmental dynamics T(s′|s, a) and reward function r(s, a, s′) are known, we can use planning algorithms like Dynamic Programming (DP).",
"However, in many ap- plications the environment’s dynamics are hard to determine.",
"As an alternative, Emotion in Reinforcement Learning Agents and Robots: A Survey.",
"7 we can use sampling-based methods to learn the policy, known as reinforcement learning.",
There is a large variety of RL approaches.,
"First, we can separate value-function methods, which try to iteratively approximate the cumulative return speciﬁed in equation (1), and policy search, which tries to directly optimize some parametrized policy.",
"Policy search shows promising results in real robotic applications (Kober and Peters, 2012).",
"However, most work in RL utilizes value-function methods, on which we also focus in this survey.",
Among value-function methods we should identify model-free versus model- based approaches.,
"In model-free RL we iteratively approximate the value-function through temporal diﬀerence (TD) learning, thereby avoiding having to learn the transition function (which is usually challenging).",
"Well-known algorithms are Q- learning (Watkins, 1989), SARSA (Rummery and Niranjan, 1994) and TD(λ) (Sutton, 1988).",
"The update equation for Q-learning is given by: Q(s, a) = Q(s, a) + α h r(s, a, s′) + γ max a′ Q(s′, a′) −Q(s, a) i (4) where α speciﬁes a learning rate.",
"With additional criteria for the learning and exploration parameters we can show this estimation procedure converges to the optimal value function (Sutton and Barto, 1998).",
"Model-based RL (Hester and Stone, 2012b) is a hybrid form of planning (like DP) and sampling (like TD learning).",
"In model-based RL, we approximate the transition and reward function from the sampled experience.",
"After acquiring knowl- edge of the environment, we can mix real sample experience with planning updates.",
"We will write M = { ˆT, ˆr} to denote the estimated model.",
"Note that a model is de- rived from the full agent-environment interaction history at time-point t, as given by gt = {s0, a0, s1, a1, s2, ...st−1, at−1, st}.",
A ﬁnal aspect we have not yet discussed is the nature of the reward func- tion.,
Traditional RL speciﬁcations assume an external reward signal (known as an ‘external Critic’).,
"However, as argued by Chentanez et al (2004), in animals the reward signal is by deﬁnition derived from neuronal activations, and the Critic therefore resides inside the organism.",
"It therefore also incorporates information from the internal environment, making all reward ‘internal’.",
Singh et al (2010) identiﬁes two types of internal reward: extrinsic internal and intrinsic internal (we will omit ‘internal’ and simply use extrinsic and intrinsic from now on).,
Extrin- sic reward is related to resources/stimuli/goals in the external world (e.g.,
"food), possibly inﬂuenced by internal variables (e.g.",
sugar level).,
"In RL terms, extrin- sic reward explicitly depends on the content of the sensory information (i.e.",
the observed state).,
"On the contrary, intrinsic reward is not dependent on external resources, but rather derived from the agent-environment history g and current model M. An example of intrinsic reward in animals is curiosity.",
"Intrinsic reward is domain-independent, i.e.",
"curiosity is not related to any external resource, but can happen at any state (dependent on the agent history g).",
"In contrast, extrinsic reward for food will never occur in domains where food does not occur.",
Intrinsic motivation has been identiﬁed to serve a developmental role to organisms.,
8 Moerland et al.,
3 Survey structure and methodology We intended to include all research papers in which reinforcement learning and emotion play a role.,
"We conducted a systematic Google Scholar search for ’Emo- tion’ AND ’Reinforcement Learning’ AND ’Computational’, and for ’Emotion’ AND ’Markov Decision Process’.",
We scanned all abstracts for the joint occurrence of emotion and learning in the proposed work.,
"When in doubt, we assessed the full article to determine inclusion.",
"Moreover, we investigated all papers citing several core papers in the ﬁeld, for example, Gadanho and Hallam (2001), Salichs and Malfaz (2012), Broekens et al (2007a) and Marinier and Laird (2008).",
This re- sulted in 52 papers included in this survey.,
A systematic overview of these papers can be found in Table 9 and 10.,
"The proposed taxonomy of emotion elicitation, type and function is shown in Table 1, also stating the associated subsection where each category is discussed.",
"The elicitation and function categories are also visually illustrated in Figure 1, a ﬁgure that is based on the motivated RL illustration (with internal Critic) intro- duced in Chentanez et al (2004).",
Figure 1 may be useful to refer back to during reading to integrate the diﬀerent ideas.,
"Finally, for each individual paper the reader can verify the associated category of emotion elicitation, type and function through the colour coding in the overview Table 9.",
"There is one important assumption throughout this work, which we want to emphasize here.",
We already introduced the distinction between extrinsic and in- trinsic motivation in RL at the end of the last section.,
"Throughout this work, we parallel extrinsic motivation with homeostasis (Section 4.1), and intrinsic moti- vation with appraisal (Section 4.2).",
"The extrinsic/intrinsic distinction is clearly part of the RL literature, while homeostasis and especially appraisal belong to the aﬀective modelling literature.",
"We group these together, as the concept of extrinsic motivation is frequently studied in combination with homeostasis, while intrinsic motivation shows large overlap with appraisal theory.",
We will identify this over- lap in the particular sections.,
"However, the point we want to stress is that the concepts are not synonyms.",
"For example, it is not clear whether some intrinsic motivation or appraisal dimensions also show homeostatic dynamics (a point at which we tend to disagree with Singh et al (2010)).",
"However, a full discussion of the overlap and diﬀerence moves towards psychology, and is beyond the scope of our computational overview.",
"We merely identify the overlap we observed in com- putational implementations, and therefore discuss both extrinsic/homeostasis and intrinsic/appraisal as single sections.",
"Table 1 Overview of categories in emotion elicitation, emotion type and emotion function.",
The number before each category identiﬁes the paragraph where the topic is discussed.,
Emotion elicitation and function are also visually illustrated in Figure 1.,
Emotion elicitation Emotion type Emotion function 4.1 Homeostasis and extrinsic motivation 4.2 Appraisal and intrinsic motivation 4.3 Value/reward-based 4.4 Hard-wired 5.1 Categorical 5.2 Dimensional 6.1 Reward modiﬁcation 6.2 State modiﬁcation 6.3 Meta-learning 6.4 Action selection 6.5 Epiphenomenon Emotion in Reinforcement Learning Agents and Robots: A Survey.,
1 Schematic representation of motivated reinforcement learning based on Chentanez et al (2004).,
"Although traditional RL assumes an external Critic (to provide the reward signal), this actually happens inside the brain of real-world organisms.",
"Thereby the Critic also incorporates, apart from external sensations, internal motivations to determine the current reward and state.",
Motivations have been derived from homeostatic variables and/or internal models.,
The Critic then feeds the state and reward to the Agent.,
The Agent usually learns a value function (Adaptive Critic) and determines the next action (Actor).,
"Note that ordinary RL, in which the reward is a fully external stimulus, is still a speciﬁc case of this scheme (with the Critic as identity function).",
"Emotion elicitation (green) has been associated to A) Homeostasis and extrinsic motivation (paragraph 4.1), B) Appraisal and intrinsic motivation (4.2), C) Reward and value function (4.3) and D) Hard-wired connections from sensations (4.4).",
"Subsequently, the elicited emotion may also inﬂuence the learning loop.",
"Emotion function (blue) has been linked to I) Reward modiﬁcation (6.1), II) State modiﬁcation (6.2), III) Meta-learning (6.3), IV) Action selection (6.4) and ﬁnally as V) Epiphenomenon (6.5).",
"4 Emotion elicitation We identify four major categories of emotion elicitation: extrinsic/homeostatic (4.1), intrinsic/appraisal (4.2), value function and reward-based (4.3), and ﬁnally hard-wired (4.4).",
10 Moerland et al.,
"4.1 Homeostasis and extrinsic motivation Several computational implementations of emotions involve homeostatic variables, drives and motivations.",
"The notion of internal drives originates from the Drive Reduction Theory developed by Hull (1943), which identiﬁes drive reduction as a central cause of learning.",
"These innate drives are also known as primary reinforcers, as their rewarding nature is hard-wired in our system (due to evolutionary beneﬁt).",
"An example of a homeostatic variable is energy/sugar level, which has a temporal dynamic, an associated drive when in deﬁcit (hunger) and can be satiated by an external inﬂuence (food intake).",
The reader might now question why machines even need something like ‘hunger’.,
"However, for a robot the current energy level shows similarity to human sugar levels (and body integrity and pain show similarity to a robot’s mechanical integrity, etc.).",
"Thereby, homeostasis is a useful concept to study in machines as well (see also the remark about bio-inspiration in the Introduction).",
"There is a vast literature on motivated reinforcement learning, see e.g.",
"Konidaris and Barto (2006) and Cos et al (2013), mainly for its potential to naturally switch between goals.",
"Early implementations of these ideas outside the reinforcement learning framework were by Ca˜namero (1997a,b).",
"We denote a homeostatic variable by ht, where t identiﬁes the dependency of this variable on time.",
"The organism’s full physiological state is captured by Ht = {h1,t, h2,t..hN,t}, where hi,t indicates the ith homeostatic variable.",
"Each homeostatic variable has a certain set point H⋆= {h⋆ 1, h⋆ 2..h⋆ N} (Keramati and Gutkin, 2011).",
"Furthermore, each homeostatic variable is aﬀected by a set of ex- ternal resources, associated to a particular action or state.",
"For example, a partic- ular homeostatic variable may increase upon resource consumption, and slightly decrease with every other action (Konidaris and Barto, 2006).",
"More formally, de- noting resource consumption by ¯a and the presence of a resource by ¯s, a simple homeostatic dynamic would be hi,t+1 = ( hi,t + ψ(st, at) if at ∈¯a, st ∈¯s hi,t −ϵ otherwise (5) for a resource eﬀect of size ψ(st, at).",
"We can also explicitly identify a drive as the diﬀerence between the current value and setpoint, i.e.",
"di,t = |h⋆ i −hi,t| (Cos et al, 2013).",
"The overall drive of the system can then be speciﬁed by Dt = N X i=1 θidi,t = N X i=1 θi|h⋆ i −hi,t| (6) where we introduced θi to specify the weight or importance of the i-th homeostatic variable.",
Most examples take the absolute diﬀerence between current value and setpoint (i.e.,
the L1 norm) as shown above.,
"However, we can consider the space of homeostatic variables H ∈RN and in principle deﬁne any distance function in this space with respect to the reference point H⋆(see e.g.",
Figure 2 for a Euclidean distance example).,
The weight of each homeostatic variable (θi) does not need to be ﬁxed in time.,
"For example, Konidaris makes it a non-linear function of the current homeostatic level hi,t and a priority parameter ρi,t: θi,t = f(hi,t, ρi,t).",
The former dependence Emotion in Reinforcement Learning Agents and Robots: A Survey.,
2 Schematic illustration of homeostasis and drives.,
The ﬁgure shows a two-dimensional homeostatic space consisting (as an example) of energy (h1) and water level (h2).,
The set point (H⋆) indicates the desired values for the homeostatic variables.,
At the current timepoint t the agent’s homeostatic status is Ht (red).,
The associated drive Dt can be visualize as the distance to the set point.,
Note that we use the Euclidean distance for the drive here (i.e.,
"Dt = ||H⋆−Ht||2), while the text describes the L1-norm example (i.e.",
"Dt = ||H⋆−Ht||1, equation 6).",
We are free to choose any distance metric in homeostatic space.,
"After taking an action the new homeostatic status becomes Ht+1 (green), in this case bringing both homeostatic levels closer to their set point.",
The diﬀerence between the drives at both timepoints has been associated to reward and joy (see Section 6.1).,
Figure partially based on Keramati and Gutkin (2011).,
allows priorities (i.e.,
"rewards) to scale non-linearly with the sensory input levels (an idea reminiscent of Prospect Theory (Kahneman and Tversky, 1979)).",
"The priority parameters ρi,t can be estimated online, for example assigning more importance to resources which are harder to obtain (i.e.",
that should get priority earlier).,
"As a ﬁnal note on homeostatic RL systems, note that internal variables need to be part of the state-space as well.",
"One can either include all homeostatic variables and learn generic Q-values, or include only the dominant drive and learn drive-speciﬁc Q-values (Konidaris and Barto, 2006).",
"The connection between drives/homeostasis and emotions is partially reﬂected in Damasio’s somatic marker hypothesis (Damasio, 1994), stating that emotions are the result of bodily sensations.",
"In general, we identify two ways in which homeostatic systems have been used to elicit emotions.",
"The ﬁrst elicits categorical emotions from a subset of homeostatic variables, while the second derives an overall well-being W from the sum of the homeostatic dimensions.",
"One of the ﬁrst RL systems deriving emotions from homeostasis was by Gadanho and Hallam (1998, 2001).",
"They describe an extensive set of internal variables (drives), including e.g.",
"hunger (rises per timestep in lack of resources), pain (rises with collisions), restlessness (rises with non-progress) and temperature (rises with high motor usage).",
"Emotions are related to these physiological variables, e.g.",
"hap- piness is derived from the frequent motor use or decreasing hunger, sadness from low energy, fear from collisions (with less sensitivity if the agent is hungry or rest- less), and anger from high restlessness.",
"Similar ideas are put forward by Coutinho et al (2005), who speciﬁes a more biological homeostasis: blood sugar (increases with food intake), endorphine (increases with play), energy (increases with bed rest), vascular volume (increases with water intake) and body integrity (decreases 12 Moerland et al.",
"Table 2 Overview of most frequently investigated homeostatic dimensions, their associated drive in case of deﬁcit, and the papers in which example implementations can be found.",
"Homeostasic variable Drive Papers Food/energy Hunger (Gadanho and Hallam, 2001), (Salichs and Malfaz, 2012), (Coutinho et al, 2005) (Von Haugwitz et al, 2012) (Goerke, 2006) (Tanaka et al, 2004) Water level Thirst (Salichs and Malfaz, 2012) (Coutinho et al, 2005) Body integrity Pain (Gadanho and Hallam, 2001) (Coutinho et al, 2005) (Tanaka et al, 2004) (Lee- Johnson et al, 2010) Activity Restlessness (Gadanho and Hallam, 2001) (Coutinho et al, 2005) (Von Haugwitz et al, 2012) Energy (movement) Sleep/tiredness (Salichs and Malfaz, 2012) (Coutinho et al, 2005) (Von Haugwitz et al, 2012) (Goerke, 2006) (Tanaka et al, 2004) Social interaction Loneliness (Salichs and Malfaz, 2012) with obstacle collision).",
"Similar examples of homeostatic emotions can be found in Von Haugwitz et al (2012), Tanaka et al (2004) and Goerke (2006).",
A second group of implementations ﬁrst deﬁnes the overall well-being (W).,
"An example of a well-being speciﬁcation is Wt = K −Dt = K − N X i=1 θi|h⋆ i −hi,t| (7) where K denotes a reference value.",
"Compared to the previous paragraph, now all internal variables (instead of subsets) are combined into a single emotion.",
"Some papers leave the speciﬁcation of well-being as their emotion (Gadanho, 2003).",
"Others actually identify the positive or negative diﬀerence in well-being as happy and unhappy (Salichs and Malfaz, 2012) or ‘hedonic value’ (Cos et al, 2013).",
"In conclusion, there have been numerous approaches to homeostatic systems in emotional implementations.",
A summary of some of the most frequently en- countered homeostatic dimensions is shown in Table 2.,
"Although most papers use slightly diﬀerent speciﬁcations for their homeostatic dimensions, it is usually a matter of labelling that does not aﬀect the underlying principle.",
Homeostatic variables provide a good way to naturally implement goal and task switching.,
"The implementation of this functionality usually involves reward modiﬁcation, which is covered in Section 6.1.",
4.2 Appraisal and intrinsic motivation Appraisal theory is an inﬂuential psychological emotion theory (see Section 2).,
Appraisals are domain independent elements that provide (aﬀective) meaning to a particular stimulus.,
"As such, they are a basis for emotion elicitation, as diﬀerent combinations of appraisal dimensions have diﬀerent associated emotions.",
"Examples of appraisal dimensions are novelty, recency, control and motivational relevance.",
"These terms of course refer to abstract cognitive concepts, but in RL literature Emotion in Reinforcement Learning Agents and Robots: A Survey.",
"13 they show a large overlap with intrinsic motivation features, being independent of a speciﬁc external resource.",
"Instead, they are functions of the agent-environment interaction history g and derived model M: ζj(s, a, s′) = fj(g, M) (8) for the jth appraisal variable.",
"Note that the current state and action are actually included in g, but we emphasize that fj(·) is not a function of the actual content of any state s (see Section 2.4 for a discussion of the extrinsic/intrinsic distinction).",
"Rather, fj(·) computes domain-independent characteristics, like ‘recency’ which may be derived from g, and ‘motivational relevance’ which can be derived by planning over M. Intrinsic motivation is an active topic in developmental robotics (Oudeyer and Kaplan, 2007).",
Singh et al (2010) shows how incorporating these dimensions as extra reward provides better task achievement compared to non-intrinsically mo- tivated agents (see Section 6.1).,
We discuss two implementations based on these ideas more extensively: Marinier and Laird (2008) and Sequeira et al (2011).,
"The work by Marinier and Laird (2008) takes a diverse set of appraisal dimensions based on Scherer’s appraisal theory (Scherer, 1999).",
"These include both sensory processing dimensions, like suddenness, intrinsic pleasantness and relevance, and comprehension and reasoning dimensions, like outcome probability, discrepancy from expectation, conduciveness, control and power.",
"The implementation by Se- queira et al (2011) uses a smaller subset of appraisal dimensions: novelty, relevance, valence and control.",
"Note that these appraisal-based papers only elicit appraisal dimensions, without specifying categorical or dimensional emotions on top (see Table 9, i.e.",
appraisal papers with empty middle column).,
"We now highlight some appraisal implementations, both to concretize their speciﬁcation in MDPs, and illustrate the diﬀerences between models.",
Sequeira et al (2011) speciﬁes ‘motivational relevance’ as inversely related to the distance to the goal.,
"If we implement a planning procedure over our model M which returns an estimated distance ˆd(s, s◦) to the goal node s◦from our current node s, then the associated appraisal variable for motivational relevance could be (Sequeira et al, 2011): ζrelevance(s) = 1 1 + ˆd(s, s◦) (9) Similarly, if we denote by c(s) the number of time-steps since node s was last visited, then we can specify a ‘recency’ feature as (Bratman et al, 2012): ζrecency(s) = 1 − 1 c(s) (10) This example intrinsic motivation vector ζ = {ζrelevance, ζrecency} is used in Sec- tion 6.1 to show its use in reward modiﬁcation.",
There are several more speciﬁcations in intrinsic motivation RL literature that reﬂect appraisal dimensions.,
"For example, Hester and Stone (2012a) maintain an ensemble of transition models (by stochastically adding new data to each model) and derive ‘model uncertainty’ from the KL-divergence (as a measure of the dis- tance between two probability distributions) between the ensemble model’s pre- dictions: 14 Moerland et al.",
"ζuncertainty(s, a) = X i̸=j DKL h Ti(s′|s, a)∥Tj(s′|s, a) i (11) for all pairs of models i and j in the ensemble.",
"As a second example from their paper, ‘novelty’ of a state-action pair is identiﬁed from the closest L1-distance to a historical observation: ζnovelty(s, a) = min <si,ai>∈g ∥⟨s, a⟩−⟨si, ai⟩∥1 (12) Recently, Houthooft et al (2016) derive ‘curiosity/surprise’ from the KL-divergence between the old and new transition models (i.e.",
"after updating based on the ob- served transition): ζcuriosity(s, a, s′) = DKL h T(ω|gt, a, s′)∥T(ω|gt) i (13) where T(ω) denotes the transition model parametrized by ω.",
"Together, Equations 9-13 illustrate how intrinsic motivation and appraisal theory have modelled similar notions, and gives a short illustration of the variety of concepts that are expressible in the MDP setting.",
It is also important to note that appraisal theory bears similarities to many ‘domain-independent’ heuristics developed in the planning community Russell et al (1995).,
"These of course include heuristics without a clear psychological or biological interpretation, but we mainly emphasize the potential for cross-breeding between diﬀerent research ﬁelds.",
"For example, some appraisal theories partition novelty into three sub-elements: familiarity, suddenness and predictability (Gratch and Marsella, 2014).",
"Each of these seem to capture diﬀerent computational concepts, and such inspiration may beneﬁt intrinsic motivation and/or planning researchers.",
"Vice-versa, psychologist could seek for results from the RL or planning literature to develop and verify psychological theory as well.",
"There are several other implementations of appraisal dimensions, e.g.",
"by Yu et al (2015), Lee-Johnson et al (2010), Williams et al (2015), Si et al (2010), Kim and Kwon (2010), Hasson et al (2011) and Moussa and Magnenat-Thalmann (2013).",
"We also encounter a few explicit social dimensions, like social fairness (Yu et al, 2015) and social accountability (Si et al, 2010), although the latter for exam- ple requires some symbolic reasoning on top of the RL paradigm.",
This illustrates how current RL algorithms (for now) have trouble learning complex social phenom- ena.,
"Some of the appraisal systems also include homeostatic variables (Yu et al, 2015).",
"Both Williams et al (2015) and Lee-Johnson et al (2010) do not mention appraisal in their paper, but their dimensions can be conceptualized as intrinsic motivation nevertheless.",
"In summary, some appraisal-based dimensions require cognitive reasoning, and are harder to implement.",
"However, dimensions like novelty, motivational relevance and intrinsic pleasantness are frequently implemented (see Table 3).",
Table 4 pro- vides a more systematic overview of the actual connections to the RL framework.,
"These features usually require learned transition functions, recency features or for- ward planning procedures over the model space, which can all be derived from the history g. Also note that a single concept may be interpreted in very diﬀerent ways (Table 4).",
"For example, control and power have been derived from the transitions function (Kim and Kwon, 2010), from the number of visits to a state (Sequeira Emotion in Reinforcement Learning Agents and Robots: A Survey.",
15 Table 3 Overview of frequently investigated appraisal dimensions.,
"Appraisal dimension Paper Novelty (Sequeira et al, 2011) (Kim and Kwon, 2010) (Si et al, 2010) (Williams et al, 2015) Recency (Marinier and Laird, 2008) Control/Power (Marinier and Laird, 2008) (Sequeira et al, 2011) (Kim and Kwon, 2010) (Si et al, 2010) (Williams et al, 2015) Motivational relevance (Marinier and Laird, 2008) (Sequeira et al, 2011) (Has- son et al, 2011) (Kim and Kwon, 2010) (Si et al, 2010) (Williams et al, 2015) Intrinsic pleasantness (Marinier and Laird, 2008) (Sequeira et al, 2011) (Lee- Johnson et al, 2010) Model uncertainty (Marinier and Laird, 2008) (Lee-Johnson et al, 2010) (Kim and Kwon, 2010) (Williams et al, 2015) Social fairness/attachment (Yu et al, 2015) (Moussa and Magnenat-Thalmann, 2013) Social accountability (Si et al, 2010) (Kim and Kwon, 2010) Table 4 Overview of the ﬁve most frequently investigated appraisal dimensions (columns) and their speciﬁc implementations in six appraisal-based papers (rows).",
The cell text indicates which event causes the associated appraisal dimension to be high.,
"Note that both Williams et al (2015) and Lee-Johnson et al (2010) do not explicitly mention appraisal theory as their inspiration, but they do derive emotions from dimensions encountered in appraisal theory.",
"Only the implementation of Marinier and Laird (2008) uses direct sensory information (for control and intrinsic pleasantness), which would better ﬁt with the hard-wired approach in Section 4.4.",
"All other speciﬁcations rely on (an aggregate of) the agent-environment interaction history, for example on an estimated transition model T(s′|s, a).",
"Novelty/ Suddenness Control/ power Motivational relevance Intrinsic pleasant- ness Model un- certainty (Kim and Kwon, 2010) Ratio of P s′ T(s′|s, a)2 and T(s′|s, a) Entropy reduction by act sel.",
"High TD - Low belief b(s) & high goal distance (Lee- Johnson et al, 2010) - - - Low mean travel time Mismatch of model and obs.",
"(Marinier and Laird, 2008) High time to last state visit Absence of obstacles Low dist.",
"to goal Absence of obstacles Low progress (Sequeira et al, 2011) Low # visits to state High # visits to state Low dist.",
"to goal Current reward/ value ratio - (Si et al, 2010) Low T of obs.",
transition Low dist.,
"to higher value state High absolute TD - - (Williams et al, 2015) Unseen/seen ratio state-space High success/fail ratio Part of task ﬁnished - Low model accuracy et al, 2011), from a forward planning procedure (Si et al, 2010) and from the overall success of the agent (Williams et al, 2015).",
"We encounter a fundamental challenge in the ﬁeld here, namely how to translate abstract cognitive concepts to explicit (broadly accepted) mathematical expressions.",
16 Moerland et al.,
4.3 Value function and reward The third branch of emotion elicitation methods in RL focusses on the value and reward functions.,
"We can generally identify four groups: value-based, temporal diﬀerence-based, average reward-based and reward-based (Table 5).",
"One of the earliest approaches to sequential decision making based on emotion was by Bozinovski (1982); Bozinovski et al (1996), who considered emotion to be the expected cumulative reward (i.e.",
the state-action value) received from taking an action in that state.,
"Thereby, Bozinovski actually developed a precursor of Q-learning grounded in emotional ideas.",
Other implementations have also consid- ered emotion as the state value.,
"For example, Matsuda et al (2011) maintains a separate value function for fear, which is updated when the agent gets penalized.",
Recent work by Jacobs et al (2014) considers the positive and negative part of the state as the hope and fear signal.,
"Another value-based approach is by Salichs and Malfaz (2012), who model the fear for a particular state as the worst historical Q-value associated with that state.",
"As such, their model remembers particular bad locations for which it should be afraid.",
A second group of value function related implementations of emotions are based on the temporal diﬀerence error (TD).,
"For Q-learning, the TD is given by δ = r(s, a, s′) + γ max a′ Q(s′, a′) −Q(s, a) (14) There has been extensive research in neuroscience on the connection between dopamine and the TD.",
"Following these ideas, there have also been implementations connecting happiness and unhappiness to the positive and negative TD, respec- tively (Moerland et al, 2016; Jacobs et al, 2014; Lahnstein, 2005).",
Models based on the temporal diﬀerence are robust against shifting the reward function by a constant (a trait that is not shared by the models of the ﬁrst group of this sec- tion).,
"More recently, Moerland et al (2016) extended these ideas by deriving hope and fear signals from anticipated temporal diﬀerences (through explicit forward simulation from the current node).",
Another branch of emotion derivations base themselves on the average re- ward.,
"For example, Broekens et al (2007a), Schweighofer and Doya (2003) and Hogewoning et al (2007) derive a valence from the ratio between short- and long- term average reward.",
"Shi et al (2012) also derives emotions from the temporal change in reward function, while Blanchard and Canamero (2005) uses the aver- age reward.",
"Other implementations interpreted the reward ifself as the emotional signal (Moren and Balkenius, 2000; Balkenius and Mor´en, 1998; Ahn and Picard, 2006).",
"In conclusion, emotions have been related to the value function, temporal dif- ference error or direct derivative of the reward function (Table 5).",
"Note that some implementations try to incorporate a time dimensions as well (besides only the reward or value signal), e.g.",
"Moerland et al (2016), Salichs and Malfaz (2012) and Broekens et al (2007b).",
"4.4 Hard-wired While all three previous groups used internal agent/robot aspects, a ﬁnal category speciﬁes hard-wired connections from sensory input to emotions.",
A ﬁrst group of Emotion in Reinforcement Learning Agents and Robots: A Survey.,
17 Table 5 Overview of elicitation methods based on value and/or reward functions.,
"Imple- mentations are either based on the raw value function, the temporal diﬀerence error, some derivative of an average reward or from the raw reward function.",
"Method Papers Value (Bozinovski, 1982; Bozinovski et al, 1996) (Matsuda et al, 2011) (Jacobs et al, 2014) (Salichs and Malfaz, 2012) Temporal diﬀerence (Moerland et al, 2016) (Jacobs et al, 2014) (Lahnstein, 2005) Average reward (Broekens et al, 2007a) (Schweighofer and Doya, 2003) (Hogewoning et al, 2007) (Shi et al, 2012) (Blanchard and Canamero, 2005) Reward (Moren and Balkenius, 2000; Balkenius and Mor´en, 1998) (Ahn and Picard, 2006) implementations use the detected emotional state of another person to inﬂuence the emotion of the agent/robot (Hoey et al, 2013) (Ficocelli et al, 2015).",
"Hasson et al (2011) uses facial expression recognition systems to detect human emotion, while Kubota and Wakisaka (2010) uses human speech input.",
"Note that if these agent emotions subsequently inﬂuence agent learning, then we come very close to learning from human emotional feedback (as brieﬂy described in Section 2.3).",
There are several other implementations that pre-specify sensation-emotion connections.,
"In general, these approaches are less generic compared to the earlier categories.",
"Some use for example fuzzy logic rules to connect input to emotions (Ayesh, 2004).",
"Another example we encountered is the previous emotional state (at t−1) inﬂuencing the current emotional state (Kubota and Wakisaka, 2010).",
"An example is the Markovian transition model between emotions in (Ficocelli et al, 2015), with similar ideas in (Zhang and Liu, 2009).",
"This is a reasonable idea for smoother emotion dynamics, but we still categorize it as hard-wired since it does not explain how initial emotions should be generated.",
"Finally, there is also overlap with previously described elicitation methods.",
"For example, Tsankova (2002) derives an emotion (frustration) directly from the collision detector.",
"This is very similar to some homeostatic speciﬁcations, but Tsankova does not include a body integrity or pain variable (i.e.",
"it is therefore not a homeostatic system, but the author does make the connection between pain or non-progress and frustration).",
"In conclusion, the hard-wired emotion elicitation does not seem to provide us any deeper understanding about emotion generation in RL agents, but the papers in this category may actually implement ideas from diﬀerent elicitation methods.",
"5 Emotion type Having discussed the methods to elicit emotions, this section discusses which types of emotions are speciﬁed.",
We cover both categorical (5.1) and dimensional (5.2) emotion models.,
"Note however that some appraisal theory-based papers only elicit appraisal dimensions, without speciﬁcally identifying emotions (see Table 9).",
18 Moerland et al.,
5.1 Categorical Most papers in the emotion and RL literature elicit categorical emotions.,
An overview of the most occurring emotions and their associated papers is presented in Table 6.,
Joy (or happiness) is the most implemented emotion by a wide vari- ety of authors.,
"We did not include the papers that specify a valence dimension (see Section 5.2), but this could also be interpreted as a happy-sad dimension.",
"A few papers (Von Haugwitz et al, 2012) (Tanaka et al, 2004) speciﬁcally address Ekman’s six universal emotions (happy, sad, fear, anger, surprise, disgust), while most papers drop the latter two emotions.",
"In general, happy, sad, fear and anger have been implemented in all elicitation categories (homeostatic, appraisal and value-based).",
"However, hope has mainly been connected to value function based systems.",
"The implementations of hope try to assess anticipation (by addressing the value function (Jacobs et al, 2014), the dynamics within a decision cycle (Lahnstein, 2005), or explicitly forward simulat- ing from the current node towards expected temporal diﬀerences (Moerland et al, 2016)).",
"Hope therefore needs a time component, a notion which is not directly available from for example an extrinsic homeostasis dimension.",
"An overview of the most often elicited emotions (happy, sad, fear and angry) is provided in Table 7.",
The table shows that diﬀerent elicitation methods have been associated to similar sets of categorical emotions.,
"For example, anger (fourth col- umn) has been associated to extrinsic homeostasis (e.g.",
"hunger), intrinsic appraisal (e.g.",
non-progress) and reward-based (decreasing received reward) elicitation.,
"Note that frustration, a closely related emotion, has been associated to obstacle detec- tion (Tsankova, 2002) and non-progress (Hasson et al, 2011) as well.",
"The other three emotions in Table 7 have also been associated to each elicitation dimension, as is easily observed from the colour coding.",
Note that Table 7 also shows how diﬀerent researchers apply diﬀerent elicitation methods within one paper (i.e.,
looking at rows instead of columns now).,
"Moreover, a few papers even combine elicitation methods for an individual emotion.",
"For example, Williams et al (2015) derives fear from a combination of pain (extrinsic) and novelty (intrinsic/appraisal).",
It is important to realize that the elicitation methods of the previous section are clearly only a framework.,
"These are not hard separations, and combining diﬀerent approaches is clearly possible (and probably necessary), as these papers nicely illustrate.",
"Finally, many included papers did not fully specify the implemented connec- tions between elicitation method and emotion type, making it diﬃcult to replicate these studies.",
"For example, Von Haugwitz et al (2012) only mentions the con- nections between homeostatic dimensions and emotions are based on fuzzy logic, but does not indicate any principles underlying the real implementation.",
"Similar problems occur in (Tanaka et al, 2004), (Ayesh, 2004) and (Obayashi et al, 2012), while (Zhou and Coggins, 2002) and (Shibata et al, 1997) leave the implemented connections unspeciﬁed.",
"5.2 Dimensional Relative to the number of implementations of categorical emotions, there is a much smaller corpus of work on dimensional emotions (Table 8).",
The most im- Emotion in Reinforcement Learning Agents and Robots: A Survey.,
19 Table 6 Overview of categorical emotion implementations.,
"Categorical emo- tion Paper Joy/happy (Gadanho and Hallam, 2001) (Von Haugwitz et al, 2012) (Ficocelli et al, 2015) (Tanaka et al, 2004) (Goerke, 2006) (Yu et al, 2015) (Lee-Johnson et al, 2010) (Williams et al, 2015) (Hasson et al, 2011) (Moussa and Magnenat-Thalmann, 2013) (Salichs and Mal- faz, 2012) (Cos et al, 2013) (Moerland et al, 2016) (Jacobs et al, 2014) (Lahnstein, 2005) (Shi et al, 2012) (El-Nasr et al, 2000) (Kubota and Wakisaka, 2010) Sad/unhappy/distress (Gadanho and Hallam, 2001) (Von Haugwitz et al, 2012) (Ficocelli et al, 2015) (Tanaka et al, 2004) (Yu et al, 2015) (Lee-Johnson et al, 2010) (Moussa and Magnenat-Thalmann, 2013) (Salichs and Mal- faz, 2012) (Moerland et al, 2016) (Jacobs et al, 2014) (Lahnstein, 2005) (El-Nasr et al, 2000) (Kubota and Wakisaka, 2010) Fear (Gadanho and Hallam, 2001) (Von Haugwitz et al, 2012) (Tanaka et al, 2004) (Goerke, 2006) (Yu et al, 2015) (Lee-Johnson et al, 2010) (Williams et al, 2015) (Salichs and Malfaz, 2012) (Moerland et al, 2016) (Jacobs et al, 2014) (Matsuda et al, 2011) (Shi et al, 2012) (El-Nasr et al, 2000) (Kubota and Wakisaka, 2010) Anger (Gadanho and Hallam, 2001) (Von Haugwitz et al, 2012) (Ficocelli et al, 2015) (Tanaka et al, 2004) (Goerke, 2006) (Yu et al, 2015) (Hasson et al, 2011) (Moussa and Magnenat-Thalmann, 2013) (Shi et al, 2012) (El-Nasr et al, 2000) (Kubota and Wakisaka, 2010) Surprise (Von Haugwitz et al, 2012) (Tanaka et al, 2004) (Lee-Johnson et al, 2010) Hope (Moerland et al, 2016) (Jacobs et al, 2014) (Lahnstein, 2005) (El- Nasr et al, 2000) Frustration (Hasson et al, 2011) (Huang et al, 2012) (Tsankova, 2002) plemented dimension is valence.",
"Not surprisingly, valence has mostly been derived from reward-based elicitation methods (Broekens et al, 2007a) (Ahn and Picard, 2006) (Zhang and Liu, 2009) (Obayashi et al, 2012) (Hogewoning et al, 2007).",
"It is also connected to a few extrinsic homeostasis papers (Coutinho et al, 2005) (Gadanho, 2003), but then it is referred to as ‘well-being’.",
"Although this is not completely the same concept, we group these together here for clarity.",
"Following the dimensional emotion models of Russell and Barrett (1999) intro- duced in Section 2.1, the second most implemented dimension is arousal.",
Arousal has been connected to extrinsic homeostatic dimensions (e.g.,
"pain and overall well-being (Coutinho et al, 2005)), appraisal-like dimensions (e.g.",
"continuation of incoming stimulus (Kuremoto et al, 2013)), and a few hard-wired implementations (Ayesh, 2004) (Guojiang et al, 2010).",
"Note that some do not use the term arousal but refer to similar concepts, e.g.",
"relaxation (Coutinho et al, 2005) and restlessness (Ayesh, 2004).",
"The only paper to extend the valence-arousal space is by Hoey et al (2013), who also include control.",
"In general, the dimensional emotion models seem somewhat under-represented compared to the categorical emotion implementations.",
"Although the implemen- tation for valence shows some consistency among papers, there is more diﬃculty to specify arousal or diﬀerent emotion dimensions.",
"Nevertheless, the continuous nature of dimensional emotion models remains appealing from an engineering per- spective.",
"A possible beneﬁt is the identiﬁcation of a desirable target area in aﬀec- tive space, towards which the agent aims to progress (Guojiang et al, 2010).",
20 Moerland et al.,
Table 7 Overview of four categorical emotion (columns) elicitations for diﬀerent papers (rows).,
The text in each cell speciﬁes the elicitation condition.,
"We observe diﬀerent cate- gories of emotion elicitation, i.e.",
"homeostatic (blue, Section 4.1), appraisal (green, 4.2) and value-based (red, 4.3).",
We see how single emotions are connected to diﬀerent elicitation meth- ods (multiple colours in single column) and how single papers use diﬀerent elicitation methods (multiple colours in single row).,
"Happy/Joy Sad/Distress Fear Anger (Gadanho and Hallam, 1998) High energy Low energy Pain High restlessness (low progress) (Goerke, 2006) All drives low - Homesick & low energy Hunger & homesick & high energy (Kim and Kwon, 2010) Goal achievement No goal achievement Pain No progress (Williams et al, 2015) Progress & control & low pain - Pain & novelty - (Salichs and Malfaz, 2012) Positive delta well-being Negative delta well-being Worst historical Q(s,a) - (Moerland et al, 2016) Positive TD Negative TD Anticipated negative TD - (Shi et al, 2012) Increasing positive reward - Increasing negative reward Decreasing positive reward (Yu et al, 2015) High well-being Egoistic agent & low well-being Agent defects & others co-orperate Agent co-orperates & others defect Table 8 Overview of dimensional emotion implementations.",
"Dimensional emotion Paper Valence (Kuremoto et al, 2013) (Ahn and Picard, 2006) (Zhang and Liu, 2009) (Broekens et al, 2007a) (Broekens, 2007) (Obayashi et al, 2012) (Hogewon- ing et al, 2007) (Hoey et al, 2013) (Guojiang et al, 2010) (Coutinho et al, 2005) Arousal (Kuremoto et al, 2013) (Obayashi et al, 2012) (Ayesh, 2004) (Hoey et al, 2013) (Guojiang et al, 2010) (Coutinho et al, 2005) Control (Hoey et al, 2013) 6 Emotion function We now discuss the ways in which emotions may inﬂuence the learning loop.",
"It turns out emotions have been implicated with all main aspects of this loop: Reward (6.1), State (6.2), Adaptive Critic (6.3) and Actor (6.4).",
"Finally, emotion has also been studied as an epiphenomenon, i.e.",
"without any eﬀect on the learning loop, but for example to communicate the learning/behavioural process to other social companions (6.5).",
These categories are visualized in Figure 1 (labels I-V).,
Note that this Section introduces the ways in which emotion may inﬂuence the RL loop Emotion in Reinforcement Learning Agents and Robots: A Survey.,
21 on a conceptual level.,
"We summarize the resulting eﬀect, for example on learning eﬃciency, in Section 7.",
6.1 Reward modiﬁcation A large group of emotional RL implementations use emotions to modify the reward function.,
These approaches add an additive term to the reward function that relies on emotions (we have only encountered additive speciﬁcations).,
The reward function is given by rt = ˜rt + r△ t (15) where ˜r(t) denotes the external reward function and r△(t) an internal reward based on emotional mechanisms.,
"In the RL community, Eq.",
"15 is known as re- ward shaping (Ng et al, 1999).",
"The internal reward can be targeted at maximizing positive emotions, but is also frequently associated to homeostatic variables or appraisal dimensions (see Sections 4.1 and 4.2 for elicitation).",
"However, the gen- eral underlying principle usually remains that agents seek to maximize positive emotions and minimize negative emotions.",
Homeostasis.,
For homeostatic systems the reward becomes dependent on the cur- rent state of the internal homeostatic variables.,
"Some implementations use the diﬀerence in overall well-being, r△ t = Wt −Wt−1 = Dt−1 −Dt (16) where the step from well-being W to overall drive D naturally follows from Equa- tion (7).",
"In this speciﬁcation, the acquisition of food does not provide any reward if the associated homeostatic variable (e.g.",
energy/sugar level) is already satiated.,
"Implementations of the above idea can be found in (Gadanho and Hallam, 2001), (Salichs and Malfaz, 2012) and (Cos et al, 2013).",
"Variants of this have focussed on using positive emotions (instead of well-being) as the reinforcement learning signal, e.g.",
"in (Gadanho and Hallam, 1998) and (Goerke, 2006).",
Appraisal-based.,
Similar ideas are used for appraisal-based reward modiﬁcations.,
"Some examples of appraisal dimension speciﬁcations were discussed in Section 4.2, with some formal examples in Equations 9-13.",
"Appraisal dimensions are related to generic concepts of the agent history (novelty, recency, consistency of observa- tions with world model) and expectations with respect to the goal (motivational relevance, intrinsic pleasantness).",
"Several studies in the intrinsically motivated re- inforcement learning literature have identiﬁed the learning and survival beneﬁt of these dimensions (Oudeyer and Kaplan, 2007; Oudeyer et al, 2007).",
Some authors therefore took appraisal theory as an inspiration to develop intrinsic motivation features.,
Speciﬁcations in this direction therefore usually take the following form: r△ t = J X j=1 φjζj(gt) (17) 22 Moerland et al.,
for J appraisal variables and φj denoting the weight of the j-th appraisal dimen- sion.,
"We could for example use the two features in Equations 9-10, specifying an agent that gets rewarded for motivational relevance and recency.",
"Note that ap- praisal speciﬁcations usually do not include the diﬀerence with (t −1), probably because they are usually assumed not to satiate (i.e.",
no underlying homeostatic dynamics).,
We also note that a reward bonus for novelty (e.g.,
"12) is in the RL literature usually referred to as ‘optimism in the face of uncertainty’, i.e.",
we want to explore where we have not been yet.,
Sequeira et al (2011) actually tries to optimize the vector of weights φ (with respect to overall goal achievement).,
"In a more recent publication, Sequeira et al (2014) also extends this work to actually learn the required appraisal dimensions through genetic programming.",
Similar ideas can be found in Marinier and Laird (2008).,
"One of the problems with both implementations is the distance-to-goal heuristic used by both emotion-based agents, which has access to additional infor- mation compared to the baseline agent (although the heuristic does not monoton- ically increase with the actual distance to goal).",
We discuss the empirical results of these papers more systematically in Section 7.,
6.2 State modiﬁcation Emotions have also been used as part of the state-space (learning emotion speciﬁc value functions and policies).,
"An example is the social robot Maggie (Castro- Gonz´alez et al, 2013).",
"When fear is elicited it becomes part of the state-space (replacing the dominant drive in a homeostatic system), which makes Maggie learn fear-speciﬁc action values.",
"Some papers explicitly write Q(s, a, e), where e denotes the emotional state, to illustrate this dependency (Ahn and Picard, 2006) (Ayesh, 2004).",
"More examples of such implementations can be found in (Zhang and Liu, 2009) (Ficocelli et al, 2015) (Obayashi et al, 2012) and (Matsuda et al, 2011).",
"Hoey developed a POMDP variant called Bayesian Aﬀect Control Theory that includes the three-dimensional emotional space (valence, control, arousal) of a companion (Hoey et al, 2013) and the agent itself (Hoey and Schr¨oder, 2015).",
"There are also implementations that use reinforcement learning to model the aﬀective state of a human or group (Kim and Cho, 2015), but note that this is a diﬀerent setting (i.e.",
RL to steer human emotional state instead of agent emotional state).,
Using emotion to modify the state can also be seen as a form of representation learning.,
"There are not many architectures that learn the modiﬁcation (most hard- code the emotion elicitation), with the exception of Williams et al (2015).",
"Their architecture has similarities to the bottle-neck structure frequently encountered in deep neural network research, for example in (deep) auto-encoders (Goodfellow et al, 2016).",
We return to the fully-learned approach in the Discussion (Section 8).,
"6.3 Meta-learning The previous two sections showed how emotion has been implicated with determin- ing both the reward and state, which together can be considered as the (Internal) Emotion in Reinforcement Learning Agents and Robots: A Survey.",
"Afterwards, the state and reward are used to learn a value function, a pro- cess that is usually referred to as the Adaptive Critic (see Figure 1).",
"The learning process requires appropriate (and tedious) scaling of learning parameters, most noteworthy the learning rate α (see Section 2.4).",
"The connection between emotion and these learning parameters was inspired by the work of Doya (2000, 2002).",
He identiﬁed neuroscientiﬁc grounding for the connection between several neurotransmitters and several reinforcement learning parameters.,
"In particular, he proposed connections between dopamine and the temporal diﬀerence error (δ), serotonin and the discount factor (γ), noradrenaline and the Boltzmann action selection temperature (β) and acetylcholine and the learning rate (α).",
This work inspired both Shi et al (2012) and Von Haugwitz et al (2012) to implement emotional systems inﬂuencing these metaparameters.,
"Shi identiﬁes the connections joy →δ, anger →β, fear →α and relief →γ, while von Haugwitz changes only the latter two to surpise →(1 −α) and fear →(1 −γ).",
"Recently, Williams et al (2015) also investigated metaparameter steering in navigation tasks.",
"Together with (Sequeira et al, 2014) they are the only ones to learn the emotional connections, and then post-characterize the emerged phe- nomena.",
Williams trains a classiﬁer connecting a set of primary reinforcers (both appraisal and homeostasis-based) to the metaparameters of their navigation algo- rithm.,
"They train two emotional nodes, and only afterwards anthropomorphicized these.",
"One node learned positive connections to progress and control and nega- tively to pain and uncertainty, while it caused the robot to increase its speed and reduce the local cost bias.",
"In contrary, their second node was elicited by pain and novelty, while it caused the opposite eﬀect of node 1.",
"They afterwards characterized these nodes as ‘happy’ and ‘fear’, respectively.",
6.4 Action selection The ﬁnal step of the RL loop involves action selection.,
"This incorporates another crucial RL challenge, being the exploration/exploitation trade-oﬀ.",
"Emotions have long been implicated with action readiness, and we actually already encountered two papers steering the Boltzmann action selection temperature β above (as it is technically also a metaparameter of the RL system).",
We next focus on those papers that speciﬁcally target action selection.,
One branch of research focusses on directly modifying the exploration param- eter.,
"Broekens et al (2007b,a) has done extensive investigations of the connections between valence and the exploration/exploitation trade-oﬀ.",
"In one implementation (Broekens et al, 2007a) selection was based on internal simulation, where a valency determined the threshold for the simulation depth.",
"In another paper (Broekens et al, 2007b) this valency directly inﬂuenced the β parameter in a Boltzmann action selection mechanism.",
"Schweighofer and Doya (2003) applied small pertur- bations to the exploration parameters based on emotion, and subsequently kept the parameters if they performed better.",
"Finally, Hogewoning et al (2007) inves- tigated a hybrid system of Broekens and Schweighofer, trying to combine their strengths.",
"Other papers use emotion to switch between multiple sets of value functions, thereby eﬀectively determining which set should currently be used for action se- 24 Moerland et al.",
"For example, both Tsankova (2002) and Hasson et al (2011) use a high frustration to switch between behaviour.",
"Similarly, Kubota and Wakisaka (2010) use several emotions to switch between the weighting of diﬀerent value functions.",
"For example, happiness leads to exploration by selecting a value function derived from inverse recency.",
"Note that such a recency feature was used in the appraisal section described previously, but there it modiﬁed the reward function, while now emotion is used to switch between value functions.",
"Although this technically leads to similar behaviour, emotion intervenes at a diﬀerent level.",
6.5 Epiphenomenon The ﬁnal category of functions of emotions seems an empty one: Epiphenomenon.,
"Several papers have studied emotion elicitation in RL, without the emotion inﬂu- encing the learning or behavioural loop.",
These papers usually focus on diﬀerent evaluation criteria as well (see Section 7).,
"Examples of papers that only elicit emo- tions are (Coutinho et al, 2005), (Goerke, 2006), (Si et al, 2010), (Kim and Kwon, 2010), (Bozinovski, 1982; Bozinovski et al, 1996), (Jacobs et al, 2014), (Lahnstein, 2005) and (Moerland et al, 2016).",
There can however still be a clear function of the emotion for the agent in a social communication perspective (node V in Figure 1).,
"Emotion may communi- cate the current learning and behavioural process, and also create empathy and user investment.",
The potential of emotions to communicate internal state and en- hance empathy is infrequently evaluated in current reinforcement learning related emotion literature.,
This seems a fruitful direction when emotions serve to make an agent or robot more sociable and likeable.,
This concludes our discussion of emotion functions in RL agents.,
"The full overview is provided in Table 10, which mainly lists the categories per paper.",
The most important connections between Sections 4-6 (i.e.,
column 1 to 3 in Table 9) were described in the text and tables (e.g.,
Table 4 and 7).,
"7 Evaluation This section systematically addresses the embodiment, test scenario and main empirical results found in the diﬀerent papers.",
A systematic overview of this section is provided in Table 10.,
"7.1 Embodiment We can grossly identify 5 embodiment categories: standard single agent, multiple agents, screen agents, simulated robot and real robot.",
The standard agent set- ting usually concerns a (gridworld) navigation simulation in some environment designed by the researcher.,
"Some agents are also designed to appear on a screen for interaction with a user (El-Nasr et al, 2000).",
Another group of embodiments concern simulated or real robots.,
"Simulated robots are based on models of exist- ing real robots, i.e.",
they usually incorporate more realistic physics and continuous controls.,
Emotion in Reinforcement Learning Agents and Robots: A Survey.,
25 There are also real robotic implementations in navigation and resource tasks.,
"However, several robotic implementations (especially those involving human inter- action) use the robot mainly as physical embodiment (without moving much, for example in a dialogue task).",
"Overall, most implementations have focussed on simu- lated agents.",
"It is important to note that most state-spaces stay relatively small, i.e.",
sensory information usually has narrow bandwidth (or is assumed to be appropri- ately pre-processed).,
"Although this facilitates interpretation, a remaining question is whether the current emotion modelling methods scale to high-dimensional and complex problems.",
"7.2 Test scenario Emotion implementations have been tested in diﬀerent scenarios: navigation tasks with resources and/or obstacles, multiple agent interaction settings and human- agent/robot interaction tasks.",
There is a wide variety of navigation tasks with additional (multiple) resources and obstacles (with associated positive and negative rewards).,
When resources and obstacles are non-stationary we usually see the terminology ‘prey’ and ‘predators’.,
Within this group we mainly see navigation tasks with a single goal and multiple obstacles (i.e.,
"‘mazes’ (Marinier and Laird, 2008) or robot navigation (Lee-Johnson et al, 2010) (Williams et al, 2015)).",
"A second group involves multiple resources, which are mostly connected to underlying homeostatic systems to investigate be- haviour switching.",
"A few tasks also speciﬁcally include virtual enemies (Sequeira et al, 2011) or humans with adversarial intentions (Castro-Gonz´alez et al, 2013) (Tanaka et al, 2004).",
"A second, much smaller group of scenarios involves multiple agents in a social simulation scenario, either a competitive (Von Haugwitz et al, 2012) (Yu et al, 2015) or co-operative one (Matsuda et al, 2011).",
The third category tests their im- plementation in interaction with humans.,
"This can either involve a human dialogue task (Ficocelli et al, 2015) (Moussa and Magnenat-Thalmann, 2013) or physical interaction with a human (Blanchard and Canamero, 2005) (Shibata et al, 1997).",
"In general, most papers have constructed their own scenario.",
"We have not seen any test scenarios being borrowed from other emotion-learning implementations, nor from the general reinforcement learning literature.",
This makes it hard to com- pare diﬀerent implementations amongst each other.,
"7.3 Main results Finally, we discuss what empirical results were found by the various authors.",
"We identify three main categories in which emotions may be useful to the agent: learning eﬃciency, emotion dynamics and human-robot interaction (HRI) (Table 10, third column).",
Learning eﬃciency.,
Most authors in emotion-RL research have focussed on learn- ing eﬃciency (see Table 10).,
"Overall, emotions have been found beneﬁcial in a variety of learning tasks.",
"Agents with emotional functionality achieved higher av- erage rewards (Gadanho and Hallam, 2001; Sequeira et al, 2014; Yu et al, 2015) or 26 Moerland et al.",
"learned faster (Marinier and Laird, 2008; Ahn and Picard, 2006; Zhang and Liu, 2009).",
"Others researchers focussed on the ability to avoid speciﬁc negative rewards, like the ability to avoid collisions (Gadanho and Hallam, 2001; Lee-Johnson et al, 2010) and navigate away from obstacles (Shi et al, 2012).",
"Other researchers report improved behaviour switching, where emotional agents better alternate between goals (Cos et al, 2013; Hasson et al, 2011; Goerke, 2006).",
"Finally, some authors speciﬁcally show improved exploration (Broekens et al, 2007b).",
"Many authors that focussed on learning performance do compare to a non-emotional baseline agent, which is of course a necessary comparison.",
"Altogether, the results show emotions may be a useful inspiration to improve learning performance of RL agents.",
Emotion dynamics.,
"A second group of researchers focusses on emotion dynamics, usually comparing the emergent emotion signals to known psychological theories.",
"For example, Jacobs et al (2014) showed patterns of habituation and extinction, Moren and Balkenius (2000) reproduced blocking, while Blanchard and Canamero (2005) observed approach and avoidance behaviour in their emotional agent.",
"Other researchers qualitatively interpret whether the emotion dynamics ﬁt the (social) interaction (Tanaka et al, 2004) (Moussa and Magnenat-Thalmann, 2013) or occurs at appropriate states in the scenario (Moerland et al, 2016).",
"Altogether, results in this category show that emotion in RL agents might be a viable tool to study emotion theories in computational settings.",
Human-robot interaction.,
"Finally, a third group of researchers focusses on human- robot interaction evaluation.",
"Their primary focus is to show how emotions may beneﬁt social interaction with humans, usually by taking questionnaires with the participants after the experiment.",
"Participants of Ficocelli et al (2015) report more eﬀective communication, participants of El-Nasr et al (2000) found the agent more convincing, and participants of Shibata et al (1997) report an increased notion of connection as well as increased perception of robot intelligence.",
Kim and Kwon (2010) describe an enhanced pleasant feeling of the participant after the human- agent interaction.,
"Therefore, there is clear indication that emotion in RL agents may beneﬁt an interactive learning setting.",
"However, there are relatively few papers in this category compared to the other two, and this may be a direction for more research.",
"8 Discussion This article surveyed the available work on emotion and reinforcement learning in agents and robots, by systematically categorizing emotion elicitation, type and function in RL agents.",
We ﬁrst summarize the main results and identify the chal- lenges encountered throughout the article.,
"Emotions have been elicited from extrinsic motivation (in combination with homeostasis), intrinsic motivation (in combination with appraisal), value and re- ward functions and as hard-wired implementation.",
"We want to emphasize again that extrinsic motivation and homeostasis are not synonyms, nor are intrinsic mo- tivations and appraisal (see Section 3).",
"The hard-wired emotion elicitation seems least useful, as it does not provide any deeper understanding about emotion gen- eration, and is by deﬁnition hand-crafted to the task.",
The other three elicitation Emotion in Reinforcement Learning Agents and Robots: A Survey.,
27 methods are useful and appear to address diﬀerent aspects of emotions.,
"Home- ostasis focusses on the inner resource status, appraisal on the inner model status and value/reward focusses on the learning process.",
They seem to cover diﬀerent aspects of emotions.,
"For example, surprise seems only elicitable from a model, joy from food requires extrinsic motivation and homeostasis, while aspects like antici- pated change need value functions.",
"Finally, note that there remains slight overlap among categories, i.e.",
"they serve as a framework, but are not mutually exclusive.",
This is also illustrated by the overlap among implementations in Table 7.,
Regarding emotion types we observed a relatively larger corpus of categori- cal implementations than dimensional models.,
"Although dimensional models are appealing from an engineering perspective, they are usually implemented in 1D (valence) or 2D (valence-arousal) space.",
This makes it challenging to implement a diverse set of emotions.,
"We do want to present a hypothesis here: dimensional and categorical emotions may ﬁt into the same framework, but at diﬀerent levels.",
"Concepts like ‘well-being’, as encountered throughout this survey, do not appear to be categorical emotions, but could be interpreted as valence.",
"However, an agent can have categorical emotions on top of a well-being/valence system, joining both emotion types in one system.",
"Similarly, arousal could be related to the speed of processing of the RL loop, also entering the RL process at a diﬀerent level.",
"Finally, emotion function could involve nearly every node in the RL loop: reward, state, value function and action selection.",
"It seems like all approaches are useful, as each element targets a diﬀerent RL challenge.",
The ﬁfth emotion function category (epiphenomenon) should get more attention because it involves a diﬀerent kind of usefulness (communicative).,
"Although quite some papers are focussing on emotion dynamics, there is less work on evaluating the potential of emotions to communicate the learning process.",
Thomaz and Breazeal (2006) found that transparency of the learner’s internal process (in their case through the robot’s gaze direction) can improve the human’s teaching.,
"We hypothesize emotional communication to express internal state may serve a similar role, which is a topic that could get more research attention in the future.",
Advice for implementation.,
We expect this article is useful to engineers who want to implement emotional functionality in their RL-based agent or robot.,
We advise to ﬁrst consider what type of functionality is desired.,
"When the goal is to have emotions visualize agent state, or have believable emotions to enhance empathy and user investment, then emotions can be implemented as an epiphenomenon (i.e.",
focus on Sections 4 and 5).,
"The reader could for example ﬁrst decide on the desired emotion types, and then check which available elicitation methods seem applicable (e.g.",
via Table 9).,
"When one desires emotion function in their agent/robot as well, then Section 6 becomes relevant.",
"We advise the reader to ﬁrst consider the desired functionality, e.g.",
"a more adaptive reward function, learning parameter tuning, or modulated exploration, and then work ‘backwards’ to emotion type and emotion elicitation.",
Readers may verify whether there are existing implementations of their requirements through the colour coding in Table 9.,
"In general, we believe researchers in the ﬁeld should start focussing on integrat- ing approaches.",
"This survey intended to provide a framework and categorization of emotion elicitation and function, but it seems likely that these categories actually jointly occur in the behavioural loop.",
We look forward to systems that integrate multiple approaches.,
"Moreover, we want to emphasize the paper by Williams et al 28 Moerland et al.",
(2015) that took a fully learned approach.,
"Their system contains nodes that were trained for their functional beneﬁt, and later on characterized for the emotion pat- terns.",
"We expect such an approach to both be more robust against the complexity problems encountered when developing integrated systems, and to transfer more easily between problem settings as well.",
Testing and quality of the ﬁeld.,
We also systematically categorized the testing sce- narios and evaluation criteria (Section 7 and Table 10).,
There are several points to be noted about the current testing.,
"First we want to stress a point already made by Ca˜namero (2003), who noted that ‘one should not put more emotion in the agent than what is required by the complexity of the system-environment interac- tion’.",
Many of the current implementations design their own (grid) world.,
"While these artiﬁcial worlds are usually well-suited to assess optimization behaviour, it is frequently hard to assess which emotions should be elicited by the agent at each point in the world.",
"On the other hand, more realistic scenarios quickly become high-dimensional, and therefore the challenge changes to a representation learn- ing problem.",
"Potentially, the advances in solving more complex AI scenarios with (deep) RL (Silver et al, 2016; Mnih et al, 2015) may provide more realistic test scenarios in the future as well.",
There are two other important observations regarding testing and evaluation.,
We have not encountered any (emotional) scenario being reproduced by other researchers.,
This appears to us as an important problem.,
"To enhance the stan- dard of the ﬁeld, researchers should start reproducing scenarios from other’s work to compare with, or borrow from diﬀerent RL literature.",
The second topic we want to emphasize is the use of diﬀerent evaluation criteria.,
"Researchers should choose whether they target learning eﬃciency, emotion dynamics or HRI criteria.",
"If learning performance is your criterion, then your implementation must include a baseline.",
"When you focus on emotion dynamics, then you should try to validate by a psychological theory, or ideally compare to empirical (human) data.",
"When you focus on human interaction criteria, then this should usually involve a ques- tionnaire.",
"Although questionnaires seems to be consistent practice already, we did observe authors reporting on a smaller subset of the questions (i.e.",
posing the risk to have a few results pop out by statistical chance).,
"This brings us to a ﬁnal problem in the ﬁeld, being the thoroughness of the pa- pers.",
Frequently we were unable to fully deduce the details of each implementation.,
"Indeed a full system description with all the details requires valuable space, but on the other hand, a well-informed colleague reading a conference paper should be able to reproduce your results.",
Only listing the homeostatic/appraisal variables and the emotions that were implemented does not provide deeper understand- ing about how the system works.,
This also makes it harder to compare between implementations.,
Diﬀerences in notational conventions and slight diﬀerences in deﬁnitions further complicate comparisons.,
"Paying attention to these aspects of reproducibility, for example sticking to conventional RL notation (Sutton and Barto, 1998), will facilitate broader uptake of the work in this ﬁeld.",
"A core challenge for the future will be to integrate all aspects into one larger system, potentially taking a fully learned approach.",
"Along the same line, it is a remaining challenge of this ﬁeld (and AI in general) to translate higher-level (psychological) concepts into implementable mathematical expressions.",
Examples Emotion in Reinforcement Learning Agents and Robots: A Survey.,
"29 of such translations can be found in Equations 9-13, and we expect comparing diﬀerent translations may help identify more consensus.",
At least the RL framework provides a common language to start comparing these translations.,
"With social robots increasingly positioned at our research horizon, we expect interest in emotion in functional agents to increase in the forthcoming years.",
"How- ever, the current implementations seldomly investigate the full social interaction.",
"Although this is a very high-level AI challenge, we believe research should fo- cus in this direction to show empirical success.",
"This involves all aspects of RL in a social context, i.e.",
"robots learning from human demonstration (LfD) (Argall et al, 2009), learning from human feedback (possibly emotional (Broekens, 2007)), human emotions inﬂuencing agent emotions, and agent emotions communicating internal processes back to humans.",
"From an aﬀective modelling perspective, it is promising to see how a cogni- tive theory like appraisal theory turns out to be well-applicable to MDP settings.",
"Apart from integrating important lines of emotion and learning research, this also illustrates how cognitive and learning theories are not mutually exclusive.",
"We hope the aﬀective modelling community will start to beneﬁt from the literature on in- trinsic motivation in RL as well (Bratman et al, 2012).",
A crucial requisite herein will be improving the types of problems that (model-based) RL can solve.,
"Many scenarios that are interesting from an aﬀective modelling viewpoint, for example high-dimensional social settings, are still challenging for RL.",
"Advances in deep reinforcement learning (Mnih et al, 2015) might make more complex scenarios available soon.",
"However, for aﬀective modelling we especially need the transition function and model-based RL (Deisenroth and Rasmussen, 2011).",
"Recent work has also shown the feasibility of high-dimensional transition function approximation (Oh et al, 2015) in stochastic domains (Moerland et al, 2017) under uncertainty (Houthooft et al, 2016).",
Further progress in this direction should make the ideas covered in this survey applicable to more complicated scenarios as well.,
9 Conclusion This article surveyed emotion modelling in reinforcement learning (RL) agents.,
The literature has been structured according to the intrinsically motivated RL framework.,
"We conclude by identifying the main beneﬁts encountered in this work for the machine learning (ML), human-robot interaction (HRI), and aﬀective mod- elling (AM) communities.",
"For machine learning, emotion may beneﬁt learning eﬃ- ciency by providing inspiration for intrinsic motivation, exploration and for meta- parameter tuning.",
"The current results should stimulate further cross-over between (intrinsic) motivation, model-based RL and emotion-RL research.",
"For HRI re- search, emotions obviously are important for social interaction.",
"More work should be done on implementing emotion models in interactive reinforcement learning algorithms, for which the survey presents a practical guideline on implementing emotions in RL agents.",
"For aﬀective modelling, we conclude that cognitive theories (like appraisal theory) can well be expressed in RL agents.",
"The general beneﬁts of RL agents (they require little assumptions, are easily applicable to all kinds of domains, and allow for learning) make them a promising test-bed for aﬀec- tive modelling research.",
This survey identiﬁes opportunities for future work with respect to implementation and evaluation of emotion models in RL agents.,
30 Moerland et al.,
"Table 9: Systematic overview of emotion elicitation, emotion type and emotion func- tion in the reinforcement learning loop (see Figure 1).",
Papers are ordered by their elicitation method (ﬁrst column).,
"Note that for homeostatic speciﬁcation, we try to use the terms mentioned in the original paper, which may sometimes refer to the drive (i.e.",
the deﬁcit in homeostatic variable) rather than the homeostatic dimen- sion itself.,
"Colour coding is based on the ﬁrst term mentioned in each cell, grouping the categories as encountered in Sections 4-6 and Table 1.",
"Paper Emotion Elicitation Emotion Type Emotion Function (Gadanho and Hal- lam, 1998, 2001) Homeostasis: hunger, pain, restlessness, tem- perature, eating, smell, warmth, proximity Categorical: happiness, sadness, fear, anger Reward modiﬁcation: positive emotion is reward (Gadanho, 2003) Homeostasis: energy, wel- fare, activity Dimensional: well-being Reward modiﬁcation: delta well-being is re- ward (Cos et al, 2013) Homeostasis: hunger, tiredness, restlessness Categorical: hedonic value Reward modiﬁcation: delta well-being is re- ward (Coutinho et al, 2005) Homeostasis: blood sugar, energy, pain, vas- cular volume, endorphine Dimensional: wellness, relaxation, fatigue Epiphenomenon (Von Haugwitz et al, 2012) Homeostasis: hunger, fa- tigue, interest Categorical: happiness, sadness, anger, surprise, fear, disgust.",
"Metalearning: reward = delta happiness, learning rate = (1-surprise), dis- count factor = (1-fear), Boltzmann temperature = anger (Tanaka et al, 2004) Homeostasis: hunger, fullness, pain, comfort, fatigue, sleepiness Categorical: happiness, sadness, anger, surprise, disgust, fear, neutral Epiphenomenon: gesture, voice, facial expression (Goerke, 2006) Homeostasis: fatigue, hunger, homesickness, curiosity Categorical: happiness, fear, anger, boredom Reward modiﬁcation: positive emotion is reward (Sequeira et al, 2011, 2014) Appraisal: valency, con- trol, novelty, motivation None Reward modiﬁcation: summed appraisals added to reward function (Marinier and Laird, 2008) Appraisal: suddenness, intrinsic pleasantness, relevance, conducive- ness, discrepancy from expectation, control, power.",
"None Reward modiﬁcation: summed appraisals is reward (Yu et al, 2015, 2013) Appraisal: social fairness Value: average reward Categorical: happiness, sadness, fear, anger Reward modiﬁcation: positive/negative emo- tion is positive/negative reward (Lee- Johnson et al, 2010, 2007) Appraisal: model mis- match Value: average achieved reward, global planned reward Homeostatic: collision Categorical: Happiness, sadness, fear, anger, sur- prise Reward modiﬁcation: change local reward (happy & suprise higher, fear & anger lower) (Williams et al, 2015) Appraisal: novelty, progress, control, uncer- tainty Homeostatic: pain.",
"Categorical: happiness, fear (post-characterized) Metalearning: happy gives positive reward bias and higher travel speed, fear giver negative reward bias and lower travel speed (Si et al, 2010) Appraisal: motivational relevance & congruence, accountability, control, novelty.",
None Epiphenomenon Emotion in Reinforcement Learning Agents and Robots: A Survey.,
"31 (Kim and Kwon, 2010) Appraisal: unexpected- ness, motive consistency, control, uncertainty, agency/accountability.. Dimensional: valence, arousal (not fully ex- plicit) Epiphenomenon: facial avatar, voice, movement of ears, music (Hasson et al, 2011) Appraisal: non-progress Human aﬀective state Categorical: frustration, anger, happiness Action selection: switch between targets (Moussa and Magnenat- Thalmann, 2013) Appraisal: desirability, attachment (OCC model) Categorical: joy, dis- tress, happy for, resent- ment, sorry for, gloating, gratitude, admiration, anger, reproach.",
"Reward modiﬁcation: reward is diﬀerence of largest positive and negative current emotion (Huang et al, 2012) Appraisal: motivational relevance + goal reach- able Categorical: Happy, sad, anger, surprise, fear, frustration Epiphenomenon (Kuremoto et al, 2013) Appraisal: distance to goal, continuation of eliciting event Dimensional: valence, arousal Action selection: sepa- rate emotional Q-value as part of total summed Q- value (Castro- Gonz´alez et al, 2013; Salichs and Malfaz, 2012) Value: worst historical Q-value + Homeostasis: energy, boredom, calm, loneli- ness Categorical: happiness, sadness, fear Reward modiﬁcation: delta well-being State modiﬁcation: fear replaces dominant moti- vation (when threshold is exceeded) (Ahn and Picard, 2006) Reward: diﬀerence be- tween experienced reward and expected immediate reward of best two avail- able actions Dimensional: feeling good, bad Action selection: emo- tional Q-value is part of total Q-value (Zhang and Liu, 2009) Reward: diﬀerence be- tween experienced reward and expected immediate reward of best action Dimensional: feeling good/bad Action selection: emo- tional Q-value is part of total Q-value (Broekens et al, 2007a,b) Reward: short versus long term average reward Dimensional: valence Action selection: emotion tunes exploration param- eter and simulation depth (Moerland et al, 2016) Value: Anticipated tem- poral diﬀerence Categorical: hope, fear Epiphenomenon (Jacobs et al, 2014) Value: temporal dif- ference and posi- tive/negative part of value Categorical: joy, dis- tress, hope, fear Epiphenomenon (Bozinovski, 1982) Value None Epiphenomenon (Moren and Balke- nius, 2000) Value None Epiphenomenon (Lahnstein, 2005) Value: temporal diﬀer- ence Categorical: happiness, sadness, hope.",
"Epiphenomenon (Obayashi et al, 2012) Reward: not explicit Hard-wired: not-explicit Dimensional: valence, arousal (with unlabelled categories) State modiﬁcation: emo- tion speciﬁc Q-value (Matsuda et al, 2011) Reward: only negative re- ward Categorical: fear Action selection: separate emotional value function is part of action selection (Schweighofer and Doya, 2003; Doya, 2002) Reward: mid versus long- term average reward None Metalearning: perturba- tion of discount, learning and temperature parame- ter based on emotion.",
32 Moerland et al.,
"(Hogewoning et al, 2007) Reward: short/mid ver- sus long-term average re- ward Dimensional: valence Action selection: emotion tunes exploration (com- bines (Broekens et al, 2007b) and (Schweighofer and Doya, 2003) with chi- square test) (Shi et al, 2012) Reward: change in re- ward signal Categorical: joy, fear, anger, relief Metalearning: joy = TD, anger = temperature, fear = learning rate, re- lief = discount parameter (connection not explicit) (Blanchard and Canamero, 2005) Reward: average Categorical: comfort Metalearning: emotion modulates the learning rate (El-Nasr et al, 2000) Value: combined with fuzzy logic Categorical: joy, sad- ness, disappointment, relief, hope, fear, pride, shame, reproach, anger, gratitude, gratiﬁcation, remorse Action selection: emo- tions are input to a fuzzy logic action selection sys- tem (Kubota and Wak- isaka, 2010) Hard-wired: from objects (users, balls, chargers, obstacles), speech and previous emotional state Categorical: happiness, sadness, fear, anger Action selection: switch between value functions (Ayesh, 2004) Hard-wired: from state through fuzzy cognitive maps Dimensional: restless, neutral, stable State modiﬁcation: emo- tion speciﬁc Q-values (Ficocelli et al, 2015) Human aﬀective state Hard-wired Categorical: happiness, neutral, sadness, angry.",
"State modiﬁcation Action selection: modify intonation of speech (Hoey and Schr¨oder, 2015) Hard-wired from object observations (social inter- action) Dimensional: valence, control, arousal State modiﬁcation: ex- tended POMDP deriva- tion with 3D emotional state (Tsankova, 2002) Hardwired: from obstacle detectors Categorical: frustration Action selection: emotion controls the balancing be- tween value functions (Zhou and Cog- gins, 2002) Hardwired: from sight of resources Homeostasis: hunger, thirst (not connected to emotion but to reward) None Reward modiﬁcation: reward calculated from maximum emotion or motivation.",
"(Doshi and Gmy- trasiewicz, 2004) Hard-wired: from sight of enemy or resource.",
"Categorical: Contented, elation, fear, panic.",
"Action selection: emotion adjust planning depth and biases considered ac- tions (Gmytrasiewicz and Lisetti, 2002) Hard-wired: Markovian transition from previous emotions and state Categorical: Coopera- tive, slightly annoyed, angry Meta-learning: emotion biases transition function Action selection: emotion biases available action subset, biases value function (Guojiang et al, 2010) Hard-wired: from exte- rior incentive like safety, threat, fancy, surprise (assumed pre-given) Dimensional: valence, arousal State modiﬁcation: 2D emotional state space Reward modiﬁcation: agent should move to de- sirable area in emotional space (implementation not speciﬁed) (Shibata et al, 1997) Not explicit Not explicit Not explicit Emotion in Reinforcement Learning Agents and Robots: A Survey.",
"33 Table 10: Systematic overview of test embodiment, scenario, evaluation criterion and main results.",
Papers are ordered according to Table 9.,
Colour coding presented for the evalution criterion column.,
"Paper Embodi- ment Scenario Criterion Main result (Gadanho and Hal- lam, 1998, 2001; Gadanho, 2003) Simulated robot Multiple resource task Learning Less collisions and higher average reward with emotional agent.",
"(Cos et al, 2013) Grid- world agent Multiple resource task Learning Emergent behavioural cycles fulﬁlling diﬀerent drives (Coutinho et al, 2005) Grid- world agent Multiple resource task - No emotion results (Von Haugwitz et al, 2012) Multiple agents Game / competi- tion Learning Increased average reward compared to non-emotional agents (Tanaka et al, 2004) Real robot Human interacting ( hitting/ padding robot) Dynamics Appropriate emotion response (fear and joy) to bad and good acting person.",
"(Goerke, 2006) Simulated robot + real robot Multiple resource task Learning Diﬀerent behaviour types with emotion functionality (Sequeira et al, 2011) Grid- world agent Resource- predator task Learning Improved average ﬁtness compared to non-appraisal agent (Marinier and Laird, 2008) Grid- world agent Maze Learning Emotional agent needs less learning episodes (Yu et al, 2015, 2013) Multiple agents Game / ne- gotiation Learning Emotional/social agents have higher av- erage reward and show co-operation (Lee- Johnson et al, 2010, 2007) Simulated robot Navigation task Learning Emotional agent has less collisions and more exploration, against a higher aver- age travel time (Williams et al, 2015) Real robot Navigation task Learning Less collisions with fear enabled, more exploration with surprise, quicker routes with happiness enabled.",
"(Si et al, 2010) Multiple agents Social interaction Dynamics Diﬀerent appraisal with deeper planning + Social accountability realistically de- rived (compared to other computational model).",
"(Kim and Kwon, 2010) Real robot Social in- teraction (question game) with human HRI Users report higher subjective feeling of interaction and higher pleasantness for emotional robot + humans correctly identify part of the underlying robot ap- praisals based on a questionnaire (Hasson et al, 2011) Real robot Multiple resource navigation task Learning Robot with emotion can switch between drives (in case of obstacles) and escape deadlocks.",
"(Moussa and Magnenat- Thalmann, 2013) Real robot Human dialogue task (while playing game) Dynamics + Learn- ing Appropriate emotion responses to friendly and unfriendly users + learn diﬀerent attitudes towards them.",
"(Huang et al, 2012) Grid- world agent Navigation task Dynamics Dynamics show how emotion elicitation varies with planning depth and goal achievement probability.",
34 Moerland et al.,
"(Kuremoto et al, 2013) Grid- world agent Predator task Learning Quicker goal achievement for emotional agent compared to non-emotional agent.",
"(Castro- Gonz´alez et al, 2013; Salichs and Mal- faz, 2012) Real robot Multiple resources task in- cluding human objects Learning + Dy- namics Less harmful interactions compared to non-fear robot + realistic fear dynamics (compared to animal) (Ahn and Picard, 2006) Agent Conditioning experiment Learning Aﬀective agent learns optimal policy faster.",
"(Zhang and Liu, 2009) Simulated robot Navigation task Learning Emotional robot needs less trials to learn the task.",
"(Broekens et al, 2007a,b) Grid- world agent Maze Learning Emotional control of simulation depth improves average return.",
Emotional con- trol of exploration improves time to goal and time to ﬁnd the global optimum.,
"(Moerland et al, 2016) Grid- world agent + Pacman Resource- predator task Dynamics Appropriate hope and fear anticipation in speciﬁc Pacman scenarios.",
"(Jacobs et al, 2014) Grid- world agent Maze Dynamics Emotion dynamics (habituation, extinc- tion) simulated realistically compared to psychological theory.",
"(Bozinovski, 1982; Bozi- novski et al, 1996) Grid- world agent Maze Learning First investigation of emotion as primary reward, shows agent is able to solve maze task.",
"(Moren and Balke- nius, 2000; Balke- nius and Mor´en, 1998) Agent Conditioning experiment Dynamics Agent shows habituation, extinction, blocking (i.e.",
"of learning signal, not emo- tion) (Lahnstein, 2005) Real- robot Multiple objects grasping task Dynamics Models dynamics within single decision cycle, shows plausible anticipation, hedo- nic experience and subsequent decay.",
"(Obayashi et al, 2012) Grid- world agent Maze Learning Emotional agent needs less steps to goal (ordinary agent does not converge).",
"(Matsuda et al, 2011) Multiple agent grid- world Co- operation task Learning Emotional agents show more co- operation and adapt better to envi- ronmental change compared to non- emotional agents.",
"(Schweighofer and Doya, 2003; Doya, 2002) Agent Conditioning experiment + Sim- ulated pendulum Learning Dynamic adaptation of meta-parameters in both static and dynamic environ- ment.",
Task not achieved for ﬁxed meta- parameters.,
"(Hogewoning et al, 2007) Grid- world agent Maze Learning Emotional agent cannot improve results of (Broekens et al, 2007b; Schweighofer and Doya, 2003) (Shi et al, 2012) Grid- world agent Obstacle and re- source task Learning + Dy- namics Emotional agent avoids obstacle better.",
Diﬀerent emotion lead to diﬀerent paths.,
"(Blanchard and Canamero, 2005) Real robot Conditioning task Dynamics Robot can imprint desirable stimuli based on comfort (reward) signal, and subsequently show approach or avoidance behaviour.",
Emotion in Reinforcement Learning Agents and Robots: A Survey.,
"35 (El-Nasr et al, 2000) Screen agent Human interaction task HRI Users perceive agent with emotional ac- tion selection as more convincing.",
"(Kubota and Wak- isaka, 2010) Simulated robot Multiple objects and human Dynamics Emotional robot avoids dangerous areas due to fear, and starts exploring when happy.",
"(Ayesh, 2004) Real robot None None None (Ficocelli et al, 2015) Real robot Human di- alogue task HRI + Dynam- ics + Learning Eﬀective emotion expression (user ques- tionnaire) + Robot changing emotions to satisfy diﬀerent drives (Hoey and Schr¨oder, 2015) Agent Social agent interaction Dynamics Model can accurately modify own dimen- sional emotion with respect to the client it is interacting with.",
"(Tsankova, 2002) Simulated robot Navigation task.",
"Learning Emotional robot reaches goal more often, but need more timesteps.",
"(Zhou and Coggins, 2002) Real robot Multiple resources Learning Emotional robot has higher average re- ward and less intermediate behaviour switching compared to non-emotional robot.",
"(Doshi and Gmy- trasiewicz, 2004) Grid- world Multiple resource, predator task Learning Emotional agent (with meta-learning) has higher average return compared to non-emotional agent.",
"(Gmytrasiewicz and Lisetti, 2002) None None (the- oretical model) None None (Guojiang et al, 2010) Agent Conditioning task Dynamics Agent moves towards beneﬁcial emo- tional state-space and stays there.",
"(Shibata et al, 1997) Real robot Human stroke/pad robot HRI Humans reported a coupling with the robot, some reported it as intelligent.",
Subjects report positive emotions them- selves.,
"References Ahn H, Picard RW (2006) Aﬀective cognitive learning and decision making: The role of emotions.",
In: EMCSR 2006: The 18th Europ.,
"Antos D, Pfeﬀer A (2011) Using emotions to enhance decision-making.",
"In: Proceedings of the Inter- national Joint Conference on Artiﬁcial Intelligence (IJCAI), vol 22, p 24 Argall BD, Chernova S, Veloso M, Browning B (2009) A survey of robot learning from demonstration.",
Robotics and autonomous systems 57(5):469–483 Ayesh A (2004) Emotionally motivated reinforcement learning based controller.,
"In: Systems, Man and Cybernetics, 2004 IEEE International Conference on, IEEE, vol 1, pp 874–878 Balkenius C, Mor´en J (1998) A computational model of emotional conditioning in the brain.",
"In: Proceedings of Workshop on Grounding Emotions in Adaptive Systems, Zurich Baumeister RF, Vohs KD, DeWall CN, Zhang L (2007) How emotion shapes behavior: Feedback, an- ticipation, and reﬂection, rather than direct causation.",
Personality and Social Psychology Review 11(2):167–203 Belavkin RV (2004) On relation between emotion and entropy.,
"In: Proceedings of the AISB’04 Symposium on Emotion, Cognition and Aﬀective Computing., AISB Press, pp 1–8 Blanchard AJ, Canamero L (2005) From imprinting to adaptation: Building a history of aﬀective interaction.",
"In: Proceedings of the 5th International Workshop on Epigenetic Robotics, Lund University Cognitive Studies, pp 23–30 Bozinovski S (1982) A self-learning system using secondary reinforcement.",
"Cybernetics and Systems Research pp 397–402 Bozinovski S, Stojanov G, Bozinovska L (1996) Emotion, embodiment, and consequence driven systems.",
"In: Proc AAAI Fall Symposium on Embodied Cognition and Action, pp 12–17 Bratman J, Singh S, Sorg J, Lewis R (2012) Strong mitigation: Nesting search for good policies within search for good reward.",
"In: Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1, International Foundation for Autonomous Agents and Multiagent Systems, pp 407–414 36 Moerland et al.",
Breazeal C (2003) Emotion and sociable humanoid robots.,
International Journal of Human- Computer Studies 59(1):119–155 Broekens J (2007) Emotion and reinforcement: aﬀective facial expressions facilitate robot learning.,
"In: Artiﬁcal Intelligence for Human Computing, Springer, pp 113–132 Broekens J, Kosters WA, Verbeek FJ (2007a) Aﬀect, anticipation, and adaptation: Aﬀect-controlled selection of anticipatory simulation in artiﬁcial adaptive agents.",
"Adaptive Behavior 15(4):397–422 Broekens J, Kosters WA, Verbeek FJ (2007b) On aﬀect and self-adaptation: Potential beneﬁts of valence-controlled action-selection.",
"In: Bio-inspired modeling of cognitive tasks, Springer, pp 357– 366 Broekens J, Bosse T, Marsella SC (2013) Challenges in Computational Modeling of Aﬀective Pro- cesses.",
"Aﬀective Computing, IEEE Transactions on 4(3):242–245 Calvo R, D’Mello S, Gratch J, Kappas A (2014) Handbook of aﬀective computing Calvo RA, D’Mello S (2010) Aﬀect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications.",
"Aﬀective Computing, IEEE Transactions on 1(1):18–37, DOI 10.1109/t-aﬀc.",
2010.1 Ca˜namero D (1997a) A hormonal model of emotions for behavior control.,
VUB AI-Lab Memo 2006 Ca˜namero D (1997b) Modeling motivations and emotions as a basis for intelligent behavior.,
"In: Proceedings of the ﬁrst international conference on Autonomous agents, ACM, pp 148–155 Ca˜namero D (2003) Designing emotions for activity selection in autonomous agents.",
"Emotions in humans and artifacts 115:148 Castro-Gonz´alez ´A, Malfaz M, Salichs MA (2013) An autonomous social robot in fear.",
"Autonomous Mental Development, IEEE Transactions on 5(2):135–151 Chentanez N, Barto AG, Singh SP (2004) Intrinsically motivated reinforcement learning.",
"In: Ad- vances in neural information processing systems, pp 1281–1288 Cos I, Ca˜namero L, Hayes GM, Gillies A (2013) Hedonic value: enhancing adaptation for motivated agents.",
"Adaptive Behavior p 1059712313486817 Coutinho E, Miranda ER, Cangelosi A (2005) Towards a model for embodied emotions.",
"In: Artiﬁcial intelligence, 2005. epia 2005. portuguese conference on, IEEE, pp 54–63 Damasio AR (1994) Descartes’ Error: Emotion, Reason and the Human Brain.",
"Grosset/Putnam Deisenroth M, Rasmussen CE (2011) PILCO: A model-based and data-eﬃcient approach to policy search.",
"In: Proceedings of the 28th International Conference on machine learning (ICML-11), pp 465–472 Doshi P, Gmytrasiewicz P (2004) Towards aﬀect-based approximations to rational planning: a decision-theoretic perspective to emotions.",
"In: Working Notes of the Spring Symposium on Ar- chitectures for Modeling Emotion: Cross-Disciplinary Foundations Doya K (2000) Metalearning, neuromodulation, and emotion.",
Aﬀective minds 101 Doya K (2002) Metalearning and neuromodulation.,
"Neural Networks 15(4):495–506 Dunn BD, Dalgleish T, Lawrence AD (2006) The somatic marker hypothesis: A critical evaluation.",
"Neuroscience & Biobehavioral Reviews 30(2):239–271, DOI 10.1016/j.neubiorev.2005.07.001, URL http://www.sciencedirect.com/science/article/pii/S0149763405001053 Ekman P, Friesen WV, O’Sullivan M, Chan A, Diacoyanni-Tarlatzis I, Heider K, Krause R, LeCompte WA, Pitcairn T, Ricci-Bitti PE, et al (1987) Universals and cultural diﬀerences in the judgments of facial expressions of emotion.",
"Journal of personality and social psychology 53(4):712 El-Nasr MS, Yen J, Ioerger TR (2000) Flame—fuzzy logic adaptive model of emotions.",
"Autonomous Agents and Multi-agent systems 3(3):219–257 Ficocelli M, Terao J, Nejat G (2015) Promoting Interactions Between Humans and Robots Using Robotic Emotional Behavior.",
"IEEE Transactions on Cybernetics pp 1–13 Fong T, Nourbakhsh I, Dautenhahn K (2003) A survey of socially interactive robots.",
"Robotics and autonomous systems 42(3):143–166 Franklin S, Madl T, D’mello S, Snaider J (2014) LIDA: A systems-level architecture for cognition, emotion, and learning.",
"Autonomous Mental Development, IEEE Transactions on 6(1):19–41 Frijda NH, Kuipers P, Ter Schure E (1989) Relations among emotion, appraisal, and emotional action readiness.",
"Journal of personality and social psychology 57(2):212 Gadanho S, Hallam J (1998) Emotion triggered learning for autonomous robots.",
DAI Research Paper Gadanho SC (2003) Learning behavior-selection by emotions and cognition in a multi-goal robot task.,
"The journal of machine learning research 4:385–412 Gadanho SC, Hallam J (2001) Robot learning driven by emotions.",
"Adaptive Behavior 9(1):42–64 Gmytrasiewicz PJ, Lisetti CL (2002) Emotions and personality in agent design and modeling.",
"In: Game theory and decision theory in agent-based systems, Springer, pp 81–95 Goerke N (2006) EMOBOT: a robot control architecture based on emotion-like internal values.",
"INTECH Open Access Publisher Goodfellow I, Bengio Y, Courville A (2016) Deep learning.",
"MIT Press Gratch J, Marsella S (2014) Appraisal models.",
"Oxford University Press Guojiang W, Xiaoxiao W, Kechang F (2010) Behavior decision model of intelligent agent based on artiﬁcial emotion.",
"In: Advanced Computer Control (ICACC), 2010 2nd International Conference on, IEEE, vol 4, pp 185–189 Hasson C, Gaussier P, Boucenna S (2011) Emotions as a dynamical system: the interplay between the meta-control and communication function of ´emotions.",
"Paladyn 2(3):111–125 Hester T, Stone P (2012a) Intrinsically motivated model learning for a developing curious agent.",
In: 2012 IEEE International Conference on Development and Learning and Epigenetic Robotics Emotion in Reinforcement Learning Agents and Robots: A Survey.,
"37 (ICDL), IEEE, pp 1–6 Hester T, Stone P (2012b) Learning and using models.",
"In: Reinforcement Learning, Springer, pp 111–141 Hoey J, Schr¨oder T (2015) Bayesian Aﬀect Control Theory of Self.",
"In: AAAI, Citeseer, pp 529–536 Hoey J, Schroder T, Alhothali A (2013) Bayesian aﬀect control theory.",
"In: Aﬀective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on, IEEE, pp 166–172 Hogewoning E, Broekens J, Eggermont J, Bovenkamp EG (2007) Strategies for aﬀect-controlled action-selection in Soar-RL.",
"In: Nature Inspired Problem-Solving Methods in Knowledge Engi- neering, Springer, pp 501–510 Houthooft R, Chen X, Duan Y, Schulman J, De Turck F, Abbeel P (2016) Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks.",
"arXiv preprint arXiv:160509674 Huang X, Du C, Peng Y, Wang X, Liu J (2012) Goal-oriented action planning in partially observ- able stochastic domains.",
"In: Cloud Computing and Intelligent Systems (CCIS), 2012 IEEE 2nd International Conference on, IEEE, vol 3, pp 1381–1385 Hull CL (1943) Principles of behavior: an introduction to behavior theory.",
"Appleton-Century Jacobs E, Broekens J, Jonker CM (2014) Emergent dynamics of joy, distress, hope and fear in reinforcement learning agents.",
"In: Adaptive Learning Agents workshop at AAMAS2014 Joﬃly M, Coricelli G (2013) Emotional valence and the free-energy principle.",
"PLOS Computational Biology 9(6) Johnson-Laird PN, Oatley K (1992) Basic emotions, rationality, and folk theory.",
"Cognition and Emotion 6(3-4):201–223 Kahneman D, Tversky A (1979) Prospect theory: An analysis of decision under risk.",
"Econometrica: Journal of the Econometric Society pp 263–291 Keramati M, Gutkin BS (2011) A reinforcement learning theory for homeostatic regulation.",
"In: Advances in neural information processing systems, pp 82–90 Kim HR, Kwon DS (2010) Computational model of emotion generation for human–robot interaction based on the cognitive appraisal theory.",
"Journal of Intelligent & Robotic Systems 60(2):263–283 Kim KH, Cho SB (2015) A Group Emotion Control System based on Reinforcement Learning.",
"SoCPaR 2015 Knox WB, Glass B, Love B, Maddox WT, Stone P (2012) How Humans Teach Agents.",
"International Journal of Social Robotics 4(4):409–421, DOI 10.1007/s12369-012-0163-x, URL http://dx.doi.",
"org/10.1007/s12369-012-0163-x Knox WB, Stone P, Breazeal C (2013) Training a Robot via Human Feedback: A Case Study, Lecture Notes in Computer Science, vol 8239, Springer International Publishing, book section 46, pp 460–470 Kober J, Peters J (2012) Reinforcement learning in robotics: A survey.",
"In: Reinforcement Learning, Springer, pp 579–610 Konidaris G, Barto A (2006) An adaptive robot motivational system.",
"In: From Animals to Animats 9, Springer, pp 346–356 Kubota N, Wakisaka S (2010) Emotional model based on computational intelligence for partner robots.",
"In: Modeling machine emotions for realizing intelligence, Springer, pp 89–108 Kuremoto T, Tsurusaki T, Kobayashi K, Mabu S, Obayashi M (2013) An improved reinforcement learning system using aﬀective factors.",
Robotics 2(3):149–164 Lahnstein M (2005) The emotive episode is a composition of anticipatory and reactive evaluations.,
"of the AISB’05 Symposium on Agents that Want and Like, pp 62–69 Laird JE (2008) Extending the Soar cognitive architecture.",
Frontiers in Artiﬁcial Intelligence and Applications 171:224 Lazarus RS (1991) Cognition and motivation in emotion.,
"American psychologist 46(4):352 LeDoux J (2003) The emotional brain, fear, and the amygdala.",
"Cellular and molecular neurobiology 23(4-5):727–738 Lee-Johnson CP, Carnegie D, et al (2007) Emotion-based parameter modulation for a hierarchical mobile robot planning and control architecture.",
"In: Intelligent Robots and Systems, 2007.",
"IEEE/RSJ International Conference on, IEEE, pp 2839–2844 Lee-Johnson CP, Carnegie D, et al (2010) Mobile robot navigation modulated by artiﬁcial emotions.",
"Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 40(2):469–480 Lhommet M, Marsella SC (2015) Expressing Emotion Through Posture and Gesture, Oxford Uni- versity Press, pp 273–285 Lisetti C, Hudlicka E (2014) Why and How to Build Emotion-based Agent Architectures Marinier R, Laird JE (2008) Emotion-driven reinforcement learning.",
"Cognitive science pp 115–120 Marsella S, Gratch J, Petta P (2010) Computational models of emotion.",
"K r Scherer, t B¨anziger & e roesch (eds), A blueprint for aﬀective computing pp 21–45 Marsella SC, Gratch J (2009) EMA: A process model of appraisal dynamics.",
"Cognitive Systems Research 10(1):70–90, URL http://www.sciencedirect.com/science/article Matsuda A, Misawa H, Horio K (2011) Decision making based on reinforcement learning and emotion learning for social behavior.",
"In: Fuzzy Systems (FUZZ), 2011 IEEE International Conference on, IEEE, pp 2714–2719 Michaud F (2002) EMIB—Computational Architecture Based on Emotion and Motivation for In- tentional Selection and Conﬁguration of Behaviour-Producing Modules.",
Cognitive Science Quar- terly 3-4:340–361 38 Moerland et al.,
"Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al (2015) Human-level control through deep reinforcement learning.",
"Nature 518(7540):529–533 Moerland TM, Broekens J, Jonker CM (2016) Fear and Hope Emerge from Anticipation in Model- based Reinforcement Learning.",
"In: Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), pp 848–854 Moerland TM, Broekens J, Jonker CM (2017) Learning Multimodal Transition Dynamics for Model- Based Reinforcement Learning.",
"arXiv preprint arXiv:170500470 Moren J, Balkenius C (2000) A computational model of emotional learning in the amygdala.",
"From animals to animats 6:115–124 Moussa MB, Magnenat-Thalmann N (2013) Toward socially responsible agents: integrating attach- ment and learning in emotional decision-making.",
"Computer Animation and Virtual Worlds 24(3- 4):327–334 Murphy RR, Lisetti CL, Tardif R, Irish L, Gage A (2002) Emotion-based control of cooperating heterogeneous mobile robots.",
"Robotics and Automation, IEEE Transactions on 18(5):744–757 Ng AY, Harada D, Russell S (1999) Policy invariance under reward transformations: Theory and application to reward shaping.",
"In: ICML, vol 99, pp 278–287 Obayashi M, Takuno T, Kuremoto T, Kobayashi K (2012) An emotional model embedded rein- forcement learning system.",
"In: Systems, Man, and Cybernetics (SMC), 2012 IEEE International Conference on, IEEE, pp 1058–1063 Ochs M, Niewiadomski R, Pelachaud C (2015) Facial Expressions of Emotions for Virtual Characters, Oxford University Press, pp 261–272 Oh J, Guo X, Lee H, Lewis RL, Singh S (2015) Action-conditional video prediction using deep networks in atari games.",
"In: Advances in Neural Information Processing Systems, pp 2863–2871 Ortony A, Clore GL, Collins A (1990) The cognitive structure of emotions.",
"Cambridge university press Osgood CE, Suci GJ, Tannenbaum PH (1964) The measurement of meaning.",
"University of Illinois Press Oudeyer PY, Kaplan F (2007) What is intrinsic motivation?",
a typology of computational approaches.,
"Frontiers in neurorobotics 1 Oudeyer PY, Kaplan F, Hafner VV (2007) Intrinsic motivation systems for autonomous mental development.",
"Evolutionary Computation, IEEE Transactions on 11(2):265–286 Paiva A, Leite I, Ribeiro T (2015) Emotion modeling for social robots, Oxford University Press, pp 296–308 Parisi D, Petrosino G (2010) Robots that have emotions.",
Adaptive Behavior 18(6):453–469 Reisenzein R (2009) Emotional experience in the computational belief–desire theory of emotion.,
"Emotion Review 1(3):214–222 Rolls ET, Baylis LL (1994) Gustatory, olfactory, and visual convergence within the primate or- bitofrontal cortex.",
"The Journal of neuroscience 14(9):5437–5452 Rolls ET, Grabenhorst F (2008) The orbitofrontal cortex and beyond: from aﬀect to decision-making.",
"Progress in neurobiology 86(3):216–244 Rumbell T, Barnden J, Denham S, Wennekers T (2012) Emotions in autonomous agents: comparative analysis of mechanisms and functions.",
"Autonomous Agents and Multi-Agent Systems 25(1):1–45 Rummery GA, Niranjan M (1994) On-line Q-learning using connectionist systems.",
"rep., Uni- versity of Cambridge, Department of Engineering Russell JA (1978) Evidence of convergent validity on the dimensions of aﬀect.",
"Journal of personality and social psychology 36(10):1152 Russell JA, Barrett LF (1999) Core aﬀect, prototypical emotional episodes, and other things called emotion: dissecting the elephant.",
"Journal of personality and social psychology 76(5):805 Russell S, Norvig P, Intelligence A (1995) A modern approach.",
"Artiﬁcial Intelligence Prentice-Hall, Egnlewood Cliﬀs 25:27 Salichs MA, Malfaz M (2012) A new approach to modeling emotions and their use on a decision- making system for artiﬁcial agents.",
"Aﬀective Computing, IEEE Transactions on 3(1):56–68 Scherer KR (1999) Appraisal theory.",
"Handbook of cognition and emotion pp 637–663 Scherer KR, Schorr A, Johnstone T (2001) Appraisal processes in emotion: Theory, methods, re- search.",
"Oxford University Press Schweighofer N, Doya K (2003) Meta-learning in reinforcement learning.",
"Neural Networks 16(1):5–9 Sequeira P, Melo FS, Paiva A (2011) Emotion-based intrinsic motivation for reinforcement learning agents.",
"In: Aﬀective computing and intelligent interaction, Springer, pp 326–336 Sequeira P, Melo FS, Paiva A (2014) Learning by appraising: an emotion-based approach to intrinsic reward design.",
"Adaptive Behavior p 1059712314543837 Shi X, Wang Z, Zhang Q (2012) Artiﬁcial Emotion Model Based on Neuromodulators and Q-learning.",
"In: Future Control and Automation, Springer, pp 293–299 Shibata T, Yoshida M, Yamato J (1997) Artiﬁcial emotional creature for human-machine interaction.",
"In: Systems, Man, and Cybernetics, 1997.",
"Computational Cybernetics and Simulation., 1997 IEEE International Conference on, IEEE, vol 3, pp 2269–2274 Si M, Marsella SC, Pynadath DV (2010) Modeling appraisal in theory of mind reasoning.",
"Au- tonomous Agents and Multi-Agent Systems 20(1):14–31 Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al (2016) Mastering the game of Go with deep neural networks Emotion in Reinforcement Learning Agents and Robots: A Survey.",
39 and tree search.,
"Nature 529(7587):484–489 Singh S, Lewis RL, Barto AG, Sorg J (2010) Intrinsically motivated reinforcement learning: An evolutionary perspective.",
"Autonomous Mental Development, IEEE Transactions on 2(2):70–82 Sutton RS (1988) Learning to predict by the methods of temporal diﬀerences.",
"Machine learning 3(1):9–44 Sutton RS, Barto AG (1998) Reinforcement learning: An introduction.",
"MIT press Cambridge Tanaka F, Noda K, Sawada T, Fujita M (2004) Associated emotion and its expression in an enter- tainment robot QRIO.",
"In: Entertainment Computing–ICEC 2004, Springer, pp 499–504 Thomaz AL, Breazeal C (2006) Teachable characters: User studies, design principles, and learning performance.",
"In: Intelligent Virtual Agents, Springer, pp 395–406 Thomaz AL, Breazeal C (2008) Teachable robots: Understanding human teaching behavior to build more eﬀective robot learners.",
"Artiﬁcial Intelligence 172(6):716–737 Thorpe S, Rolls E, Maddison S (1983) The orbitofrontal cortex: neuronal activity in the behaving monkey.",
Experimental Brain Research 49(1):93–115 Tsankova DD (2002) Emotionally inﬂuenced coordination of behaviors for autonomous mobile robots.,
"In: Intelligent Systems, 2002.",
Proceedings.,
"2002 First International IEEE Symposium, IEEE, vol 1, pp 92–97 Velasquez J (1998) Modeling emotion-based decision-making.",
"Emotional and intelligent: The tangled knot of cognition pp 164–169 Vinciarelli A, Pantic M, Heylen D, Pelachaud C, Poggi I, D’Errico F, Schr¨oder M (2012) Bridging the gap between social animal and unsocial machine: A survey of social signal processing.",
"Aﬀective Computing, IEEE Transactions on 3(1):69–87 Von Haugwitz R, Kitamura Y, Takashima K (2012) Modulating reinforcement-learning parameters using agent emotions.",
"In: Soft Computing and Intelligent Systems (SCIS) and 13th International Symposium on Advanced Intelligent Systems (ISIS), 2012 Joint 6th International Conference on, IEEE, pp 1281–1285 Watkins CJCH (1989) Learning from delayed rewards.",
"PhD thesis, University of Cambridge England Wiering M, Van Otterlo M (2012) Reinforcement learning.",
"Adaptation, Learning, and Optimization 12 Williams H, Lee-Johnson C, Browne WN, Carnegie DA (2015) Emotion inspired adaptive robotic path planning.",
"In: Evolutionary Computation (CEC), 2015 IEEE Congress on, IEEE, pp 3004– 3011 Yu C, Zhang M, Ren F (2013) Emotional multiagent reinforcement learning in social dilemmas.",
"In: PRIMA 2013: Principles and Practice of Multi-Agent Systems, Springer, pp 372–387 Yu C, Zhang M, Ren F, Tan G (2015) Emotional Multiagent Reinforcement Learning in Spatial Social Dilemmas.",
"IEEE Transactions on Neural Networks and Learning Systems 26(12):3083–3096 Zhang H, Liu S (2009) Design of autonomous navigation system based on aﬀective cognitive learning and decision-making.",
"In: Robotics and Biomimetics (ROBIO), 2009 IEEE International Confer- ence on, IEEE, pp 2491–2496 Zhou W, Coggins R (2002) Computational models of the amygdala and the orbitofrontal cortex: A hierarchical reinforcement learning system for robotic control.",
"In: AI 2002: Advances in Artiﬁcial Intelligence, Springer, pp 419–430",
"entropy Article MEMe: An Accurate Maximum Entropy Method for Efﬁcient Approximations in Large-Scale Machine Learning Diego Granziol 1,2,∗,‡, Binxin Ru 1,2,∗,‡, Stefan Zohren 1,2, Xiaowen Doing 1,2, Michael Osborne 1,2 and Stephen Roberts 1,2 1 Machine Learning Research Group, University of Oxford, Walton Well Rd, Oxford OX2 6ED, UK; diego@robots.ox.ac.uk (D.G.",
); robin@robots.ox.ac.uk (B.R.,
); zohren@robots.ox.ac.uk (S.Z.,
); xdong@robots.ox.ac.uk (X.D.,
); mosb@robots.ox.ac.uk (M.O.,
); sjrob@robots.ox.ac.uk (S.R.),
"2 Oxford-Man Institute of Quantitative Finance, Walton Well Rd, Oxford OX2 6ED, UK * Correspondence: diego@robots.ox.ac.uk (D.G.",
); robin@robots.ox.ac.uk (B.R.),
‡ These authors contributed equally to this work.,
Received: 30 April 2019; Accepted: 29 May 2019; Published: 31 May 2019 Abstract: Efﬁcient approximation lies at the heart of large-scale machine learning problems.,
"In this paper, we propose a novel, robust maximum entropy algorithm, which is capable of dealing with hundreds of moments and allows for computationally efﬁcient approximations.",
"We showcase the usefulness of the proposed method, its equivalence to constrained Bayesian variational inference and demonstrate its superiority over existing approaches in two applications, namely, fast log determinant estimation and information-theoretic Bayesian optimisation.",
Keywords: Maximum entropy; Log determinant estimation; Bayesian optimisation 1.,
Introduction Algorithmic scalability is an important component of modern machine learning.,
"Making high quality inference on large, feature rich datasets under a constrained computational budget is arguably the primary goal of the learning community.",
"This, however, comes with signiﬁcant challenges.",
"On the one hand, the exact computation of linear algebraic quantities may be prohibitively expensive, such as that of the log determinant.",
"On the other hand, an analytic expression for the quantity of interest may not exist at all, such as the case for the entropy of a Gaussian mixture model, and approximate methods are often both inefﬁcient and inaccurate.",
These highlight the need for efﬁcient approximations especially in solving large-scale machine learning problems.,
"In this paper, to address this challenge, we propose a novel, robust maximum entropy algorithm, stable for a large number of moments, surpassing the limit of previous maximum entropy algorithms [1–3].",
"We show that the ability to handle more moment information, which can be calculated cheaply either analytically or with the use of stochastic trace estimation, leads to signiﬁcantly enhanced performance.",
We showcase the effectiveness of the proposed algorithm by applying it to log determinant estimation [4–6] and entropy term approximation in the information-theoretic Bayesian optimisation [7–9].,
"Speciﬁcally, we reformulate the log determinant estimation into an eigenvalue spectral estimation problem so that we can estimate the log determinant of a symmetric positive deﬁnite matrix via computing the maximum entropy spectral density of its eigenvalues.",
"Similarly, we learn the maximum entropy spectral density for the Gaussian mixture and then approximate the entropy of the Gaussian mixture via the entropy of the Entropy 2019, 21, 551; doi:10.3390/e21010551 www.mdpi.com/journal/entropy arXiv:1906.01101v1 [stat.ML] 3 Jun 2019 Entropy 2019, 21, 551 2 of 19 maximum entropy spectral density, which provides an analytic upper bound.",
"Furthermore, in developing our algorithm, we establish equivalence between maximum entropy methods and constrained Bayesian variational inference [10].",
"The main contributions of the paper are as follows: • We propose a maximum entropy algorithm, which is stable and consistent for hundreds of moments, surpassing other off-the-shelf algorithms with a limit of a small number of moments.",
"Based on this robust algorithm, we develop a new Maximum Entropy Method (MEMe) which improves upon the scalability of existing machine learning algorithms by efﬁciently approximating computational bottlenecks using maximum entropy and fast moment estimation techniques; • We establish the link between maximum entropy methods and variational inference under moment constraints, hence connecting the former to well-known Bayesian approximation techniques; • We apply MEMe to the problem of estimating the log determinant, crucial to inference in determinental point processes [11], and to that of estimating the entropy of a Gaussian mixture, important to state-of-the-art information-theoretic Bayesian optimisation algorithms.",
"Theoretical Framework The method of maximum entropy, hereafter referred to as MaxEnt [12], is a procedure for generating the most conservative estimate of a probability distribution with the given information and the most non-committal one with regard to missing information [13].",
"Intuitively, in a bounded domain, the most conservative distribution, i.e., the distribution of maximum entropy, is the one that assigns equal probability to all the accessible states.",
"Hence, the method of maximum entropy can be thought of as choosing the ﬂattest, or most equiprobable distribution, satisfying the given moment constraints.",
"To determine the maximally entropic density q(x), we maximise the entropic functional S=− Z q(x) log q(x)dx − m ∑ i=0 αi  Z q(x)xidx −µi  , (1) with respect to q(x), where the second term with µi = Ep[xi] for some density p(x) are the power moment constraints on the density q(x), {αi} are the corresponding Lagrange multipliers, and m is the number of moments.",
"The ﬁrst term in Equation (1) is referred to as the Boltzmann–Shannon–Gibbs (BSG) entropy, which has been applied in multiple ﬁelds, ranging from condensed matter physics [14] to ﬁnance [15].",
"For the case of i ≤2, the Lagrange multipliers can be calculated analytically; for i ≥3, they must be determined numerically.",
"In this section, we ﬁrst establish links between the method of MaxEnt and Bayesian variational inference.",
We then describe fast moment estimation techniques.,
"Finally, we present the proposed MaxEnt algorithm.",
Maximum Entropy as Constrained Bayesian Variational Inference The work of Bretthorst [16] makes the claim that the method of maximum entropy (MaxEnt) is fundamentally at odds with Bayesian inference.,
"At the same time, variational inference [17] is a widely used approximation technique that falls under the category of Bayesian learning.",
"In this section, we show that the method of maximum relative entropy [18] is equivalent to constrained variational inference, thus establishing the link between MaxEnt and Bayesian approximation.",
"Entropy 2019, 21, 551 3 of 19 2.1.1.",
"Variational Inference Variational methods [10,17] in machine learning pose the problem of intractable density estimation from the application of Bayes’ rule as a functional optimisation problem: p(z|x) = p(x|z)p(z) p(x) ≈q(z), (2) where p(z) and p(z|x) represent the prior and posterior distributions of the random variable z, respectively.",
"Variational inference therefore seeks to ﬁnd q(z) as an approximation of the posterior p(z|x), which has the beneﬁt of being a strict bound to the true posterior.",
"Typically, while the functional form of p(x|z) is known, calculating p(x) = R p(x|z)p(z)dz is intractable.",
"Using Jensen’s inequality, we can show that: log p(x) ≥Eq[log p(x, z)] −Eq[log q(z)].",
"(3) Furthermore, the reverse Kullback-Leibler (KL) divergence between the posterior and the variational distribution, DKL(q|p), can be written as: log p(x) = Eq[log p(x, z)] −Eq[log q(z)] + DKL(q|p).",
"(4) Hence, maximising the evidence lower bound is equivalent to minimising the reverse KL divergence between p and q.",
"MaxEnt is equivalent to Constrained Variational Inference Minimising the reverse KL divergence between our posterior q(x) and our prior q0(x) as is done in variational inference, with respect to q(x): DKL(q|q0) = −H(q) − Z x∈D q(x) log q0(x)dx, (5) where H(q) denotes the differential entropy of the density q(x), such that R q(x)dx = 1 and R q(x)xidx = µi.",
"By the theory of Lagrangian duality, the convexity of the KL divergence, and the afﬁne nature of the moment constraints, we maximise the dual form [19] of Equation (5): −H(q) − Z q(x) log q0(x)dx − m ∑ i=0 αi Z q(x)xidx −µi  (6) with respect to q(x) or, alternatively, we minimise H(q)+ Z q(x) log q0(x)dx + m ∑ i=0 αi Z q(x)xidx −µi  .",
"(7) In the ﬁeld of information physics, the minimisation of Equation (7) is known as the method of relative entropy [18].",
"It can be derived as the unique functional satisfying the axioms of locality, coordinate invariance, sub-system invariance and objectivity.",
"The restriction to a functional is derived from considering the set of all distributions Q = {qj(x)} compatible with the constraints and devising a transitive ranking scheme (Transitive ranking means if A > B and B > C, then A > C.).",
"Furthermore, it can be shown that Newton’s laws, non-relativistic quantum mechanics, and Bayes’ rule can all be derived under this formalism [18].",
"In the case of a ﬂat prior, Entropy 2019, 21, 551 4 of 19 we reduce to the method of maximum entropy with moment constraints [13] as q0(x) is a constant.",
"Hence, MaxEnt and constrained variational inference are equivalent.",
"Fast Moment Estimation Techniques As shown in Section 2.1, constrained variational inference requires the knowledge of the moments of the density being approximated to make inference.",
"In this section, we discuss fast moment estimation techniques for both the general case and the case for which analytic expressions exist.",
"Moments of a Gaussian Mixture In some problems, there exist analytic expressions which can be used to compute the moments, One popular example is the Gaussian distribution (and the mixture of Gaussians).",
"Speciﬁcally, for the one-dimensional Gaussian, we can ﬁnd an analytic expression for the moments: Z ∞ −∞xme−(x−µ)2 2σ2 dx = m ∑ i=0 ( √ 2σ)i+1 m i  µm−iζi, (8) where µ and σ are the mean and standard deviation of the Gaussian, respectively, and: ζi = Z ∞ −∞yie−y2dy = ( (i−1)!",
"!√π 2i/2 , i even, [1/2(i −1)]!, i odd.",
(9) where (i −1)!!,
denotes double factorial.,
"Hence, for a mixture of k Gaussians with mean µk and standard deviation σk, the n-th moment is analytic: ∑ k wk m ∑ i=0 ( √ 2σk)i+1 m i  µm−i k ζi.",
(10) where wk is the weight of k−th Gaussian in the mixture.,
Moments of the Eigenvalue Spectrum of a Matrix Stochastic trace estimation is an effective way of cheaply estimating the moments of the eigenvalue spectrum of a given matrix K ∈Rn×n [20].,
The essential idea is that we can estimate the non-central moments of the spectrum by using matrix-vector multiplications.,
"Recall that for any multivariate random variable z with mean m and variance Σ, we have: E(zTz) = mTm + Σ m=0,Σ=I ======= I, (11) where for the second equality we have assumed (without loss of generality) that z possesses zero mean and unit variance.",
"By the linearity of trace and expectation, for any number of moments m ≥0 we have: n ∑ s=1 λm s = Tr(IKm) = E(zKmzT), (12) Entropy 2019, 21, 551 5 of 19 where {λs} are the eigenvalues of K. In practice, we approximate the expectation in Equation (12) with a set of probing vectors and a simple Monte Carlo average, i.e., for d random unit vectors {zj}d j=1: E(zKmzT) ≈1 d  d ∑ j=1 zjKmzT j  , (13) where we take the product of the matrix K with the vector zT j a total of m times and update zT j each time, so as to avoid the costly matrix multiplication with O(n3) complexity.",
"This allows us to calculate the non-central moment expectations in O(dmn2) complexity for dense matrices, or O(dm × nnz) complexity for sparse matrices, where d × m ≪n and nnz is the number of non-zero entries in the matrix.",
"The random vector zj can be drawn from any distribution, such as a Gaussian.",
The resulting stochastic trace estimation algorithm is outlined in Algorithm 1.,
"Algorithm 1 Stochastic Trace Estimation [20] 1: Input: Matrix K ∈Rn×n, Number of Moments m, Number of Probe Vectors d 2: Output: Moment Expectations ˆµi, ∀i ≤m 3: Initialise ˆµi = 0 ∀i 4: for j = 1, .",
", d do 5: Initialise zj = rand(1, n) 6: for i = 1, .",
", m do 7: zT j = KzT j 8: ˆµi = ˆµi + 1 dzjzT j 9: end for 10: end for 2.3.",
"The Proposed MaxEnt Algorithm In this section, we develop an algorithm for determining the maximum relative entropy density given moment constraints.",
"Under the assumption of a ﬂat prior in a bounded domain, this reduces to a generic MaxEnt density, which we use in various examples.",
"As we discussed previously, in order to obtain the MaxEnt distribution, we maximise the generic objective of Equation (1).",
"In practice, we instead minimise the dual form of this objective [19], which can be written as follows: S(q, q0) = Z x∈D q0(x) exp(−[1 + m ∑ i=0 αixi])dx + m ∑ i=0 αiµi.",
"(14) Notice that this dual objective admits analytic expressions for both the gradient and the Hessian: ∂S(q, q0) ∂αj = µj − Z x∈D q0(x)xj exp(−[1 + m ∑ i=0 αixi])dx, (15) ∂2S(q, q0) ∂αj∂αk = Z x∈D q0(x)xj+k exp(−[1 + m ∑ i=0 αixi])dx.",
"(16) Entropy 2019, 21, 551 6 of 19 For a ﬂat prior in a bounded domain, which we use as a prior for the spectral densities of large matrices and for Gaussian mixture models, q0(x) is a constant that can be dropped out.",
"With the notation qα(x) = exp(−[1 + m ∑ i=0 αixi]), (17) we obtain the MaxEnt algorithm in Algorithm 2.",
"The input {µi} of Algorithm 2 are estimated using fast moment estimation techniques, as explained in Section 2.2.2.",
"In our implementation, we use Python’s SciPy Newton-conjugate gradient (CG) algorithm to solve the minimisation in step 4 (line 6), having ﬁrstly computed the gradient within a tolerance level of ϵ as well as the Hessian.",
"To make the Hessian better conditioned so as to achieve convergence, we symmetrise it and add jitter of intensity η = 10−8 along the diagonal.",
We estimate the given integrals with quadrature using a grid size of 10−4.,
"Empirically, we observe that the algorithm is not overly sensitive to these choices.",
"In particular, we ﬁnd that, for well conditioned problems, the need for jitter is obviated and a smaller grid size works ﬁne; in the case of worse conditioning, jitter helps improve convergence and the grid size becomes more important, where a smaller grid size leads to a computationally more intensive implementation.",
"Algorithm 2 The Proposed MaxEnt Algorithm 1: Input: Moments {µi}, Tolerance Level ϵ, Jitter variance in Hessian η = 10−8 2: Output: Coefﬁcients {αi} 3: Initialise αi = 0 4: Compute gradient: gj = µj −R 1 0 qα(x)xjdx 5: Compute Hessian: Hjk =R 1 0 qα(x)xj+kdx, H = 1 2(H + HT)+ηI 6: Minimize R 1 0 qα(x)dx + ∑i αiµi using Conjugate Gradients until ∀j: gj < ϵ Given that any polynomial sum can be written as ∑i αixi = ∑i βi fi(x), where fi(x) denotes another polynomial basis, whether we choose to use power moments in our constraints or another polynomial basis, such as the Chebyshev or Legendre basis, does not change the entropy or solution of our objective.",
"However, the performance of optimisers working on these different formulations may vary [21].",
"For simplicity, we have kept all the formulas in terms of power moments.",
"However, we ﬁnd vastly improved performance and conditioning when we switch to orthogonal polynomial bases (so that the errors between moment estimations are uncorrelated), as shown in Section 3.2.2.",
We implement both Chebyshev and Legendre moments in our Lagrangian and ﬁnd similar performance for both.,
"Applications We apply the proposed algorithm to two problems of interest, namely, log determinant estimation and Bayesian optimisation.",
In both cases we demonstrate substantial speed up and improvement in performance.,
"Log Determinant Estimation Calculation of the log determinant is a common hindrance in machine learning, appearing in Gaussian graphical models [22], Gaussian processes [23], variational inference [24], metric and kernel learning [25], Markov random ﬁelds [26] and determinantal point processes [11].",
"For a large positive deﬁnite matrix K ∈Rn×n, the canonical solution involves the Cholesky decomposition of K = LLT.",
"The log determinant is then trivial to calculate as log det(K) = 2 ∑n i=1 log Lii, where Lii is the ii-th entry of L. This computation Entropy 2019, 21, 551 7 of 19 invokes a computational complexity of O(n3) and storage complexity of O(n2), which becomes prohibitive for large n, i.e., n > 104.",
"Log Determinant Estimation as a Spectral Estimation Problem using MaxEnt Any symmetric positive deﬁnite (PD) matrix K is diagonalisable by a unitary matrix U, i.e., K = UTDU, where D is the diagonal matrix of eigenvalues of K. Hence we can write the log determinant as: log det(K) = log∏ i λi = n ∑ i=1 log λi = nEp(log λ), (18) where we have used the cyclicity of the determinant and Ep(log λ) denotes the expectation under the spectral measure.",
"The latter can be written as: Ep(log λ) = Z λmax λmin p(λ) log λdλ = Z λmax λmin n ∑ i=1 1 nδ(λ −λi) log λdλ, (19) where λmax and λmin correspond to the largest and smallest eigenvalues, respectively.",
"Given that the matrix is PD, we know that λmin > 0 and we can divide the matrix by an upper bound of the eigenvalue, λmax ≤λu, via the Gershgorin circle theorem [27] such that: log det(K) = nEp(log λ′) + n log λu, (20) where λ′ = λ/λu and λu = maxi(∑n j=1 |Kij|), i.e., the maximum sum of the rows of the absolute of the matrix K. Hence we can comfortably work with the transformed measure: Z λmax/λu λmin/λu p(λ′) log λ′dλ′ = Z 1 0 p(λ′) log λ′dλ′, (21) where the spectral density p(λ′) is 0 outside of its bounds of [0, 1].",
We therefore arrive at the following maximum entropy method (MEMe) for log determinant estimation detailed in Algorithm 3.,
"Algorithm 3 MEMe for Log Determinant Estimation 1: Input: PD Symmetric Matrix K ∈Rn×n, Number of Moments m, Number of Probe Vectors d, Tolerance Level ϵ 2: Output: Log Determinant Approximation log det(K) 3: λu = maxi(∑n j=1 |Kij|) 4: B = K/λu 5: Compute moments, {µi}, via Stochastic Trace Estimation (Algorithm 1) with inputs (B, m, d) 6: Compute coefﬁcients, {αi}, via MaxEnt (Algorithm 2) with inputs ({µi}, ϵ) 7: Compute q(λ) = exp [−(1 + ∑i αiλi)] 8: Estimate log det(K) ≈n R log(λ)q(λ)dλ + n log(λu) Entropy 2019, 21, 551 8 of 19 3.1.2.",
"Experiments Log Determinant Estimation for Synthetic Kernel Matrix To speciﬁcally compare against commonly used Chebyshev [4] and Lanczos approximations [28] to the log determinant, and see how their accuracy degrades with worsening condition number, we generate a typical squared exponential kernel matrix [23], K ∈Rn×n, using the Python GPy package with 6 dimensional Gaussian inputs with a variety of realistic uniform length-scales.",
"We then add noise of variance η = 10−8 along the diagonal of K. We use m = 30 moments and d = 50 Hutchinson probe vectors [20] in MEMe for the log determinant estimation, and display the absolute relative estimation error for different approaches in Table 1.",
"We see that, for low condition numbers, the beneﬁt of framing the log determinant as an optimisation problem is marginal, whereas for large condition numbers, the beneﬁt becomes substantial, with orders of magnitude better results than competing methods.",
"Relative estimation error for MEMe, Chebyshev, and Lanczos approaches, with various length-scale l and condition number κ on the squared exponential kernel matrix K ∈R1000×1000.",
κ l MEMe Chebyshev Lanczos 3 × 101 0.05 0.0014 0.0037 0.0024 1.1 × 103 0.15 0.0522 0.0104 0.0024 1.0 × 105 0.25 0.0387 0.0795 0.0072 2.4 × 106 0.35 0.0263 0.2302 0.0196 8.3 × 107 0.45 0.0284 0.3439 0.0502 4.2 × 108 0.55 0.0256 0.4089 0.0646 4.3 × 109 0.65 0.00048 0.5049 0.0838 1.4 × 1010 0.75 0.0086 0.5049 0.1050 4.2 × 1010 0.85 0.0177 0.5358 0.1199 Log Determinant Estimation on Real Data The work in Granziol and Roberts [29] has shown that the addition of an extra moment constraint cannot increase the entropy of the MaxEnt solution.,
"For the problem of log determinant, this signiﬁes that the entropy of the spectral approximation should decrease with the addition of every moment constraint.",
We implement the MaxEnt algorithm proposed in Bandyopadhyay et al.,
"[2], which we refer to as OMxnt, in the same manner as applied for log determinant estimation in Fitzsimons et al.",
"[30], and compare it against the proposed MEMe approach.",
"Speciﬁcally, we show results on the Ecology dataset [31], with n = 999, 999, for which the true log determinant can be calculated.",
"For OMxnt, we see that after the initial decrease, the error (Figure 1b) begins to increase for m > 3 moments and the entropy (Figure 1a) increases at m = 6 and m = 12 moments.",
"For the proposed MEMe algorithm, the performance is vastly improved in terms of estimation error (Figure 1d); furthermore, the error continually decreases with increasing number of moments, and the entropy (Figure 1c) decreases smoothly.",
This demonstrates the superiority both in terms of consistency and performance of our novel algorithm over established existing alternatives.,
Bayesian Optimisation Bayesian Optimisation (BO) is a powerful tool to tackle global optimisation challenges.,
"It is particularly useful when the objective function is unknown, non-convex and very expensive to evaluate [32], and has been successfully applied in a wide range of applications including automatic machine learning [33–36], robotics [37,38] and experimental design [39].",
"When applying BO, we need to choose Entropy 2019, 21, 551 9 of 19 5 10 15 No.",
of Moments 6.86 6.87 6.88 6.89 6.9 6.91 Entropy Entropy (a) OMxnt: Entropy vs Moments 5 10 15 No.,
of Moments 0 0.02 0.04 0.06 0.08 Relative Error Relative Error (b) OMxnt: Relative Error vs Moments 5 10 15 No.,
of Moments -3 -2.5 -2 -1.5 -1 -0.5 Entropy Entropy (c) MEMe: Entropy vs Moments 5 10 15 No.,
of Moments 0 0.02 0.04 0.06 0.08 Relative Error Relative Error (d) MEMe: Relative Error vs Moments Figure 1.,
Comparison of the Classical (OMxnt) and our novel (MEMe) MaxEnt algorithms in log determinant estimation on real data.,
The entropy value (a) and estimation error (b) of OMxnt are shown in the top row.,
Those of the MEMe are shown in (c) and (d) in the bottom row.,
a statistical prior to model the objective function and deﬁne an acquisition function which trades off exploration and exploitation to recommend the next query location [40].,
The generic BO algorithm is presented in Algorithm 6 in Appendix B.,
"In the literature, one of the most popular prior models is the Gaussian processes (GPs), and the most recent class of acquisition functions that demonstrate state-of-the-art empirical performance is the information-theoretic ones [7–9,41].",
MaxEnt for Information-Theoretic Bayesian Optimisation Information-theoretic BO methods select the next query point that maximises information gain about the unknown global optimiser/optimum.,
"Despite their impressive performance, the computation of their acquisition functions involves an intractable term of Gaussian mixture entropy, as shown in Equation (22), if we perform a fully Bayesian treatment for the GP hyperparameters.",
"Accurate approximation of this Gaussian mixture entropy term is crucial for the performance of information-theoretic BO, but can be difﬁcult and/or expensive.",
"In this paper, we propose an efﬁcient approximation of the Gaussian mixture entropy by using MEMe, which allows for efﬁcient computation of the information-theoretic acquisition functions.",
"Entropy 2019, 21, 551 10 of 19 As an concrete example, we consider the application of MEMe for Fast Information-Theoretic Bayesian Optimisation (FITBO) [9] as described in Algorithm 5, which is a highly practical information-based BO method proposed recently.",
"The FITBO acquisition function has the following form: α(x|Dt) = H h 1 M M ∑ j p(y|I(j)) i −1 M M ∑ j H  p(y|I(j))  , (22) where p(y|I(j)) = p(y|Dt, x, θ(j)) is the predictive posterior distribution for y conditioned on the observed data Dt, the test location x, and a hyperparameter sample θ(j).",
"The ﬁrst entropy term is the entropy of a Gaussian mixture, where M is the number of Gaussian components.",
The entropy of a Gaussian mixture does not have a closed-form solution and needs to be approximated.,
"In FITBO, the authors approximate the quantity using numerical quadrature and moment matching (This corresponds to the maximum entropy solution for two moment constraints as well as the normalization constraint.).",
"In this paper, we develop an effective analytic upper bound of the Gaussian mixture entropy using MEMe, which is derived from the non-negative relative entropy between the true density p(x) and the MaxEnt distribution q(x) [29]: DKL(p||q) = −H(p) + H(q) ≥0, (23) hence H(p) ≤H(q).",
"(24) Notice that q(x) shares the same moment constraints as p(x); furthermore, the entropy of the MaxEnt distribution q(x) can be derived analytically: H(q) = 1 + m ∑ i=0 αiµi, (25) where {µi} are the moments of a Gaussian mixture, which can be computed analytically using Equation (10), and {αi} are the Lagrange multipliers that can be obtained by applying Algorithm 2.",
The overall algorithm for approximating the Gaussian mixture entropy is then summarised in Algorithm 4.,
"In Appendix B.1, we also prove that the moments of the Gaussian mixture uniquely determine its density, and the bound becomes tighter with every extra moment constraint: in the m →∞limit, the entropy of the MaxEnt distribution converges to the true Gaussian mixture entropy.",
"Hence, the use of a moment-based MaxEnt approximation can be justiﬁed [3].",
"Algorithm 4 MEMe for Approximating Gaussian Mixture Entropy 1: Input: A univariate Gaussian mixture GM = 1 M ∑M j N (y; mj, σ2 j ) with mean mj and variance σ2 j 2: Output: HGM ≈H  1 M ∑M j N (y; mj, σ2 j )  3: Compute the moments of GM, {µi}, analytically using Equation (10) 4: Compute the Lagrange multipliers, {αi}, using Algorithm 2 5: HGM ≈−(1 + ∑i αiE[yi]) = −(1 + ∑i αiµi) Entropy 2019, 21, 551 11 of 19 Algorithm 5 MEMe for FITBO 1: Input: Observed data Dt 2: Output: Acquisition function αn(x|Dt) 3: Sample M hyperparameters n θ(j)o 4: for j = 1, .",
", M do 5: Approximately compute p(y|Dt, x, θ(j)) = N (y; mj, Kj) and its entropy H h p(y|Dt, x, θ(j)) i 6: end for 7: Approximate H  1 M ∑M j N (y; mj, Kj)  with MEMe following Algorithm 4 8: Compute α(x|Dt) as in Equation (22) 3.2.2.",
"Experiments Entropy of the Gaussian Mixture in Bayesian Optimisation We ﬁrst test a diverse set of methods to approximate the entropy of two sets of Gaussian mixtures, which are generated using FITBO with 200 hyperparameter samples and 400 hyperparameter samples on a 2D problem.",
"Speciﬁcally, we compare the following approaches for entropy approximation: MaxEnt methods using 10 and 30 power moments (MEMe-10 and MEMe-30), MaxEnt methods using 10 and 30 Legendre moments (MEMeL-10 and MEMeL-30), variational upper bound (VUB) [42], method proposed in [43] with 2 Taylor expansion terms (Huber-2), Monte Carlo with 100 samples (MC-100), and simple moment matching (MM).",
"We evaluate the performance in terms of the approximation error, i.e., the relative error between the approximated entropy value and the true entropy value, the latter of which is estimated via expensive numerical integration.",
The results of the mean approximation errors by all methods over 80 different Gaussian mixtures are shown in Table 2 (The version with the standard deviation of errors is presented as Table A1 in Appendix B.2.),
We can see clearly that all MEMe approaches outperform other methods in terms of the approximation error.,
"Among the MEMe approaches, the use of Legendre moments, which apply orthogonal Legendre polynomial bases, outperforms the use of simple power moments.",
Mean fractional error in approximating the entropy of the mixture of M Gaussians using various methods.,
Methods M=200 M=400 MEMe-10 1.24 × 10−2 1.38 × 10−2 MEMe-30 1.13 × 10−2 1.06 × 10−2 MEMeL-10 1.01 × 10−2 0.85 × 10−2 MEMeL-30 0.50 × 10−2 0.36 × 10−2 VUB 22.0 × 10−2 29.1 × 10−2 Huber-2 24.7 × 10−2 35.5 × 10−2 MC-100 1.60 × 10−2 2.72 × 10−2 MM 2.78 × 10−2 3.22 × 10−2 The mean runtime taken by all approximation methods over 80 different Gaussian mixtures are shown in Table 3 (The version with the standard deviation is presented as Table A2 in Appendix B.2.).,
"To ensure a fair comparison, all the methods are implemented in MATLAB and all the timing tests are performed on a 2.3GHz Intel Core i5 processor.",
"As we expect, the moment matching technique, which Entropy 2019, 21, 551 12 of 19 enables us to obtain an analytic approximation for the Gaussian mixture entropy, is the fastest method.",
MEMe approaches are signiﬁcantly faster than the rest of approximation methods.,
This demonstrates that MEMe approaches are highly efﬁcient in terms of both approximation accuracy and computational speed.,
"Among all the MEMe approaches, we choose to apply MaxEnt with 10 Legendre moments in the BO for the next set of experiments, as it is able to achieve lower approximation error than MaxEnt with higher power moments while preserving the computational beneﬁt of FITBO.",
Mean runtime of approximating the entropy of the mixture of M Gaussians using various methods.,
Methods M=200 M=400 MEMe-10 1.38 × 10−2 1.48 × 10−2 MEMe-30 2.59 × 10−2 3.21 × 10−2 MEMeL-10 1.70 × 10−2 1.75 × 10−2 MEMeL-30 4.18 × 10−2 4.66 × 10−2 VUB 12.9 × 10−2 50.7 × 10−2 Huber-2 20.9 × 10−2 82.2 × 10−2 MC-100 10.6 × 10−2 40.1 × 10−2 MM 2.71 × 10−5 2.87 × 10−5 Information-Theoretic Bayesian Optimisation We now test the effectiveness of MEMe for information-theoretic BO.,
We ﬁrst illustrate the entropy approximation performance of MEMe using a 1D example.,
"In Figure 2, the top plot shows the objective function we want to optimise (red dash line) and the posterior distribution of our surrogate model (blue solid line and shaded area).",
"The bottom plot shows the acquisition functions computed based on Equation (22) using the same surrogate model but three different methods for Gaussian mixture entropy approximation, i.e., expensive numerical quadrature or Quad (red solid line), MM (black dash line), and MEMe using 10 Legendre moments (green dash line).",
"In BO, the next query location is obtained by maximising the acquisition function, therefore the location instead of the magnitude of the modes of the acquisition function matters most.",
We can see from the bottom plot that MEMeL-10 results in an approximation that well matches the true acquisition function obtained by Quad.,
"As a result, MEMeL-10 manages to recommend the same next query location as Quad.",
"In comparison, the loose upper bound of the MM method, though successfully capturing the locations of the peak values, fails to correctly predict the global maximiser of the true acquisition function.",
MM therefore recommends a query location that is different from the optimal choice.,
"As previously mentioned, the acquisition function in information-theoretic BO represents the information gain about the global optimum by querying at a new location.",
The sub-optimal choice of the next query location thus imposes a penalty on the optimisation performance as seen in Figure 3.,
"Entropy 2019, 21, 551 13 of 19 Figure 2.",
Bayesian Optimisation (BO) on a 1D toy example with acquisition functions computed by different approximation methods.,
"In the top subplot, the red dash line is the unknown objective function, the black crosses are the observed data points, and the blue solid line and shaded area are the posterior mean and variance, respectively, of the GP surrogate that we use to model the latent objective function.",
"The coloured triangles are the next query point recommended by the BO algorithms, which correspond to the maximiser of the acquisition functions in the bottom subplot.",
"In the bottom plot, the red solid line, black dash line, and green dotted line are the acquisition functions computed by Quad, MM, and MEMe using 10 Legendre moments, respectively.",
0 10 20 30 40 50 No.,
of Function Evaluations 0.3 0.4 0.5 0.6 Log10 IR FITBO-Quad FITBO-MM FITBO-MEMeL-10 (a) Michalewicz-5D 0 10 20 30 40 50 No.,
of Function Evaluations 0.4 0.8 1.2 1.6 Log10 IR FITBO-Quad FITBO-MM FITBO-MEMeL-10 (b) Hartmann-6D Figure 3.,
Performance of various versions of FITBO on 2 benchmark test problems: (a) Michalewicz-5D function and (b) Hartmann-6D function.,
The immediate regret (IR) on the y-axis is expressed in the logarithm to the base 10.,
"In the next set of experiments, we evaluate the optimisation performance of three versions of FITBO that use different approximation methods.",
"Speciﬁcally, FITBO-Quad denotes the version that Entropy 2019, 21, 551 14 of 19 uses expensive numerical quadrature to approximate the entropy of the Gaussian mixture, FITBO-MM denotes the one using simple moment matching, and FITBO-MEMeL denotes the one using MEMe with 10 Legendre moments.",
"We test these BO algorithms on two challenging optimisation problems, i.e., the Michalewicz-5D function [44] and the Hartmann-6D function [45], and measure the optimisation performance in terms of the immediate regret (IR): IR = | f ∗−ˆf |, which measures the absolute difference between the true global minimum value f ∗and the best guess of the global minimum value ˆf by the BO algorithm.",
The average (median) result over 10 random initialisations for each experiment is shown in Figure 3.,
"It is evident that the MEMe approach (FITBO-MEMeL-10), which better approximates the Gaussian mixture entropy, leads to a superior performance of the BO algorithm compared to the BO algorithm using simple moment matching technique (FITBO-MM).",
"Conclusion In this paper, we established the equivalence between the method of maximum entropy and Bayesian variational inference under moment constraints, and proposed a novel maximum entropy algorithm (MEMe) that is stable and consistent for a large number of moments.",
"We apply MEMe in two applications, i.e., log determinant estimation and Bayesian optimisation, to demonstrate its effectiveness and superiority over state-of-the-art approaches.",
The proposed algorithm can further beneﬁt a wide range of large-scale machine learning applications where efﬁcient approximation is of crucial importance.,
"Author Contributions: Conceptualization, D.G.",
"; formal analysis, investigation, methodology, software and writing–original draft, D.G.",
"; supervision and writing–review and editing, S.Z., X.D., M.O.",
Funding: B.R.,
"would like to thank the Oxford Clarendon Fund and the Oxford-Man Institute of Quantitative Finance; D.G., S.Z.",
and X.D would like to thank the Oxford-Man Institute of Quantitative Finance for ﬁnancial support; S.R.,
would like to thank the UK Royal Academy of Engineering and the Oxford-Man Institute of Quantitative Finance Conﬂicts of Interest: The authors declare no conﬂict of interest.,
Abbreviations The following abbreviations are used in this manuscript: MEMe Maximum entropy method MaxEnt Maximum entropy PD Positive deﬁnite CG Conjugate gradient OMxnt Old MaxEnt algorithm proposed by Bandyopadhyay et al.,
[2] BO Bayesian optimisation GP Gaussian process FITBO Fast information-theoretic Bayesian optimisation GM Gaussian mixture VUB Variational upper bound Huber Method proposed by Huber et al.,
"[43] for estimating the Gaussian mixture entropy MC Monte Carlo sampling MM Moment matching Quad Numerical quadrature IR Immediate regret Entropy 2019, 21, 551 15 of 19 Appendix A Polynomial Approximations to the Log Determinant Recent work [4–6] has considered incorporating knowledge of the non-central moments (Also using stochastic trace estimation.)",
"of a normalised eigenspectrum by replacing the logarithm with a ﬁnite polynomial expansion, Z 1 0 p(λ) log(λ)dλ = Z 1 0 p(λ) log(1 −(1 −λ))dλ.",
"(A1) Given that log(λ) is not analytic at λ = 0, it can be seen that, for any density with a large mass near the origin, a very large number of polynomial expansions, and thus moment estimates, will be required to achieve a good approximation, irrespective of the choice of basis.",
"Appendix A.1 Taylor Approximations are Unsound In the case of a Taylor expansion, Equation (A1) can be written as, − Z 1 0 p(λ) ∞ ∑ i=1 (1 −λ)i i ≈− Z 1 0 p(λ) m ∑ i=1 (1 −λ)i i .",
"(A2) The error in this approximation can be written as the difference of the two sums, − ∞ ∑ i=m+1 Ep(1 −λ)i i , (A3) where we have used the Taylor expansion of log(1 −x) and Ep denotes the expectation under the spectral measure.",
"We begin with complete ignorance about the spectral density p(λ) (other than its domain [0, 1]) and by some scheme after seeing the ﬁrst m non-central moment estimates we propose a surrogate density q(λ).",
"The error in our approximation can be written as, Z 1 0 [p(λ) −q(λ)] log(λ)dλ = Z 1 0 −[p(λ) −q(λ)] ∞ ∑ i=1 (1 −λ)i i dλ.",
"(A4) For this error to be equal to that of our Taylor expansion, Equation (A3), our implicit surrogate density must have the ﬁrst m non-central moments of (1 −λ) identical to the true spectral density p(λ) and all others 0.",
"For any PD matrix K, for which Ep(1 −λ)i > 0, ∀i ≤m, (We ignore the trivial case of a Dirac distribution at λ = 1, which is of no practical interest.)",
"for Equation (A4) to be equal to Equation (A3), we must have, Z 1 0 q(λ) ∞ ∑ i=m+1 (1 −λ)i i dλ = 0.",
"(A5) Given that 0 ≤λ ≤1 and that we have removed the trivial case of the spectral density (and by implication its surrogate) being a delta function at λ = 1, the sum is manifestly positive and hence q(λ) < 0 for some λ, which violates the deﬁnition of a density.",
Appendix B Bayesian Optimisation The generic algorithm for Bayesian optimisation can be summarised in Algorithm 6 .,
"Entropy 2019, 21, 551 16 of 19 Algorithm 6 Bayesian Optimisation 1: Input: A black-box function y, Initial observed data D0 2: Output: The best guess about the global optimiser ˆxN 3: for n = 0, .",
", N do 4: Select xn+1 = arg maxx∈X αn(x|Dt) 5: Query y at xn+1 to obtain yn+1 6: Dn+1 ←Dt ∪(xn+1, fn+1) 7: Update model p(y|x, Dn+1) = GP (y; m(·), K(·, ·)) 8: end for 9: ˆxN = arg maxx∈X mN(x) Appendix B.1 Are Moments Sufﬁcient to Fully Describe the Problem?",
"It was shown in Billingsley [46] that, for a probability measure µ having ﬁnite moments of all orders αk = R ∞ −∞xkµ(dx), if the power series ∑k αk/k!",
"has a positive radius of convergence, then µ is the only probability measure with the moments {αi}.",
"Informally, a Gaussian has ﬁnite moments of all orders; therefore, any ﬁnite combination of Gaussians must necessarily possess ﬁnite moments of all orders hence the above condition is satisﬁed.",
We explicitly show this for the case of a one-dimensional Gaussian as follows.,
We take the location parameter as mi and standard deviation as σi.,
"It can be seen that the 2k-th moment of the Gaussian is given as: G2k = 2k ∑ 2p=0 2k 2p  m2(k−p) i β−p i , (A6) where we make use of the fact that all odd central power moments of the Gaussian are 0.",
"Hence for the Gaussian mixture model we have: GM2k = N ∑ i=1 wi 2k ∑ 2p=0 2k 2p  m2(k−p) i β−p i , (A7) where {wi} are the weights for the components that satisfy ∑i wi = 1 and 0 ≤wi ≤1.",
Notice that µi is upper bounded by a quantity greater than 1 and βi is lower bounded by a quantity smaller than 1.,
"Furthermore, the following relationship holds: ∑2k 2p=0 (2k 2p) < (k + 1) (2k)!",
"Therefore, the expression in Equation (A7) can be upper bounded as: GM2k < N(k + 1) (2k)!",
")2(µ2k maxβ−k i ) , (A8) which is smaller than (2k)!",
"in the k →∞limit by taking the logarithm: log N 2k + log(k + 1) 2k + log µmax + | log βi| 2 ≤log k. (A9) Entropy 2019, 21, 551 17 of 19 Appendix B.2 Experimental Results on Approximating the Gaussian Mixture Entropy The mean and standard deviation of the approximation error and the runtime taken by all approximation methods over 80 different Gaussian mixtures are shown in Table A1 and Table A2 respectively.",
Fractional error in approximating the entropy of Gaussian mixtures using various methods.,
Methods M=200 M=400 MEMe-10 1.24 × 10−2 1.38 × 10−2 (±4.12 × 10−2) (±5.46 × 10−2) MEMe-30 1.13 × 10−2 1.06 × 10−2 (±3.68 × 10−2 ) (±4.47 × 10−2 ) MEMeL-10 1.01 × 10−2 0.85 × 10−2 (±3.68 × 10−2 ) (±3.81 × 10−2 ) MEMeL-30 0.50 × 10−2 0.36 × 10−2 (±2.05 × 10−2 ) (±1.62 × 10−2 ) Variational 22.0 × 10−2 29.1 × 10−2 Upper Bound (±28.0 × 10−2 ) (±78.6 × 10−2) Huber-2 24.7 × 10−2 35.5 × 10−2 (±46.1 × 10−2) (±130.4 × 10−2 ) MC-100 1.60 × 10−2 2.72 × 10−2 (±3.80 × 10−2 ) (±11.9 × 10−2) Moment 2.78 × 10−2 3.22 × 10−2 Matching (±4.85 × 10−2 ) (±7.94 × 10−2) Table A2.,
Runtime of approximating the entropy of Gaussian mixtures using various methods.,
Methods M=200 M=400 MEMe-10 1.38 × 10−2 1.48 × 10−2 (±2.59 × 10−3) (±2.22 × 10−3) MEMe-30 2.59 × 10−2 3.21 × 10−2 (±4.68 × 10−3 ) (±5.17 × 10−3 ) MEMeL-10 1.70 × 10−2 1.75 × 10−2 (±3.00 × 10−3 ) (±3.17 × 10−3 ) MEMeL-30 4.18 × 10−2 4.66 × 10−2 (±5.63 × 10−3 ) (±5.00 × 10−3 ) Variational 12.9 × 10−2 50.7 × 10−2 Upper Bound (±8.22 × 10−3 ) (±10.4 × 10−3) Huber-2 20.9 × 10−2 82.2 × 10−2 (±7.54 × 10−3) (±14.8 × 10−3 ) MC-100 10.6 × 10−2 40.1 × 10−2 (±6.19 × 10−3 ) (±17.8 × 10−3) Moment 2.71 × 10−5 2.87 × 10−5 Matching (±1.16 × 10−4 ) (±1.19 × 10−4) References 1.,
"Granziol, D.; Roberts, S.J.",
Entropic determinants of massive matrices.,
"In Proceedings of the 2017 IEEE International Conference on Big Data, Boston, MA, USA, 11–14 December 2017; pp.",
"Bandyopadhyay, K.; Bhattacharya, A.K.",
"; Biswas, P.; Drabold, D. Maximum entropy and the problem of moments: A stable algorithm.",
"E 2005, 71, 057701.",
"Entropy 2019, 21, 551 18 of 19 3.",
"; Papanicolaou, N. Maximum entropy in the problem of moments.",
"1984, 25, 2404–2417.",
"Han, I.; Malioutov, D.; Shin, J.",
Large-scale log-determinant computation through stochastic Chebyshev expansions.,
"In Proceedings of the 32nd International Conference on International Conference on Machine Learning, Lille, France, 6–11 July 2015; pp.",
"Dong, K.; Eriksson, D.; Nickisch, H.; Bindel, D.; Wilson, A.G. Scalable Log Determinants for Gaussian Process Kernel Learning.",
"In Proceedings of the 31st Conference on Neural Information Processing Systems, Long Beach, CA, USA, 4–9 December 2017; pp.",
"Zhang, Y.; Leithead, W.E.",
Approximate implementation of the logarithm of the matrix determinant in Gaussian process regression.,
"2007, 77, 329–348.",
"Hernández-Lobato, J.M.",
"; Hoffman, M.W.",
"; Ghahramani, Z. Predictive entropy search for efﬁcient global optimization of black-box functions.",
"In Proceedings of the 27st Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8–13 December 2014; pp.",
"Wang, Z.; Jegelka, S. Max-value Entropy Search for Efﬁcient Bayesian Optimization.",
"arXiv 2017, doi:arXiv:1703.01968.",
"Ru, B.; McLeod, M.; Granziol, D.; Osborne, M.A.",
Fast Information-theoretic Bayesian Optimisation.,
"In Proceedings of the 2018 International Conference on Machine Learning, Stockholm, Sweden, 10–15 July 2018; pp.",
"; Roberts, S.J.",
A tutorial on variational Bayesian inference.,
"2012, 38, 85–95.",
"Kulesza, A. Determinantal Point Processes for Machine Learning.",
Trends Mach.,
"2012, 5, 123–286.",
"Pressé, S.; Ghosh, K.; Lee, J.; Dill, K.A.",
Principles of Maximum Entropy and Maximum Caliber in Statistical Physics.,
"2013, 85, 1115–1141.",
"Jaynes, E.T.",
Information Theory and Statistical Mechanics.,
"1957, 106, 620–630.",
"Gifﬁn, A.; Cafaro, C.; Ali, S.A.",
Application of the Maximum Relative Entropy method to the Physics of Ferromagnetic Materials.,
"2016, 455, 11 – 26.",
"Neri, C.; Schneider, L. Maximum Entropy Distributions inferred from Option Portfolios on an Asset.",
Finance Stoch.,
"2012, 16, 293–318.",
"Bretthorst, G.L.",
The maximum entropy method of moments and Bayesian probability theory.,
"2013, 1553, 3–15.",
"Beal, M.J. Variational algorithms for approximate Bayesian inference.",
"Master’s Thesis, University of London, London, UK, 2003.",
"Caticha, A. Entropic Inference and the Foundations of Physics (monograph commissioned by the 11th Brazilian Meeting on Bayesian Statistics–EBEB-2012, 2012.",
"; Vandenberghe, L. Convex Optimization; Cambridge University Press, Cambridge, UK, 2009.",
"Hutchinson, M.F.",
A stochastic estimator of the trace of the inﬂuence matrix for laplacian smoothing splines.,
"1990, 19, 433–450.",
"Skilling, J.",
The eigenvalues of mega-dimensional matrices.,
"In Maximum Entropy and Bayesian Methods; Springer, Berlin, Germany, 1989; pp.",
"Friedman, J.; Hastie, T.; Tibshirani, R. Sparse inverse covariance estimation with the graphical lasso.",
"Biostatistics 2008, 9, 432–441.",
"Rasmussen, C.E.",
"; Williams, C.K.",
"Gaussian Processes for Machine Learning; The MIT Press: Cambridge, MA, USA, 2006, pp.",
"MacKay, D.J.",
"Information Theory, Inference and Learning Algorithms; Cambridge University Press: Cambridge, UK, 2003.",
"Van Aelst, S.; Rousseeuw, P. Minimum volume ellipsoid.",
Wiley Interdiscip.,
"2009, 1, 71–82.",
"Wainwright, M.J.; Jordan, M.I.",
Log-determinant relaxation for approximate inference in discrete Markov random ﬁelds.,
IEEE Trans.,
Signal Process.,
"2006, 54, 2099–2109.",
"Gershgorin, S.A. Über die Abgrenzung der Eigenwerte einer Matrix.",
"Izvestija Akademii Nauk SSSR Serija Matematika 1931, 6, 749–754.",
"Entropy 2019, 21, 551 19 of 19 28.",
"Ubaru, S.; Chen, J.; Saad, Y.",
Fast Estimation of tr( f (A)) via Stochastic Lanczos Quadrature.,
SIAM J. Matrix Anal.,
"2016, 38, 1075–1099.",
"Granziol, D.; Roberts, S. An Information and Field Theoretic approach to the Grand Canonical Ensemble.",
"arXiv 2017, [arXiv:1703.10099].",
"Fitzsimons, J.; Granziol, D.; Cutajar, K.; Osborne, M.; Filippone, M.; Roberts, S. Entropic Trace Estimates for Log Determinants.",
"arXiv 2017, [arXiv:1704.07223].",
"Davis, T.A.",
The University of Florida sparse matrix collection.,
"2011, 38, 1.",
"Hennig, P.; Osborne, M.A.",
"; Girolami, M. Probabilistic numerics and uncertainty in computations.",
"A 2015, 471, 20150142.",
"Bergstra, J.S.",
"; Bardenet, R.; Bengio, Y.; Kégl, B. Algorithms for hyper-parameter optimization.",
"In Proceedings of the 24th International Conference on Neural Information Processing Systems, Granada, Spain, 12–15 December 2011; pp.",
"Snoek, J.; Larochelle, H.; Adams, R.P.",
Practical bayesian optimization of machine learning algorithms.,
"In Proceedings of the 25th International Conference on Neural Information Processing Systems, Lake Tahoe, NV, USA, 3–6 December 2012; pp.",
"Thornton, C.; Hutter, F.; Hoos, H.H.",
"; Leyton-Brown, K. Auto-WEKA: Combined selection and hyperparameter optimization of classiﬁcation algorithms.",
"In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, IL, USA, 11–14 August 2013; pp.",
"Hoffman, M.; Shahriari, B.; Freitas, N. On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning.",
"In Proceedings of the 17th International Conference on Artiﬁcial Intelligence and Statistics, Reykjavik, Iceland, 22–24 April 2014; pp.",
"Lizotte, D.J.",
"; Wang, T.; Bowling, M.H.",
"; Schuurmans, D. Automatic Gait Optimization with Gaussian Process Regression.",
"IJCAI 2007, 7, 944–949.",
"Martinez-Cantin, R.; de Freitas, N.; Doucet, A.; Castellanos, J.A.",
Active policy learning for robot planning and exploration under uncertainty.,
Robotics Sci.,
"2007, 3, 321–328.",
"Azimi, J.; Jalali, A.; Fern, X.",
Hybrid batch Bayesian optimization.,
"arXiv 2012, doi:arXiv:1202.5597.",
"Brochu, E.; Cora, V.M.",
"; de Freitas, N. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning.",
"arXiv 2010, doi:arXiv:1012.2599.",
"Hennig, P.; Schuler, C.J.",
Entropy search for information-efﬁcient global optimization.,
"2012, 13, 1809–1837.",
"Hershey, J.R.; Olsen, P.A.",
Approximating the Kullback Leibler divergence between Gaussian mixture models.,
"In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Honolulu, HI, USA, 15–20 April 2007.",
"Huber, M.F.",
"; Bailey, T.; Durrant-Whyte, H.; Hanebeck, U.D.",
On entropy approximation for Gaussian mixture random vectors.,
"In Proceedings of the IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, Seoul, South Korea, 20–22 August 2008; pp.",
"Molga, M.; Smutnicki, C. Test functions for optimization needs.",
Available online: http://www.zsd.ict.pwr.,
wroc.pl/ﬁles/docs/functions.pdf (available online 30 May 2005).,
"Dixon, L.C.W.",
The global optimization problem.,
An introduction.,
Toward Glob.,
"1978, 2, 1–15.",
"Billingsley, P. Probability and Measure; Wiley: Hoboken, NJ, USA, 2012. c⃝2019 by the authors.",
"Licensee MDPI, Basel, Switzerland.",
This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).,
"Generalizing Machine Learning Evaluation through the Integration of Shannon Entropy and Rough Set Theory Olga Cherednichenko1, Dmytro Chernyshov 2, Dmytro Sytnikov2 and Polina Sytnikova2 1 University of Lyon 2, 5 avenue Mendès, Lyon, 69676, France 2 National University of RadioElectronics, Nauky ave. 14, Kharkiv, 61166, Ukraine Abstract This research paper delves into the innovative integration of Shannon entropy and rough set theory, presenting a novel approach to generalize the evaluation approach in machine learning.",
"The conventional application of entropy, primarily focused on information uncertainty, is extended through its combination with rough set theory to offer a deeper insight into data's intrinsic structure and the interpretability of machine learning models.",
"We introduce a comprehensive framework that synergizes the granularity of rough set theory with the uncertainty quantification of Shannon entropy, applied across a spectrum of machine learning algorithms.",
"Our methodology is rigorously tested on various datasets, showcasing its capability to not only assess predictive performance but also to illuminate the underlying data complexity and model robustness.",
"The results underscore the utility of this integrated approach in enhancing the evaluation landscape of machine learning, offering a multi-faceted perspective that balances accuracy with a profound understanding of data attributes and model dynamics.",
"This paper contributes a groundbreaking perspective to machine learning evaluation, proposing a method that encapsulates a holistic view of model performance, thereby facilitating more informed decision-making in model selection and application.",
"Keywords Machine learning, entropy, information theory, rough set theory, model evaluation 1 1.",
"Introduction In the evolving landscape of machine learning, the quest for robust evaluation metrics that transcend mere predictive accuracy is paramount.",
"This research delves into an innovative integration of two mathematical concepts: Shannon entropy[1] and rough set theory[2], to forge a novel pathway in machine learning evaluation.",
"Shannon entropy, a cornerstone in information theory, quantifies the uncertainty or the informational content within a system.",
"It has been extensively applied across various domains, offering insights into the unpredictability or the inherent informational richness of datasets.",
"Rough set theory, on the other hand, provides a framework to deal with vagueness and indiscernibility in data, enabling the analysis of data's granularity and the discernment of patterns within an ambiguous informational landscape[3,4].",
The intersection of these two theories presents a fertile ground for advancing machine learning evaluation.,
"Traditional metrics, while effective in gauging model performance, often overlook the nuanced interplay of data features and their collective impact on the learning process.",
"The integration of entropy and rough set theory proposes a more holistic approach, considering not just the outcome but the informational dynamics and structural intricacies of the data being processed.",
"The primary objective of this research is to establish a methodological framework that employs this integration to offer a more nuanced and comprehensive evaluation of machine COLINS-2024: 8th International Conference on Computational Linguistics and Intelligent Systems, April 12–13, 2024, Lviv, Ukraine olga.cherednichenko@univ-lyon2.fr (O. Cherednichenko); dmytro.chernyshov@nure.ua (D. Chernyshov); dmytro.sytnikov@nure.ua (D. Sytnikov); polina.sytnikova@nure.ua (P. Sytnikova) 0000-0002-9391-5220 (O. Cherednichenko); 0009-0003-2773-7467 (D. Chernyshov); 0000-0003-1240-7900 (D. Sytnikov); 0000-0002-6688-4641 (P. Sytnikova) © 2024 Copyright for this paper by its authors.",
Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).,
CEUR Workshop Proceedings ceur-ws.org ISSN 1613-0073 learning models.,
"By embedding Shannon entropy's measure of uncertainty within the granular perspective of rough set theory, this framework aims to illuminate aspects of model behavior and data structure that are typically obscured in conventional evaluations.",
"Through a comprehensive analysis, we aim to demonstrate the efficacy and applicability of our approach, culminating in a discussion of potential applications, challenges, and future directions for this line of research.",
"In doing so, this paper aspires to contribute a new lens through which machine learning models can be evaluated, enriching the toolkit available to researchers and practitioners in the field.",
Related works 2.1.,
"Entropy in machine learning Entropy, a concept from thermodynamics and information theory, plays a pivotal role in understanding the uncertainty and informational content within a dataset in machine learning.",
"Originating from Claude Shannon's seminal work in 1948[1], entropy quantifies the unpredictability or randomness of a system.",
"In the context of machine learning, it provides a measure of the impurity or diversity of the attributes or classes within a dataset.",
"Shannon's entropy, defined as: (1) , where is the probability of occurrence of the i-th element in the dataset, serves as a foundational metric in various machine learning algorithms, particularly in decision tree classifiers (Figure 1.).",
"In these algorithms, entropy helps in determining the optimal points for splitting the data, thereby enhancing the model's ability to classify or predict outcomes accurately.",
The application of entropy extends beyond tree-based models.,
"It is instrumental in feature selection, where the goal is to identify the most informative features that contribute to the predictive power of a model.",
"By evaluating the entropy of different feature subsets, machine learning practitioners can eliminate redundant or irrelevant features, simplifying the model without sacrificing performance.",
Figure 1: A visual representation of entropy in a decision tree.,
"Furthermore, entropy is employed in clustering algorithms to assess the homogeneity of clusters.",
"A lower entropy value indicates that the cluster contains predominantly similar instances, while a higher value suggests a mixture of different instances, signaling the need for further refinement in the clustering process.",
"In the realm of information theory, the concept of joint entropy and conditional entropy also provides insights into the relationships between variables.",
"Joint entropy, , quantifies the uncertainty of a pair of random variables, while conditional entropy, , measures the uncertainty of a variable given the knowledge of another.",
These metrics are crucial in understanding the dependencies and interactions among features in a dataset.,
The significance of entropy in machine learning is not just theoretical; it has practical implications in model evaluation and comparison[5].,
"By analyzing the entropy of model predictions, researchers can gain insights into the model's confidence and its ability to generalize from training to unseen data.",
"This is particularly relevant in the evaluation of probabilistic models, where entropy can indicate the model's certainty in its predictions.",
"Expanding further on the role of entropy in machine learning, it's crucial to understand its application in the context of uncertainty quantification[6] and how it guides the learning process in algorithms beyond the decision trees and clustering mentioned previously[7].",
"Entropy's role in machine learning extends into the realms of unsupervised learning, particularly in the optimization of models such as autoencoders[8] and in the evaluation of neural network architectures.",
"In autoencoders, for instance, entropy can be used to measure the effectiveness of the data compression and reconstruction process, indicating how well the network has captured the essential information of the input data[9].",
"In neural networks, entropy is a key factor in understanding and optimizing the information flow[10].",
"It can be used to analyze the layers of a network, providing insights into which layers are contributing most to the reduction in uncertainty about the output[11].",
"This can guide the design of more efficient and effective network architectures, optimizing the depth and width of the network to balance complexity and performance.",
"Additionally, entropy plays a pivotal role in reinforcement learning.",
"In environments where agents must make decisions under uncertainty, entropy can serve as a measure of the randomness in the agent's policy, providing a balance between exploration (trying new things) and exploitation (leveraging known strategies).",
"High entropy in the policy indicates more explorative behavior, which is particularly beneficial in the early stages of learning or in highly dynamic environments[12].",
"The concept of cross-entropy is also fundamental in machine learning, especially in classification tasks[13].",
"It measures the difference between two probability distributions - the true distribution and the predicted distribution, serving as a loss function in classification problems, particularly in training deep learning models.",
"By minimizing cross-entropy, models are trained to improve their predictions, aligning them more closely with the true data distribution.",
"Moreover, in the evaluation of generative models, such as Generative Adversarial Networks (GANs), entropy helps in assessing the diversity of the generated samples[14].",
"It ensures that the model generates a variety of outputs, not just replicating a subset of the training data, which is crucial for the effectiveness and realism of the generated samples.",
"In summary, entropy serves as a versatile tool in machine learning, aiding in decision-making, feature selection, model evaluation, and providing a deeper understanding of the data's inherent structure.",
Its ability to quantify uncertainty and diversity is invaluable in the quest to develop robust and interpretable machine learning models.,
"Rough set theory in data analysis Rough set theory, introduced by Zdzisław Pawlak in the early 1980s[2], provides a mathematical framework to deal with vagueness and indiscernibility in information systems.",
"It is particularly useful in the realm of data analysis for handling imprecise or incomplete information, offering a robust alternative to traditional statistical methods.",
The fundamental concept in rough set theory is the approximation sets.,
"Given an information system, any subset of the universe can be approximated using two sets, the lower and upper approximations.",
"The lower approximation of denoted as (2), is the set of all elements that are certainly in based on the available information.",
"Conversely, the upper approximation of , denoted as (3), comprises elements that possibly belong to .",
"Formally, these approximations are defined as follows: (2) (3) Here, represents the equivalence class of under an equivalence relation, which groups together indiscernible elements (elements that cannot be distinguished using the available attributes).",
"Rough set theory also introduces the concept of the boundary region, which is the set difference between the upper and lower approximations.",
"The boundary region, denoted as , represents the set of elements for which we cannot decisively determine whether they belong to or not: (4) In data analysis, these concepts allow for the classification of data into three regions: the positive, negative, and boundary regions, corresponding to the lower approximation, its complement, and the boundary region, respectively.",
One of the key strengths of rough set theory is its ability to reduce data complexity without significant loss of information[15].,
"Through attribute reduction, it identifies the essential features necessary for data classification, eliminating redundant or irrelevant attributes.",
"This process not only simplifies the data but also enhances the interpretability of the resulting models, making it a valuable tool in exploratory data analysis and decision-making.",
"In machine learning, rough set theory has been applied to various tasks, including feature selection, rule generation, and pattern recognition.",
"By providing a mechanism to deal with uncertainty and partial knowledge, it complements probabilistic and fuzzy approaches, offering a different perspective on data analysis[5].",
"In conclusion, rough set theory offers a unique lens through which to view data analysis, emphasizing granularity, discernibility, and interpretability.",
"Its integration into machine learning paves the way for more nuanced and informed approaches to model evaluation and decision- making, reinforcing the importance of understanding the intricacies of data in the era of big data and complex models.",
Figure 2: A conceptual diagram illustrating basic concepts in rough set theory.,
In conclusion the integration of Shannon entropy and rough set theory presents a significant advancement in the methodology of machine learning evaluation.,
"By merging these two concepts, researchers and practitioners can gain deeper insights into the informational dynamics of data and the performance of machine learning models.",
Method This section delineates the methods and techniques employed to integrate Shannon entropy and rough set theory for enhancing machine learning model evaluation.,
"The methodology is structured to systematically address the research problem, providing a clear path from theoretical underpinnings to practical application.",
"The integration of Shannon entropy and rough set theory represents a pioneering approach in the realm of machine learning, aiming to enhance the interpretability and efficacy of model evaluation[16].",
"While entropy measures the uncertainty or randomness in information, rough set theory provides a framework for dealing with ambiguity and granularity in data sets.",
The convergence of these two theories offers a multifaceted lens through which the complexity and structure of data can be analyzed more profoundly.,
"Shannon entropy, traditionally used to quantify the amount of information in a system, can be applied to the subsets of data delineated by rough set theory.",
"In this context, entropy can measure the information content within the boundaries of rough sets, offering insights into the distribution and significance of data attributes.",
"This application allows for a nuanced assessment of data, highlighting the interplay between various features and their impact on the information structure.",
"It has been demonstrated that by applying entropy within the framework of rough set theory yields results that resonate closely with the characteristics of the boundary region, as depicted in Figure 3.",
"This alignment underscores a pivotal aspect of our integrated approach: as the granularity decreases, the outcomes derived from the entropy calculations for granular data begin to mirror those obtained from the analysis of the boundary region in rough set theory Upon establishing the theoretical basis for integrating Shannon entropy with rough set theory, the next phase involves the practical application of these concepts to machine learning datasets.",
"The data is first subjected to a granulation process, where it is divided into subsets based on the equivalence relations dictated by rough set theory.",
"This granulation is crucial as it forms the basis for subsequent entropy calculations, allowing us to examine the data at varying levels of granularity.",
Figure 3: Boundary region and entropy change over decreasing granularity.,
"Once the data is granulated, Shannon entropy is computed for each subset to quantify the informational content present within these granules.",
"This step is pivotal as it provides a measure of the uncertainty or randomness associated with each granule, offering valuable insights into the underlying structure of the data.",
"The entropy values obtained from this process are then analyzed in conjunction with the boundary regions defined by rough set theory, enabling a comprehensive evaluation of the data's complexity and informational content.",
"Through this detailed methodological approach, the research aims to demonstrate the value of combining Shannon entropy and rough set theory in enhancing the evaluation of machine learning models, offering a new perspective that considers the intricate interplay between data complexity and model performance.",
"Experiments This section outlines the experimental framework designed to demonstrate the applicability of the proposed technique, integrating Shannon entropy and rough set theory for the evaluation of machine learning models.",
The experiments are structured to validate the methodology's effectiveness and to illustrate its potential in offering deeper insights into model performance and data complexity.,
"The experiments are conducted across a variety of datasets, selected to cover a broad spectrum of domains and complexities: 1.",
"“Titanic”[17] is a classic dataset in machine learning, the Titanic dataset includes passenger information from the ill-fated Titanic voyage.",
"The objective is to predict survival outcomes based on various features like age, sex, class, fare, and more.",
"This dataset allows us to explore how the proposed method handles binary classification tasks with relatively straightforward, structured data.",
"“Microsoft Malware Detection”[18] is a dataset that is used for detecting malware, it presents a more complex challenge with a higher dimensionality space.",
It consists of characteristics of software to determine whether it is malicious or benign.,
"The complexity and feature richness of this dataset provide an opportunity to evaluate the proposed method's effectiveness in handling intricate, high-dimensional data.",
"Figure 4: Entropy over granularity curves for “Titanic” dataset Each dataset undergoes a standard preprocessing pipeline, including data cleaning, granulation, feature scaling and the computation of entropy within the rough set framework.",
Machine learning models are then trained on these datasets to ensure a comprehensive evaluation across different types of algorithms.,
"After preprocessing each dataset, we apply the proposed entropy-rough set framework to create granulated views of the data, which then inform the training and evaluation of various machine learning models.",
"For each task—classification, regression, clustering, and dimensionality reduction—the models are evaluated using both traditional metrics and our novel entropy-rough set-based metric.",
"For granularity, we will utilize the machine learning models themselves, analyzing how they segment the dataset based on their intrinsic mechanisms of creating granular subsets.",
This evaluation will focus on understanding the models' inherent data partitioning behavior and its impact on the overall model performance.,
"The experimental results illustrate the performance trends of four different machine learning models: Decision Tree, Random Forest, Logistic Regression, and KNN, as their complexity or parameter tuning is varied.",
"Figure 5: Entropy over granularity curves for “Malaware Detection” dataset Upon closer examination, with the x-axis representing the exponential scale of data split, the trends observed in the performance of the machine learning models acquire a new dimension of interpretation, emphasizing the impact of data volume on model performance Figure 4 and 5.",
Decision tree: The initial low performance of the decision tree model at lower bits of data suggests that it is not capable of capturing essential patterns with limited information.,
"However, as the amount of data increases exponentially, the model's performance improves, suggesting that the decision tree may be capable of capturing more patterns with more information provided.",
Random forest: The gradual improvement in the random forest model's performance across an increasing volume of data implies that it is more robust to overfitting than the decision tree.,
This could be due to the model's increasing difficulty in generalizing as the data complexity grows with more bits of data.,
Logistic regression: The logistic regression model's relatively stable performance at the lower end of the data scale suggests it requires a minimal amount of data to establish its predictive patterns.,
"However, the subsequent improvement indicates that additional precision, especially when increasing exponentially, does not necessarily translate to improved performance.",
KNN: The KNN model's performance improvement with increased data bits is particularly noteworthy.,
"The model benefits from larger data volumes, possibly because more data provides a better context for its instance-based learning approach, allowing for more accurate neighborhood estimations and, consequently, better performance.",
The results were analyzed using a combination of statistical methods and qualitative assessments.,
The analysis aimed to illustrate not only the performance improvements but also how the entropy and rough set-based metrics provided deeper insights into the model's interaction with the data.,
"Significantly, the experiments showed that in cases where traditional metrics suggested multiple optimal hyperparameter configurations, the entropy and rough set-derived metrics often identified a single configuration that offered superior performance in terms of generalization, robustness, and interpretability.",
"These findings underscore the potential of the proposed method to act as a crucial decision- making tool in hyperparameter tuning, offering a more nuanced approach that goes beyond conventional performance metrics.",
The results convincingly demonstrate that the integration of Shannon entropy and rough set theory can lead to the selection of hyperparameter configurations that not only optimize predictive performance but also ensure that the model is more aligned with the underlying data structure and complexity.,
"Discussions The experimental results provide a nuanced understanding of how different machine learning models adapt to increasing volumes of data, as represented in an exponential scale of granular data.",
"This section discusses the implications of these findings, juxtaposing them with existing research to draw broader conclusions about model behavior and data scalability.",
"The findings from this research emphasize the dynamic interplay between data characteristics and model performance, underscoring the necessity for a holistic approach to machine learning that considers both the quantitative and qualitative aspects of data and algorithms.",
"Decision tree and random forest, both models demonstrate an improvement in performance with increased model capacity, which seem intuitive as more data typically aids in model training.",
This phenomenon aligns with research suggesting that decision tree-based models can benefit from complex data.,
"The random forest's more gradual improvement compared to the decision tree could be attributed to its ensemble nature, providing a built-in mechanism to combat overfitting, albeit not entirely negating the effect of data volume.",
"The stability of logistic regression at lower data volumes and its subsequent decline resonate with studies indicating that logistic regression models, being linear classifiers, have limited capacity to benefit from massive data if the underlying relationships in the data are not linear or if the additional data does not introduce new information.",
"This observation is crucial for practitioners, emphasizing the need to balance data volume with the inherent model capacity.",
"The improvement in KNN's performance with more data contrasts with the other models, highlighting its unique dependency on data volume for performance enhancement.",
"This aligns with the understanding that KNN models, which rely on neighborhood-based decision-making, inherently scale their performance with more data points, improving the model's ability to make informed predictions based on a richer context.",
The observed trends contribute to the broader discourse on the scalability of machine learning models with respect to data volume.,
Previous studies have emphasized the importance of matching model complexity with data complexity to avoid overfitting or underfitting.,
"Our findings corroborate this perspective, demonstrating that an exponential increase in data volume does not uniformly translate to linear improvement of performance.",
"Moreover, the distinctive behavior of KNN in our experiments underscores the importance of model selection in the context of data availability.",
"While some models like KNN thrive on larger datasets, others may not leverage additional data effectively after a certain threshold.",
"This observation is particularly relevant in the era of big data, where the temptation to indiscriminately increase dataset sizes is prevalent.",
The results from these experiments offer practical insights for machine learning practitioners: 1.,
Model Selection: Practitioners should carefully consider the nature of their data and the corresponding model's capacity to handle data volume when selecting a machine learning algorithm.,
"Data Preparation: The findings highlight the need for judicious data preprocessing and granulation, especially when dealing with large datasets, to ensure that models are not overwhelmed by data volume[19,20].",
"Performance Evaluation: The integration of Shannon entropy and rough set theory for model evaluation provides a novel perspective that goes beyond traditional performance metrics, offering a deeper understanding of how models interact with data.",
"The application of this method can also be instrumental in the domain of hyperparameter optimization, providing a novel perspective to evaluate and select the optimal set of hyperparameters for machine learning models.",
"In hyperparameter optimization, the objective is to find the set of hyperparameters that yields the best model performance, which can often be a challenging and computationally intensive process.",
"In this context, the proposed method can be used as an additional criterion in hyperparameter tuning algorithms, such as grid search, random search, or Bayesian optimization.",
"By evaluating the entropy and rough set-derived metrics alongside conventional performance measures, practitioners can gain deeper insights into the hyperparameter effects, potentially identifying configurations that not only optimize predictive performance but also enhance model interpretability and robustness.",
"For instance, in a scenario where multiple hyperparameter configurations result in similar accuracy, the entropy and rough set-based metrics could be the deciding factor, favoring configurations that yield models with better generalization properties or more interpretable structures.",
"This approach could lead to more informed decision-making in hyperparameter selection, ultimately resulting in models that are not only high-performing but also more aligned with the underlying data structure and complexity.",
"Incorporating this method into hyperparameter optimization processes could significantly enhance the efficiency and effectiveness of model tuning, providing a richer set of criteria to guide the search for optimal hyperparameters and contributing to the development of more sophisticated and nuanced machine learning models.",
"By applying this method, it's possible to assess the impact of different hyperparameter configurations on the model's ability to capture and utilize the information within the data.",
"This method can provide a more granular view of how changes in hyperparameters affect the model's structure and performance, beyond traditional evaluation metrics.",
"This approach offers a multifaceted perspective that enhances traditional evaluation metrics, enabling a deeper understanding of a model's interaction with data.",
"In classification tasks, this method can reveal subtle nuances in how models manage class boundaries, especially in cases of imbalanced datasets or overlapping class distributions.",
"By assessing the entropy and rough set-based metrics, we can gauge a model's ability to discern between classes effectively, not just its overall accuracy.",
This can lead to improved model designs that are more sensitive to the intrinsic complexities of the data.,
"For regression tasks, the integration of these theories helps in understanding how models cope with noise and outliers.",
"It can provide insights into the robustness of the model, indicating how changes in hyperparameters affect the model's ability to generalize from the training data to unseen data, which is crucial for predictive accuracy in real-world applications.",
"In the realm of unsupervised learning, such as clustering and dimensionality reduction, the proposed method introduces a novel approach to evaluate the quality of the clustering or the representation of data in reduced-dimensional spaces.",
"It allows us to assess whether the essential structure of the data is preserved or if important information is lost, thus guiding the tuning of hyperparameters to achieve more meaningful and interpretable results.",
"Moreover, this approach promotes the development of new hyperparameter tuning algorithms that integrate entropy and rough set-derived metrics into their optimization criteria.",
Such algorithms could potentially automate the process of finding hyperparameters that not only optimize traditional performance metrics but also ensure that the model captures the underlying data structure efficiently and effectively.,
"Conclusions This research explored the integration of Shannon entropy and rough set theory as a novel method for evaluating machine learning models, extending its application across various tasks including classification, regression, clustering, dimensionality reduction, compression, and hyperparameter optimization.",
"The experimental results demonstrated the method's potential to provide deeper insights into model performance and data structure, offering a multifaceted perspective that complements traditional evaluation metrics.",
"In classification and regression tasks, the method revealed nuanced differences in how models handle increasing data complexity and volume, highlighting the potential risks of overfitting and underfitting in models like decision trees and logistic regression.",
"For KNN, the method illustrated an improved performance with increased data, underscoring the model's dependency on data volume for its effectiveness.",
"In clustering and dimensionality reduction, the proposed approach offered a novel metric to assess the quality of clusters and the information preservation in reduced-dimensional spaces, respectively.",
These applications underscored the method's versatility and its ability to enhance the interpretability and efficacy of unsupervised learning tasks.,
"The research also highlighted the method's applicability in compression, where it can serve as a tool to evaluate the loss of information, and in hyperparameter optimization, where it provides additional criteria to guide the selection of optimal hyperparameters.",
"The integration of these concepts enhances the capacity to discern the subtle intricacies of model performance and data interaction, providing a richer, more granular perspective on machine learning efficacy and reliability.",
"This is particularly vital as the field moves towards more complex, data-driven decision-making processes, where the stakes of model accuracy and reliability are higher.",
"The ability to evaluate and fine-tune models with such precision is a crucial step forward, ensuring that machine learning systems can be trusted and relied upon in diverse applications, from healthcare to autonomous vehicles.",
"Overall, the integration of Shannon entropy and rough set theory presents a promising avenue for advancing machine learning model evaluation.",
"It not only enriches the toolkit available to practitioners and researchers but also opens up new possibilities for refining machine learning models to achieve better performance, robustness, and interpretability.",
The prospects for this line of research are expansive.,
"Future work can delve into more extensive applications, explore the integration of this method with advanced machine learning models, and investigate its potential in guiding the development of new algorithms.",
"By building on the foundational work presented here, subsequent research can further elucidate the complexities of model-data interactions, driving the evolution of machine learning towards more sophisticated and nuanced methodologies.",
"Acknowledgements The research study depicted in this paper is funded by the French National Research Agency (ANR), project ANR-19-CE23-0005 BI4people (Business intelligence for the people) References [1] C. E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal 27 (3) (1948) 379–423.",
doi:10.1002/j.1538-7305.1948.tb01338.x.,
"[2] Z. Pawlak, Rough sets, Int.",
11 (1982) 341–356.,
"[3] Z. Wang, X. Zhang, J. Deng, The uncertainty measures for covering rough set models, Soft Computing 24 (2020) 11909–11929.",
doi:10.1007/s00500-020-05098-x.,
"[4] M. A. Geetha, D. P. Acharjya, N. Ch.",
"S. N. Iyengar, Algebraic properties and measures of uncertainty in rough set on two universal sets based on multi-granulation, in: Proceedings of the 6th ACM India Computing Convention, ACM, 2013, doi: 10.1145/2522548.2523168.",
"[5] J. Xu, K. Qu, X. Meng, Y.",
"Sun, Q. Hou, Feature selection based on multiview entropy measures in multiperspective rough set, International Journal of Intelligent Systems 37 (2022) 7200– 7234. doi:10.1002/int.22878.",
"[6] J. Gawlikowski et al., A Survey of Uncertainty in Deep Neural Networks, arXiv preprint arXiv:2107.03342, 2021. doi:10.48550/ARXIV.2107.03342.",
"[7] A. R. Asadi, An Entropy-Based Model for Hierarchical Learning, arXiv preprint arXiv:2212.14681, 2022. doi:10.48550/arXiv.2212.14681.",
"[8] J. Mosiński, P. Biliński, T. Merritt, A. Ezzerg, D. Korzekwa, AE-Flow: AutoEncoder Normalizing Flow, arXiv:2312.16552, 2023. doi:10.48550/ arXiv.2312.16552.",
"[9] T. Ge, J. Hu, L. Wang, X. Wang, S.-Q.",
"Chen, F. Wei, In-context Autoencoder for Context Compression in a Large Language Model, arXiv, 2023. doi:10.48550/ARXIV.2307.06945.",
"[10] R. Shwartz-Ziv, Information Flow in Deep Neural Networks, arXiv preprint arXiv:2202.06749, 2022. doi:10.48550/arXiv.2202.06749.",
"[11] A. Thuy, D. F. Benoit, Explainability through uncertainty: Trustworthy decision-making with neural networks, European Journal of Operational Research (2023).",
doi:10.1016/j.ejor.2023.09.009.,
"[12] D. Tiapkin et al., Fast Rates for Maximum Entropy Exploration, arXiv preprint arXiv:2303.08059, 2023. doi:10.48550/ arXiv.2303.08059.",
"[13] A. Mao, M. Mohri, Y. Zhong, Cross-Entropy Loss Functions: Theoretical Analysis and Applications, arXiv (2023).",
doi:10.48550/ arXiv.2304.07288.,
"[14] D. Reshetova, Y. Bai, X. Wu, A. Ozgur, Understanding Entropic Regularization in GANs, arXiv:2111.01387, 2021. doi:10.48550/ arXiv.2111.01387.",
"[15] D. Sitnikov, O. Ryabov, An Algebraic Approach To Defining Rough Set Approximations And Generating Logic Rules, in: Data Mining V, WIT Press, 2004, pp.",
(specific page numbers if available).,
doi:10.2495/data040171.,
"[16] L. Zhao, Y. Yao, Subsethood Measures of Spatial Granules, arXiv, 2023. doi:10.48550/ arXiv.2309.02662.",
"[17] W. Cukierski, Titanic - Machine Learning from Disaster, 2012.",
URL: https://kaggle.com/competitions/titanic.,
"[18] R. Ronen, M. Radu, C. Feuerstein, E. Yom-Tov, M. Ahmadi, Microsoft Malware Classification Challenge, arXiv (2018).",
doi:10.48550/ARXIV.1802.10135.,
"[19] D. Chernyshov, D. Sytnikov, Binary classification based on a combination of rough set theory and decision trees, Innovative Technologies and Scientific Solutions for Industries, (2023).",
doi:10.30837/itssi.2023.26.087.,
"[20] G. Chiaselotti, T. Gentile, F. Infusino, Decision systems in rough set theory: A set operatorial perspective, J. Algebra Its Appl.",
18 (01) (2019) 1950004. doi:10.1142/s021949881950004x.,
"Learn to Accumulate Evidence from All Training Samples: Theory and Practice Deep Pandey 1 Qi Yu 1 Abstract Evidential deep learning, built upon belief the- ory and subjective logic, offers a principled and computationally efficient way to turn a determin- istic neural network uncertainty-aware.",
The resul- tant evidential models can quantify fine-grained uncertainty using the learned evidence.,
"To en- sure theoretically sound evidential models, the ev- idence needs to be non-negative, which requires special activation functions for model training and inference.",
"This constraint often leads to infe- rior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets.",
"To unveil the real cause of this undesired behavior, we theoreti- cally investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation func- tions create zero evidence regions, which prevent the model to learn from training samples falling into such regions.",
A deeper analysis of eviden- tial activation functions based on our theoretical underpinning inspires the design of a novel regu- larizer that effectively alleviates this fundamental limitation.,
Extensive experiments over many chal- lenging real-world datasets and settings confirm our theoretical findings and demonstrate the effec- tiveness of our proposed approach.,
"Introduction Deep Learning (DL) models have found great success in many real-world applications such as speech recognition (Kamath et al., 2019), machine translation (Singh et al., 2017), and computer vision (Voulodimos et al., 2018).",
"How- ever, these highly expressive models may easily fit the noise in the training data, which leads to overconfident predictions (Nguyen et al., 2015).",
"The challenge is further compounded when learning from limited labeled data, which is common 1Rochester Institute of Technology.",
Correspondence to: Qi Yu <qi.yu@rit.edu>.,
"Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA.",
"PMLR 202, 2023.",
Copyright 2023 by the author(s).,
"for applications from specialized domain (e.g., medicine, public safety, and military operations) where data collec- tion and annotation is highly costly.",
Accurate uncertainty quantification is essential for successful application of DL models in these domains.,
"To this end, DL models have been augmented to become uncertainty-aware (Gal & Ghahra- mani, 2016; Blundell et al., 2015; Pearce et al., 2020).",
"How- ever, commonly used extensions require expensive sampling operations (Gal & Ghahramani, 2016; Blundell et al., 2015), which significantly increase the computational costs (Lak- shminarayanan et al., 2017).",
"The recently developed evidential models bring together evidential theory (Shafer, 1976; Jøsang, 2016) and deep neural architectures that turn a deterministic neural network uncertainty-aware.",
"By leveraging the learned evidence, evi- dential models are capable of quantifying fine-grained un- certainty that helps to identify the sources of ‘unknowns’.",
"Furthermore, since only lightweight modifications are intro- duced to existing DL architectures, additional computational costs remain minimum.",
"Such evidential models have been successfully extended to classification (Sensoy et al., 2018), regression (Amini et al., 2020), meta-learning (Pandey & Yu, 2022a), and open-set recognition (Bao et al., 2021) settings.",
"Cifar100 Result Despite the attractive uncertainty quantifica- tion capacity, eviden- tial models are only able to achieve a pre- dictive performance on par with standard deep architectures in rela- tively simple learning problems.",
They suffer from a sig- nificant performance drop when facing large datasets with more complex features even in the common classification setting.,
"As shown in Figure 1, an evidential model using ReLU activation and an evidential MSE loss (Sensoy et al., 2018) only achieves 36% test accuracy on Cifar100, which is almost 40% lower than a standard model trained using softmax.",
"Additionally, most evidential models can easily break down with minor architecture changes and/or have a much stronger dependency on hyperparameter tuning to achieve reasonable predictive performance.",
The experiment section provides more details on these failure cases.,
1 arXiv:2306.11113v2 [cs.LG] 24 Jun 2023 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Figure 2.,
Visualization of zero-evidence region for evidential mod- els with ReLU activation in a binary classification setting.,
Existing models fail to learn from samples that are mapped to such zero- evidence region (shared area at the bottom left quadrant).,
"To train uncertainty-aware evidential models that can also predict well, we perform a novel theoretical analysis with a focus on the standard classification setting to unveil the underlying cause of the performance gap.",
Our theoreti- cal results show that existing evidential models learn sub- optimally compared to corresponding softmax counterparts.,
Such sub-optimal training is mainly attributed to the inher- ent learning deficiency of evidential models that prevents them from learning across all training samples.,
"More specif- ically, they are incapable to acquire new knowledge from training samples mapped to “zero-evidence regions” in the evidence space, where the predicted evidence reduces to zero.",
The sub-optimal learning phenomenon is illustrated in Figure 2 (detailed discussion is presented in Section 4.2).,
We analyze different variants of evidential models present in the existing literature and observe this limitation across all the models and settings.,
Our theoretical results inspire the design of a novel Regularized Evidential model (RED) that includes positive evidence regularization in its train- ing objective to battle the learning deficiency.,
"Our major contributions can be summarized as follows: • We identify a fundamental limitation of evidential models, i.e., lack the capability to learn from any data samples that lie in the “zero-evidence” region in the evidence space.",
• We theoretically show the superiority of evidential models with exp activation over other activation functions.,
• We conduct novel evidence regularization that enables evidential models to avoid the “zero-evidence” region so that they can effectively learn from all training samples.,
"• We carry out experiments over multiple challenging real- world datasets to empirically validate the presented theory, and show the effectiveness of our proposed ideas.",
Related Works Uncertainty Quantification in Deep Learning.,
Accu- rate quantification of predictive uncertainty is essential for development of trustworthy Deep Learning (DL) models.,
"Deep ensemble techniques (Pearce et al., 2020; Lakshmi- narayanan et al., 2017) have been developed for uncer- tainty quantification.",
An ensemble of neural networks is constructed and the agreement/disagreement across the en- semble components is used to quantify different uncertain- ties.,
"Ensemble-based methods significantly increase the number of model parameters, which are computationally expensive at both training and test times.",
"Alternatively, Bayesian neural networks (Gal & Ghahramani, 2016)(Blun- dell et al., 2015)(Mobiny et al., 2021) have been devel- oped that consider a Bayesian formalism to quantify dif- ferent uncertainties.",
"For instance, (Blundell et al., 2015) use Bayes-by-backdrop to learn a distribution over neural network parameters, whereas (Gal & Ghahramani, 2016) enable dropout during inference phase to obtain predictive uncertainty.",
Bayesian methods resort to some form of ap- proximation to address the intractability issue in marginal- ization of latent variables.,
"Moreover, these methods are also computationally expensive as they require sampling for uncertainty quantification.",
Evidential Deep Learning.,
Evidential models introduce a conjugate higher-order evidential prior for the likelihood dis- tribution that enables the model to capture the fine-grained uncertainties.,
"For instance, Dirichlet prior is introduced over the multinomial likelihood for evidential classification (Bao et al., 2021; Zhao et al., 2020), and NIG prior is in- troduced over the Gaussian likelihood (Amini et al., 2020; Pandey & Yu, 2022b) for the evidential regression models.",
"Adversarial robustness (Kopetzki et al., 2021) and calibra- tion (Tomani & Buettner, 2021) of evidential models have also been well studied.",
"Usually, these models are trained with evidential losses in conjunction with heuristic evidence regularization to guide the uncertainty behavior (Pandey & Yu, 2022a; Shi et al., 2020) in addition to reasonable gen- eralization performance.",
"Some evidential models assume access to out-of-distribution data during training (Malinin & Gales, 2019; 2018) and use the OOD data to guide the un- certainty behavior.",
"A recent survey (Ulmer, 2021) provides a thorough review of the evidential deep learning field.",
"In this work, we focus on evidential classification models and consider settings where no OOD data is used during model training to make the proposed approach more broadly applicable to practical real-world situations.",
Learning Deficiency of Evidential Models 3.1.,
Preliminaries and problem setup Standard classification models use a softmax transformation on the output from the neural network FΘ for input x to ob- tain the class probabilities in K-class classification problem.,
Such models are trained with the cross-entropy based loss.,
"2 Learn to Accumulate Evidence from All Training Samples: Theory and Practice For a given training sample (x, y), the loss is given by Lcross = − K X k=1 yk log(smk) (1) where smk is the softmax output.",
These models have achieved state-of-the-art performance on many benchmark problems.,
A detailed gradient analysis shows that they can effectively learn from all training data samples (see Ap- pendix A).,
"Nevertheless, these models lack a systematic mechanism to quantify different sources of uncertainty, a highly desired property in many real-world problems.",
Graphical model for Evidential Deep Learning Evidential classification models formulate training as an evidence acquisition process and consider a higher-order Dirichlet prior Dir(p|α) over the predictive Multino- mial distribution Mult(y|p).,
"Different from a standard Bayesian formulation which optimizes Type II Maximum Likelihood to learn the Dirichlet hyperparameter (Bishop & Nasrabadi, 2006), evidential models directly predict α using data features x and then generate the prediction y by marginalizing the Multinomial parameter p. Figure 3 de- scribes this generative process.",
Such higher-order prior en- ables the model to systematically quantify different sources of uncertainty.,
"In evidential models, the softmax layer of the standard neural networks is replaced by a non-negative activation function A, where A(x) ≥0 ∀x ∈[−∞, ∞], such that for input x, the neural network model FΘ with parameters Θ can output evidence e for different classes.",
Dirichlet prior α is evaluated as α = e+1 to ensure α ≥1.,
The trained evidential model outputs Dirichlet parameters α for input x that can quantify fine-grained uncertainties in addition to the prediction y.,
"Mathematically, for K−class classification problem, Evidence(e) = A(FΘ(x)) = A(o) (2) Dirichlet Parameter(α) = e + 1 (3) Dirichlet Strength(S) = K + K X k=1 ek (4) The activation function A(·) assumes three common forms to transform the neural network output into evidence: (1) ReLU(·) = max(0, ·), (2) SoftPlus(·) = log(1 + exp(·)), and (3) exp(·).",
Evidential models assign input sample to that class for which the output evidence is greatest.,
"Moreover, they quantify the confidence in the prediction for K class classification prob- lem through vacuity ν (i.e., measure of lack of confidence in the prediction) computed as Vacuity(ν) = K S (5) For any training sample (x, y), the evidential models aim to maximize the evidence for the correct class, minimize the evidence for the incorrect classes, and output accurate confi- dence.",
"To this end, three variants of evidential loss functions have been proposed (Sensoy et al., 2018): 1) Bayes risk with sum of squares loss, 2) Bayes risk with cross-entropy loss, and 3) Type II Maximum Likelihood loss.",
"Please refer to equations (21), (22), and (23) in the Appendix for the spe- cific forms of these losses.",
"Additionally, incorrect evidence regularization terms are introduced to guide the model to output low evidence for classes other than the ground truth class (See Appendix C for discussion on the regularization).",
"With evidential training, accurate evidential deep learning models are expected to output high evidence for the correct class, low evidence for all other classes, and output very high vacuity for unseen/out-of-distribution samples.",
"Theoretical Analysis of Learning Deficiency in Evidential Learning To identify the underlying reason that causes the perfor- mance gap of evidential models as described earlier, we consider a K class classification problem and a represen- tative evidential model trained using Bayes risk with sum of squares loss given in (21).",
We first provide an important definition that is critical for our theoretical analysis.,
Definition 1 (Zero-Evidence Region).,
A Zero-evidence sample is a data sample for which the model outputs zero evidence for all classes.,
A region in the evidence space that contains zero-evidence samples is a zero-evidence region.,
"For a reasonable evidential model, novel data samples not yet seen during training, difficult data samples, and out-of- distribution samples should become zero-evidence samples.",
"Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero.",
Consider an input x with one-hot ground truth label y.,
"Let the ground truth class index be gt, i.e., ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0.",
"Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively.",
"In this evidential model, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (6) 3 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule: ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok (7) Based on the actual form of A, we have three cases: Case I: ReLU(·) to transform logits to evidence ek = ReLU(ok) =⇒∂ek ∂ok = ( 1 if ok > 0 0 otherwise (8) For a zero-evidence sample, the logits ok satisfy the rela- tionship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 Case II: SoftPlus(·) to transform logits to evidence ek = log(exp(ok) + 1) =⇒ ∂ek ∂ok = Sigmoid(ok) (9) For a zero-evidence sample, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Case III: exp(·) to transform logits to evidence ek = exp(ok) =⇒ ∂ek ∂ok = exp(ok) = αk −1 (10) For a zero-evidence sample, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient in (7) to counterbalance these zero-approaching gradients.",
"So, for zero-evidence training samples, for any node k, ∂LMSE(x, y) ∂ok = 0 (11) Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
This implies that the evidential models fail to learn from a zero-evidence data sample.,
"For completeness, we present the analysis of standard clas- sification models in Appendix A, detailed proof of the evi- dential models trained using Bayes risk with sum of squares error along with other evidential lossses in Appendix B, and impact of incorrect evidence regularization in Appendix C. Remark: Evidential models can not learn from a train- ing sample that the model has never seen and for which the model accurately outputs “I don’t know”, i.e., ek = 0 ∀k ∈[1, K].",
Such samples are expected and likely to be present during model training.,
"However, the supervised in- formation in such training data points is completely missed by evidential models so they fail to acquire any new knowl- edge from all such training data samples (i.e., data samples in zero-evidence region of the evidence space).",
Corollary 1.,
Incorrect evidence regularization can not help evidential models learn from zero-evidence samples.,
"Intuitively, the incorrect evidence regularization encourages the model to output zero evidence for all classes other than the ground truth class and the regularization does not have any impact on the evidence for the ground truth class.",
"So, the regularization updates the model parameters such that the model is likely to map input samples closer to zero- evidence region in the evidence space.",
"Thus, the regular- ization does not address the failure of evidential models to learn from zero evidence samples.",
"For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential acti- vation function leads to a larger gradident update on the model parameters than softplus and ReLu.",
"Limited by space, we present the proof of Theorem 2 along with additional analysis in the Appendix D. The proof fol- lows the gradient analysis of the exponential, Softplus, and ReLU based models.",
It implies that the the training of evidential models is most effective with the exponential activation function.,
"Intuitively, the ReLU based activation completely destroys all the information in the negative logits, and has largest region in evidence space in which training data have zero evidence.",
"Softplus activation improves over the ReLU, and compared to ReLU, has smaller region in evidence space where training data have zero evidence.",
"However, Softplus based evidential models fail to cor- rect the acquired knowledge when the model has strong wrong evidence.",
"Moreover, these models are likely to suf- fer from vanishing gradients problem when the number of classes increases (i.e., classification problem becomes more challenging).",
"Finally, exponential activation has the smallest zero-evidence region in the evidence space without suffering from the issues of SoftPlus based evidential models.",
Avoiding Zero-Evidence Regions Through Correct Evidence Regularization We now consider an evidential model with exponential func- tion to transform the logits into evidence.,
"We propose a novel vacuity-guided correct evidence regularization term Lcor(x, y) = −λcor log(αgt −1) (12) where λcor = ν = K S represents the regularization term whose value is given by the magnitude of the vacuity output by the evidential model and αgt −1 represents the predicted evidence for the ground truth class.",
The regularization term λcor determines the relative importance of the correct 4 Learn to Accumulate Evidence from All Training Samples: Theory and Practice evidence regularization term compared to the evidential loss and incorrect evidence regularization and is treated as constant during model parameter update.,
"Correct evidence regularization Lcor(x, y) can address the issue of learning from zero-evidence train- ing samples.",
"The proposed regularization term Lcor(x, y) does not contain any evidence terms other than the evidence for the ground truth node.",
"So, the gradient of the regularization for nodes other than the ground truth node will be 0 i.e.",
"∂Lcor(x,y) ∂ok k̸=gt = 0 and there will be no update on these nodes.",
"For the ground truth node gt, ygt = 1, the gradient is given by ∂Lcor(x, y) ∂ogt = ∂  −λcor log(αgt −1)  ∂ogt (13) = −λcor ∂log(αgt −1) ∂αgt × ∂αgt ∂ogt (14) = − λcor (αgt −1)(αgt −1) = −λcor (15) The gradient value equals the magnitude of the vacuity.",
"The vacuity is bounded in the range [0, 1], and zero-evidence sample, the vacuity is maximum, leading to the greatest gradient value of ∂Lcor(x,y) ∂ogt = −1.",
"In other words, the reg- ularization encourages the model to update the parameters such that the correct evidence αgt −1 increases.",
"As the model evidence increases, the vacuity decreases, and the contribution of the regularization Lcor(x, y) is minimized.",
"Thus, the proposed regularization enables the evidential model to learn from zero-evidence samples.",
Evidential Model Training We formulate an overall objective used to train the pro- posed Regularized evidential model (RED).,
"Essentially, the evidential model is trained to maximize the correct evi- dence, minimize the incorrect evidence, and avoid the zero- evidence region during training.",
"The overall loss is L(x, y) = Levid(x, y) + η1Linc(x, y) + Lcor(x, y) (16) where Levid(x, y) is the loss based on the evidential framework given by (21), (23), or (22) (See Appendix B), Linc(x, y) represents the incorrect evidence regularization (See Appendix Section C), Lcor(x, y) represents the pro- posed novel correct evidence regularization term in (12), and η1 = λ1 × min(1.0, epoch index/10) controls the impact of incorrect evidence regularization to the overall model training.",
"In this work, we consider the forward-KL based incorrect evidence regularization given in (42) based on (Sensoy et al., 2018).",
Evidence Space Visualization Figure 4.,
Evidence space visualization to demonstrate the effec- tiveness of the proposed method.,
Figure 2 visualizes the evidence space in ReLU-based ev- idential models by considering the pre-ReLU output in a binary classification setting.,
"Ideally, all samples that belong to Class 1 should be mapped to the blue region (region of high evidence for Class 1, low evidence for all other classes), all samples that belong to Class 2 should be mapped to the red region, and all out-of distribution samples should be mapped to the zero-evidence region (no evidence for all classes).",
"To realize this goal, the models are trained using the evidential loss Levid with incorrect evidence regular- ization Linc.",
"However, there is no update to the evidential model from such samples of zero-evidence region.",
Model’s prior belief of “I don’t know” for such samples does not get updated even after being exposed to the true label.,
"For the samples with high incorrect evidence and low correct evidence, evidential model aims to correct itself.",
"However, many such samples are likely to get mapped to the zero- evidence region (as shown by blue and orange arrows in Figure 2) after which there is no update to the model.",
Such fundamental limitation holds true for all evidential models.,
The evidence space visualization for RED is shown in Figure 4 to illustrate how it addresses the above limitation.,
Cor- rect evidence regularization (indicated by green arrows) is weighted by the magnitude of the vacuity and is maximum in the zero-evidence region.,
"In this problematic region, the proposed regularization fully dominates the model update as there is no update to the model from the two loss com- ponents (Levid and Linc) in (16).",
"As the sample gets far away from the zero evidence region, the vacuity decreases proportionally, the impact of the proposed regularization to model update becomes insignificant, and the evidential losses (Levid & Linc) guide the model training.",
"In this way, RED can effectively learn from all training samples irrespective of the model’s existing evidence.",
5 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 5.,
Experiments Datasets and setup.,
"We consider the standard supervised classification problem with MNIST (LeCun, 1998), Ci- far10, and Cifar100 datasets (Krizhevsky et al., 2009), and few-shot classification with mini-ImageNet dataset (Vinyals et al., 2016).",
"We employ the LeNet model for MNIST, ResNet18 model (He et al., 2016) for Cifar10/Cifar100, and ResNet12 model (He et al., 2016) for mini-ImageNet.",
We first conduct experiments to demonstrate the learning deficiency of existing evidential models to confirm our the- oretical findings.,
We then evaluate the proposed correct evidence regularization to show its effectiveness.,
We finally conduct ablation studies to investigate the impact of evi- dential losses on model generalization and the uncertainty quantification of the proposed evidential model.,
"Limited by space, additional clarifications, experiment results includ- ing few-shot classification experiments, experiments over challenging tiny-Imagenet datasett with Swin Transformer, hyperparameter details, and discussions are presented in the Appendix.",
Learning Deficiency of Evidential Models Sensitivity to the change of the architecture.,
"We first consider a toy illustrative experiment with two frameworks: 1) standard softmax, 2) evidential learning, and experiment with the LeNet (LeCun et al., 1999) model considered in EDL (Sensoy et al., 2018) with a minor modification to the architecture: no dropout in the model.",
"To construct the toy dataset, we randomly select 4 labeled data points from the MNIST training dataset as shown in the Figure 5.",
"For the evidential model, we use ReLU to transform the network outputs to evidence, and train the model with MSE-based evidential loss (Sensoy et al., 2018) given in (21) without incorrect evidence regularization.",
We train both models using only these 4 training data points.,
Figure 6 compares the training accuracy and training loss trends of the evidential model with the standard softmax model (trained with the cross-entropy loss).,
"Before any training, both models have 0% accuracy and the loss is high as expected.",
"For the evidential model, in the first few iter- ations, the model learns from the training dataset, and the model’s accuracy increases to 50%.",
"Afterward, the eviden- tial model fails to learn as the evidential model maps two of the training data samples to the zero-evidence region.",
"Even in such a trivial setting, the evidential model fails to fit the 4 training data points showing their learning deficiency that empirically verifies the conclusion in Theorem 1.",
It is also worth noting that the range of the evidential model’s loss is significantly smaller than the standard model.,
"This is mainly due to the bounded nature of the evidential MSE loss(i.e., it is bounded in the range [0, 2]) (a detailed theoretical analy- sis of the evidential losses is provided in the Appendix).",
"In contrast, the standard model trained with cross-entropy loss GT : 3 GT : 5 GT : 2 GT : 6 Figure 5.",
Toy dataset with 4 data points.,
0 2 4 6 8 Iterations (× 10) 0.00 0.25 0.50 0.75 1.00 Accuracy Standard model Evidential model (a) Training accuracy trend 0 2 4 6 8 Iterations (× 10) 0 1 2 Loss Standard Model Evidential Model (b) Training loss trend Figure 6.,
"Training of standard and evidential models easily fits the trivial dataset, obtains near 0 loss, and perfect accuracy of 100% after a few iterations of training.",
0 25 50 75 100 Iteration 0 2 4 Evidence 3 5 2 6 Figure 7.,
"Zero-evidence trend during model training Additionally, we visualize the zero-evidence data samples for the toy dataset setting.",
We plot the total evidence for each training sample as training progresses for the first 100 iterations.,
The total evidence trend as training progresses for the first 100 iterations is shown in Figure 7.,
"The ev- idential model’s predictions are correct for data samples with ground truth labels of 3 and 6, and incorrect for the remaining two data samples.",
"After few iterations of training, the remaining two samples have zero total evidence (i.e.",
"samples are mapped to zero evidence region), the model never learns from them, and the model only achieves overall 50% training accuracy even after 100 iterations.",
"Clearly, the evidential model continues to output zero evidence for two of the training examples and fails to learn from them.",
Such learning deficiency of evidential models limits their extension to challenging settings.,
"In contrast, the standard model easily overfits the 4 training examples and achieves 100% accuracy.",
Sensitivity to hyperparameter tuning.,
"In this experi- ment, evidential models are trained using evidential losses given in (21), (22), or (23) with incorrect evidence regular- ization to guide the model for accurate uncertainty quan- 6 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Figure 8.",
Impact of different incorrect evidence regularization strengths to the test set accuracy on Cifar100 dataset tification.,
We study the impact of the incorrect evidence regularization λ1 to the evidential model’s performance using Cifar100.,
The result shows that the generalization performance of evidential models is highly sensitive to λ1 values.,
"To illustrate, we consider the Type II Maximum Likelihood loss in (23) with different λ1 to control KL reg- ularization (results on other loss functions are presented in the Appendix).",
"As shown in Figure 8, when some regular- ization is introduced, evidential model’s test performance improves slightly.",
"However, when strong regularization is used, the model focuses strongly on minimizing the incor- rect evidence.",
"Such regularization causes the model to push many training samples into or close to the zero-evidence regions, which hurts the model’s learning capabilities.",
"In contrast, the proposed model can continue to learn from samples in zero-evidence regions, which shows its robust- ness to incorrect evidence regularization.",
"Moreover, our model has stable performance across all hyperparameter settings as it can effectively learn from all training samples.",
Challenging datasets and settings.,
We next consider standard classification models for the Cifar100 dataset and 1-shot classification with the mini-ImageNet dataset.,
We develop evidential extensions of the classification models using Type II Maximum Likelihood loss given in (23) with- out any incorrect evidence regularization and use ReLU to transform logits to evidence.,
"As shown in Figure 10, com- pared to the standard classification model, the evidential model’s predictive performance is sub-optimal (almost 20% lower for both classification problems).",
"This is mainly due to the fact that evidential model maps many of the training data points to zero-evidence region, which is equivalent to the model saying “I don’t know to which class this sample belongs” and stopping to learn from them.",
"Consequently, the model fails to acquire new knowledge (i.e., update itself), even after being exposed to correct supervision (the label information).",
"In these cases, instead of learning, the eviden- tial model chooses to ignore the training data on which it does not have any evidence and remains to be ignorant.",
Visualization of zero-evidence samples.,
We next show the 2-dimensional visualization of the latent representation for the randomly selected 500 training examples based on Figure 9.,
Zero-Evidence Sample Visualization (a) Cifar100 Results (b) 1-Shot Results Figure 10.,
Learning trends in complex classification problems the tSNE plot for ReLU based evidential model trained on the Cifar100 dataset with λ1 = 0.1.,
Figure 9 plot visualizes the latent embedding of zero evidence (Zero E) training sam- ples with non-zero evidence (Non-Zero E) training samples.,
"As can be seen, both zero and non-zero evidence samples ap- pear to be dispersed, overlap at different regions, and cover a large area in the embedding space.",
This further confirms the challenge of effectively learning from these samples 5.2.,
Effectiveness of the RED Evidential activation function.,
We first experiment with different activation functions for the evidential models to show the superior predictive performance and generalization capability of exp activation validating our Theorem 2.,
We consider evidential models trained with evidential log loss given by (23) in Table 1 (Additional results along with hy- perparameter details are presented in Appendix Section F).,
"As can be seen, exp activation to transform network outputs into evidence leads to superior performance compared to ReLU and Softplus based transformations.",
"Furthermore, our proposed model with correct evidence regularization further improves over the exp-based evidential models as it enables the evidential model to continue learning from zero-evidence samples.",
Classification performance comparison Model MNIST Cifar10 Cifar100 ReLU 98.19±0.08 41.43±19.60 61.27±3.79 SoftPlus 98.21±0.05 95.18±0.11 74.48±0.17 exp 98.79±0.02 95.11±0.10 76.12±0.04 RED(Ours) 99.10±0.02 95.24±0.06 76.43±0.21 We next present the test set performance change as training 7 Learn to Accumulate Evidence from All Training Samples: Theory and Practice progresses with MNIST dataset and two different evidential losses in Figure 11 where we observe similar results.,
"The exp activation shows superior performance, as it has small- est zero-evidence region, and does not suffer from many learning issues present in other activation functions.",
(a) Evidential MSE loss (b) Evidential Log loss Figure 11.,
Impact of evidential activation functions to the Test Accuracy Correct evidence regularization.,
We now study the im- pact of the proposed correct evidence regularization using the MNIST and Cifar100 classification problems.,
"We con- sider the evidential baseline model that uses exp activation to acquire evidence, and is trained with Type II Maximum Likelihood based loss with different incorrect evidence reg- ularization strengths.",
We introduce the proposed novel cor- rect evidence regularization to the model.,
"As can be seen in Figure 12, the model with correct-evidence regularization has superior generalization performance compared to the baseline evidential model.",
"This is mainly due to the fact that with proposed correct evidence regularization, the evi- dential model can also learn from the zero-evidence training samples to acquire new knowledge instead of ignoring them.",
Our proposed model considers knowledge from all the train- ing data and aims to acquire new knowledge to improve its generalization instead of ignoring the samples on which it has no knowledge.,
"Finally, even though strong incorrect evidence regularization hurts the model’s generalization, the proposed model is robust and generalizes better, empirically validating our Theorem 3.",
"Limited by space, we present additional results in Appendix F.3.2.",
Zero-evidence Sample Anaysis.,
"Similar to the toy MNIST zero-evidence analysis, we consider the Cifar100 dataset, and carry out the analysis for this complex dataset/setting.",
"Instead of focusing on a few training ex- amples, we present the average statistics of the evidence (E) for the 50,000 training samples in the 100 class classi- fication problem for a model trained for 200 epochs using a log-based evidential loss in (23) with λ1 = 1.0.",
"For ref- erence, the samples with less than 0.01 average evidence (i.e., E ≤0.01) are samples on which the model is not confident (i.e., having a high vacuity of ν ≥0.99), and are close to the ideal zero-evidence region.",
"Our proposed RED model effectively avoids such zero evidence regions, and has the lowest number of samples (i.e.",
"only 0.06% of total training dataset compared to 58.96% of SoftPlus based, (a) Trend for λ1 = 1.0 (b) Trend for λ1 = 10.0 (c) Trend for λ1 = 0.1 (d) Trend for λ1 = 1.0 Figure 12.",
"Impact of correct evidence regularization to test accu- racy: (a), (b) - MNIST Results; (c), (d) - Cifar100 Results and 100% of ReLU based evidential models) in very low evidence regions.",
Zero-Evidence Analysis for Complex Dataset-Setting Model E ≤.01 E ≤0.1 E ≤1.0 E > 1.0 ReLU 50000 50000 50000 0 SoftPlus 29483 32006 49938 62 Exp 48318 49881 49949 51 RED 30 16322 25154 24846 5.3.,
Ablation Study Impact of loss function.,
We next study the impact of the evidential loss function on the model’s performance using MNIST and CIFAR100 classification problems.,
"We consider all three activations: ReLU, SoftPlus, and exp to transform neural network outputs to evidence and carry out experiments over CIFAR100 with identical model and settings.",
"As seen in Table 3, the generalization performance of evidential model is consistently sub-optimal when trained with evidential MSE loss given by (21) compared to the two other evidential losses (22) & (23).",
This is consistent across all three evidence activation functions.,
"This is mainly due to the bounded nature of the evidential MSE loss (21): for all training samples, evidential MSE loss is bounded in the range of [0, 2].",
Type II Maximum Likelihood loss given in (23) and cross-entropy based evidential loss given in (22) show comparable empirical results.,
"Next, we consider exp activation and conduct experiments over the MNIST dataset for incorrect evidence regulariza- tion strengths of λ1 = 0&1.",
We again observe similar results where the training with the Evidential MSE loss in (21) leads to sub-optimal test performance.,
"Additional re- sults, along with theoretical analysis are presented in the Appendix.",
"In the subsequent experiments, we consider the Type II Maximum Likelihood loss (23) for evidential model training due to its simplicity and some theoretical advan- 8 Learn to Accumulate Evidence from All Training Samples: Theory and Practice tages (see Appendix E).",
We leave a thorough investigation of these two evidential losses ((22) & (23)) as future work.,
Impact of evidential losses on classification performance Loss ReLU SoftPlus exp RED(Ours) MSE(21) 31.49±0.3 15.74±0.5 42.95±0.7 75.73±0.3 CE (22) 68.62±2.4 74.44±0.1 76.23±0.1 76.35±0.1 Log(23) 61.27±3.8 74.48±0.1 76.12±0.1 76.43±0.2 (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 Figure 13.,
Impact of evidential losses on test set accuracy Figure 14.,
Accuracy-Vacuity curve Study of uncertainty information.,
We now investigate the uncertainty behavior of the proposed evidential model with Cifar100 experiments.,
We present the Accuracy- Vacuity curve for different incorrect evidence regulariza- tion strengths (λ1) in Figure 14.,
"Vacuity reflects the lack of confidence in the predictions, and the accuracy of effec- tive evidential model should increase with lower vacuity threshold.",
"Without any incorrect evidence regularization (i.e., λ1 = 0), the evidential model is highly confident on its predictions and all test samples are concentrated on the low vacuity region.",
"As the incorrect evidence regularization strength is increased, the model outputs more accurate confi- dence in the predictions.",
"Strong incorrect evidence regular- ization hurts the generalization over the test set as indicated by low accuracy when all test samples are considered (i.e., vacuity threshold of 1.0).",
"In all cases, the evidential model shows reasonable uncertainty behavior: the model’s test set accuracy increases as the vacuity threshold is decreased.",
"Next, we look at the accuracy of the evidential models on their top-K % most confident predictions over the test set.",
Table 4 shows the accuracy trend of Top-K (%) confident samples.,
Consider the most confident 20% samples (cor- responding to 2000 test samples of Cifar100 dataset).,
The proposed model leads to highest accuracy (of 99.35%) com- pared to all the models.,
Similar trend is seen for different K values where the proposed model shows comparable to superior results demonstrating its accurate uncertainty quantification capability.,
"Accuracy on Top-K% confident samples (%) Model 10% 20% 30% 50% 80% 100% ReLU 98.50 98.30 97.27 90.60 71.54 61.27 SoftPlus 99.10 98.75 98.30 95.86 85.56 74.48 exp 99.40 98.95 98.50 96.52 86.46 76.12 RED 99.60 99.35 98.83 96.24 86.38 76.43 We next consider out-of-distribution (OOD) detection ex- periments for the Cifar100-trained evidential model using SVHN dataset (as OOD) (Netzer et al., 2011).",
"As seen in Table 5, the evidential models, on average, output very high vacuity for the OOD samples, showing the potential for OOD detection.",
Out-of-Distribution sample detection Model InD Vacuity OOD Vacuity (SVHN) exp 0.3227 0.7681 RED (Ours) 0.2729 0.7552 We present the AUROC score for Cifar100 trained models with SVHN dataset test set as the OOD samples in Table 6.,
"In AUROC calculation, we use the maximum softmax score for the standard model, and predicted vacuity score for all the evidential models.",
"As can be seen, the exp-based model outperforms all other activation functions, and the proposed model RED can learn from all the training samples that leads to the best performance.",
AUROC for Cifar100-SVHN experiment Model ReLU SoftPlus Standard exp RED AUROC 0.7430 0.8058 0.8669 0.8804 0.8833 6.,
"Conclusion In this paper, we theoretically investigate the evidential mod- els to identify their learning deficiency, which makes them fail to learn from zero-evidence regions.",
We then show the superiority of the evidential model with exp evidential activation over the ReLU and SoftPlus based models.,
"We further analyze the evidential losses, and introduce a novel correct evidence regularization over the exp-based ev- idential model.",
"The proposed model effectively pushes the training samples out of the zero-evidence regions, leading to superior learning capabilities.",
We conduct extensive experi- ments that empirically validate all theoretical claims while demonstrating the effectiveness of the proposed approach.,
Acknowledgements This research was supported in part by an NSF IIS award IIS-1814450 and an ONR award N00014-18-1-2875.,
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.,
"9 Learn to Accumulate Evidence from All Training Samples: Theory and Practice References Amini, A., Schwarting, W., Soleimany, A., and Rus, D. Deep evidential regression.",
"Advances in Neural Informa- tion Processing Systems, 33:14927–14937, 2020.",
"Bao, W., Yu, Q., and Kong, Y. Evidential deep learning for open set action recognition.",
"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
"13349–13358, 2021.",
"Bishop, C. M. and Nasrabadi, N. M. Pattern recognition and machine learning, volume 4.",
"Springer, 2006.",
"Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network.",
"In International conference on machine learning, pp.",
"PMLR, 2015.",
"Charpentier, B., Z¨ugner, D., and G¨unnemann, S. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts.",
"Advances in Neural Information Processing Systems, 33:1356–1367, 2020.",
"Chen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta- baseline: Exploring simple meta-learning for few-shot learning.",
"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
"9062–9071, 2021.",
"Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks.",
"In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp.",
"Gal, Y. and Ghahramani, Z.",
Dropout as a bayesian approx- imation: Representing model uncertainty in deep learn- ing.,
"In international conference on machine learning, pp.",
"PMLR, 2016.",
"He, K., Zhang, X., Ren, S., and Sun, J.",
Deep residual learn- ing for image recognition.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.",
"770–778, 2016.",
"Huynh, E. Vision transformers in 2022: An update on tiny imagenet.",
"arXiv preprint arXiv:2205.10660, 2022.",
"Jøsang, A. Subjective logic, volume 3.",
"Springer, 2016.",
"Kamath, U., Liu, J., and Whitaker, J.",
"Deep learning for NLP and speech recognition, volume 84.",
"Springer, 2019.",
"Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.",
"arXiv preprint arXiv:1412.6980, 2014.",
"Knopp, K. Weierstrass’s factor-theorem.",
"In Theory of Functions: Part II, pp.",
"Dover, 1996.",
"Kopetzki, A.-K., Charpentier, B., Z¨ugner, D., Giri, S., and G¨unnemann, S. Evaluating robustness of predictive un- certainty estimation: Are dirichlet-based models reliable?",
"In International Conference on Machine Learning, pp.",
"PMLR, 2021.",
"Krizhevsky, A., Hinton, G., et al.",
Learning multiple layers of features from tiny images.,
"Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles.",
"Advances in neural information processing systems, 30, 2017.",
The mnist database of handwritten digits.,
http://yann.,
"com/exdb/mnist/, 1998.",
"LeCun, Y., Haffner, P., Bottou, L., and Bengio, Y. Ob- ject recognition with gradient-based learning.",
"In Shape, contour and grouping in computer vision, pp.",
"Springer, 1999.",
"Malinin, A. and Gales, M. Predictive uncertainty estima- tion via prior networks.",
"Advances in neural information processing systems, 31, 2018.",
"Malinin, A. and Gales, M. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness.",
"Advances in Neural Information Processing Systems, 32, 2019.",
"Mobiny, A., Yuan, P., Moulik, S. K., Garg, N., Wu, C. C., and Van Nguyen, H. Dropconnect is effective in modeling uncertainty of bayesian deep networks.",
"Scientific reports, 11(1):1–14, 2021.",
"Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y.",
"Reading digits in natural images with unsupervised feature learning, 2011.",
"Nguyen, A., Yosinski, J., and Clune, J.",
Deep neural net- works are easily fooled: High confidence predictions for unrecognizable images.,
"In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp.",
"427–436, 2015.",
"Pandey, D. S. and Yu, Q. Multidimensional belief quantifi- cation for label-efficient meta-learning.",
"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pp.",
"14391–14400, June 2022a.",
"Pandey, D. S. and Yu, Q. Evidential conditional neural processes.",
"arXiv preprint arXiv:2212.00131, 2022b.",
"Pearce, T., Leibfried, F., and Brintrup, A.",
Uncertainty in neural networks: Approximately bayesian ensembling.,
"In International conference on artificial intelligence and statistics, pp.",
"PMLR, 2020.",
"10 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Sensoy, M., Kaplan, L., and Kandemir, M. Evidential deep learning to quantify classification uncertainty.",
"Advances in neural information processing systems, 31, 2018.",
"Shafer, G. A mathematical theory of evidence, volume 42.",
"Princeton university press, 1976.",
"Shi, W., Zhao, X., Chen, F., and Yu, Q. Multifaceted uncertainty estimation for label-efficient deep learning.",
"Advances in neural information processing systems, 33, 2020.",
"Singh, S. P., Kumar, A., Darbari, H., Singh, L., Rastogi, A., and Jain, S. Machine translation using deep learning: An overview.",
"In 2017 international conference on computer, communications and electronics (comptelix), pp.",
"IEEE, 2017.",
"Tomani, C. and Buettner, F. Towards trustworthy predictions from deep neural networks with fast adversarial calibra- tion.",
"In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.",
"9886–9896, 2021.",
"Ulmer, D. A survey on evidential deep learning for single-pass uncertainty estimation.",
"arXiv preprint arXiv:2110.03051, 2021.",
"Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.",
Matching networks for one shot learning.,
"Advances in neural information processing systems, 29, 2016.",
"Voulodimos, A., Doulamis, N., Doulamis, A., and Protopa- padakis, E. Deep learning for computer vision: A brief review.",
"Computational intelligence and neuroscience, 2018, 2018.",
"Zhao, X., Chen, F., Hu, S., and Cho, J.-H.",
Uncertainty aware semi-supervised learning on graph data.,
"Advances in Neural Information Processing Systems, 33:12827– 12836, 2020.",
"11 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Appendix Organization of the Appendix • In Section A, we present an analysis of standard classification models trained with cross-entropy loss to show their learning capabilities.",
"• In Section B, we present a complete proof of Theorem 1 for different evidential losses that demonstrates the inability of evidential models to learn from zero-evidence samples.",
"• In Section C, we describe different incorrect evidence regularizations used in the existing literature and carry out a gradient analysis to study their impact on evidential model learning.",
"• In Section D, we present the proof for Theorem 2 that shows the superiority of exp activation over the SoftPlus and ReLU functions to transform logits to evidence.",
"• In Section E, we analyze the evidential losses that reveals the theoretical limitation of evidential models trained using Bayes risk with sum of squares loss.",
"• In Section F, we present additional experiment results, clarifications, hyperparameter details, and discuss some limitations along with possible future works.",
The source code for the experiments carried out in this work is attached in the supplementary materials and is available at the link: https://github.com/pandeydeep9/EvidentialResearch2023 A.,
Standard Classification Model Consider a standard cross-entropy based model for K−class classification.,
Let the overall network be represented by fΘ(.,
"), and let o = fΘ(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y.",
The output after the softmax layer is given by smi = exp(oi) PK k=1 exp(ok) = exp(oi) Sce (17) Where Sce = PK i=1 exp(oi).,
The model is trained with cross-entropy loss.,
"For a given sample (x, y), the loss is given by Lcross-entropy = − K X k=1 yk log(smk) = − K X k=1 h ykok −yk log  K X i=1 exp(oi) i (18) = log Sce − K X k=1 ykok (19) Now, looking at the gradient of this loss with respect to the pre-softmax values o gradk = ∂Lcross-entropy ∂ok =  1 Sce ∂Sce ∂ok −yk  = exp(ok) Sce −yk  = smk −yk (20) Analysis of the gradients For Standard Classification Model.",
"The gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as 0 ≤smk ≤1 and yk ∈{0, 1}.",
The model is updated using gradient descent based optimization objectives.,
"For input x, the neural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y̸=gt = 0.",
"When yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value.",
"Only when smi = 0, the gradient is zero, and the model is not updated.",
"In all other cases when smi ̸= 0, there is a non-zero gradient dependent on smi, and the model is updated to minimize the smi as expected.",
"12 Learn to Accumulate Evidence from All Training Samples: Theory and Practice When yi = 1, the gradient signal is gradi = smi −1 and the model optimizes the parameters to minimize this value.",
"As smi ∈[0, 1], only when the model outputs a large logit on i (corresponding to the ground truth class) and small logit for all other nodes, smi = 1, the gradient is zero, and the model is not updated.",
"In all other cases when smi < 1, there is a non-zero gradient dependent on smi and the model is updated to maximize the smi and minimize all other sm̸=i as expected.",
The gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables learning from all the training data samples.,
"B. Evidential Classification Models Theorem 1: Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero.",
"In the main paper, we considered a K−class classification problem and a representative evidential model trained using Bayes risk with sum of squares loss (Eqn.",
21) in the proof.,
"Following 3 variants of evidential losses ((Sensoy et al., 2018)) have been commonly used in evidential classification works: 1.",
"Bayes risk with sum of squares loss (i.e., Evidential MSE loss) (Zhao et al., 2020) LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (21) 2.",
"Bayes risk with cross-entropy loss (i.e., Evidential CE loss)(Charpentier et al., 2020) LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  (22) 3.",
"Type II Maximum Likelihood loss (i.e., Evidential log loss)(Pandey & Yu, 2022a) LLog(x, y) = K X k=1 yk  log(S) −log(αk)  (23) For completeness, we consider all three loss functions used in evidential classification models and carry out their analysis.",
Gradient of Evidential Activation Functions A(.),
"Three non-linear functions are proposed and commonly used in the existing literature to transform the neural network output to evidence: 1) ReLU function, 2) SoftPlus function, and 3) Exponential function.",
"In this section, we compute the gradients of the evidence output ei from these non-linear activation functions with respect to the logit input oi 1.",
"= max(0, .)",
"ek = ReLU(ok) = max(0, ok) =⇒∂ek ∂ok = ( 0 if ok ≤0 1 otherwise (24) 2.",
= SoftPlus(.),
= log(1 + exp(.)),
ek = log(exp(ok) + 1) =⇒∂ek ∂ok = 1 1 + exp(−ok) = Sigmoid(ok) (25) 3.,
ek = exp(ok) =⇒∂ek ∂ok = exp(ok) = ek = αk −1 (26) 13 Learn to Accumulate Evidence from All Training Samples: Theory and Practice B.2.,
"Evidential Model Trained using Bayes risk with sum of squares loss (i.e., Eqn.",
Consider an input x with one-hot ground truth label of y.,
Let the ground truth class be g i.e.,
"ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0.",
"Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively.",
"In this evidential framework, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) = 1 −2αgt S + P k α2 k S2 + 2 P i P j αiαj S2(S + 1) (27) = 2 −2αgt S − 2 P i P j αiαj S(S + 1) (28) Now, consider different components of the loss and compute the gradients of the components with respect to Dirichlet parameters α, ∂αgt S ∂αgt = 1 S −αgt S2 & ∂αgt S ∂α̸=gt = −αgt S2 =⇒∂αgt S ∂αk = yk S −αgt S2 The gradient of the variance term is the same for all the K Dirichlet parameters and is given by ∂ P i P j αiαj S(S+1) ∂αk = (S −αk) S(S + 1) − (2S + 1) P i P j αiαj (S2 + S)2 Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule as ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = −  2∂αk S ∂αk −2 ∂ P i P j αiαj S(S+1) ∂αk  × ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok Case I: ReLU(.)",
"to transform logits to evidence ek = ReLU(ok) = max(0, ok) =⇒∂ek ∂ok = ( 1 if ok > 0 o otherwise (29) For zero-evidence sample with ReLU(.)",
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 Case II: SoftPlus(.)",
to transform logits to evidence ek = log(exp(ok) + 1) =⇒∂ek ∂ok = Sigmoid(ok) (30) Case II: exp(.),
to transform logits to evidence ek = exp(ok) =⇒∂ek ∂ok = exp(ok) = αk −1 (31) For zero-evidence sample with SoftPlus(.),
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
For zero-evidence sample with exp(.),
"used to transform the logits to evidence, αk → 1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
29) to counterbalance these zero-approaching gradients.,
"So, for zero-evidence samples, ∂LMSE(x, y) ∂ok = 0 (32) Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
"Thus, the evidential models fail to learn from such zero-evidence samples.",
14 Learn to Accumulate Evidence from All Training Samples: Theory and Practice B.3.,
"Evidential Model Trained using Type II Maximum Likelihood formulation of Evidential loss (i.e., Eqn.",
23) Consider a K−class evidential classification model that trains the model using Type II Maximum Likelihood formulation of the evidential loss.,
"Consider an input x with one-hot ground truth label of y, PK k=1 yk = 1.",
"For this evidential framework, the Type II Maximum Likelihood loss is given by LLog(x, y) = K X k=1 yk  log(S) −log(αk)  = log S − K X k=1 yk log αk (33) Taking the gradient of the loss with the logits o, we get gradk = ∂LLog(x, y) ∂ok = 1 S ∂S ∂ok −yk 1 αk ∂αk ∂ok =  1 S −yk αk ∂ek ∂ok (34) Case I: ReLU(.)",
to transform logits to evidence For any zero-evidence sample with ReLU(.),
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LLog(x,y) ∂ok = 0 ∀k ∈[1, K] Case II: SoftPlus(.)",
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 25, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  Sigmoid(ok) (35) Case III: exp(.)",
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 26, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (ek) =  1 S −yk αk  (αk −1) (36) For zero-evidence sample with SoftPlus(.)",
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Similarly, for zero-evidence sample with exp(.)",
"used to transform the logits to evidence, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
35 and Eqn.,
36 ) to counterbalance these zero-approaching gradient terms.,
"Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
"Thus, the evidential models trained with Type II Maximum Likelihood formulation of the evidential loss fail to learn from such zero-evidence samples.",
"Evidential Model Trained using Bayes risk with cross-entropy formulation of Evidential loss (i.e., Eqn.",
22) Consider a K−class evidential classification model that trains model using Bayes risk with cross-entropy loss for evidential learning (Eqn.,
"Consider an input x with one-hot ground truth label of y, PK k=1 yk = 1.",
"For this evidential framework, the loss is given by LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  = Ψ(S) −Ψ(αgt) (37) Where αgt represents the output Dirichlet parameter for the ground truth class i.e.",
"ygt = 1, y̸=gt = 0, and Ψ(.)",
"represents the Digamma function, and for z ≥1, is given by Ψ(z) = d dz log Γ(z) = d dz  −γz −log z + ∞ X n=1  z n −log  1 + z n  = −γ −1 z + z ∞ X n=1 1 n(n + z) 15 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Here, γ is the Euler–Mascheroni constant, and Γ(.)",
"is the gamma function, Using Weierstass’s definition of gamma function (Knopp, 1996) for values outside negative integers that is given by Γ(z) = e−γz z ∞ Y n=1  1 + z n −1 e z n Using the definition of the digamma functions, the loss updates as LCE(x, y) = Ψ(S) −Ψ(αgt) = 1 αgt −1 S + S ∞ X n=1 1 n(n + S) −αgt ∞ X n=1 1 n(n + αgt) (38) The derivative of the digamma function is bounded and is given by ∂Ψ(z) ∂z = ∂ ∂z  −γ −1 z + ∞ X n=1 1 n − 1 n + z  = 1 z2 + ∞ X n=1 1 (n + z)2 1 z2 <∂Ψ(z) ∂z < 1 z2 + π2 6 , z ≥1 With this, we can compute the gradients of the loss with respect to the logits as gradk = ∂LCE(x, y) ∂ok = ∂ ∂αk  Ψ(S) −Ψ(αgt) ∂αk ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2 ∂ek ∂ok (39) Case I: ReLU(.)",
to transform logits to evidence For any zero-evidence sample with ReLU(.),
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LCE(x,y) ∂ok = 0 ∀k ∈[1, K] Case II: SoftPlus(.)",
to transform logits to evidence.,
Considering Eqn.,
"25 and Eqn 39, the gradient of the loss with respect to the logits becomes gradk = ∂LCE(x, y) ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2  Sigmoid(ok) (40) Case III: exp(.)",
to transform logits to evidence.,
Considering Eqn.,
"26 and Eqn 39, the gradient of the loss with respect to the logits becomes gradk = ∂LCE(x, y) ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2  (αk −1) (41) For zero-evidence sample with SoftPlus(.)",
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Similarly, for zero-evidence sample with exp(.)",
"used to transform the logits to evidence, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
29) to counterbalance these zero-approaching gradient terms.,
The gradient of the loss with respect to all the nodes is zero for all the considered cases.,
"Since the gradient of the loss with respect to all the nodes is zero for all three cases, there is no update to the model from such samples.",
"Thus, the evidential models fail to learn from such zero-evidence samples in all cases.",
"C. Regularization in the Evidential Classification Models Based on the evidence e, beliefs b, and the Dirichlet parameters α, various regularization terms have been introduced that aim to penalize the incorrect evidence/incorrect belief of the model, leading to the model with accurate uncertainty estimates.",
"Here, we briefly summarize the key regurlaizations: 16 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 1.",
"Introduce a forward KL regularization term as in EDL (Sensoy et al., 2018) that regularizes the model to output no incorrect evidence.",
"LEDL reg(x, y) = KL  Dir(p|˜α)||Dir(p|1)  = log  Γ PK k=1 ˜αk Γ(K) QK k=1 Γ˜αk  + K X k=1 (˜αk −1)  ψ(˜αk) −ψ  K X j=1 ˜αj  (42) Where ˜α = y + (1 −y) ⊙α = (˜α1, ˜α2, ...˜αN) parameterize a dirichlet distribution, ˜αi=gt = 1, ˜αi = αi∀i ̸= gt.",
"Here, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., Dir(p|˜α) to be flat which is possible when there is no incorrect evidence.",
"42, we can see that the regularization term, introduces digamma functions for the loss and may require evaluation of higher-order polygamma functions for challenging problems (e.g.",
"involving bi-level optimizations as in MAML (Finn et al., 2017)).",
"Introduce an incorrect evidence regularization term as in ADL (Shi et al., 2020) that is the sum of the incorrect evidence for a sample LADL reg(x, y) = K X k=1  e ⊙(1 −y)  k = K X k=1 ek × (1 −yk) (43) Here, ⊙represents element-wise product.",
The evidence for a class ek is only restricted to be non-negative and can take large positive values leading to large variation in the overall loss.,
"Introduce incorrect belief-based regularization as in Units-ML (Pandey & Yu, 2022a) LUnits reg (x, y) = K X k=1   e S ⊙(1 −y)  k = K X k=1 ek S × (1 −yk) (44) The regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake is.",
All three regularizations aim to guide the model such that the incorrect evidence is minimized (ideally close to zero).,
These regularizations help the evidential model acquire desired uncertainty quantification capabilities in evidential models.,
Such guidance is expected to update the model such that it maps input samples near zero-evidence regions in the evidence space.,
"Thus, the regularization does not help address the issue of learning from zero-evidence samples and is likely to hurt the model’s learning capabilities.",
Gradient Analysis of the Incorrect Evidence Regularizations The regularization terms use ground truth information to consider only the incorrect evidence.,
"Thus, the gradient of the regularization loss with respect to the ground truth node αgt is 0.",
"In this analysis, we consider the gradient with respect to non-ground truth nodes i.e.",
"αk, and ok, k ̸= gt.",
Gradient for EDL regularization (Eqn.,
"42 ) LEDL reg(x, y) = KL  Dir(p|˜α)||Dir(p|1)  = log  Γ PK k=1 ˜αk Γ(K) QK k=1 Γ˜αk  + K X k=1 (˜αk −1)  ψ(˜αk) −ψ  K X j=1 ˜αj  = log Γ(S −αgt) −log Γ(K) − K X k=1 log Γ˜αk + K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  (45) 17 Learn to Accumulate Evidence from All Training Samples: Theory and Practice ∂LEDL reg(x, y) ∂αk = ∂ ∂αk  log Γ(S −αgt) −log Γ(K) − K X k=1 log Γ˜αk + K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  = ψ(S −αgt) −ψ(αk) + ∂ ∂αk  K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  = ψ(S −αgt) −ψ(αk) + ψ(αk) −ψ(S −αgt) + (αk −1) ∂ ∂αk  ψ(˜αk) −ψ(S −αgt)  = (αk −1) ∂ ∂αk  ψ(αk) −ψ(S −αgt)  = (αk −1)  ψ1(αk) −ψ1(S −αgt)  Where ψ1 is the trigamma function.",
"Further, using the definition of trigamma function, ∂LEDL reg(x, y) ∂αk = (αk −1)  ψ1(αk) −ψ1(S −αgt)  = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (46) Now, the gradients with respect to the logits ok becomes ∂LEDL reg(x, y) ∂ok = ∂LEDL reg(x, y) ∂αk ∂αk ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  × ∂ek ∂ok (47) Case I: ReLU(.)",
to transform logits to evidence.,
The gradients with respect to the logits ok for zero evidence is zero.,
"For all non-zero evidence, the gradient updates as ∂ek ∂ok = 1∀ek > 0 and ∂LEDL reg(x, y) ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (48) Now, when αk →∞, the value of the gradient ∂LEDL reg(x,y) ∂ok →0.",
There is close to zero model update from regularization for very large incorrect evidence.,
Case II: SoftPlus(.),
to transform logits to evidence.,
The gradients with respect to the logits ok is given by the sigmoid i.e.,
"∂ek ∂ok = sigmoid(ok) , limok→∞ ∂ek ∂ok = 1, and ∂LEDL reg(x, y) ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  σ(αk −1) (49) Now, similar to ReLU, when αk →∞, the value of the gradient ∂LEDL reg(x,y) ∂ok →0.",
There is close to zero model update from regularization for very large incorrect evidence.,
Case III: exp(.),
to transform logits to evidence.,
"When using exponential non-linearity to transform the neural network output to evidence, the αk is given by αk = exp(ok) + 1, ∂αk ∂ok = αk −1.",
"Now the gradients with respect to the neural network output ok becomes: ∂L2 reg(x, y) ∂ok = ∂L2 reg(x, y) ∂αk × ∂αk ∂ok = (αk −1)2  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (50) Here, the gradient values increase as αk →∞, and the gradient values do not vanish.",
"Simply, as the incorrect evidence becomes very large, the model updates also become large in the accurate direction.",
"Thus, considering Case I, II, and II, we see that the incorrect evidence-based regularization with forward KL divergence is not effective in regions of incorrect evidence when using ReLu and SoftPlus functions to transform logits to evidence.",
This issue of correcting very large incorrect evidence does not appear when using exp function to transform the logits into evidence.,
18 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 2.,
"Gradient for ADL regularization ((Shi et al., 2020) ) LADL reg(x, y) = K X k=1  e ⊙(1 −y)  k = K X k=1 ek × (1 −yk) = S −K −αgt + 1 (51) Considering the gradient of the regularization with respect to the parameters αk, k ̸= gt, and corresponding logits ok, we get ∂LADL reg(x, y) ∂αk = 1 =⇒∂LADL reg(x, y) ∂ok = ∂ek ok (52) When considering the exp function to transform logits to evidence, ∂ek ok = ek = exp(ok) and the gradient value becomes very large when the model’s predicted incorrect evidence value is large.",
This may lead to exploding gradients and stability issues in the model training.,
"For ReLU and SoftPlus functions, the gradients in positive evidence regions are ∂ek ok = 1, and ∂ek ok = σ(ok) respectably.",
"Thus, the gradient and corresponding model updates for high incorrect evidence are as desired.",
"Gradient analysis of incorrect belief regularization term as in Units-ML(Pandey & Yu, 2022a) LUnits reg (x, y) = K X k=1   e S ⊙(1 −y)  k = K X k=1 ek S × (1 −yk) = 1 S  S −K −αgt + 1  (53) The regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake which may limit its effectiveness.",
"Next, the gradient of the regularization with respect to the parameters αk, and logits ok is given by ∂LUnits reg (x, y) ∂αk = ∂  1 S  S −K −αgt + 1  ∂αk = αgt + K −1 S2 = egt + K (K + PK k=1 ek)2 (54) ∂L3 reg(x, y) ∂ok = ∂LUnits reg (x, y) ∂αk × ∂αk ∂ok = egt + K S2 × ∂ek ∂ok (55) The gradient value decreases as the number of classes K in the classification problem increases.",
"For all three transformations: ReLU, SoftPlus, and exp to transform logits to evidence, the gradients will go to zero as the incorrect evidence increases i.e.",
"ek →∞and S →∞=⇒ ∂L3 reg(x,y) ∂ok →0.",
"So, the regularization may be ineffective when the incorrect evidence is very high.",
"D. Impact of Non-linear Transformation Theorem 2: For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential activation function leads to a larger gradident update on the model parameters than softplus and ReLu.",
"Consider an evidential loss L, which is formally defined in Eqns.",
"(21), (22), and (23), is used to train the evidential model, let o, e ∈RK denote the neural network output vector before applying the activation A, and the evidence vector, respectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, ∀k ∈[K], we have: 1.",
ReLu: ∂L1 ∂w = X k ∂L1 ∂ek ∂ek ∂ok ∂ok ∂w = 0 (see Eqn.,
SoftPlus: ∂L2 ∂w = X k ∂L2 ∂ek ∂ek ∂ok ∂ok ∂w = X k ∂L2 ∂ek ∂ok ∂w Sigmoid(ok) ( see Eqn.,
"9), 19 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 3.",
Exponential: ∂L3 ∂w = X k ∂L3 ∂ek ∂ek ∂ok ∂ok ∂w = X k ∂L3 ∂ek ∂ok ∂w exp(ok) = X k ∂L3 ∂ek ∂ok ∂w {[1 + exp(ok)]Sigmoid(ok)} (see Eqn.,
"10) Thus, we have ∂L3 ∂w ≥∂L2 ∂w ≥∂L1 ∂w , which implies that A = exp leads to a larger update to the network than both Softplus and ReLu.",
This completes the proof.,
Now we carry out an analysis of the three activations.,
Analysis: Consider a representative K−class evidential classification model that trains using Type II Maximum Likelihood evidential loss.,
"Consider an input x with one-hot label of y, PK k=1 yk = 1.",
"For this evidential framework, the Type II Maximum Likelihood loss (LLog(x, y)) and its gradient with the logits o ( Eqn.",
"34) are given by LLog(x, y) = log S − K X k=1 yk log αk & gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk ∂ek ∂ok (56) Case I and II: ReLU(.)",
and SoftPlus(.),
to transform logits to evidence.,
• Zero evidence region: For ReLU(.),
"based evidential models, if the logits value for class k i.e.",
"ok is negative, then the corresponding evidence for class k i.e.",
"ek = 0, ∂ek ∂ok = 0 & gradk = ∂LLog(x,y) ∂ok = 0.",
"So, there is no update to the model through the nodes that output negative logits value.",
In the case of SoftPlus(.),
"based evidential models, there is no update to the model when training samples lie in zero-evidence regions.",
This is possible in the condition of ok →−∞.,
"In other cases, there will be some small finite small update in the accurate direction from the gradient.",
• Range of gradients: The range of gradients for both ReLU(.),
and SoftPlus(.),
based evidential models are identical.,
"Considering the gradient for the ground truth node i.e.yk = 1, the range of gradients is [ 1 K −1, 0].",
For all other nodes other than the ground truth node i.e.,
"yk = 0, the range of gradients is [0, 1 K ].",
"So, for classification problems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth class will be bounded in a small range and is likely to be very small.",
• High incorrect evidence region: If the evidence for class k is very large i.e.,
"ek →∞, then for ReLU(.",
"), ∂ek ok = 1, and for SoftPlus(.",
"), ∂ek ok = Sigmoid(ok) →1, 1 αk = 1 ek+1 →0, 1 S →0, & gradk = ∂LLog(x,y) ∂ok →0.",
"For large positive model evidence, there is no update to the corresponding node of the neural network.",
"The evidence can be further broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect evidence (corresponding to the evidence for any other class other than the ground truth class).",
"When the correct class evidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which is desired.",
"When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.",
"However, the evidential models with ReLU and Softplus fail to minimize incorrect evidence when the incorrect evidence value is large.",
These necessities the need for incorrect evidence regularization terms.,
Case III: exp(.),
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 26, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (ek) =  1 S −yk αk  (αk −1) (57) • Zero evidence region: In case of exp(.)",
"based evidential models, except in the extreme cases of αk →∞, there will be some signal to guide the model.",
In cases outside the zero-evidence region (i.e.,
"outside αk →∞), there will be some finite small update in the accurate direction from the gradient.",
"Moreover, for same evidence values, the gradient of exp based model is larger than the SoftPlus based evidential model by a factor of 1 + exp(ok).",
"Compared to SoftPlus models, the larger gradient is expected to help the model learn faster in low-evidence regions.",
"• Range of gradients: For the ground truth node i.e.yk = 1, the range of gradients is [−1, 0].",
For all nodes other than the ground truth node i.e.,
"yk = 0, the range of gradients is [0, 1].",
"Thus, the gradients are expected to be more expressive and accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models.",
20 Learn to Accumulate Evidence from All Training Samples: Theory and Practice • High evidence region: If the evidence for class k is very large i.e.,
"ek →∞, then αk −1 ≈αk and gradk = smk −yk.",
"In other words, the model’s gradient updates become identical to the standard classification model (see Section A) without any learning issues.",
"Due to smaller zero-evidence region, more expressive gradients, and no issue of learning in high incorrect evidence region, the exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based evidential models.",
"E. Analysis of Evidential Losses Here, we analyze the three variants of evidential loss.",
"As seen in Section D, exp function is expected to be superior to ReLU and SoftPlus functions to transform the logits to evidence.",
"Thus, in this section, we consider exp function to transform the logits into evidence.",
"However, the analysis holds true for all three functions.",
Bayes risk with the sum of squares loss (Eqn.,
"21) LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (58) The loss can be simplified as LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (59) = 1 −2αgt S + P k α2 k S2 + 2 P i P j αiαj S2(S + 1) (60) = 1 −2αgt S + P k α2 k + 2 P i P j αiαj S2 + 2 P i P j αiαj S2(S + 1) − 2 P i P j αiαj S2 (61) = 2 −2αgt S + 2 P i P j αiαj S2 h 1 (S + 1) −1 i (62) = 2 −2αgt S − 2 P i P j αiαj S(S + 1) (63) The range of the two components in the loss is 0 ≤2αgt S + 2 P i P j αiαj S(S+1) ≤2 and the loss is bounded in the range [0, 2].",
"In other words, the loss for any sample in the entire sample space is bounded in the range of [0, 2] no matter how severe the mistake is.",
Such bounded loss is expected to restrict the model’s learning capacity.,
Bayes risk with cross-entropy loss (Eqn.,
"22) LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  = Ψ(S) −Ψ(αgt) (64) Where Ψ(.)",
"is the Digamma function, and Γ is the gamma function.",
"The functions and their gradients are defined as Γ(z) = e−γz z ∞ Y n=1  1 + z n −1 e z n (65) Ψ(z) = d dz log Γ(z) = d dz  −γz −log z + ∞ X n=1  z n −log  1 + z n  (66) = −γ −1 z + ∞ X n=1 1 n − 1 n + z (67) ∂Ψ(z) ∂z = ∂ ∂z  −γ −1 z + ∞ X n=1 1 n − 1 n + z  = 1 z2 + ∞ X n=1 1 (n + z)2 (68) 21 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Now, the Bayes risk with cross-entropy loss becomes LCE(x, y) = Ψ(S) −Ψ(αgt) (69) = 1 αgt −1 S + S ∞ X n=1 1 n(n + S) −αgt ∞ X n=1 1 n(n + αgt) (70) Both the infinite sums (P∞ n=1 1 n(n+S) and P∞ n=1 1 n(n+αgt)) converge and lie in the range of 0 to π2 6 .",
The minimum possible value of this loss is 0 when αgt →∞&S ≈αgt.,
The maximum possible value is ∞when only S →∞.,
"The loss lies in the range [0, ∞] and is more expressive compared to MSE-based evidential loss.",
Considering the gradient of the loss with respect to the ground truth node (i.e.,
"αgt, ygt = 1), ∂LCE(x, y) ∂αgt = ∂ ∂αgt Ψ(S) −Ψ(αgt) = 1 S2 + ∞ X n=1 1 (n + S)2 −1 α2 gt − ∞ X n=1 1 (n + αgt)2 (71) As αgt < S, the gradient is always negative.",
"Thus, the model aims to maximize the correct evidence αgt.",
Considering the gradient of the loss with respect to nodes not corresponding to the ground truth (i.e.,
"αk, k ̸= gt, yk = 0), ∂LCEx, y) ∂αk = ∂ ∂αk Ψ(S) −Ψ(αgt) = ∂Ψ(S) ∂S ∂S ∂αk = 1 S2 + ∞ X n=1 1 (n + S)2 (72) ∂LCEx, y) ∂ok = ∂LCEx, y) ∂αk × αk ok =  1 S2 + ∞ X n=1 1 (n + S)2  (αk −1) (73) The gradient at nodes that do not correspond to ground truth is always non-negative.",
"However, this gradient is also minimum and 0 when S →∞& αk →∞.",
This is an undesired behavior as the model may be encouraged to always increase the evidence for all the classes.,
"Moreover, the gradient is zero and there is no update to the nodes when S →∞, & αk →∞.",
"So, the incorrect evidence regularization to penalize the incorrect evidence is essential for the evidential model trained with this loss.",
Type II Maximum Likelihood loss (Eqn.,
"23) LLog(x, y) = K X k=1 yk  log(S) −log(αk)  = log(S) −log(αgt) (74) The loss is bounded in the range of [0, ∞] as the loss is minimum and 0 when αgt →S →∞, and maximum loss when αgt << S & S →∞.",
"Thus, the loss is more expressive compared to MSE based evidential loss.",
"Now, the gradient of the loss is given by ∂LLog(x, y) ∂ok = 1 S ∂S ∂ok −yk 1 αk ∂αk ∂ok =  1 S −yk αk ∂ek ∂ok =  1 S −yk αk  (αk −1) (75) Here, when S →∞& αk →∞, the gradient becomes ∂LLog(x,y) ∂ok →(1−yk).",
"This is highly desirable behavior for the model as it aims to minimize the evidence for the incorrect class and there will be no update to the node corresponding to the ground truth class if αk = αgt, ygt = 1.",
"Thus, the Type II based issue is expected to be superior to the other two losses as the range of loss is optimal (i.e.",
"in the range [0, ∞]), and no learning issue arises for samples with high incorrect evidence.",
"F. Additional Experiments and Results We first present the details of the models, hyperparameter settings, clarification regarding dead neuron issue, and experiments used in the work in Section F.1.",
"We then present additional results and discussions, including Few-shot classification, and 200-class tiny-ImageNet Classification results, that show the effectiveness of the proposed model RED in Section F.3.",
Finally discuss some limitations and potential future works in Section F.4.,
22 Learn to Accumulate Evidence from All Training Samples: Theory and Practice F.1.,
"Hyperparameter details For Table 1 results, λ1 = 1.0 was used for MNIST experiments, λ1 = 0.1 was used for Cifar10 experiments, and λ1 = 0.001 was used for Cifar100 experiments.",
"Table 8, 9, and 10 present complete results across the hyperparameter values and experiment settings.",
"MNIST model was trained on the LeNet model (Sensoy et al., 2018) for 50 epochs, and Cifar10/Cifar100 models were trained on Resnet-18 based classifier (He et al., 2016) for 200 epochs.",
"Few-shot classification experiments were carried out with λ1 = 0.1 using Resnet-12 based classifier (Chen et al., 2021).",
All results presented in this work are from local reproduction.,
"MNIST models were trained with learning rate of 0.0001 and Adam optimizer (Kingma & Ba, 2014), and all remaining models were trained with learning rate of 0.1 and Stochastic Gradient Descent optimizer with momentum.",
Tabular results represent the mean and standard deviation from 3 independent runs of the model.,
"In the proposed model RED, correct evidence regularization is weighted by the parameter λcor whose value is given by the predicted vacuity ν. λcor is treated as hyperparameter, i.e., constant weighting term in the loss during model update.",
"Dead Neuron Issue Clarification Instead of using ReLU as an activation function in a standard deep neural network, evidential models introduce ReLU as non-negative transformation function in the output layer to ensure that the predicted evidence is non-negative to satisfy the requirement of evidential theory.",
"This non-negative evidence vector parameterizes a Dirichlet prior for fine-grained uncertainty quantification that covers second-order uncertainty, including vacuity and dissonance.",
We theoretically and empirically show the learning deficiency of ReLU based evidential models and justify the advantage of using an exponential function to output (non-negative) evidence.,
We further introduce a correct evidence regularization term in the loss that addresses the learning deficiency from zero-evidence samples.,
"The “dead neuron” issue in the activation functions has been studied, and ReLU variations such as Exponential Linear Unit, Parametric ReLU, and Leaky ReLU have been developed to address the issue.",
"But, these activation functions will not be theoretically sound in the evidential framework as they are can lead to negative evidences.",
"In this case, they can not serve as Dirichlet parameters that are interpreted as pseudo counts.",
Effectiveness of Regularized Evidential Model (RED) F.3.1.,
EVIDENTIAL ACTIVATION FUNCTION.,
"In this section, we present additional results (for section 5.2) with the MNIST classification problem using the LeNet model to empirically validate Theorem 2.",
"We carry out experiments for evidential models trained using all three evidential losses: Evidential MSE loss in (21), Evidential cross-entropy loss in (22), and Evidential Log loss in (23) with λ1 = {0.0, 1.0, &10.0}.",
"As can be seen in Figure 15, 16, and 17, using exp activation for transforming logits to evidence leads to superior performance in all settings compared to ReLU and Softplus based evidential models that empirically validates Theorem 2.",
(a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 15.,
Impact of Evidential Activation to the test set accuracy of the model trained with MSE based evidential loss (Eqn.,
CORRECT EVIDENCE REGULARIZATION We introduce the novel correct evidence regularization term to train the evidential model (Section 4.1).,
"In this section, we present additional results for the evidential model that uses exp activation.",
"We trained the model using evidential losses with different incorrect evidence regularization strengths ( λ1 = 0, 1.0 & 10.0).",
"As can be seen( Figure 18, and 19), the model with proposed correct-evidence regularization leads to improved generalization compared to the baseline model 23 Learn to Accumulate Evidence from All Training Samples: Theory and Practice (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 16.",
Impact of Evidential Activation to test set accuracy of the model trained with cross-entropy based evidential loss (Eqn.,
22) (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 17.,
Impact of Evidential Activation to the test set accuracy of the model trained with Type II based evidential loss (Eqn.,
23) as the proposed correct-evidence regularization term enables the evidential model to learn from zero-evidence samples instead of ignoring them.,
"Moreover, even though strong incorrect evidence regularization hurts both model’s generalization, the proposed regularization leads to a more robust model that generalizes better.",
"Finally, the MSE-based evidential model is hurt the most with strong incorrect evidence regularization as thee MSE based evidential loss is bounded in the range [0, 2], and the incorrect evidence-regularization term may easily dominate the overall loss compared to other evidential losses.",
This can be seen in Figure 18(c) where the incorrect evidence regularization strength is large i.e.,
λ1 = 10.0 and the evidential model fails to train.,
"Due to strong incorrect evidence regularization, the model may have learned to map all training samples to zero-evidence region.",
"However, with the proposed regularization, the model continues to learn and achieves good generalization performance.",
(a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 18.,
Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidential model( Trained with Eqn.,
FEW-SHOT CLASSIFICATION EXPERIMENTS Ideas presented in this work address the fundamental limitation of evidential classification framework that enables the evidential model to acquire knowledge from all the training samples.,
"Using these ideas, evidential framework can be extended to challenging classification problems to the reasonable predictive performance.",
"To this end, we experiment with few-shot classification using 1-shot and 5-shot classification for the mini-ImageNet dataset (Vinyals et al., 2016).",
We 24 Learn to Accumulate Evidence from All Training Samples: Theory and Practice (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 19.,
Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidnetial model (Trained with Eqn.,
"22) consider the ResNet-12 backbone, classifier-baseline model (Chen et al., 2021), and its evidential extension.",
Table 7 shows the results for 1-shot and 5-shot classification experiments.,
"As can be seen, the ReLU and Softplus based evidential models have suboptimal performance as they avoid many training samples of the zero-evidence region.",
"In contrast, the exp model has a better learning capacity that leads to superior performance.",
"Finally, the proposed model RED can learn from all training samples, which leads to the best generalization performance among all the evidential models.",
Few-Shot Classification Accuracy comparison: mini-ImageNet dataset Standard CE Model: 1 Shot: 57.9±0.2%; 5-Shot: 76.9±0.2% 1-Shot Experiments Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 38.78±3.75 51.60±0.40 57.11±0.09 56.27±0.15 λ1 = 0.100 31.15±1.69 48.87±0.21 56.43±0.03 58.03±0.39 λ1 = 1.000 20.00±0.00 43.81±0.56 27.43±0.88 54.68±0.45 5-Shot Experiments Regularization ReLU SoftPlus exp Ours λ1 = 0.000 52.66±5.32 67.22±0.17 75.87±0.09 75.31±0.13 λ1 = 0.100 43.95±3.72 66.14±0.05 74.08±0.13 76.05±0.17 λ1 = 1.000 20.00±0.00 61.96±0.61 34.01±1.46 72.32±0.20 F.3.4.,
"COMPLEX DATASET/MODEL EXPERIMENTS We also carry out experiment for a challenging 200-class classification problem over Tiny-ImageNet based on (Huynh, 2022).",
"We adapt the Swin Transformer to be evidential, and train all the models for 20 epochs with Evidential log loss (Eqn.",
"In this setting, ReLU based evidential model achieves 85.25% accuracy, softplus based model achieves 85.15 % accuracy, the exponential model improves over both to achieve 89.93 % accuracy, and our proposed model RED outperforms all the evidential models to achieve the greatest accuracy of 90.14%, empirically validating our theoretical analysis.",
Limitations and Future works We carried out a theoretical investigation of the Evidential Classification models to identify their fundamental limitation: their inability to learn from zero evidence regions.,
The empirical study in this work is based on classification problems.,
We next plan to extend the ideas to develop Evidential Segmentation and Evidential Object Detection models.,
"Moreover, this work identifies limitations of Evidential MSE loss in (21), and we plan to carry out a thorough theoretical analysis to analyze other evidential losses given in (23) and (22)).",
"The proposed evidential model, similar to existing evidential classification models, requires hyperparameter tuning for λ1 i.e.",
the incorrect evidence regularization hyperparameter.,
"In addition, extending evidential models to noisy and incomplete data settings and investigating the benefits of leveraging uncertainty information could be interesting future work.",
"Finally, It will be an interesting future work to extend the analysis and evidential models to tasks beyond classification, for instance to build effective evidential segmentation and object detection models.",
25 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Table 8.,
Classification performance comparison: MNIST dataset Standard CE Model: 99.21±0.03% Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 97.06±0.19 97.07±0.24 98.85±0.03 98.82±0.04 λ1 = 1.000 98.19±0.08 98.21±0.05 98.79±0.02 99.10±0.02 λ1 = 10.000 83.17±4.54 80.37±18.70 98.14±0.07 98.84±0.03 Evidential CE loss λ1 = 0.000 97.03±0.21 97.09±0.21 98.84±0.02 98.81±0.01 λ1 = 1.000 98.27±0.02 98.36±0.02 98.87±0.03 99.12±0.02 λ1 = 10.000 97.46±1.02 97.14±1.42 98.31±0.07 98.84±0.04 Evidential MSE loss λ1 = 0.000 96.18±0.02 96.20±0.03 98.42±0.03 98.41±0.06 λ1 = 1.000 97.41±0.22 97.45±0.16 98.35±0.05 99.02±0.00 λ1 = 10.000 19.93±6.98 27.14±6.37 27.17±3.72 98.76±0.03 Table 9.,
Classification performance comparison: Cifar10 Dataset Standard CE Model: 95.43±0.02% Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 43.83±14.60 95.19±0.10 95.35±0.02 95.03±0.14 λ1 = 0.100 41.43±19.60 95.18±0.11 95.11±0.10 95.24±0.06 λ1 = 1.000 38.42±15.64 94.94±0.22 93.95±0.06 94.78±0.17 λ1 = 10.000 10.00±0.00 32.42±6.99 23.29±5.24 90.96±0.35 λ1 = 50.000 10.00±0.00 10.00±0.00 12.47±3.49 65.09±0.74 Evidential CE loss λ1 = 0.000 79.19±16.06 95.32±0.17 95.38±0.10 95.40±0.14 λ1 = 0.100 75.97±20.56 95.12±0.05 95.33±0.03 95.08±0.07 λ1 = 1.000 75.83±20.74 94.99±0.08 94.65±0.04 94.74±0.11 λ1 = 10.000 10.00±0.00 89.63±0.38 56.54±4.80 91.71±0.23 λ1 = 50.000 10.00±0.00 27.03±2.62 25.33±6.66 62.98±0.84 Evidential MSE loss λ1 = 0.000 95.43±0.05 95.35±0.15 95.10±0.04 94.92±0.12 λ1 = 0.100 95.15±0.10 95.04±0.05 95.14±0.03 95.03±0.13 λ1 = 1.000 49.68±29.48 93.51±0.03 18.98±1.82 94.90±0.20 λ1 = 10.000 10.00±0.00 10.00±0.00 10.00±0.00 90.15±0.71 λ1 = 50.000 10.00±0.00 10.00±0.00 10.00±0.00 27.11±24.20 26 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Table 10.,
Classification performance comparison: Cifar100 dataset Standard CE Model: 75.67 ± 0.11 Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 56.69±5.83 73.85±0.20 76.25±0.16 76.26±0.27 λ1 = 0.001 61.27±3.79 74.48±0.17 76.12±0.04 76.43±0.21 λ1 = 0.010 54.20±5.93 75.56±0.43 76.02±0.16 76.14±0.09 λ1 = 0.100 20.29±4.54 75.67±0.22 72.72±0.26 74.62±0.21 λ1 = 1.000 1.00±0.00 37.60±0.82 2.59±0.52 68.62±0.03 λ1 = 2.000 1.00±0.00 1.57±0.35 0.97±0.06 62.33±0.52 Evidential CE loss λ1 = 0.000 66.37±3.47 73.73±0.38 75.91±0.20 76.19±0.22 λ1 = 0.001 68.62±2.41 74.44±0.08 76.23±0.09 76.35±0.06 λ1 = 0.010 71.94±0.66 75.45±0.12 75.95±0.14 76.13±0.24 λ1 = 0.100 67.25±1.84 75.75±0.21 74.02±0.09 74.69±0.13 λ1 = 1.000 1.00±0.00 73.10±0.20 37.36±0.73 69.40±0.16 λ1 = 2.000 1.00±0.00 52.99±0.56 12.94±1.11 63.93±0.34 Evidential MSE loss λ1 = 0.000 35.76±2.81 20.45±1.41 75.70±0.47 75.55±0.24 λ1 = 0.001 31.49±0.31 15.74±0.47 42.95±0.76 75.73±0.27 λ1 = 0.010 13.60±2.44 1.00±0.00 1.00±0.00 75.35±0.16 λ1 = 0.100 1.00±0.00 1.00±0.00 1.00±0.00 74.00±0.13 λ1 = 1.000 1.00±0.00 1.00±0.00 1.00±0.00 66.61±0.46 λ1 = 2.000 1.00±0.00 1.00±0.00 1.00±0.00 63.01±0.83 27,
The Modern Mathematics of Deep Learning∗ Julius Berner† Philipp Grohs‡ Gitta Kutyniok§ Philipp Petersen‡ Abstract We describe the new ﬁeld of mathematical analysis of deep learning.,
This ﬁeld emerged around a list of research questions that were not answered within the classical framework of learning theory.,
"These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which ﬁne aspects of an architecture aﬀect the behavior of a learning task in which way.",
We present an overview of modern approaches that yield partial answers to these questions.,
"For selected approaches, we describe the main ideas in more detail.",
Contents 1 Introduction 2 1.1 Notation .,
4 1.2 Foundations of learning theory .,
4 1.3 Do we need a new theory?,
17 2 Generalization of large neural networks 22 2.1 Kernel regime .,
23 2.2 Norm-based bounds and margin theory .,
24 2.3 Optimization and implicit regularization .,
25 2.4 Limits of classical theory and double descent .,
27 3 The role of depth in the expressivity of neural networks 29 3.1 Approximation of radial functions .,
29 3.2 Deep ReLU networks .,
31 3.3 Alternative notions of expressivity .,
32 4 Deep neural networks overcome the curse of dimensionality 34 4.1 Manifold assumption .,
34 4.2 Random sampling .,
35 4.3 PDE assumption .,
36 5 Optimization of deep neural networks 39 5.1 Loss landscape analysis .,
39 5.2 Lazy training and provable convergence of stochastic gradient descent .,
41 ∗A version of this review paper appears as a chapter in the book “Mathematical Aspects of Deep Learning” by Cambridge University Press.,
"†Faculty of Mathematics, University of Vienna.",
"‡Faculty of Mathematics and Research Network DataScience@UniVienna, University of Vienna.",
"§Department of Mathematics, Ludwig Maximilian University of Munich, and Department of Physics and Technology, University of Tromsø.",
1 arXiv:2105.04026v2 [cs.LG] 8 Feb 2023 6 Tangible eﬀects of special architectures 44 6.1 Convolutional neural networks .,
45 6.2 Residual neural networks .,
46 6.3 Framelets and U-Nets .,
47 6.4 Batch normalization .,
49 6.5 Sparse neural networks and pruning .,
50 6.6 Recurrent neural networks .,
52 7 Describing the features a deep neural network learns 52 7.1 Invariances and the scattering transform .,
52 7.2 Hierarchical sparse representations .,
53 8 Eﬀectiveness in natural sciences 55 8.1 Deep neural networks meet inverse problems .,
55 8.2 PDE-based models .,
56 1 Introduction Deep learning has undoubtedly established itself as the outstanding machine learning technique of recent times.,
This dominant position was claimed through a series of overwhelming successes in widely diﬀerent application areas.,
"Perhaps the most famous application of deep learning and certainly one of the ﬁrst where these techniques became state-of-the-art is image classiﬁcation [LBBH98, KSH12, SLJ+15, HZRS16].",
"In this area, deep learning is nowadays the only method that is seriously considered.",
The prowess of deep learning classiﬁers goes so far that they often outperform humans in image labelling tasks [HZRS15].,
"A second famous application area is the training of deep-learning-based agents to play board games or computer games, such as Atari games [MKS+13].",
"In this context, probably the most prominent achievement yet is the development of an algorithm that beat the best human player in the game of Go [SHM+16, SSS+17]— a feat that was previously unthinkable owing to the extreme complexity of this game.",
"Besides, even in multiplayer, team-based games with incomplete information deep-learning-based agents nowadays outperform world-class human teams [BBC+19, VBC+19].",
"In addition to playing games, deep learning has also led to impressive breakthroughs in the natural sciences.",
"For example, it is used in the development of drugs [MSL+15], molecular dynamics [FHH+17], or in high-energy physics [BSW14].",
One of the most astounding recent breakthroughs in scientiﬁc applications is the development of a deep-learning-based predictor for the folding behavior of proteins [SEJ+20].,
This predictor is the ﬁrst method to match the accuracy of lab-based methods.,
"Finally, in the vast ﬁeld of natural language processing, which includes the subtasks of understanding, summarizing, or generating text, impressive advances were made based on deep learning.",
"Here, we refer to [YHPC18] for an overview.",
"One technique that recently stood out is based on a so-called transformer neural network [BCB15, VSP+17].",
"This network structure gave rise to the impressive GPT-3 model [BMR+20] which not only creates coherent and compelling texts but can also produce code, such as, for the layout of a webpage according to some instructions that a user inputs in plain English.",
"Transformer neural networks have also been successfully employed in the ﬁeld of symbolic mathematics [SGHK18, LC19].",
"In this article, we present and discuss the mathematical foundations of the success story outlined above.",
"More precisely, our goal is to outline the newly emerging ﬁeld of mathematical analysis of deep learning.",
"To accurately describe this ﬁeld, a necessary preparatory step is to sharpen our deﬁnition of the term deep learning.",
"For the purposes of this article, we will use the term in the following narrow sense: Deep learning refers to techniques where deep neural networks1 are trained with gradient-based methods.",
"This narrow 1We will deﬁne the term neural network later but, for this deﬁnition, one can view it as a parametrized family of functions with a diﬀerentiable parametrization.",
2 deﬁnition is helpful to make this article more concise.,
"We would like to stress, however, that we do not claim in any way that this is the best or the right deﬁnition of deep learning.",
"Having ﬁxed a deﬁnition of deep learning, three questions arise concerning the aforementioned emerging ﬁeld of mathematical analysis of deep learning: To what extent is a mathematical theory necessary?",
Is it truly a new ﬁeld?,
What are the questions studied in this area?,
Let us start by explaining the necessity of a theoretical analysis of the tools described above.,
"From a scientiﬁc perspective, the primary reason why deep learning should be studied mathematically is simple curiosity.",
"As we will see throughout this article, many practically observed phenomena in this context are not explained theoretically.",
"Moreover, theoretical insights and the development of a comprehensive theory are often the driving force underlying the development of new and improved methods.",
Prominent examples of mathematical theories with such an eﬀect are the theory of ﬂuid mechanics which is an invaluable asset to the design of aircraft or cars and the theory of information that aﬀects and shapes all modern digital communication.,
"In the words of Vladimir Vapnik2: “Nothing is more practical than a good theory”, [Vap13, Preface].",
"In addition to being interesting and practical, theoretical insight may also be necessary.",
"Indeed, in many applications of machine learning, such as medical diagnosis, self-driving cars, and robotics, a signiﬁcant level of control and predictability of deep learning methods is mandatory.",
"Also, in services, such as banking or insurance, the technology should be controllable to guarantee fair and explainable decisions.",
Let us next address the claim that the ﬁeld of mathematical analysis of deep learning is a newly emerging area.,
"In fact, under the aforementioned deﬁnition of deep learning, there are two main ingredients of the technology: deep neural networks and gradient-based optimization.",
The ﬁrst artiﬁcial neuron was already introduced in 1943 in [MP43].,
This neuron was not trained but instead used to explain a biological neuron.,
The ﬁrst multi-layered network of such artiﬁcial neurons that was also trained can be found in [Ros58].,
"Since then, various neural network architectures have been developed.",
We will discuss these architectures in detail in the following sections.,
"The second ingredient, gradient-based optimization, is made possible by the observation that due to the graph-based structure of neural networks the gradient of an objective function with respect to the parameters of the neural network can be computed eﬃciently.",
"This has been observed in various ways, see [Kel60, Dre62, Lin70, RHW86].",
"Again, these techniques will be discussed in the upcoming sections.",
"Since then, techniques have been improved and extended.",
"As the rest of the manuscript is spent reviewing these methods, we will keep the discussion of literature at this point brief.",
"Instead, we refer to some overviews of the history of deep learning from various perspectives: [LBH15, Sch15, GBC16, HH19].",
"Given the fact that the two main ingredients of deep neural networks have been around for a long time, one would expect that a comprehensive mathematical theory has been developed that describes why and when deep-learning-based methods will perform well or when they will fail.",
"Statistical learning theory [AB99, Vap99, CS02, BBL03, Vap13] describes multiple aspects of the performance of general learning methods and in particular deep learning.",
We will review this theory in the context of deep learning in Subsection 1.2 below.,
"Hereby, we focus on classical, deep learning-related results that we consider well-known in the machine learning community.",
"Nonetheless, the choice of these results is guaranteed to be subjective.",
"We will ﬁnd that the presented, classical theory is too general to explain the performance of deep learning adequately.",
"In this context, we will identify the following questions that appear to be diﬃcult to answer within the classical framework of learning theory: Why do trained deep neural networks not overﬁt on the training data despite the enormous power of the architecture?",
What is the advantage of deep compared to shallow architectures?,
Why do these methods seemingly not suﬀer from the curse of dimensionality?,
"Why does the optimization routine often succeed in ﬁnding good solutions despite the non-convexity, non-linearity, and often non-smoothness of the problem?",
Which aspects of an architecture aﬀect the performance of the associated models and how?,
Which features of data are learned by deep architectures?,
Why do these methods perform as well as or better than specialized numerical tools in natural sciences?,
The new ﬁeld of mathematical analysis of deep learning has emerged around questions like the ones listed above.,
"In the remainder of this article, we will collect some of the main recent advances to answer these questions.",
"Because this ﬁeld of mathematical analysis of deep learning is incredibly active and new material is added at breathtaking speeds, a brief survey on recent advances in this area is guaranteed to miss not only 2This claim can be found earlier in a non-mathematical context in the works of Kurt Lewin [Lew43].",
3 a couple of references but also many of the most essential ones.,
"Therefore, we do not strive for a complete overview, but instead, showcase several fundamental ideas on a mostly intuitive level.",
"In this way, we hope to allow the reader to familiarize themselves with some exciting concepts and provide a convenient entry-point for further studies.",
"1.1 Notation We denote by N the set of natural numbers, by Z the set of integers and by R the ﬁeld of real numbers.",
"For N ∈N, we denote by [N] the set {1, .",
"For two functions f, g: X →[0, ∞), we write f ≲g, if there exists a universal constant c such that f(x) ≤cg(x) for all x ∈X.",
"In a pseudometric space (X, dX ), we deﬁne the ball of radius r ∈(0, ∞) around a point x ∈X by BdX r (x) or Br(x) if the pseudometric dX is clear from the context.",
"By ∥· ∥p, p ∈[1, ∞], we denote the ℓp-norm, and by ⟨·, ·⟩the Euclidean inner product of given vectors.",
By ∥· ∥op we denote the operator norm induced by the Euclidean norm and by ∥· ∥F the Frobenius norm of given matrices.,
"For p ∈[1, ∞], s ∈[0, ∞), d ∈N, and X ⊂Rd, we denote by W s,p(X) the Sobolev-Slobodeckij space, which for s = 0 is just a Lebesgue space, i.e., W 0,p(X) = Lp(X).",
"For measurable spaces X and Y, we deﬁne M(X, Y) to be the set of measurable functions from X to Y.",
"We denote by ˆg the Fourier transform3 of a tempered distribution g. For probabilistic statements, we will assume a suitable underlying probability space with probability measure P. For an X-valued random variable X, we denote by E[X] and V[X] its expectation and variance and by PX the image measure of X on X, i.e., PX(A) = P(X ∈A) for every measurable set A ⊂X.",
"If possible, we use the corresponding lowercase letter to denote the realization x ∈X of the random variable X for a given outcome.",
"We write Id for the d-dimensional identity matrix and, for a set A, we write 1A for the indicator function of A, i.e., 1A(x) = 1 if x ∈A and 1A(x) = 0 else.",
"1.2 Foundations of learning theory Before we continue to describe recent developments in the mathematical analysis of deep learning methods, we start by providing a concise overview of the classical mathematical and statistical theory underlying machine learning tasks and algorithms which, in their most general form, can be formulated as follows.",
Deﬁnition 1.1 (Learning - informal).,
"Let X, Y, and Z be measurable spaces.",
"In a learning task, one is given data in Z and a loss function L: M(X, Y) × Z →R.",
"The goal is to choose a hypothesis set F ⊂M(X, Y) and construct a learning algorithm, i.e., a mapping A: [ m∈N Zm →F, that uses training data s = (z(i))m i=1 ∈Zm to ﬁnd a model fs = A(s) ∈F that performs well on the training data s and also generalizes to unseen data z ∈Z.",
"Here, performance is measured via the loss function L and the corresponding loss L(fs, z) and, informally speaking, generalization means that the out-of-sample performance of fs at z behaves similar to the in-sample performance on s. Deﬁnition 1.1 is deliberately vague on how to measure generalization performance.",
"Later, we will often study the expected out-of-sample performance.",
"To talk about expected performance, a data distribution needs to be speciﬁed.",
We will revisit this point in Assumption 1.10 and Deﬁnition 1.11.,
"For simplicity, we focus on one-dimensional, supervised prediction tasks with input features in Euclidean space as deﬁned in the following.",
Deﬁnition 1.2 (Prediction task).,
"In a prediction task, we have that Z := X × Y, i.e., we are given training data s = ((x(i), y(i)))m i=1 that consist of input features x(i) ∈X and corresponding labels y(i) ∈Y.",
"For one-dimensional regression tasks with Y ⊂R, we consider the quadratic loss L(f, (x, y)) = (f(x) −y)2 and, 3Respecting common notation, we will also use the hat symbol to denote the minimizer of the empirical risk bfs in Deﬁnition 1.8 but this clash of notation does not cause any ambiguity.",
"4 for binary classiﬁcation tasks with Y = {−1, 1}, we consider the 0-1 loss L(f, (x, y)) = 1(−∞,0)(yf(x)).",
"We assume that our input features are in Euclidean space, i.e., X ⊂Rd with input dimension d ∈N.",
"In a prediction task, we aim for a model fs : X →Y, such that, for unseen pairs (x, y) ∈X × Y, fs(x) is a good prediction of the true label y.",
"However, note that large parts of the presented theory can be applied to more general settings.",
Remark 1.3 (Learning tasks).,
"Apart from straightforward extensions to multi-dimensional prediction tasks and other loss functions, we want to mention that unsupervised and semi-supervised learning tasks are often treated as prediction tasks.",
"More precisely, one transforms unlabeled training data z(i) into features x(i) = T1(z(i)) ∈X and labels y(i) = T2(z(i)) ∈Y using suitable transformations T1 : Z →X, T2 : Z →Y.",
"In doing so, one asks for a model fs approximating the transformation T2 ◦T −1 1 : X →Y which is, e.g., done in order to learn feature representations or invariances.",
"Furthermore, one can consider density estimation tasks, where X = Z, Y := [0, ∞], and F consists of probability densities with respect to some σ-ﬁnite reference measure µ on Z.",
One then aims for a probability density fs that approximates the density of the unseen data z with respect to µ.,
"One can perform L2(µ)- approximation based on the discretization L(f, z) = −2f(z) + ∥f∥2 L2(µ) or maximum likelihood estimation based on the surprisal L(f, z) = −log(f(z)).",
"In deep learning the hypothesis set F consists of realizations of neural networks Φa(·, θ), θ ∈P, with a given architecture a and parameter set P. In practice, one uses the term neural network for a range of functions that can be represented by directed acyclic graphs, where the vertices correspond to elementary almost everywhere diﬀerentiable functions parametrizable by θ ∈P and the edges symbolize compositions of these functions.",
"In Section 6, we will review some frequently used architectures, in the other sections, however, we will mostly focus on fully connected feedforward (FC) neural networks as deﬁned below.",
Deﬁnition 1.4 (FC neural network).,
"A fully connected feedforward neural network is given by its architecture a = (N, ϱ), where L ∈N, N ∈NL+1, and ϱ: R →R.",
"We refer to ϱ as the activation function, to L as the number of layers, and to N0, NL, and Nℓ, ℓ∈[L −1], as the number of neurons in the input, output, and ℓ-th hidden layer, respectively.",
"We denote the number of parameters by P(N) := L X ℓ=1 NℓNℓ−1 + Nℓ and deﬁne the corresponding realization function Φa : RN0 × RP (N) →RNL which satisﬁes for every input x ∈RN0 and parameters θ = (θ(ℓ))L ℓ=1 = ((W (ℓ), b(ℓ)))L ℓ=1 ∈ L× ℓ=1 (RNℓ×Nℓ−1 × RNℓ) ∼= RP (N) that Φa(x, θ) = Φ(L)(x, θ), where Φ(1)(x, θ) = W (1)x + b(1), ¯Φ(ℓ)(x, θ) = ϱ  Φ(ℓ)(x, θ)  , ℓ∈[L −1], and Φ(ℓ+1)(x, θ) = W (ℓ+1) ¯Φ(ℓ)(x, θ) + b(ℓ+1), ℓ∈[L −1], (1.1) and ϱ is applied componentwise.",
"We refer to W (ℓ) ∈RNℓ×Nℓ−1 and b(ℓ) ∈RNℓas the weight matrices and bias vectors, and to ¯Φ(ℓ) and Φ(ℓ) as the activations and pre-activations of the Nℓneurons in the ℓ-th layer.",
The width and depth of the architecture are given by ∥N∥∞and L and we call the architecture deep if L > 2 and shallow if L = 2.,
"The underlying directed acyclic graph of FC networks is given by compositions of the aﬃne linear maps x 7→W (ℓ)x + b(ℓ), ℓ∈[L], with the activation function ϱ intertwined, see Figure 1.1.",
"Typical activation 5 x1 x2 x3 Φ(1) 1 Φ(1) 2 Φ(1) 3 Φ(1) 4 x 7→W (1)x + b(1) ¯Φ(1) 1 ¯Φ(1) 2 ¯Φ(1) 3 ¯Φ(1) 4 ϱ Φ(2) 1 Φ(2) 2 Φ(2) 3 Φ(2) 4 Φ(2) 5 Φ(2) 6 x 7→W (2)x + b(2) ¯Φ(2) 1 ¯Φ(2) 2 ¯Φ(2) 3 ¯Φ(2) 4 ¯Φ(2) 5 ¯Φ(2) 6 ϱ Φa x 7→W (3)x + b(3) Figure 1.1: Graph (grey) and (pre-)activations of the neurons (white) of a deep fully connected feedforward neural network Φa : R3 × R53 7→R with architecture a = ((3, 4, 6, 1), ϱ) and parameters θ = ((W (ℓ), b(ℓ))3 ℓ=1.",
"functions used in practice are variants of the rectiﬁed linear unit (ReLU) given by ϱR(x) := max{0, x} and sigmoidal functions ϱ ∈C(R) satisfying ϱ(x) →1 for x →∞and ϱ(x) →0 for x →−∞, such as the logistic function ϱσ(x) := 1/(1+e−x) (often referred to as the sigmoid function).",
See also Table 1 for a comprehensive list of widely used activation functions.,
Remark 1.5 (Neural networks).,
"If not further speciﬁed, we will use the term (neural) network, or the abbreviation NN, to refer to FC neural networks.",
"Note that many of the architectures used in practice (see Section 6) can be written as special cases of Deﬁnition 1.4 where, e.g., speciﬁc parameters are prescribed by constants or shared with other parameters.",
"Furthermore, note that aﬃne linear functions are NNs with depth L = 1.",
"We will also consider biasless NNs given by linear mappings without bias vector, i.e., b(ℓ) = 0, ℓ∈[L].",
"In particular, any NN can always be written without bias vectors by redeﬁning x → x 1  , (W (ℓ), b(ℓ)) →  W (ℓ) b(ℓ) 0 1  , ℓ∈[L −1], and (W (L), b(L)) →  W (L) b(L) .",
"To enhance readability we will often not specify the underlying architecture a = (N, ϱ) or the parameters θ ∈ RP (N) and use the term NN to refer to the architecture as well as the realization functions Φa(·, θ): RN0 →RNL or Φa : RN0 ×RP (N) →RNL.",
"However, we want to emphasize that one cannot infer the underlying architecture or properties like magnitude of parameters solely from these functions as the mapping (a, θ) 7→Φa(·, θ) is highly non-injective.",
"As an example, we can set W (L) = 0 which implies Φa(·, θ) = b(L) for all architectures a = (N, ϱ) and all values of (W (ℓ), b(ℓ))L−1 ℓ=1 .",
"In view of our considered prediction tasks in Deﬁnition 1.2, this naturally leads to the following hypothesis sets of neural networks.",
Deﬁnition 1.6 (Hypothesis sets of neural networks).,
"Let a = (N, ϱ) be a NN architecture with input dimension N0 = d, output dimension NL = 1, and measurable activation function ϱ.",
"For regression tasks the corresponding hypothesis set is given by Fa =  Φa(·, θ): θ ∈RP (N) and for classiﬁcation tasks by Fa,sgn =  sgn(Φa(·, θ)): θ ∈RP (N) , where sgn(x) := ( 1, if x ≥0, −1, if x < 0.",
"6 Name Given as a function of x ∈R by Plot linear x Heaviside / step function 1(0,∞)(x) logistic / sigmoid 1 1+e−x rectiﬁed linear unit (ReLU) max{0, x} power rectiﬁed linear unit max{0, x}k for k ∈N parametric ReLU (PReLU) max{ax, x} for a ≥0, a ̸= 1 exponential linear unit (ELU) x · 1[0,∞)(x) + (ex −1) · 1(−∞,0)(x) softsign x 1+|x| inverse square root linear unit x · 1[0,∞)(x) + x √ 1+ax2 · 1(−∞,0)(x) for a > 0 inverse square root unit x √ 1+ax2 for a > 0 tanh ex−e−x ex+e−x arctan arctan(x) softplus ln(1 + ex) Gaussian e−x2/2 Table 1: List of commonly used activation functions.",
"7 Note that we compose the output of the NN with the sign function in order to obtain functions mapping to Y = {−1, 1}.",
This can be generalized to multi-dimensional classiﬁcation tasks by replacing the sign by an argmax function.,
"Given a hypothesis set, a popular learning algorithm is empirical risk minimization (ERM), which minimizes the average loss on the given training data, as described in the next deﬁnitions.",
Deﬁnition 1.7 (Empirical risk).,
"For training data s = (z(i))m i=1 ∈Zm and a function f ∈M(X, Y), we deﬁne the empirical risk by bRs(f) := 1 m m X i=1 L(f, z(i)).",
Deﬁnition 1.8 (ERM learning algorithm).,
"Given a hypothesis set F, an empirical risk minimization algorithm Aerm chooses4 for training data s ∈Zm a minimizer bfs ∈F of the empirical risk in F, i.e., Aerm(s) ∈arg min f∈F bRs(f).",
(1.2) Remark 1.9 (Surrogate loss and regularization).,
"Note that, for classiﬁcation tasks, one needs to optimize over non-diﬀerentiable functions with discrete outputs in (1.2).",
"For NN hypothesis sets Fa,sgn one typically uses the corresponding hypothesis set for regression tasks Fa to ﬁnd an approximate minimizer bf surr s ∈Fa of 1 m m X i=1 Lsurr(f, z(i)), where Lsurr : M(X, R) × Z →R is a surrogate loss guaranteeing that sgn( bf surr s ) ∈arg minf∈Fa,sgn bRs(f).",
"A frequently used surrogate loss is the logistic loss5 given by Lsurr(f, z) = log  1 + e−yf(x) .",
"In various learning tasks one also adds regularization terms to the minimization problem in (1.2), such as penalties on the norm of the parameters of the NN, i.e., min θ∈RP (N) bRs(Φa(·, θ)) + α∥θ∥2 2, where α ∈(0, ∞) is a regularization parameter.",
"Note that in this case the minimizer depends on the chosen parameters θ and not only on the realization function Φa(·, θ), see also Remark 1.5.",
"Coming back to our initial, informal description of learning in Deﬁnition 1.1, we have now outlined potential learning tasks in Deﬁnition 1.2, NN hypothesis sets in Deﬁnition 1.6, a metric for the in-sample performance in Deﬁnition 1.7, and a corresponding learning algorithm in Deﬁnition 1.8.",
"However, we are still lacking a mathematical concept to describe the out-of-sample (generalization) performance of our learning algorithm.",
"This question has been intensively studied in the ﬁeld of statistical learning theory, see Section 1 for various references.",
"In this ﬁeld one usually establishes a connection between unseen data z and the training data s = (z(i))m i=1 by imposing that z and z(i), i ∈[m], are realizations of independent samples drawn from the same distribution.",
Assumption 1.10 (Independent and identically distributed data).,
"We assume that z(1), .",
", z(m), z are realizations of i.i.d.",
"random variables Z(1), .",
"4For simplicity, we assume that the minimum is attained which, for instance, is the case if F is a compact topological space on which b Rs is continuous.",
"Hypothesis sets of NNs F(N,ϱ) constitute a compact space if, e.g., one chooses a compact parameter set P ⊂RP (N) and a continuous activation function ϱ.",
"One could also work with approximate minimizers, see [AB99].",
5This can be viewed as cross-entropy between the label y and the output of f composed with a logistic function ϱσ.,
In a multi-dimensional setting one can replace the logistic function with a softmax function.,
"8 In this formal setting, we can compute the average out-of-sample performance of a model.",
"Recall from our notation in Section 1.1 that we denote by PZ the image measure of Z on Z, which is the underlying distribution of our training data S = (Z(i))m i=1 ∼Pm Z and unknown data Z ∼PZ.",
Deﬁnition 1.11 (Risk).,
"For a function f ∈M(X, Y), we deﬁne6 the risk by R(f) := E  L(f, Z)  = Z Z L(f, z) dPZ(z).",
"Deﬁning S := (Z(i))m i=1, the risk of a model fS = A(S) is thus given by R(fS) = E  L(fS, Z)|S  .",
"For prediction tasks, we can write Z = (X, Y ), such that the input features and labels are given by an X-valued random variable X and a Y-valued random variable Y , respectively.",
"Note that for classiﬁcation tasks the risk equals the probability of misclassiﬁcation R(f) = E[1(−∞,0)(Y f(X))] = P[f(X) ̸= Y ].",
"For noisy data, there might be a positive, lower bound on the risk, i.e., an irreducible error.",
"If the lower bound on the risk is attained, one can also deﬁne the notion of an optimal solution to a learning task.",
Deﬁnition 1.12 (Bayes-optimal function).,
"A function f ∗∈M(X, Y) achieving the smallest risk, the so-called Bayes risk R∗:= inf f∈M(X,Y) R(f), is called a Bayes-optimal function.",
"For the prediction tasks in Deﬁnition 1.2, we can represent the risk of a function with respect to the Bayes risk and compute the Bayes-optimal function, see, e.g., [CZ07, Propositions 1.8 and 9.3].",
Lemma 1.1 (Regression and classiﬁcation risk).,
"For a regression task with V[Y ] < ∞, the risk can be decomposed into R(f) = E  (f(X) −E[Y |X])2 + R∗, f ∈M(X, Y), (1.3) which is minimized by the regression function f ∗(x) = E[Y |X = x].",
"For a classiﬁcation task, the risk can be decomposed into R(f) = E  |E[Y |X]|1(−∞,0)(E[Y |X]f(X))  + R∗, f ∈M(X, Y), which is minimized by the Bayes classiﬁer f ∗(x) = sgn(E[Y |X = x]).",
"As our model fS is depending on the random training data S, the risk R(fS) is a random variable and we might aim7 for R(fS) small with high probability or in expectation over the training data.",
The challenge for the learning algorithm A is to minimize the risk by only using training data but without knowing the underlying distribution.,
"One can even show that for every learning algorithm there exists a distribution where convergence of the expected risk of fS to the Bayes risk is arbitrarily slow with respect to the number of samples m [DGL96, Theorem 7.2].",
Theorem 1.13 (No free lunch).,
"Let am ∈(0, ∞), m ∈N, be a monotonically decreasing sequence with a1 ≤1/16.",
Then for every learning algorithm A of a classiﬁcation task there exists a distribution PZ such that for every m ∈N and training data S ∼Pm Z it holds that E  R(A(S))  ≥R∗+ am.,
"6Note that this requires z 7→L(f, z) to be measurable for every f ∈M(X, Y), which is the case for our considered prediction tasks.",
"7In order to make probabilistic statements on R(fS) we assume that R(fS) is a random variable, i.e., measurable.",
"This is, e.g., the case if F constitutes a measurable space and s 7→A(s) and f →R|F are measurable.",
9 Figure 1.2: Illustration of the errors (A)–(C) in the decomposition of (1.4).,
"It shows an exemplary risk bR (blue) and empirical risk bRs (red) with respect to the projected space of measurable functions M(X, Y).",
Note that the empirical risk and thus εgen and εopt depend on the realization s = (z(i))m i=1 of the training data S ∼Pm Z .,
Theorem 1.13 shows the non-existence of a universal learning algorithm for every data distribution PZ and shows that useful bounds must necessarily be accompanied by a priori regularity conditions on the underlying distribution PZ.,
"Such prior knowledge can then be incorporated in the choice of the hypothesis set F. To illustrate this, let f ∗ F ∈arg minf∈F R(f) be a best approximation in F, such that we can bound the error R(fS) −R∗= R(fS) −bRS(fS) + bRS(fS) −bRS(f ∗ F) + bRS(f ∗ F) −R(f ∗ F) + R(f ∗ F) −R∗ ≤εopt + 2εgen + εapprox (1.4) by (A) an optimization error εopt := bRS(fS) −bRS( bfS) ≥bRS(fS) −bRS(f ∗ F), with bfS as in Deﬁnition 1.8, (B) a (uniform8) generalization error εgen := supf∈F |R(f) −bRS(f)| ≥max{R(fS) −bRS(fS), bRS(f ∗ F) − R(f ∗ F)}, and (C) an approximation error εapprox := R(f ∗ F) −R∗, see also Figure 1.2.",
"The approximation error is decreasing when enlarging the hypothesis set, but taking F = M(X, Y) prevents controlling the generalization error, see also Theorem 1.13.",
"This suggests a sweet-spot for the complexity of our hypothesis set F and is usually referred to as the bias-variance trade-oﬀ, see also Figure 1.4 below.",
"In the next sections, we will sketch mathematical ideas to tackle each of the errors in (A)–(C) in the context of deep learning.",
Observe that we bound the generalization and optimization error with respect to the empirical risk bRS and its minimizer bfS which is motivated by the fact that in deep-learning-based applications one typically tries to minimize variants of bRS.,
1.2.1 Optimization The ﬁrst error in the decomposition of (1.4) is the optimization error: εopt.,
This error is primarily inﬂuenced by the numerical algorithm A that is used to ﬁnd the model fs in a hypothesis set of NNs for given training data s ∈Zm.,
We will focus on the typical setting where such an algorithm tries to approximately minimize the empirical risk bRs.,
"While there are many conceivable methods to solve this minimization problem, by far the most common are gradient-based methods.",
The main reason for the popularity of gradient-based 8Although this uniform deviation can be a coarse estimate it is frequently considered to allow for the application of uniform laws of large numbers from the theory of empirical processes.,
"10 methods is that for FC networks as in Deﬁnition 1.4, the accurate and eﬃcient computation of pointwise derivatives ∇θΦa(x, θ) is possible by means of automatic diﬀerentiation, a speciﬁc form of which is often referred to as the backpropagation algorithm [Kel60, Dre62, Lin70, RHW86, GW08].",
"This numerical scheme is also applicable in general settings, such as, when the architecture of the NN is given by a general directed acyclic graph.",
"Using these pointwise derivatives, one usually attempts to minimize the empirical risk bRs by updating the parameters θ according to a variant of stochastic gradient descent (SGD), which we shall review below in a general formulation: Algorithm 1: Stochastic gradient descent Input : Diﬀerentiable function r: Rp →R, sequence of step-sizes ηk ∈(0, ∞), k ∈[K], Rp-valued random variable Θ(0).",
Output : Sequence of Rp-valued random variables (Θ(k))K k=1.,
"for k = 1, .",
", K do Let D(k) be a random variable such that E[D(k)|Θ(k−1)] = ∇r(Θ(k−1)); Set Θ(k) := Θ(k−1) −ηkD(k); end If D(k) is chosen deterministically in Algorithm 1, i.e., D(k) = ∇r(Θ(k−1)), then the algorithm is known as gradient descent.",
"To minimize the empirical loss, we apply SGD with r: RP (N) →R set to r(θ) = bRs(Φa(·, θ)).",
"More concretely, one might choose a batch-size m′ ∈N with m′ ≤m and consider the iteration Θ(k) := Θ(k−1) −ηk m′ X z∈S′ ∇θL(Φa(·, Θ(k−1)), z), (1.5) where S′ is a so-called mini-batch of size |S′| = m′ chosen uniformly9 at random from the training data s. The sequence of step-sizes (ηk)k∈N is often called learning rate in this context.",
"Stopping at step K, the output of a deep learning algorithm A is then given by fs = A(s) = Φa(·, ¯θ), where ¯θ can be chosen to be the realization of the last parameter Θ(K) of (1.5) or a convex combination of (Θ(k))K k=1 such as the mean.",
Algorithm 1 was originally introduced in [RM51] in the context of ﬁnding the root of a nondecreasing function from noisy measurements.,
Shortly afterwards this idea was applied to ﬁnd a unique minimum of a Lipschitz-regular function that has no ﬂat regions away from the global minimum [KW52].,
"In some regimes, we can guarantee convergence of SGD at least in expectation, see [NY83, NJLS09, SSSSS09], [SDR14, Section 5.9], [SSBD14, Chapter 14].",
One prototypical convergence guarantee that is found in the aforementioned references in various forms is stated below.,
Theorem 1.14 (Convergence of SGD).,
"Let p, K ∈N and let r: Rp ⊃B1(0) →R be diﬀerentiable and convex.",
"Further let (Θ(k))K k=1 be the output of Algorithm 1 with initialization Θ(0) = 0, step-sizes ηk = K−1/2, k ∈[K], and random variables (D(k))K k=1 satisfying that ∥D(k)∥2 ≤1 almost surely for all k ∈[K].",
"Then E[r(¯Θ)] −r(θ∗) ≤ 1 √ K , where ¯Θ := 1 K PK k=1 Θ(k) and θ∗∈arg minθ∈B1(0) r(θ).",
Theorem 1.14 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict convexity.,
"If r is not convex, then convergence to a global minimum can in general not be guaranteed.",
"In fact, in that case, stochastic gradient descent may converge to a local, non-global minimum, see Figure 1.3 for an example.",
9We remark that in practice one typically picks S′ by selecting a subset of training data in a way to cover the full training data after one epoch of ⌈m/m′⌉many steps.,
"This, however, does not necessarily yield an unbiased estimator D(k) of ∇θr(Θ(k−1)) given Θ(k−1).",
11 Figure 1.3: Examples of the dynamics of gradient descent (left) and stochastic gradient descent (right) for an objective function with one non-global minimum next to the global minimum.,
We see that depending on the initial condition and also on ﬂuctuations in the stochastic part of SGD the algorithm can fail or succeed in ﬁnding the global minimum.,
"Moreover, gradient descent, i.e., the deterministic version of Algorithm 1, will stop progressing if at any point the gradient of r vanishes.",
"This is the case in every stationary point of r. A stationary point is either a local minimum, a local maximum, or a saddle point.",
"One would expect that if the direction of the step D(k) in Algorithm 1 is not deterministic, then the random ﬂuctuations may allow the iterates to escape saddle points.",
"Indeed, results guaranteeing convergence to local minima exist under various conditions on the type of saddle points that r admits, [NJLS09, GL13, GHJY15, LSJR16, JKNvW20].",
"In addition, many methods that improve the convergence by, for example, introducing more elaborate step-size rules or a momentum term have been established.",
"We shall not review these methods here, but instead refer to [GBC16, Chapter 8] for an overview.",
"1.2.2 Approximation Generally speaking, NNs, even FC NNs (see Deﬁnition 1.4) with only L = 2 layers, are universal approximators, meaning that under weak conditions on the activation function ϱ they can approximate any continuous function on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89, LLPS93].",
Theorem 1.15 (Universal approximation theorem).,
"Let d ∈N, let K ⊂Rd be compact, and let ϱ ∈L∞ loc(R) be an activation function such that the closure of the points of discontinuity of ϱ is a Lebesgue null set.",
"Further let eF := [ n∈N F((d,n,1),ϱ) be the corresponding set of two-layer NN realizations.",
Then it holds that C(K) ⊂cl( eF) (where the closure is taken with respect to the topology induced by the L∞(K)-norm) if and only if there does not exist a polynomial p: R →R with p = ϱ almost everywhere.,
"The theorem can be proven by the theorem of Hahn–Banach, which implies that eF being dense in some 12 real normed vector space S is equivalent to the following condition: For all non-trivial functionals F ∈S′ \{0} from the topological dual space of S there exist parameters w ∈Rd and b ∈R such that F(ϱ(⟨w, ·⟩+ b)) ̸= 0.",
"In case of S = C(K) we have by the Riesz–Markov–Kakutani representation theorem that S′ is the space of signed Borel measures on K, see [Rud06].",
"Therefore, Theorem 1.15 holds, if ϱ is such that, for a signed Borel measure µ, Z K ϱ(⟨w, x⟩+ b) dµ(x) = 0 (1.6) for all w ∈Rd and b ∈R implies that µ = 0.",
An activation function ϱ satisfying this condition is called discriminatory.,
It is not hard to see that any sigmoidal ϱ is discriminatory.,
"Indeed, assume that ϱ satisﬁes (1.6) for all w ∈Rd and b ∈R.",
"Since for every x ∈Rd it holds that ϱ(ax+b) →1(0,∞)(x)+ϱ(b)1{0}(x) for a →∞, we conclude by superposition and passing to the limit that for all c1, c2 ∈R and w ∈Rd, b ∈R Z K 1[c1,c2](⟨w, x⟩+ b) dµ(x) = 0.",
"Representing the exponential function x 7→e−2πix as the limit of sums of elementary functions yields that R K e−2πi(⟨w,x⟩+b) dµ(x) = 0 for all w ∈Rd, b ∈R.",
"Hence, the Fourier transform of µ vanishes which implies that µ = 0.",
Theorem 1.15 addresses a uniform approximation problem on a general compact set.,
"If we are given a ﬁnite number of points and only care about good approximation at these points, then one can ask if this approximation problem is potentially simpler.",
"Below we see that, if the number of neurons is larger or equal to the number of data points, then one can always interpolate, i.e., exactly ﬁt the data on a given ﬁnite number of points.",
Proposition 1.1 (Interpolation).,
"Let d, m ∈N, let x(i) ∈Rd, i ∈[m], with x(i) ̸= x(j) for i ̸= j, let ϱ ∈C(R), and assume that ϱ is not a polynomial.",
"Then, there exist parameters θ(1) ∈Rm×d × Rm with the following property: For every k ∈N and every sequence of labels y(i) ∈Rk, i ∈[m], there exist parameters θ(2) = (W (2), 0) ∈Rk×m × Rk for the second layer of the NN architecture a = ((d, m, k), ϱ) such that Φa(x(i), (θ(1), θ(2))) = y(i), i ∈[m].",
Let us sketch the proof in the following.,
"First, note that Theorem 1.15 also holds for functions g ∈C(K, Rm) with multi-dimensional output by approximating each one-dimensional component x 7→(g(x))i and stacking the resulting networks.",
"Second, one can add an additional row containing only zeros to the weight matrix W (1) of the approximating neural network as well as an additional entry to the vector b(1).",
The eﬀect of this is that we obtain an additional neuron with constant output.,
"Since ϱ ̸= 0, we can choose b(1) such that the output of this neuron is not zero.",
"Therefore, we can include the bias vector b(2) of the second layer into the weight matrix W (2), see also Remark 1.5.",
"Now choose g ∈C(Rm, Rm) to be a function satisfying g(x(i)) = e(i), i ∈[m], where e(i) ∈Rm denotes the i-th standard basis vector.",
"By the discussion before there exists a neural network architecture ˜a = ((d, n, m), ϱ) and parameters ˜θ = ((f W (1),˜b(1)), (f W (2), 0)) such that ∥Φ˜a(·, ˜θ) −g∥L∞(K) < 1 m, (1.7) where K is a compact set with x(i) ∈K, i ∈[m].",
Let us abbreviate the output of the activations in the ﬁrst layer evaluated at the input features by eA := h ϱ(f W (1)(x(1)) + ˜b(1))) .,
ϱ(f W (1)(x(m)) + ˜b(1))) i ∈Rn×m.,
"The equivalence of the max and operator norm and (1.7) establish that ∥f W (2) eA −Im∥op ≤m max i,j∈[m] (f W (2) eA −Im)i,j = m max j∈[m] ∥Φ˜a(x(j), ˜θ) −g(x(j))∥∞< 1, 13 where Im denotes the m × m identity matrix.",
"Thus, the matrix f W (2) eA ∈Rm×m needs to have full rank and we can extract m linearly independent rows from eA resulting in an invertible matrix A ∈Rm×m.",
"Now, we deﬁne the desired parameters θ(1) for the ﬁrst layer by extracting the corresponding rows from f W (1) and ˜b(1) and the parameters θ(2) of the second layer by W (2) :=  y(1) .",
y(m) A−1 ∈Rk×m.,
"This proves that with any discriminatory activation function we can interpolate arbitrary training data (x(i), y(i)) ∈Rd × Rk, i ∈[m], using a two-layer NN with m hidden neurons, i.e., O(m(d + k)) parameters.",
One can also ﬁrst project the input features to a one-dimensional line where they are separated and then apply Proposition 1.1 with d = 1.,
"For nearly all activation functions, this can be represented by a three-layer NN using only O(d + mk) parameters10.",
"Beyond interpolation results, one can obtain a quantitative version of Theorem 1.15 if one knows additional regularity properties of the Bayes optimal function f ∗, such as smoothness, compositionality, and symmetries.",
"For surveys on such results, we refer the reader to [DHP20, GRK20].",
"For instructive purposes, we review one such result, which can be found in [Mha96, Theorem 2.1], below: Theorem 1.16 (Approximation of smooth functions).",
"Let d, k ∈N and p ∈[1, ∞].",
Further let ϱ ∈C∞(R) and assume that ϱ is not a polynomial.,
"Then there exists a constant c ∈(0, ∞) with the following property: For every n ∈N there exist parameters θ(1) ∈Rn×d × Rn for the ﬁrst layer of the NN architecture a = ((d, n, 1), ϱ) such that for every g ∈W k,p((0, 1)d) it holds that inf θ(2)∈R1×n×R ∥Φa(·, (θ(1), θ(2))) −g∥Lp((0,1)d) ≤cn−d k ∥g∥W k,p((0,1)d).",
"Theorem 1.16 shows that NNs achieve the same optimal approximation rates that, for example, spline- based approximation yields for smooth functions.",
The idea behind this theorem is based on a strategy that is employed repeatedly throughout the literature.,
This is the idea of re-approximating classical approximation methods by NNs and thereby transferring the approximation rates of these methods to NNs.,
"In the example of Theorem 1.16, approximation by polynomials is used.",
"The idea is that due to the non-vanishing derivatives of the activation function11, one can approximate every univariate polynomial via divided diﬀerences of the activation function.",
"Speciﬁcally, accepting unbounded parameter magnitudes, for any activation function ϱ: R →R which is p-times diﬀerentiable at some point λ ∈R with ϱ(p)(λ) ̸= 0, one can approximate the monomial x 7→xp on a compact set K ⊂R up to arbitrary precision by a ﬁxed-size NN via rescaled p-th order diﬀerence quotients as lim h→0 sup x∈K p X i=0 (−1)i p i  hpϱ(p)(λ)ϱ  (p/2 −i)hx + λ  −xp = 0.",
(1.8) Let us end this subsection by clarifying the connection of the approximation results above to the error decomposition of (1.4).,
"Consider, for simplicity, a regression task with quadratic loss.",
"Then, the approximation error εapprox equals a common L2-error εapprox = R(f ∗ F) −R∗(∗) = Z X (f ∗ F(x) −f ∗(x))2 dPX(x) (∗) = min f∈F ∥f −f ∗∥2 L2(PX) ≤min f∈F ∥f −f ∗∥2 L∞(X), where the identities marked by (∗) follow from Lemma 1.1.",
"Hence, Theorem 1.15 postulates that εapprox →0 for increasing NN sizes, whereas Theorem 1.16 additionally explains how fast εapprox converges to 0.",
"10To avoid the m × d weight matrix (without using shared parameters as in [ZBH+17]) one interjects an approximate one- dimensional identity [PV18, Deﬁnition 2.5], which can be arbitrarily well approximated by a NN with architecture a = ((1, 2, 1), ϱ) given that ϱ′(λ) ̸= 0 for some λ ∈R, see (1.8) below.",
"11The Baire category theorem ensures that for a non-polynomial ϱ ∈C∞(R) there exists λ ∈R with ϱ(p)(λ) ̸= 0 for all p ∈N, see, e.g., [Don69, Chapter 10].",
"14 1.2.3 Generalization Towards bounding the generalization error εgen = supf∈F |R(f) −bRS(f)|, one observes that, for every f ∈F, Assumption 1.10 ensures that L(f, Z(i)), i ∈[m], are i.i.d.",
random variables.,
"Thus, one can make use of concentration inequalities to bound the deviation of the empirical risk bRS(f) = 1 m Pm i=1 L(f, Z(i)) from its expectation R(f).",
"For instance, assuming boundedness12 of the loss, Hoeﬀding’s inequality [Hoe63] and a union bound directly imply the following generalization guarantee for countable, weighted hypothesis sets F, see, e.g., [BBL03].",
"Theorem 1.17 (Generalization bound for countable, weighted hypothesis sets).",
"Let m ∈N, δ ∈(0, 1) and assume that F is countable.",
"Further let p be a probability distribution on F and assume that L(f, Z) ∈[0, 1] almost surely for every f ∈F.",
Then with probability 1−δ (with respect to repeated sampling of Pm Z -distributed training data S) it holds for every f ∈F that |R(f) −bRS(f)| ≤ r ln(1/p(f)) + ln(2/δ) 2m .,
"While the weighting p needs to be chosen before seeing the training data, one could incorporate prior information on the learning algorithm A.",
"For ﬁnite hypothesis sets without prior information, setting p(f) = 1/|F| for every f ∈F, Theorem 1.17 implies that, with high probability, it holds that εgen ≲ r ln(|F|) m .",
"(1.9) Again, one notices that, in line with the bias-variance trade-oﬀ, the generalization bound is increasing with the size of the hypothesis set |F|.",
"Although in practice the parameters θ ∈RP (N) of a NN are discretized according to ﬂoating-point arithmetic, the corresponding quantities |Fa| or |Fa,sgn| would be huge and we need to ﬁnd a replacement for the ﬁniteness condition.",
We will focus on binary classiﬁcation tasks and present a main result of VC theory which is to a great extent derived from the work of Vladimir Vapnik and Alexey Chervonenkis [VC71].,
"While in (1.9) we counted the number of functions in F, we now reﬁne this analysis to the number of functions restricted to a ﬁnite subset of X, given by the growth function growth(m, F) := max (x(i))m i=1∈X m |{f|(x(i))m i=1 : f ∈F}|.",
"The growth function can be interpreted as the maximal number of classiﬁcation patterns in {−1, 1}m which functions in F can realize on m points and thus growth(m, F) ≤2m.",
"The asymptotic behavior of the growth function is determined by a single intrinsic dimension of our hypothesis set F, the so-called VC-dimension VCdim(F) := sup  m ∈N ∪{0}: growth(m, F) = 2m , which deﬁnes the largest number of points such that F can realize any classiﬁcation pattern, see, e.g., [AB99, BBL03].",
"There exist various results on VC-dimensions of NNs with diﬀerent activation functions, see, for instance, [BH89, KM97, BMM98, Sak99].",
We present the result of [BMM98] for piecewise polynomial activation functions ϱ.,
"It establishes a bound on the VC-dimension of hypothesis sets of NNs for classiﬁcation tasks F(N,ϱ),sgn that scales, up to logarithmic factors, linear in the number of parameters P(N) and quadratic in the number of layers L. Theorem 1.18 (VC-dimension of neural network hypothesis sets).",
Let ϱ be a piecewise polynomial activation function.,
"Then there exists a constant c ∈(0, ∞) such that for every L ∈N and N ∈NL+1 it holds that VCdim(F(N,ϱ),sgn) ≤c  P(N)L log(P(N)) + P(N)L2 .",
"12Note that for our classiﬁcation tasks in Deﬁnition 1.2 it holds that L(f, Z) ∈{0, 1} for every f ∈F.",
"For the regression tasks, one typically assumes boundedness conditions, such as |Y | ≤c and supf∈F |f(X)| ≤c almost surely for some c ∈(0, ∞), which yields that supf∈F |L(f, Z)| ≤4c2.",
"15 Given (x(i))m i=1 ∈X m, there exists a partition of RP (N) such that Φ(x(i), ·), i ∈[m], are polynomials on each region of the partition.",
The proof of Theorem 1.18 is based on bounding the number of such regions and the number of classiﬁcation patterns of a set of polynomials.,
"A ﬁnite VC-dimension ensures the following generalization bound [Tal94, AB99]: Theorem 1.19 (VC-dimension generalization bound).",
"There exists a constant c ∈(0, ∞) with the following property: For every classiﬁcation task as in Deﬁnition 1.2, every Z-valued random variable Z, and every m ∈N, δ ∈(0, 1) it holds with probability 1 −δ (with respect to repeated sampling of Pm Z -distributed training data S) that sup f∈F |R(f) −bRS(f)| ≤c r VCdim(F) + log(1/δ)) m .",
"In summary, using NN hypothesis sets F(N,ϱ),sgn with a ﬁxed depth and piecewise polynomial activation ϱ for a classiﬁcation task, with high probability it holds that εgen ≲ r P(N) log(P(N)) m .",
"(1.10) In the remainder of this section we will sketch a proof of Theorem 1.19 and, in doing so, present further concepts and complexity measures connected to generalization bounds.",
"We start by observing that McDiarmid’s inequality [McD89] ensures that εgen is sharply concentrated around its expectation, i.e., with probability 1 −δ it holds that13 εgen −E  εgen ≲ r log(1/δ) m .",
(1.11) To estimate the expectation of the uniform generalization error we employ a symmetrization argu- ment [GZ84].,
"Deﬁne G := L ◦F := {L(f, ·): f ∈F}, let eS = ( eZ(i))m i=1 ∼Pm Z be a test data set independent of S, and note that R(f) = E[ bReS(f)].",
"By properties of the conditional expectation and Jensen’s inequality it holds that E  εgen = E h sup f∈F |R(f) −bRS(f)| i = E h sup g∈G 1 m m X i=1 E  g( eZ(i)) −g(Z(i))|S  i ≤E h sup g∈G 1 m m X i=1 g( eZ(i)) −g(Z(i)) i = E h sup g∈G 1 m m X i=1 τi  g( eZ(i)) −g(Z(i))  i ≤2E h sup g∈G 1 m m X i=1 τig(Z(i)) i , where we used that multiplications with Rademacher variables (τ1, .",
", τm) ∼U({−1, 1}m) only amount to interchanging Z(i) with eZ(i) which has no eﬀect on the expectation, since Z(i) and eZ(i) have the same distribution.",
"The quantity Rm(G) := E h sup g∈G 1 m m X i=1 τig(Z(i)) i is called the Rademacher complexity14 of G. One can also prove a corresponding lower bound [vdVW97], i.e., Rm(G) − 1 √m ≲E  εgen ≲Rm(G).",
"(1.12) 13For precise conditions to ensure that the expectation of εgen is well-deﬁned, we refer the reader to [vdVW97, Dud14].",
"14Due to our decomposition in (1.4), we want to uniformly bound the absolute value of the diﬀerence between the risk and the empirical risk.",
It is also common to just bound supf∈F R(f) −b RS(f) leading to a deﬁnition of the Rademacher complexity without the absolute values which can be easier to deal with.,
16 Now we use a chaining method to bound the Rademacher complexity of F by covering numbers on diﬀerent scales.,
"Speciﬁcally, Dudley’s entropy integral [Dud67, LT91] implies that Rm(G) ≲E h Z ∞ 0 r log Nα(G, dS) m dα i , (1.13) where Nα(G, dS) := inf n |G|: G ⊂G, G ⊂ [ g∈G BdS α (g) o denotes the covering number with respect to the (random) pseudometric given by dS(f, g) = d(Z(i))m i=1(f, g) := v u u t 1 m m X i=1  f(Z(i)) −g(Z(i)) 2.",
"For the 0-1 loss L(f, z) = 1(−∞,0)(yf(x)) = (1 −f(x)y)/2, we can get rid of the loss function by the fact that Nα(G, dS) = N2α(F, d(X(i))m i=1).",
"(1.14) The proof is completed by combining the inequalities in (1.11), (1.12), (1.13) and (1.14) with a result of David Haussler [Hau95] which shows that for α ∈(0, 1) we have log(Nα(F, d(X(i))m i=1)) ≲VCdim(F) log(1/α).",
(1.15) We remark that this resembles a typical behavior of covering numbers.,
"For instance, the logarithm of the covering number log(Nα(M)) of a compact d-dimensional Riemannian manifold M essentially scales like d log(1/α).",
"Finally, note that there exists a similar bound to the one in (1.15) for bounded regression tasks making use of the so-called fat-shattering dimension [MV03, Theorem 1].",
1.3 Do we need a new theory?,
"Despite the already substantial insight that the classical theories provide, a lot of open questions remain.",
We will outline these questions below.,
The remainder of this article then collects modern approaches to explain the following issues: Why do large neural networks not overﬁt?,
"In Subsection 1.2.2, we have observed that three-layer NNs with commonly used activation functions and only O(d + m) parameters can interpolate any training data (x(i), y(i)) ∈Rd ×R, i ∈[m].",
"While this speciﬁc representation might not be found in practice, [ZBH+17] indeed trained convolutional15 NNs with ReLU activation function and about 1.6 million parameters to achieve zero empirical risk on m = 50000 training images of the CIFAR10 dataset [KH09] with 32 × 32 pixels per image, i.e., d = 1024.",
"For such large NNs, generalization bounds scaling with the number of parameters P(N) as the VC-dimension bound in (1.10) are vacuous.",
"However, they observed close to state-of-the-art generalization performance16.",
"Generally speaking, NNs in practice are observed to generalize well despite having more parameters than training samples (usually referred to as overparametrization) and approximately interpolating the training data (usually referred to as overﬁtting).",
"As we cannot perform any better on the training data, there is no trade-oﬀ between ﬁt to training data and complexity of the hypothesis set F happening, seemingly contradicting the classical bias-variance trade-oﬀof statistical learning theory.",
"This is quite surprising, especially given the following additional empirical observations in this regime, see [NTS14, ZBH+17, NBMS17, BHMM19, NKB+20]: 15The basic deﬁnition of a convolutional NN will be given in Section 6.",
In [ZBH+17] more elaborate versions such as an Inception architecture [SLJ+15] are employed.,
"16In practice one usually cannot measure the risk R(fs) and instead evaluates the performance of a trained model fs by b R˜s(fs) using test data ˜s, i.e., realizations of i.i.d.",
random variables distributed according to PZ and drawn independently of the training data.,
In this context one often calls Rs(fs) the training error and R˜s(fs) the test error.,
"17 0 6 12 18 d 0.00 0.05 0.10 linear regression ( = 0) test train 0 20 40 d 10 10 10 1 108 0 6 12 18 d 0.00 0.05 0.10 ridge regression ( = 0.001) test train 0 20 40 d 10 2 10 1 0.5 0.0 0.5 x 0 1 2 d = 40 = 0 = 0.001 f * training data Figure 1.4: The ﬁrst plot (and its semi-log inset) shows median and interquartile range of the test and training errors of ten independent linear regressions with m = 20 samples, polynomial input features X = (1, Z, .",
", Zd) of degree d ∈[40], and labels Y = f ∗(Z) + ν, where Z ∼U([−0.5, 0.5]), f ∗is a polynomial of degree three, and ν ∼N(0, 0.01).",
This clearly reﬂects the classical u-shaped bias-variance curve with a sweet-spot at d = 3 and drastic overﬁtting beyond the interpolation threshold at d = 20.,
"However, the second plot shows that we can control the complexity of our hypothesis set of linear models by restricting the Euclidean norm of their parameters using ridge regression with a small regularization parameter α = 10−3, i.e., minimizing the regularized empirical risk 1 m Pm i=1(Φ(X(i), θ) −Y (i))2 + α∥θ∥2 2, where Φ(·, θ) = ⟨θ, ·⟩.",
Corresponding examples of bfs are depicted in the last plot.,
"Zero training error on random labels: Zero empirical risk can also be achieved for random labels using the same architecture and training scheme with only slightly increased training time: This suggests that the considered hypothesis set of NNs F can ﬁt arbitrary binary labels, which would imply that VCdim(F) ≈m or Rm(F) ≈1 rendering our uniform generalization bounds in Theorem 1.19 and in (1.12) vacuous.",
"Lack of explicit regularization: The test error depends only mildly on explicit regularization like norm- based penalty terms or dropout (see [G´er17] for an explanation of diﬀerent regularization methods): As such regularization methods are typically used to decrease the complexity of F, one might ask if there is any implicit regularization (see Figure 1.4), constraining the range of our learning algorithm A to some smaller, potentially data-dependent subset, i.e., A(s) ∈eFs ⊊F.",
Dependence on the optimization: The same NN trained to zero empirical risk using diﬀerent variants of SGD or starting from diﬀerent initializations can exhibit diﬀerent test errors: This indicates that the dynamics of gradient descent and properties of the local neighborhood around the model fs = A(s) might be correlated with generalization performance.,
Interpolation of noisy training data: One still observes low test error when training up to approximately zero empirical risk using a regression (or surrogate) loss on noisy training data.,
"This is particularly interesting, as the noise is captured by the model but seems not to hurt generalization performance.",
"Further overparametrization improves generalization performance: Further increasing the NN size can lead to even lower test error: Together with the previous item, this might ask for a diﬀerent treatment of models complex enough to ﬁt the training data.",
"According to the traditional lore “The training error tends to decrease whenever we increase the model complexity, that is, whenever we ﬁt the data harder.",
"However with too much ﬁtting, the model adapts itself too closely to the training data, and will not generalize well (i.e., have large test error)”, [HTF01].",
"While this ﬂawlessly describes the situation for certain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here.",
"In summary, this suggests that the generalization performance of NNs depends on an interplay of the data distribution PZ combined with properties of the learning algorithm A, such as the optimization procedure and its range.",
"In particular, classical uniform bounds as in Item (B) of our error decomposition might only 18 deliver insuﬃcient explanation, see also [NK19].",
The mismatch between predictions of classical theory and the practical generalization performance of deep NNs is often referred to as generalization puzzle.,
In Section 2 we will present possible explanations for this phenomenon.,
What is the role of depth?,
"We have seen in Subsection 1.2.2 that NNs can closely approximate every function if they are suﬃciently wide [Cyb89, Fun89, HSW89].",
"There are additional classical results that even provide a trade-oﬀbetween the width and the approximation accuracy [CLM94, Mha96, MP99].",
"In these results, the central concept is the width of a NN.",
"In modern applications, however at least as much focus if not more lies on the depth of the underlying architectures, which can have more than 1000 layers [HZRS16].",
"After all, the depth of NNs is responsible for the name of deep learning.",
This consideration begs the question of whether there is a concrete mathematically quantiﬁable beneﬁt of deep architectures over shallow NNs.,
"Indeed, we will see eﬀects of depth at many places throughout this manuscript.",
"However, one of the aspects of deep learning that is most clearly aﬀected by deep architectures is the approximation theoretical aspect.",
"In this framework, we will discuss in Section 3 multiple approaches that describe the eﬀect of depth.",
Why do neural networks perform well in very high-dimensional environments?,
We have seen in Subsection 1.2.2 and will see in Section 3 that from the perspective of approximation theory deep NNs match the performance of the best classical approximation tool in virtually every task.,
"In practice, we observe something that is even more astounding.",
"In fact, NNs seem to perform incredibly well on tasks that no classical, non-specialized approximation method can even remotely handle.",
The approximation problem that we are talking about here is that of approximation of high-dimensional functions.,
"Indeed, the classical curse of dimensionality [Bel52, NW09] postulates that essentially every approximation method deteriorates exponentially fast with increasing dimension.",
"For example, for the uniform approximation error of 1-Lipschitz continuous functions on a d-dimensional unit cube in the uniform norm, we have a lower bound of Ω(p−1/d), for p →∞, when approximating with a continuous scheme17 of p free parameters [DeV98].",
"On the other hand, in most applications, the input dimensions are massive.",
"For example, the following datasets are typically used as benchmarks in image classiﬁcation problems: MNIST [LBBH98] with 28 × 28 pixels per image, CIFAR-10/CIFAR-100 [KH09] with 32×32 pixels per image and ImageNet [DDS+09, KSH12] which contains high-resolution images that are typically down-sampled to 256 × 256 pixels.",
"Naturally, in real-world applications, the input dimensions may well exceed those of these test problems.",
"However, already for the simplest of the test cases above, the input dimension is d = 784.",
"If we use d = 784 in the aforementioned lower bound for the approximation of 1-Lipschitz functions, then we require O(ε−784) parameters to achieve a uniform error of ε ∈(0, 1).",
Already for moderate ε this value will quickly exceed the storage capacity of any conceivable machine in this universe.,
"Considering the aforementioned curse of dimensionality, it is puzzling to see that NNs perform adequately in this regime.",
"In Section 4, we describe three approaches that oﬀer explanations as to why deep NN-based approximation is not rendered meaningless in the context of high-dimensional input dimensions.",
Why does stochastic gradient descent converge to good local minima despite the non-convexity of the problem?,
"As mentioned in Subsection 1.2.1, a convergence guarantee of stochastic gradient descent to a global minimum is typically only given if the underlying objective function admits some form of convexity.",
"However, the empirical risk of a NN, i.e., bRs(Φ(·, θ)), is typically not a convex function with respect to the parameters θ.",
"For a simple intuitive reason why this function fails to be convex, it is instructive to consider the following example.",
17One can achieve better rates at the cost of discontinuous (with respect to the function to be approximated) parameter assignment.,
This can be motivated by the use of space-ﬁlling curves.,
"In the context of NNs with piecewise polynomial activation functions, a rate of p−2/d can be achieved by very deep architectures [Yar18a, YZ20].",
19 Figure 1.5: Two-dimensional projection of the loss landscape of a neural network with four layers and ReLU activation function on four diﬀerent scales.,
"From top-left to bottom-right, we zoom into the global minimum of the landscape.",
Example 1.20.,
"Consider the NN Φ(x, θ) = θ1ϱR(θ3x + θ5) + θ2ϱR(θ4x + θ6), θ ∈R6, x ∈R, with the ReLU activation function ϱR(x) = max{0, x}.",
"It is not hard to see that the two parameter values θ = (1, −1, 1, 1, 1, 0) and ¯θ = (−1, 1, 1, 1, 0, 1) produce the same realization function18, i.e., Φ(·, θ) = Φ(·, ¯θ).",
"However, since (θ + ¯θ)/2 = (0, 0, 1, 1, 1/2, 1/2), we conclude that Φ(·, (θ + ¯θ)/2) = 0.",
"Clearly, for the data s = ((−1, 0), (1, 1)), we now have that bRs(Φ(·, θ)) = bRs(Φ(·, ¯θ)) = 0 and bRs  Φ(·, (θ + ¯θ)/2)  = 1 2, showing the non-convexity of bRs.",
18This corresponds to interchanging the two neurons in the hidden layer.,
In general it holds that the realization function of a FC NN is invariant under permutations of the neurons in a given hidden layer.,
"20 Given this non-convexity, Algorithm 1 faces serious challenges.",
"Firstly, there may exist multiple suboptimal local minima.",
"Secondly, the objective may exhibit saddle points, some of which may be of higher order, i.e., the Hessian vanishes.",
"Finally, even if no suboptimal local minima exist, there may be extensive areas of the parameter space where the gradient is very small, so that escaping these regions can take a very long time.",
"These issues are not mere theoretical possibilities, but will almost certainly arise.",
"For example, [AHW96, SS18] show the existence of many suboptimal local minima in typical learning tasks.",
"Moreover, for ﬁxed-sized NNs, it has been shown in [BEG19, PRV20], that with respect to Lp-norms the set of NNs is generally a very non-convex and non-closed set.",
"Also, the map θ 7→Φa(·, θ) is not a quotient map, i.e., not continuously invertible when accounting for its non-injectivity.",
"In addition, in various situations ﬁnding the global optimum of the minimization problem is shown to be NP-hard in general [BR89, Jud90, ˇS´ım02].",
"In Figure 1.5 we show the two-dimensional projection of a loss landscape, i.e., the projection of the graph of the function θ 7→bRs(Φ(·, θ)).",
It is apparent from the visualization that the problem exhibits more than one minimum.,
"We also want to add that in practice one neglects that the loss is only almost everywhere diﬀerentiable in case of piecewise smooth activation functions, such as the ReLU, although one could resort to subgradient methods [KL18].",
"In view of these considerations, the classical framework presented in Subsection 1.2.1 oﬀers no explanation as to why deep learning works in practice.",
"Indeed, in the survey [OM98, Section 1.4] the state of the art in 1998 was summarized by the following assessment: “There is no formula to guarantee that (1) the NN will converge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.” Nonetheless, in applications, not only would an explanation of when and why SGD converges be extremely desirable, convergence is also quite often observed even though there is little theoretical explanation for it in the classical set-up.",
"In Section 5, we collect modern approaches explaining why and when convergence occurs and can be guaranteed.",
Which aspects of a neural network architecture aﬀect the performance of deep learning?,
"In the introduction to classical approaches to deep learning above, we have seen that in classical results, such as in Theorem 1.16, only the eﬀect of few aspects of the NN architectures are considered.",
In Theorem 1.16 only the impact of the width of the NN was studied.,
"In further approximation theorems below, e.g., in Theorems 2.1 and 3.2, we will additionally have a variable depth of NNs.",
"However, for deeper architectures, there are many additional aspects of the architecture that could potentially aﬀect the performance of the model for the associated learning task.",
"For example, even for a standard FC NN with L layers as in Deﬁnition 1.4, there is a lot of ﬂexibility in choosing the number of neurons (N1, .",
", NL−1) ∈NL−1 in the hidden layers.",
One would expect that certain choices aﬀect the capabilities of the NNs considerably and some choices are preferable over others.,
"Note that, one aspect of the neural network architecture that can have a profound eﬀect on the performance, especially regarding approximation theoretical aspects of the performance, is the choice of the activation function.",
"For example, in [MP99, Yar21] activation functions were found that allow uniform approximation of continuous functions to arbitrary accuracy with ﬁxed-size neural networks.",
"In the sequel we will, however, focus on architectural aspects other than the activation function.",
"In addition, practitioners have invented an immense variety of NN architectures for speciﬁc problems.",
"These include NNs with convolutional blocks [LBBH98], with skip connections [HZRS16], sparse connections [ZAP16, BBC17], batch normalization blocks [IS15], and many more.",
"In addition, for sequential data, recurrent connections are used [RHW86] and these often have forget mechanisms [HS97] or other gates [CvMG+14] included in their architectures.",
The choice of an appropriate NN architecture is essential to the success of many deep learning tasks.,
"This goes so far, that frequently an architecture search is applied to ﬁnd the most suitable one [ZL17, PGZ+18].",
"In most cases, though, the design and choice of the architecture is based on the intuition of the practitioner.",
"Naturally, from a theoretical point of view, this situation is not satisfactory.",
"Instead, it would be highly desirable to have a mathematical theory guiding the choice of NN architectures.",
"More concretely, one would wish for mathematical theorems that identify those architectures that work for a speciﬁc problem and those that will yield suboptimal results.",
"In Section 6, we discuss various results that explain theoretically quantiﬁable eﬀects of certain aspects or building blocks of NN architectures.",
21 Which features of data are learned by deep architectures?,
It is commonly believed that the neurons of NNs constitute feature extractors in diﬀerent levels of abstraction that correspond to the layers.,
"This belief is partially grounded in experimental evidence as well as in drawing connections to the human visual cortex, see [GBC16, Chapter 9.10].",
"Understanding the features that are learned can, in a way, be linked to understanding the reasoning with which a NN-based model ended up with its result.",
"Therefore, analyzing the features that a NN learns constitutes a data-aware approach to understanding deep learning.",
"Naturally, this falls outside of the scope of the classical theory, which is formulated in terms of optimization, generalization, and approximation errors.",
"One central obstacle towards understanding these features theoretically is that, at least for practical problems, the data distribution is unknown.",
"However, one often has partial knowledge.",
One example is that in image classiﬁcation it appears reasonable to assume that any classiﬁer is translation and rotation invariant as well as invariant under small deformations.,
"In this context, it is interesting to understand under which conditions trained NNs admit the same invariances.",
Biological NNs such as the visual cortex are believed to be evolved in a way that is based on sparse multiscale representations of visual information [OF96].,
"Again, a fascinating question is whether NNs trained in practice can be shown to favor such multiscale representations based on sparsity or if the architecture is theoretically linked to sparse representations.",
We will discuss various approaches studying the features learned by neural networks in Section 7.,
Are neural networks capable of replacing highly specialized numerical algorithms in natural sciences?,
"Shortly after their successes in various data-driven tasks in data science and AI applications, NNs have been used also as a numerical ansatz for solving highly complex models from the natural sciences which may be combined with data driven methods.",
This per se is not very surprising as many such models can be formulated as optimization problems where the common deep learning paradigm can be directly applied.,
What might be considered surprising is that this approach seems to be applicable to a wide range of problems which have previously been tackled by highly specialized numerical methods.,
"Particular successes include the data-driven solution of ill-posed inverse problems [AM¨OS19] which have, for example, led to a fourfold speedup in MRI scantimes [ZKS+18] igniting the research project fastmri.org.",
"Deep-learning-based approaches have also been very successful in solving a vast array of diﬀerent partial diﬀerential equation (PDE) models, especially in the high-dimensional regime [EY18, RPK19, HSN20, PSMF20] where most other methods would suﬀer from the curse of dimensionality.",
"Despite these encouraging applications, the foundational mechanisms governing their workings and limitations are still not well understood.",
In Subsection 4.3 and Section 8 we discuss some theoretical and practical aspects of deep learning methods applied to the solution of inverse problems and PDEs.,
"2 Generalization of large neural networks In the following, we will shed light on the generalization puzzle of NNs as described in Subsection 1.3.",
"We focus on four diﬀerent lines of research which, of course, do not cover the wide range of available results.",
"In fact, we had to omit a discussion of a multitude of important works, some of which we reference in the following paragraph.",
"First, let us mention extensions of the generalization bounds presented in Subsection 1.2.3 making use of local Rademacher complexities [BBM05] or dropping assumptions on boundedness or rapidly decaying tails [Men14].",
"Furthermore, there are approaches to generalization which do not focus on the hypothesis set F, i.e., the range of the learning algorithm A, but the way A chooses its model fs.",
"For instance, one can assume that fs does not depend too strongly on each individual sample (algorithmic stability [BE02, PRMN04]), only on a subset of the samples (compression bounds [AGNZ18]), or satisﬁes local properties (algorithmic robustness [XM12]).",
"Finally, we refer the reader to [JNM+20] and the references mentioned therein for an empirical study of various measures related to generalization.",
"Note that many results on generalization capabilities of NNs can still only be proven in simpliﬁed settings, e.g., for deep linear NNs, i.e., ϱ(x) = x, or basic linear models, i.e., one-layer NNs.",
"Thus, we start by 22 emphasizing the connection of deep, nonlinear NNs to linear models (operating on features given by a suitable kernel) in the inﬁnite width limit.",
"2.1 Kernel regime We consider a one-dimensional prediction setting where the loss L(f, (x, y)) depends on x ∈X only through f(x) ∈Y, i.e., there exists a function ℓ: Y × Y →R such that L(f, (x, y)) = ℓ(f(x), y).",
"For instance, in case of the quadratic loss we have that ℓ(ˆy, y) = (ˆy −y)2.",
"Further, let Φ be a NN with architecture (N, ϱ) = ((d, N1, .",
", NL−1, 1), ϱ) and let Θ0 be a RP (N)-valued random variable.",
"For simplicity, we evolve the parameters of Φ according to the continuous version of gradient descent, so-called gradient ﬂow, given by dΘ(t) dt = −∇θ bRs(Φ(·, Θ(t))) = −1 m m X i=1 ∇θΦ(x(i), Θ(t))Di(t), Θ(0) = Θ0, (2.1) where Di(t) := ∂ℓ(ˆy,y(i)) ∂ˆy |ˆy=Φ(x(i),Θ(t)) is the derivative of the loss with respect to the prediction at input feature x(i) at time t ∈[0, ∞).",
"The chain rule implies the following dynamics of the NN realization dΦ(·, Θ(t)) dt = −1 m m X i=1 KΘ(t)(·, x(i))Di(t) (2.2) and its empirical risk d bRs(Φ(·, Θ(t)) dt = −1 m2 m X i=1 m X j=1 Di(t)KΘ(t)(x(i), x(j))Dj(t), (2.3) where Kθ, θ ∈RP (N), is the so-called neural tangent kernel (NTK) Kθ : Rd × Rd →R, Kθ(x1, x2) =  ∇θΦ(x1, θ) T ∇θΦ(x2, θ).",
"(2.4) Now let σw, σb ∈(0, ∞) and assume that the initialization Θ0 consists of independent entries, where entries corresponding to the weight matrix and bias vector in the ℓ-th layer follow a normal distribution with zero mean and variances σ2 w/Nℓand σ2 b, respectively.",
"Under weak assumptions on the activation function, the central limit theorem implies that the pre-activations converge to i.i.d.",
"centered Gaussian processes in the inﬁnite width limit N1, .",
", NL−1 →∞, see [LBN+18, MHR+18].",
"Similarly, also KΘ0 converges to a deterministic kernel K∞which stays constant in time and only depends on the activation function ϱ, the depth L, and the initialization parameters σw and σb [JGH18, ADH+19, Yan19, LXS+20].",
"Thus, within the inﬁnite width limit, gradient ﬂow on the NN parameters as in (2.1) is equivalent to functional gradient ﬂow in the reproducing kernel Hilbert space (HK∞, ∥· ∥K∞) corresponding to K∞, see (2.2).",
"By (2.3), the empirical risk converges to a global minimum as long as the kernel evaluated at the input features, ¯K∞:= (K∞(x(i), x(j)))m i,j=1 ∈Rm×m, is positive deﬁnite (see, e.g., [JGH18, DLL+19] for suitable conditions) and the ℓ(·, y(i)) are convex and lower bounded.",
"For instance, in case of the quadratic loss the solution of (2.2) is then given by Φ(·, Θ(t)) = C(t)(y(i))m i=1 +  Φ(·, Θ0) −C(t)(Φ(x(i), Θ0))m i=1  , (2.5) where C(t) :=  (K∞(·, x(i)))m i=1 T ( ¯K∞)−1(Im −e−2 ¯ K∞t m ).",
"As the initial realization Φ(·, Θ0) constitutes a centered Gaussian process, the second term in (2.5) follows a normal distribution with zero mean at each input.",
"In the limit t →∞, its variance vanishes on the input features x(i), i ∈[m], and the ﬁrst term convergences to the minimum kernel-norm interpolator, i.e., to the solution of min f∈HK∞∥f∥K∞ s.t.",
f(x(i)) = y(i).,
"23 Therefore, within the inﬁnite width limit, the generalization properties of the NN could be described by the generalization properties of the minimizer in the reproducing kernel Hilbert space corresponding to the kernel K∞[BMM18, LR20, LRZ20, GMMM21, Li21].",
"This so-called lazy training, where a NN essentially behaves like a linear model with respect to the nonlinear features x 7→∇θΦ(x, θ), can already be observed in the non-asymptotic regime, see also Subsection 5.2.",
"For suﬃciently overparametrized (P(N) ≫m) and suitably initialized models, one can show that Kθ(0) is close to K∞at initialization and Kθ(t) stays close to Kθ(0) throughout training, see [DZPS18, ADH+19, COB19, DLL+19].",
"The dynamics of the NN under gradient ﬂow in (2.2) and (2.3) can thus be approximated by the dynamics of the linearization of Φ at initialization Θ0, given by Φlin(·, θ) := Φ(·, Θ0) + ⟨∇θΦ(·, Θ0), θ −Θ0⟩, which motivates to study the behavior of linear models in the overparametrized regime.",
"2.2 Norm-based bounds and margin theory For piecewise linear activation functions, one can improve upon the VC-dimension bounds in Theorem 1.18 and show that, up to logarithmic factors, the VC-dimension is asymptotically bounded both above and below by P(N)L, see [BHLM19].",
The lower bound shows that the generalization bound in Theorem 1.19 can only be non-vacuous if the number of samples m scales at least linearly with the number of NN parameters P(N).,
"However, heavily overparametrized NNs used in practice seem to generalize well outside of this regime.",
One solution is to bound other complexity measures of NNs taking into account various norms on the parameters and avoid the direct dependence on the number of parameters [Bar98].,
"For instance, we can compute bounds on the Rademacher complexity of NNs with positively homogeneous activation function, where the Frobenius norm of the weight matrices is bounded, see also [NTS15].",
"Note that, for instance, the ReLU activation is positively homogeneous, i.e., it satisﬁes that ϱR(λx) = λϱR(x) for all x ∈R and λ ∈(0, ∞).",
Theorem 2.1 (Rademacher complexity of neural networks).,
"Let d ∈N, assume that X = B1(0) ⊂Rd, and let ϱ be a positively homogeneous activation function with Lipschitz constant 1.",
"We deﬁne the set of all biasless NN realizations with depth L ∈N, output dimension 1, and Frobenius norm of the weight matrices bounded by C ∈(0, ∞) as eFL,C :=  Φ(N,ϱ)(·, θ): N ∈NL+1, N0 = d, NL = 1, θ = ((W (ℓ), 0))L ℓ=1 ∈RP (N), ∥W (ℓ)∥F ≤C .",
"Then for every m ∈N it holds that Rm( eFL,C) ≤C(2C)L−1 √m .",
The term 2L−1 depending exponentially on the depth can be reduced to √ L or completely omitted by invoking also the spectral norm of the weight matrices [GRS18].,
"Further, observe that for L = 1, i.e., linear classiﬁers with bounded Euclidean norm, this bound is independent of the input dimension d. Together with (1.12), this motivates why the regularized linear model in Figure 1.4 did perform well in the overparametrized regime.",
"The proof of Theorem 2.1 is based on the contraction property of the Rademacher complexity [LT91] which establishes that Rm(ϱ ◦eFℓ,C) ≤2Rm( eFℓ,C), ℓ∈N.",
"We can iterate this together with the fact that for every τ ∈{−1, 1}m, and x ∈RNℓ−1 it holds that sup ∥W (ℓ)∥F ≤C m X i=1 τiϱ(W (ℓ)x) 2 = C sup ∥w∥2≤1 m X i=1 τiϱ(⟨w, x⟩) .",
"24 In summary, one establishes that Rm( eFL,C) = C mE  sup f∈e FL−1,C m X i=1 τiϱ(f(X(i))) 2  ≤C(2C)L−1 m E  m X i=1 τiX(i) 2  , which by Jensen’s inequality yields the claim.",
"Recall that for classiﬁcation problems one typically minimizes a surrogate loss Lsurr, see Remark 1.9.",
"This suggests that there could be a trade-oﬀhappening between complexity of the hypothesis class Fa and the corresponding regression ﬁt underneath, i.e., the margin M(f, z) := yf(x) by which a training example z = (x, y) has been classiﬁed correctly by f ∈Fa, see [BFT17, NBS18, JKMB19].",
"For simplicity, let us focus on the ramp surrogate loss with conﬁdence γ > 0, i.e., Lsurr γ (f, z) := ℓγ(M(f, z)), where ℓγ(t) := 1(−∞,γ](t) −t γ 1[0,γ](t), t ∈R.",
Note that the ramp function ℓγ is 1/γ-Lipschitz continuous.,
"Using McDiarmid’s inequality and a sym- metrization argument similar to the proof of Theorem 1.19, combined with the contraction property of the Rademacher complexity, yields the following bound on the probability of misclassiﬁcation: With probability 1 −δ for every f ∈Fa it holds that P[sgn(f(X)) ̸= Y ] ≤E  Lsurr γ (f, Z)  ≲1 m m X i=1 Lsurr γ (f, Z(i)) + Rm(Lsurr γ ◦Fa) + r ln(1/δ) m ≲1 m m X i=1 1(−∞,γ)(Y (i)f(X(i))) + Rm(M ◦Fa) γ + r ln(1/δ) m = 1 m m X i=1 1(−∞,γ)(Y (i)f(X(i))) + Rm(Fa) γ + r ln(1/δ) m .",
This shows the trade-oﬀbetween the complexity of Fa measured by Rm(Fa) and the fraction of training data that has been classiﬁed correctly with a margin of at least γ.,
"In particular this suggests, that (even if we classify the training data correctly with respect to the 0-1 loss) it might be beneﬁcial to further increase the complexity of Fa to simultaneously increase the margins by which the training data has been classiﬁed correctly and thus obtain a better generalization bound.",
"2.3 Optimization and implicit regularization The optimization algorithm, which is usually a variant of SGD, seems to play an important role for the generalization performance.",
"Potential indicators for good generalization performance are high speed of convergence [HRS16] or ﬂatness of the local minimum to which SGD converged, which can be characterized by the magnitude of the eigenvalues of the Hessian (or approximately as the robustness of the minimizer to adversarial perturbations on the parameter space), see [KMN+17].",
"In [DR17, NBMS17] generalization bounds depending on a concept of ﬂatness are established by employing a PAC-Bayesian framework, which can be viewed as a generalization of Theorem 1.17, see [McA99].",
"Further, one can also unite ﬂatness and norm-based bounds by the Fisher–Rao metric of information geometry [LPRS19].",
"Let us motivate the link between generalization and ﬂatness in the case of simple linear models: We assume that our model takes the form ⟨θ, ·⟩, θ ∈Rd, and we will use the abbreviations r(θ) := bRs(⟨θ, ·⟩) and γ(θ) := min i∈[m] M(⟨θ, ·⟩, z(i)) = min i∈[m] y(i)⟨θ, x(i)⟩ throughout this subsection to denote the empirical risk and the margin for given training data s = ((x(i), y(i)))m i=1.",
We assume that we are solving a classiﬁcation task with the 0-1 loss and that our training 25 data is linearly separable.,
This means that there exists a minimizer ˆθ ∈Rd such that r(ˆθ) = 0.,
"We observe that δ-robustness in the sense that max θ∈Bδ(0) r(ˆθ + θ) = r(ˆθ) = 0 implies that 0 < min i∈[m] y(i)⟨ˆθ −δy(i) x(i) ∥x(i)∥2 , x(i)⟩≤γ(ˆθ) −δ min i∈[m] ∥x(i)∥2, see also [PKL+17].",
This lower bound on the margin γ(ˆθ) then ensures generalization guarantees as described in Subsection 2.2.,
"Even without explicit19 control on the complexity of Fa, there do exist results showing that SGD acts as implicit regularization [NTS14].",
This is motivated by linear models where SGD converges to the minimal Euclidean norm solution for the quadratic loss and in the direction of the hard margin support vector machine solution for the logistic loss on linearly separable data [SHN+18].,
"Note that convergence to minimum norm or maximum margin solutions in particular decreases the complexity of our hypothesis set and thus improves generalization bounds, see Subsection 2.2.",
"While we have seen this behavior of gradient descent for linear regression already in the more general context of kernel regression in Subsection 2.1, we want to motivate the corresponding result for classiﬁcation tasks in the following.",
"We focus on the exponential surrogate loss Lsurr(f, z) = ℓ(M(f, z)) = e−yf(x) with ℓ(z) = e−z, but similar observations can be made for the logistic loss deﬁned in Remark 1.9.",
"We assume that the training data is linearly separable, which guarantees the existence of ˆθ ̸= 0 with γ(ˆθ) > 0.",
"Then for every linear model ⟨θ, ·⟩, θ ∈Rd, it holds that ˆθ, ∇θr(θ)⟩= 1 m m X i=1 ℓ′(y(i)⟨θ, x(i)⟩) | {z } <0 y(i)⟨ˆθ, x(i)⟩ | {z } >0 .",
"A critical point ∇θr(θ) = 0 can therefore be approached if and only if for every i ∈[m] we have ℓ′(y(i)⟨θ, x(i)⟩) = −e−y(i)⟨θ,x(i)⟩→0, which is equivalent to ∥θ∥2 →∞and γ(θ) > 0.",
"Let us now deﬁne rβ(θ) := ℓ−1(r(βθ)) β , θ ∈Rd, β ∈(0, ∞), and observe that rβ(θ) = −log(r(βθ)) β →γ(θ), β →∞.",
"(2.6) Due to this property, rβ is often referred to as the smoothed margin [LL19, JT19b].",
"We evolve θ according to gradient ﬂow with respect to the smoothed margin r1, i.e., dθ(t) dt = ∇θr1(θ(t)) = − 1 r(θ(t))∇θr(θ(t)), which produces the same trajectory as gradient ﬂow with respect to the empirical risk r under a rescaling of the time t. Looking at the evolution of the normalized parameters ˜θ(t) = θ(t)/∥θ(t)∥2, the chain rule establishes that d˜θ(t) dt = P˜θ(t) ∇θrβ(t)(˜θ(t)) β(t) with β(t) := ∥θ(t)∥2 and Pθ := Id −θθT , θ ∈Rd.",
"19Note that also diﬀerent architectures can exhibit vastly diﬀerent inductive biases [ZBH+20] and also within the architecture diﬀerent parameters have diﬀerent importance, see [FC18, ZBS19] and Proposition 6.2.",
"26 This shows that the normalized parameters perform projected gradient ascent with respect to the function rβ(t), which converges to the margin due to (2.6) and the fact that β(t) = ∥θ(t)∥2 →∞when approaching a critical point.",
This motivates that during gradient ﬂow the normalized parameters implicitly maximize the margin.,
"See [GLSS18a, GLSS18b, LL19, NLG+19, CB20, JT20] for a precise analysis and various extensions, e.g., to homogeneous or two-layer NNs and other optimization geometries.",
"To illustrate one research direction, we present an exemplary result in the following.",
"Let Φ = Φ(N,ϱ) be a biasless NN with parameters θ = ((W (ℓ), 0))L ℓ=0 and output dimension NL = 1.",
"For given input features x ∈RN0, the gradient ∇W (ℓ)Φ = ∇W (ℓ)Φ(x, θ) ∈RNℓ−1×Nℓwith respect to the weight matrix in the ℓ-th layer satisﬁes that ∇W (ℓ)Φ = ϱ(Φ(ℓ−1)) ∂Φ ∂Φ(ℓ+1) ∂Φ(ℓ+1) ∂Φ(ℓ) = ϱ(Φ(ℓ−1)) ∂Φ ∂Φ(ℓ+1) W (ℓ+1) diag  ϱ′(Φ(ℓ))  , where the pre-activations (Φ(ℓ))L ℓ=1 are given as in (1.1).",
"Evolving the parameters according to gradient ﬂow as in (2.1) and using an activation function ϱ with ϱ(x) = ϱ′(x)x, such as the ReLU, this implies that diag  ϱ′(Φ(ℓ))  W (ℓ)(t) dW (ℓ)(t) dt T = dW (ℓ+1)(t) dt T W (ℓ+1)(t) diag  ϱ′(Φ(ℓ))  .",
"(2.7) Note that this ensures the conservation of balancedness between weight matrices of adjacent layers, i.e., d dt  ∥W (ℓ+1)(t)∥2 F −∥W (ℓ)(t)∥2 F  = 0, see [DHL18].",
"Furthermore, for deep linear NNs, i.e., ϱ(x) = x, the property in (2.7) implies conservation of alignment of left and right singular spaces of W (ℓ) and W (ℓ+1).",
"This can then be used to show implicit pre- conditioning and convergence of gradient descent [ACH18, ACGH19] and that, under additional assumptions, gradient descent converges to a linear predictor that is aligned with the maximum margin solution [JT19a].",
"2.4 Limits of classical theory and double descent There is ample evidence that classical tools from statistical learning theory alone, such as Rademacher averages, uniform convergence, or algorithmic stability may be unable to explain the full generalization capabilities of NNs [ZBH+17, NK19].",
It is especially hard to reconcile the classical bias-variance trade-oﬀwith the observation of good generalization performance when achieving zero empirical risk on noisy data using a regression loss.,
"On top of that, this behavior of overparametrized models in the interpolation regime turns out not to be unique to NNs.",
"Empirically, one observes for various methods (decision trees, random features, linear models) that the test error decreases even below the sweet-spot in the u-shaped bias-variance curve when further increasing the number of parameters [BHMM19, GJS+20, NKB+20].",
"This is often referred to as the double descent curve or benign overﬁtting, see Figure 2.1.",
"For special cases, e.g., linear regression or random feature regression, such behavior can even be proven, see [HMRT19, MM19, BLLT20, BHX20, MVSS20].",
In the following we analyze this phenomenon in the context of linear regression.,
"Speciﬁcally, we focus on a prediction task with quadratic loss, input features given by a centered Rd-valued random variable X, and labels given by Y = ⟨θ∗, X⟩+ ν, where θ∗∈Rd and ν is a centered random variable independent of X.",
"For training data S = ((X(i), Y (i)))m i=1, we consider the empirical risk minimizer bfS = ⟨ˆθ, ·⟩with minimum Euclidean norm of its parameters ˆθ or, equivalently, the limit of gradient ﬂow with zero initialization.",
"Using (1.3) and a bias-variance decomposition we can write E[R( bfS)|(X(i))m i=1] −R∗= E[∥bfS −f ∗∥L2(PX)|(X(i))m i=1] = (θ∗)T PE[XXT ]Pθ∗+ E[ν2]Tr  Σ+E[XXT ]  , where Σ := Pm i=1 X(i)(X(i))T , Σ+ denotes the Moore–Penrose inverse of Σ, and P := Id −Σ+Σ is the orthogonal projector onto the kernel of Σ.",
"For simplicity, we focus on the variance Tr  Σ+E[XXT ]  , which can 27 Figure 2.1: This illustration shows the classical, underparametrized regime in green, where the u-shaped curve depicts the bias-variance trade-oﬀas explained in Section 1.2.",
"Starting with complexity of our algorithm A larger than the interpolation threshold we can achieve zero empirical risk bRs(fs) (training error), where fs = A(s).",
"Within this modern interpolation regime, the risk R(fs) (test error) might be even lower than at the classical sweet spot.",
"Whereas complexity(A) traditionally refers to the complexity of the hypothesis set F, there is evidence that also the optimization scheme and the data is inﬂuencing the complexity leading to deﬁnitions like complexity(A) := max  m ∈N: E  bRS(A(S))  ≤ε with S ∼Pm Z , for suitable ε > 0 [NKB+20].",
This illustration is based on [BHMM19].,
be viewed as setting θ∗= 0 and E[ν2] = 1.,
Assuming that X has i.i.d.,
"entries with unit variance and bounded ﬁfth moment, the distribution of the eigenvalues of 1 mΣ+ in the limit d, m →∞with d m →κ ∈(0, ∞) can be described via the Marchenko–Pastur law.",
"Therefore, the asymptotic variance can be computed explicitly as Tr  Σ+E[XXT ]  →1 −max{1 −κ, 0} |1 −κ| for d, m →∞ with d m →κ, almost surely, see [HMRT19].",
This shows that despite interpolating the data we can decrease the risk in the overparametrized regime κ > 1.,
"In the limit d, m →∞, such benign overﬁtting can also be shown for more general settings (including lazy training of NNs), some of which even achieve their optimal risk in the overparametrized regime [MM19, MZ20, LD21].",
"For normally distributed input features X such that E[XXT ] has rank larger than m, one can also compute the behavior of the variance in the non-asymptomatic regime [BLLT20].",
"Deﬁne k∗:= min{k ≥0: P i>k λi λk+1 ≥cm}, where λ1 ≥λ2 ≥· · · ≥λd ≥0 are the eigenvalues of E[XXT ] in decreasing order and c ∈(0, ∞) is a universal constant.",
"Assuming that k∗/m is suﬃciently small, with high probability it holds that Tr  Σ+E[XXT ]  ≈k∗ m + m P i>k∗λ2 i (P i>k∗λi)2 .",
"28 0 50 100 150 d 0.0 0.5 1.0 variance Figure 2.2: The expected variance of linear regression in (2.8) with d ∈ [150] and Xi ∼U({−1, 1}), i ∈[150], where Xi = X1 for i ∈{10, .",
", 20} ∪ {30, .",
", 50} and all other coordinates are independent.",
This precisely characterizes the regimes for benign overﬁtting in terms of the eigenvalues of the covariance matrix E[XXT ].,
"Furthermore, it shows that adding new input feature coordinates and thus increasing the number of parameters d can lead to either an increase or decrease of the risk.",
"To motivate this phenomenon, which is considered in much more depth in [CMBK20], let us focus on a single sample m = 1 and features X that take values in X = {−1, 1}d. Then it holds that Σ+ = X(1)(X(1))T ∥X(1)∥4 = X(1)(X(1))T d2 and thus E  Tr  Σ+E[XXT ]  = 1 d2 E  XXT  2 F .",
"(2.8) In particular, this shows that incrementing the input feature dimen- sions d 7→d + 1 one can increase or decrease the risk depending on the correlation of the coordinate Xd+1 with respect to the previous coordinates (Xi)d i=1, see also Figure 2.2.",
"Generally speaking, overparametrization and perfectly ﬁtting noisy data does not exclude good generalization performance, see also [BRT19].",
"However, the risk crucially depends on the data distribution and the chosen algorithm.",
"3 The role of depth in the expressivity of neural networks The approximation theoretical aspect of a NN architecture, responsible for the approximation component εapprox := R(f ∗ F) −R∗of the error R(fS) −R∗in (1.4), is probably one of the most well-studied parts of the deep learning pipe-line.",
The achievable approximation error of an architecture most directly describes the power of the architecture.,
"As mentioned in Subsection 1.3, many classical approaches only study the approximation theory of NNs with few layers, whereas modern architectures are typically very deep.",
A ﬁrst observation into the eﬀect of depth is that it can often compensate for insuﬃcient width.,
"For example, in the context of the universal approximation theorem, it was shown that very narrow NNs are still universal if instead of increasing the width, the number of layers can be chosen arbitrarily [HS17, Han19, KL20].",
"However, if the width of a NN falls below a critical number, then the universality will not hold any longer.",
"Below, we discuss three additional observations that shed light on the eﬀect of depth on the approximation capacities or alternative notions of expressivity of NNs.",
"3.1 Approximation of radial functions One technique to study the impact of depth relies on the construction of speciﬁc functions which can be well approximated by NNs of a certain depth, but require signiﬁcantly more parameters when approximated to the same accuracy by NNs of smaller depth.",
"In the following we present one example for this type of approach, which can be found in [ES16].",
Theorem 3.1 (Power of depth).,
"Let ϱ ∈{ϱR, ϱσ, 1(0,∞)} be the ReLU, the logistic, or the Heaviside function.",
"Then there exist constants c, C ∈(0, ∞) with the following property: For every d ∈N with d ≥C there exist a probability measure µ on Rd, a three-layer NN architecture a = (N, ϱ) = ((d, N1, N2, 1), ϱ) with ∥N∥∞≤Cd5, and corresponding parameters θ∗∈RP (N) with ∥θ∗∥∞≤CdC and ∥Φa(·, θ∗)∥L∞(Rd) ≤2 such that for every n ≤cecd it holds that inf θ∈RP ((d,n,1)) ∥Φ((d,n,1),ϱ)(·, θ) −Φa(·, θ∗)∥L2(µ) ≥c.",
"29 In fact, the activation function in Theorem 3.1 is only required to satisfy mild conditions and the result holds, for instance, also for more general sigmoidal functions.",
"The proof of Theorem 3.1 is based on the construction of a suitable radial function g: Rd →R, i.e., g(x) = ˜g(∥x∥2 2) for some ˜g: [0, ∞) →R, which can be eﬃciently approximated by three-layer NNs but approximation by only a two-layer NN requires exponentially large complexity, i.e., the width being exponential in d. The ﬁrst observation of [ES16] is that g can typically be well approximated on a bounded domain by a three-layer NN, if ˜g is Lipschitz continuous.",
"Indeed, for the ReLU activation function it is not diﬃcult to show that, emulating a linear interpolation, one can approximate a univariate C-Lipschitz function uniformly on [0, 1] up to precision ε by a two-layer architecture of width O(C/ε).",
"The same holds for smooth, non-polynomial activation functions due to Theorem 1.16.",
"This implies that the squared Euclidean norm, as a sum of d univariate functions, i.e., [0, 1]d ∋x 7→Pd i=1 x2 i , can be approximated up to precision ε by a two-layer architecture of width O(d2/ε).",
"Moreover, this shows that the third layer can eﬃciently approximate ˜g, establishing approximation of g on a bounded domain up to precision ε using a three-layer architecture with number of parameters polynomial in d/ε.",
"The second step in [ES16] is to choose g in such a way that the realization of any two-layer neural network Φ = Φ((d,n,1),ϱ)(·, θ) with width n not being exponential in d is on average (with respect to the probability measure µ) a constant distance away from g. Their argument is heavily based on ideas from Fourier analysis and will be outlined below.",
"In this context, let us recall that we denote by ˆf the Fourier transform of a suitable function or, more generally, tempered distribution f. Assuming that the square-root ϕ of the density function associated with the probability measure µ as well as Φ and g are well-behaved, the Plancherel theorem yields that ∥Φ −g∥2 L2(µ) = ∥Φϕ −gϕ∥2 L2(Rd) = c Φϕ −c gϕ 2 L2(Rd).",
"(3.1) Next, the speciﬁc structure of two-layer NNs is used, which implies that for every j ∈[n] there exists wj ∈Rd with ∥wj∥2 = 1 and ϱj : R →R (subsuming the activation function ϱ, the norm of wj, and the remaining parameters corresponding to the j-th neuron in the hidden layer) such that Φ is of the form Figure 3.1: This illustration shows the largest possible support (blue) of c Φϕ, where ˆϕ = 1Br(0) and Φ is a shallow neural network with architec- ture N = (2, 4, 1) and weight matrix W (1) = [w1 .",
w4]T in the ﬁrst layer.,
Any radial function with enough of its L2-mass located at high frequencies (indicated by the red area) cannot be well approximated by Φϕ.,
"Φ = n X j=1 ϱj(⟨wj, ·⟩) = n X j=1 (ϱj ⊗1Rd−1) ◦Rwj.",
"The second equality follows by viewing the action of the j-th neuron as a tensor product of ϱj and the indicator function 1Rd−1(x) = 1, x ∈Rd−1, composed with a d-dimensional rotation Rwj ∈SO(d) which maps wj to the ﬁrst standard basis vector e(1) ∈Rd.",
"Noting that the Fourier transform respects linearity, rotations, and tensor products, we can compute ˆΦ = n X j=1 (ˆϱj ⊗δRd−1) ◦Rwj, where δRd−1 denotes the Dirac distribution on Rd−1.",
"In particular, the support of ˆΦ has a particular star-like shape, namely Sn j=1 span{wj}, which are in fact lines passing through the origin.",
"Now we choose ϕ to be the inverse Fourier transform of the indicator function of a ball Br(0) ⊂Rd with vol(Br(0)) = 1, ensuring that ϕ2 is a valid probability density for µ as µ(Rd) = ∥ϕ2∥L1(Rd) = ∥ϕ∥2 L2(Rd) = ∥ˆϕ∥2 L2(Rd) = ∥1Br(0)∥2 L2(Rd) = 1.",
"Using the convolution theorem, this choice of ϕ yields that supp( c Φϕ) = supp(ˆΦ ∗ˆϕ) ⊂ n [ j=1 (span{wj} + Br(0)) .",
30 Thus the lines passing through the origin are enlarged to tubes.,
"It is this particular shape which allows the construction of some g so that ∥c Φϕ−c gϕ∥2 L2(Rd) can be suitably lower bounded, see also Figure 3.1.",
"Intriguingly, the peculiar behavior of high-dimensional sets now comes into play.",
"Due to the well known concentration of measure principle, the variable n needs to be exponentially large for the set Sn j=1 (span{wj} + Br(0)) to be not sparse.",
"If it is smaller, one can construct a function g so that the main energy content of c gϕ has a certain distance from the origin, yielding a lower bound for ∥c Φϕ −c gϕ∥2 and hence ∥Φ −g∥2 L2(µ), see (3.1).",
"One key technical problem is the fact that such a behavior for ˆg does not immediately imply a similar behavior of c gϕ, requiring a quite delicate construction of g. 3.2 Deep ReLU networks 0 1 8 2 8 3 8 4 8 5 8 6 8 7 8 1 0 1 32 2 32 3 32 4 32 5 32 6 32 7 32 8 32 g I1 g I1 I2 I1 g I2 I3 I2 Figure 3.2: Interpolation In of [0, 1] ∋ x 7→g(x) := x −x2 on 2n + 1 equidis- tant points, which can be represented as a sum In = Pn k=1 Ik −Ik−1 = Pn k=1 hk 22k of n sawtooth functions.",
Each sawtooth function hk = hk−1◦h in turn can be written as a k-fold composition of a hat function h. This illustration is based on [EPGB19].,
"Maybe for no activation function is the eﬀect of depth clearer than for the ReLU activation function ϱR(x) = max{0, x}.",
"We refer to corresponding NN architectures (N, ϱR) as ReLU (neural) networks (ReLU NNs).",
"A two-layer ReLU NN with one-dimensional input and output is a function of the form Φ(x) = n X i=1 w(2) i ϱR(w(1) i x + b(1) i ) + b(2), x ∈R, where w(1) i , w(2) i , b(1) i , b(2) ∈R for i ∈[n].",
It is not hard to see that Φ is a continuous piecewise aﬃne linear function.,
"Moreover, Φ has at most n + 1 aﬃne linear pieces.",
"On the other hand, notice that the hat function h: [0, 1] →[0, 1], x 7→2ϱR(x) −4ϱR(x −1 2) = ( 2x, if 0 ≤x < 1 2, 2(1 −x), if 1 2 ≤x ≤1, is a NN with two layers and two neurons.",
Telgarsky observed that the n-fold convolution hn(x) := h ◦· · · ◦h produces a sawtooth function with 2n spikes [Tel15].,
"In particular, hn admits 2n aﬃne linear pieces with only 2n many neurons.",
"In this case, we see that deep ReLU NNs are in some sense exponentially more eﬃcient in generating aﬃne linear pieces.",
"Moreover, it was noted in [Yar17] that the diﬀerence of interpolations of [0, 1] ∋x 7→x −x2 at 2n + 1 and 2n−1 + 1 equidistant points equals the scaled sawtooth function hn 22n , see Figure 3.2.",
"This allows to eﬃciently implement approximative squaring and, by polarization, also approximative multiplication using ReLU NNs.",
"Composing these simple functions one can approximate localized Taylor polynomials and thus smooth functions, see [Yar17].",
"We state below a generalization [GKP20] of the result of [Yar17] which includes more general norms, but for p = ∞and s = 0 coincides with the original result of Dmitry Yarotsky.",
Theorem 3.2 (Approximation of Sobolev-regular functions).,
"Let d, k ∈N with k ≥2, let p ∈[1, ∞], s ∈[0, 1], B ∈(0, ∞), and let ϱ be a piecewise linear activation function with at least one break-point.",
"Then there exists a constant c ∈(0, ∞) with the following property: For every ε ∈(0, 1/2) there exists a NN architecture a = (N, ϱ) with P(N) ≤cε−d/(k−s) log(1/ε) such that for every function g ∈W k,p((0, 1)d) with ∥g∥W k,p((0,1)d) ≤B it holds that inf θ∈RP (N) ∥Φa(θ, ·) −g∥W s,p((0,1)d) ≤ε.",
31 The ability of deep ReLU neural networks to emulate multiplication has also been employed to reap- proximate wide ranges of high-order ﬁnite element spaces.,
In [OPS20] and [MOPS20] it was shown that deep ReLU neural networks are capable of achieving the approximation rates of hp-ﬁnite element methods.,
"Concretely, this means that for piecewise analytic functions, which appear, for example, as solutions of elliptic boundary and eigenvalue problems with analytic data, exponential approximation rates can be achieved.",
"In other words, the number of parameters of neural networks to approximate such a function in the W 1,2-norm up to an error of ε is logarithmic in ε. Theorem 3.2 requires the depth of the NN to grow.",
"In fact, it can be shown that the same approximation rate cannot be achieved with shallow NNs.",
"Indeed, there exists a certain optimal number of layers and, if the architecture has fewer layers than optimal, then the NNs need to have signiﬁcantly more parameters, to achieve the same approximation ﬁdelity.",
"This has been observed in many diﬀerent settings in [LS17, SS17, Yar17, PV18, EPGB19].",
We state here the result of [Yar17]: Theorem 3.3 (Depth-width approximation trade-oﬀ).,
"Let d, L ∈N with L ≥2 and let g ∈C2([0, 1]d) be a function which is not aﬃne linear.",
"Then there exists a constant c ∈(0, ∞) with the following property: For every ε ∈(0, 1) and every ReLU NN architecture a = (N, ϱR) = ((d, N1, .",
", NL−1, 1), ϱR) with L layers and ∥N∥1 ≤cε−1/(2(L−1)) neurons it holds that inf θ∈RP (N) ∥Φa(·, θ) −g∥L∞([0,1]d) ≥ε.",
depth width Figure 3.3: Standard feed-forward neural network.,
"For certain approximation results, depth and width need to be in a ﬁxed relationship to achieve optimal results.",
This results is based on the observation that ReLU NNs are piecewise aﬃne linear.,
The number of pieces they admit is linked to their capacity of approximating functions that have non-vanishing curvature.,
"Using a construction similar to the ex- ample at the beginning of this subsection, it can be shown that the number of pieces that can be gener- ated using an architecture ((1, N1, .",
", NL−1, 1), ϱR) scales roughly like QL−1 ℓ=1 Nℓ.",
"In the framework of the aforementioned results, we can speak of a depth-width trade-oﬀ, see also Fig- ure 3.3.",
A ﬁne-grained estimate of achievable rates for freely varying depths has also been established in [She20].,
3.3 Alternative notions of expressivity Conceptual approaches to study the approximation power of deep NNs besides the classical approximation framework usually aim to relate structural properties of the NN to the “richness” of the set of possibly expressed functions.,
"One early result in this direction is [MPCB14] which describes bounds on the number of aﬃne linear regions of a ReLU NN Φ(N,ϱR)(·, θ).",
"In a simpliﬁed setting, we have seen estimates on the number of aﬃne linear pieces already at the beginning of Subsection 3.2.",
"Aﬃne linear regions can be deﬁned as the connected components of RN0 \ H, where H is the set of non-diﬀerentiability of the realization20 Φ(N,ϱR)(·, θ).",
"A reﬁned analysis on the number of such regions was, for example, conducted by [HvdG19].",
It is found that deep ReLU neural networks can exhibit signiﬁcantly more regions than their shallow counterparts.,
"20One can also study the potentially larger set of activation regions given by the connected components of RN0 \  ∪L−1 ℓ=1 ∪Nℓ i=1Hi,ℓ  , where Hi,ℓ:= {x ∈RN0 : Φ(ℓ) i (x, θ) = 0}, with Φ(ℓ) i as in (1.1), is the set of non-diﬀerentiability of the activation of the i-th neuron in the ℓ-th layer.",
"In contrast to the linear regions, the activation regions are necessarily convex [RPK+17, HR19].",
"32 Figure 3.4: Shape of the trajectory t 7→Φ((2,n,...,n,2),ϱR)(γ(t), θ) of the output of a randomly initialized network with 0, 3, 10 hidden layers.",
The input curve γ is the circle given in the leftmost image.,
The hidden layers have n = 20 neurons and the variance of the initialization is taken as 4/n.,
"The reason for this eﬀectiveness of depth is described by the following analogy: Through the ReLU each neuron Rd ∋x 7→ϱR(⟨x, w⟩+ b), w ∈Rd, b ∈R, splits the space into two aﬃne linear regions separated by the hyperplane {x ∈Rd : ⟨x, w⟩+ b = 0}.",
"A shallow ReLU NN Φ((d,n,1),ϱR)(·, θ) with n neurons in the hidden layer therefore produces a number of regions deﬁned through n hyperplanes.",
"Using classical bounds on the number of regions deﬁned through hyperplane arrangements [Zas75], one can bound the number of aﬃne linear regions by Pd j=0  n j  .",
Deepening neural networks then corresponds to a certain folding of the input space.,
Through this interpretation it can be seen that composing NNs can lead to a multiplication of the number of regions of the individual NNs resulting in an exponential eﬃciency of deep neural networks in generating aﬃne linear regions21.,
This approach was further developed in [RPK+17] to a framework to study expressivity that to some extent allows to include the training phase.,
One central object studied in [RPK+17] are so-called trajectory lengths.,
"In this context, one analyzes how the length of a non-constant curve in the input space changes in expectation through the layers of a NN.",
The authors ﬁnd an exponential dependence of the expected curve length on the depth.,
"Let us motivate this in the special case of a ReLU NN with architecture a = ((N0, n, .",
", n, NL), ϱR) and depth L ∈N.",
"Given a non-constant continuous curve γ : [0, 1] →RN0 in the input space, the length of the trajectory in the ℓ-th layer of the NN Φa(·, θ) is then given by Length(¯Φ(ℓ)(γ(·), θ)), ℓ∈[L −1], where ¯Φ(ℓ)(·, θ) is the activation in the ℓ-th layer, see (1.1).",
"Here the length of the curve is well-deﬁned since ¯Φ(ℓ)(·, θ)) is continuous and therefore ¯Φ(ℓ)(γ(·), θ) is continuous.",
"Now, let the parameters Θ1 of the NN Φa be initialized independently so that the entries corresponding to the weight matrices and bias vectors follow a normal distribution with zero mean and variances 1/n and 1, respectively.",
"It is not hard to see, e.g., by Proposition 1.1, that the probability that ¯Φ(ℓ)(·, Θ1) will map γ to a non-constant curve is positive and hence, for ﬁxed ℓ∈[L −1], E  Length(¯Φ(ℓ)(γ(·), Θ1))  = c > 0.",
"Let σ ∈(0, ∞) and consider a second initialization Θσ, where we change the variances of the entries corresponding to the weight matrices and bias vectors to σ2/n and σ2, respectively.",
"Recall that the ReLU is positively homogeneous, i.e., we have that ϱR(λx) = λϱR(x) for all λ ∈(0, ∞).",
"Then it is clear that ¯Φ(ℓ)(·, Θσ) ∼σℓ¯Φ(ℓ)(·, Θ1), 21However, to exploit this eﬃciency with respect to the depth, one requires highly oscillating pre-activations which in turn can only be achieved with a delicate selection of parameters.",
"In fact, it can be shown that through random initialization the expected number of activation regions per unit cube depends mainly on the number of neurons in the NN, rather than its depth [HR19].",
"33 i.e., the activations corresponding to the two initialization strategies are identically distributed up to the factor σℓ.",
"Therefore, we immediately conclude that E  Length(¯Φ(ℓ)(γ(·), Θσ))  = σℓc.",
"This shows that the expected trajectory length depends exponentially on the depth of the NN, which is in line with the behavior of other notions of expressivity [PLR+16].",
In [RPK+17] this result is also extended to the tanh activation function and the constant c is more carefully resolved.,
"Empirically one also ﬁnds that the shapes of the trajectories become more complex in addition to becoming longer on average, see Figure 3.4.",
4 Deep neural networks overcome the curse of dimensionality M Figure 4.1: Illustration of a one- dimensional manifold M embedded in R3.,
For every point x ∈M there exists a neighborhood in which the manifold can be linearly projected onto its tangent space at x such that the corresponding inverse function is diﬀerentiable.,
"In Subsection 1.3, one of the main puzzles of deep learning that we identiﬁed was the surprising performance of deep architectures on problems where the input dimensions are very high.",
"This perfor- mance cannot be explained in the framework of classical approx- imation theory, since such results always suﬀer from the curse of dimensionality [Bel52, DeV98, NW09].",
"In this section, we present three approaches that oﬀer explana- tions of this phenomenon.",
"As before, we had to omit certain ideas which have been very inﬂuential in the literature to keep the length of this section under control.",
"In particular, an important line of reasoning is that functions to be approximated often have composi- tional structures which NNs may approximate very well as reviewed in [PMR+17].",
"Note that also a suitable feature descriptor, factor- ing out invariances, might lead to a signiﬁcantly reduced eﬀective dimension, see Subsection 7.1.",
4.1 Manifold assumption A ﬁrst remedy to the high-dimensional curse of dimensionality is what we call the manifold assumption.,
"Here it is assumed that we are trying to approximate a function g: Rd ⊃X →R, where d is very large.",
"However, we are not seeking to optimize with respect to the uniform norm or a regular Lp space, but instead consider a measure µ which is supported on a d′-dimensional manifold M ⊂X.",
Then the error is measured in the Lp(µ)-norm.,
Here we consider the case where d′ ≪d.,
"This setting is appropriate if the data z = (x, y) of a prediction task is generated from a measure supported on M × R. This set-up or generalizations thereof have been fundamental in [CM18, SCC18, CJLZ19, SH19, CK20, NI20].",
"Let us describe an exemplary approach, where we consider locally Ck-regular functions and NNs with ReLU activation functions below: 1.",
"Describe the regularity of g on the manifold: Naturally, we need to quantify the regularity of the function g restricted to M in an adequate way.",
The typical approach would be to make a deﬁnition via local coordinate charts.,
"If we assume that M is an embedded submanifold of X, then locally, i.e., in a neighborhood of a point x ∈M, the orthogonal projection of M onto the d′-dimensional tangent space TxM is a diﬀeomorphism.",
The situation is depicted in Figure 4.1.,
"Assuming M to be compact, we can choose a ﬁnite set of open balls (Ui)p i=1 that cover M and on which the local projections γi onto the respective tangent spaces as described above exists and are diﬀeomorphisms.",
Now we can deﬁne the regularity of g via classical regularity.,
"In this example, we say that g ∈Ck(M) if g ◦γ−1 i ∈Ck(γi(M ∩Ui)) for all i ∈[p].",
"Construct localization and charts via neural networks: According to the construction of local coordinate charts in Step 1, we can write g as follows: g(x) = p X i=1 φi(x)  g ◦γ−1 i (γi(x))  =: p X i=1 ˜gi(γi(x), φi(x)), x ∈M, (4.1) where φi is a partition of unity such that supp(φi) ⊂Ui.",
"Note that γi is a linear map, hence representable by a one-layer NN.",
"Since multiplication is a smooth operation, we have that if g ∈Ck(M) then ˜gi ∈Ck(γi(M ∩Ui) × [0, 1]).",
The partition of unity φi needs to be emulated by NNs.,
"For example, if the activation function is the ReLU, then such a partition can be eﬃciently constructed.",
"Indeed, in [HLXZ20] it was shown that such NNs can represent linear ﬁnite elements exactly with ﬁxed-size NNs and hence a partition of unity subordinate to any given covering of M can be constructed.",
"Use a classical approximation result on the localized functions: By some form of Whitney’s extension theorem [Whi34], we can extend each ˜gi to a function ¯gi ∈Ck(X ×[0, 1]) which by classical results can be approximated up to an error of ε > 0 by NNs of size O(ε−(d′+1)/k) for ε →0, see [Mha96, Yar17, SCC18].",
"Use the compositionality of neural networks to build the ﬁnal network: We have seen that every component in the representation (4.1), i.e., ˜gi, γi, and φi can be eﬃciently represented by NNs.",
"In addition, composition and summation are operations which can directly be implemented by NNs through increasing their depth and widening their layers.",
"Hence (4.1) is eﬃciently—i.e., with a rate depending only on d′ instead of the potentially much larger d—approximated by a NN.",
"Overall, we see that NNs are capable of learning local coordinate transformations and therefore reduce the complexity of a high-dimensional problem to the underlying low-dimensional problem given by the data distribution.",
"4.2 Random sampling Already in 1992, Andrew Barron showed that under certain seemingly very natural assumptions on the function to approximate, a dimension-independent approximation rate by NNs can be achieved [Bar92, Bar93].",
"Speciﬁcally, the assumption is formulated as a condition on the Fourier transform of a function and the result is as follows.",
Theorem 4.1 (Approximation of Barron-regular functions).,
Let ϱ: R →R be the ReLU or a sigmoidal function.,
"Then there exists a constant c ∈(0, ∞) with the following property: For every d, n ∈N, every probability measure µ supported on B1(0) ⊂Rd, and every g ∈L1(Rd) with Cg := R Rd ∥ξ∥2|ˆg(ξ)| dξ < ∞it holds that inf θ∈RP ((d,n,1)) ∥Φ((d,n,1),ϱ)(·, θ) −g∥L2(µ) ≤ c √nCg, Note that the L2-approximation error can be replaced by an L∞-estimate over the unit ball at the expense of a factor of the order of √ d on the right-hand side.",
"The key idea behind Theorem 4.1 is the following application of the law of large numbers: First, we observe that, per assumption, g can be represented via the inverse Fourier transform, as g −g(0) = Z Rd ˆg(ξ)(e2πi⟨·,ξ⟩−1) dξ = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨·,ξ⟩−1) 1 Cg ∥ξ∥2ˆg(ξ) dξ = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨·,ξ⟩−1) dµg(ξ), (4.2) 35 where µg is a probability measure.",
"Then it is further shown in [Bar92] that there exist (Rd × R)-valued random variables (Ξ, eΞ) such that (4.2) can be written as g(x) −g(0) = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨x,ξ⟩−1) dµg(ξ) = CgE  Γ(Ξ, eΞ)(x)  , x ∈Rd, (4.3) where for every ξ ∈Rd, ˜ξ ∈R the function Γ(ξ, ˜ξ): Rd →R is given by Γ(ξ, ˜ξ) := s(ξ, ˜ξ)(1(0,∞)(−⟨ξ/∥ξ∥2, ·⟩−˜ξ) −1(0,∞)(⟨ξ/∥ξ∥2, ·⟩−˜ξ)) with s(ξ, ˜ξ) ∈{−1, 1}.",
"Now, let ((Ξ(i), eΞ(i)))i∈N be i.i.d.",
"random variables with (Ξ(1), eΞ(1)) ∼(Ξ, eΞ).",
"Then, Bienaym´e’s identity and Fubini’s theorem establish that E "" g −g(0) −Cg n n X i=1 Γ(Ξ(i), eΞ(i)) 2 L2(µ) # = Z B1(0) V "" Cg n n X i=1 Γ(Ξ(i), eΞ(i))(x) # dµ(x) = C2 g R B1(0) V  Γ(Ξ, eΞ)(x)  dµ(x) n ≤(2πCg)2 n , (4.4) where the last inequality follows from combining (4.3) with the fact that |e2πi⟨x,ξ⟩−1|/∥ξ∥2 ≤2π, x ∈B1(0).",
"This implies that there exists a realization ((ξ(i), ˜ξ(i)))i∈N of the random variables ((Ξ(i), eΞ(i)))i∈N that achieves L2-approximation error of n−1/2.",
"Therefore, it remains to show that NNs can well approximate the functions ((Γ(ξ(i), ˜ξ(i)))i∈N.",
"Now it is not hard to see that the function 1(0,∞) and hence functions of the form Γ(ξ, ˜ξ), ξ ∈Rd, ˜ξ ∈R, can be arbitrarily well approximated with a ﬁxed-size, two-layer NN with a sigmoidal or ReLU activation function.",
"Thus, we obtain an approximation rate of n−1/2 when approximating functions with one ﬁnite Fourier moment by two-layer NNs with n hidden neurons.",
"It was pointed out already in the dissertation of Emmanuel Cand`es [Can98] that the approximation rate of NNs for Barron-regular functions is also achievable by n-term approximation with complex exponentials, as is apparent by considering (4.2).",
"However, for deeper NNs, the results also extend to high-dimensional non- smooth functions, where Fourier-based methods are certain to suﬀer from the curse of dimensionality [CPV20].",
"In addition, the random sampling idea above was extended in [EMW19b, EMWW20, EW20b, EW20c] to facilitate dimension-independent approximation of vastly more general function spaces.",
"Basically, the idea is to use (4.3) as an inspiration and deﬁne the generalized Barron space as all functions that may be represented as E  1(0,∞)(⟨Ξ, ·⟩−eΞ)  for any random variable (Ξ, eΞ).",
"In this context, deep and compositional versions of Barron spaces were introduced and studied in [BK18, EMW19a, EW20a], which considerably extend the original theory.",
4.3 PDE assumption Another structural assumption that leads to the absence of the curse of dimensionality in some cases is that the function we are trying to approximate is given as the solution to a partial diﬀerential equation.,
"It is by no means clear that this assumption leads to approximation without the curse of dimensionality, since most standard methods, such as ﬁnite elements, sparse grids, or spectral methods typically suﬀer from the curse of dimensionality.",
"This is not merely an abstract theoretical problem: Very recently, in [AHNB+20] it was shown that two diﬀerent gold standard methods for solving the multi-electron Schr¨odinger equation produce completely diﬀerent interaction energy predictions when applied to large delocalized molecules.",
Classical numerical representations are simply not expressive enough to accurately represent complicated high-dimensional structures such as wave functions with long-range interactions.,
"Interestingly, there exists an emerging body of work that shows that NNs do not suﬀer from these shortcomings and enjoy superior expressivity properties as compared to standard numerical representations.",
"36 Such results include, for example, [GHJVW20, GS20, HJKN20] for (linear and semilinear) parabolic evolution equations, [GH22] for stationary elliptic PDEs, [GH21] for nonlinear Hamilton–Jacobi–Bellman equations, or [KPRS19] for parametric PDEs.",
"In all these cases, the absence of the curse of dimensionality in terms of the theoretical approximation power of NNs could be rigorously established.",
"One way to prove such results is via stochastic representations of the PDE solutions, as well as associated sampling methods.",
"We illustrate the idea for the simple case of linear Kolmogorov PDEs, that is the problem of representing the function g: Rd × [0, ∞) →R satisfying22 ∂g ∂t (x, t) = 1 2Tr  σ(x, t)[σ(x, t)]∗∇2 xg(x, t)  + ⟨µ(x, t), ∇xg(x, t)⟩, g(x, 0) = ϕ(x), (4.5) where the functions ϕ: Rd →R (initial condition) and σ: Rd →Rd×d, µ: Rd →Rd (coeﬃcient functions) are continuous and satisfy suitable growth conditions.",
"A stochastic representation of g is given via the Ito processes (Sx,t)t≥0 satisfying dSx,t = µ(Sx,t)dt + σ(Sx,t)dBt, Sx,0 = x, (4.6) where (Bt)t≥0 is a d-dimensional Brownian motion.",
"Then g is described via the Feynman–Kac formula which states that g(x, t) = E[ϕ(Sx,t)], x ∈Rd, t ∈[0, ∞).",
"(4.7) Roughly speaking, a NN approximation result can be proven by ﬁrst approximating, via the law of large numbers, g(x, t) = E[ϕ(Sx,t)] ≈1 n n X i=1 ϕ(S(i) x,t), (4.8) where (S(i) x,t)n i=1 are i.i.d.",
"random variables with S(1) x,t ∼Sx,t.",
"Care has to be taken to establish such an approximation uniformly in the computational domain, for example, for every (x, t) in the unit cube [0, 1]d × [0, 1], see (4.4) for a similar estimate and [GHJVW20, GS20] for two general approaches to ensure this property.",
"Aside from this issue, (4.8) represents a standard Monte Carlo estimator which can be shown to be free of the curse of dimensionality.",
"As a next step, one needs to establish that realizations of the processes (x, t) 7→Sx,t can be eﬃciently approximated by NNs.",
"This can be achieved by emulating a suitable time-stepping scheme for the SDE (4.6) by NNs which, roughly speaking, can be done without incurring the curse of dimensionality whenever the coeﬃcient functions µ, σ can be approximated by NNs without incurring the curse of dimensionality and some growth conditions hold true.",
"In a last step one assumes that the initial condition ϕ can be approximated by NNs without incurring the curse of dimensionality which, by the compositionality of NNs and the previous step, directly implies that realizations of the processes (x, t) 7→ϕ(Sx,t) can be approximated by NNs without incurring the curse of dimensionality.",
By (4.8) this implies a corresponding approximation result for the solution of the Kolmogorov PDE g in (4.5).,
"Informally, we have discovered a regularity result for linear Kolmogorov equations, namely that (modulo some technical conditions on µ, σ), the solution g of (4.5) can be approximated by NNs without incurring the curse of dimensionality whenever the same holds true for the initial condition ϕ, as well as the coeﬃcient functions µ, σ.",
"In other words, the property of being approximable by NNs without curse of dimensionality is preserved under the ﬂow induced by the PDE (4.5).",
"Some comments are in order: 22The natural solution concept to this type of PDEs is the viscosity solution concept, a thorough study of which can be found in [HHJ15].",
37 Assumption on the initial condition: One may wonder if the assumption that the initial condition ϕ can be approximated by NNs without incurring the curse of dimensionality is justiﬁed.,
This is at least the case in many applications in computational ﬁnance where the function ϕ typically represents an option pricing formula and (4.5) represents the famous Black–Scholes model.,
"It turns out that nearly all common option pricing formulas are constructed from iterative applications of linear maps and maximum/minimum functions—in other words, in many applications in computational ﬁnance, the initial condition ϕ can be exactly represented by a small ReLU NN.",
101 102 input dimension 107 108 109 1010 1011 x cx2.36 #parameters avg.,
#steps ± 2 std.,
Figure 4.2: Computational complexity as number of neural network parameters times number of SGD steps to solve heat equations of varying dimensions up to a speciﬁed precision.,
"According to the ﬁt above, the scaling is polynomial in the dimension [BDG20].",
"Generalization and optimization error: The Feynman–Kac representation (4.7) directly implies that g(·, t) can be computed as the Bayes opti- mal function of a regression task with input fea- tures X ∼U([0, 1]d) and labels Y = ϕ(SX,t), which allows for an analysis of the generalization error as well as implementations based on ERM algo- rithms [BGJ20, BBG+21].",
"While it is in principle possible to analyze the approximation and generalization error, the analysis of the computational cost and/or convergence of corresponding SGD algorithms is completely open.",
"Some promising numerical results exist, see, for instance, Figure 4.2, but the stable training of NNs approximating PDEs to very high accuracy (that is needed in several applications such as quantum chemistry) remains very challenging.",
The recent work [GV21] has even proven several impossibility results in that direction.,
"Extensions and abstract idea: Similar techniques may be used to prove expressivity results for nonlinear PDEs, for example, using nonlinear Feynman–Kac-type representations of [PP92] in place of (4.7) and multilevel Picard sampling algorithms of [EHJK19] in place of (4.8).",
We can also formulate the underlying idea in an abstract setting (a version of which has also been used in Subsection 4.2).,
"Assume that a high-dimensional function g: Rd →R admits a probabilistic representation of the form g(x) = E[Yx], x ∈Rd, (4.9) for some random variable Yx which can be approximated by an iterative scheme Y(L) x ≈Yx and Y(ℓ) x = Tℓ(Y(ℓ−1) x ), ℓ= 1, .",
", L, with dimension-independent convergence rate.",
"If we can approximate realizations of the initial mapping x 7→Y0 x and the maps Tℓ, ℓ∈[L], by NNs and the numerical scheme is stable enough, then we can also approximate Y(L) x using compositionality.",
Emulating a uniform Monte-Carlo approximator of (4.9) then leads to approximation results for g without curse of dimensionality.,
"In addition, one can choose a Rd-valued random variable X as input features and deﬁne the corresponding labels by YX to obtain a prediction task, which can be solved by means of ERM.",
"Other methods: There exist a number of additional works related to the approximation capacities of NNs for high-dimensional PDEs, for example, [EGJS18, LTY19, SZ19].",
"In most of these works, the proof technique consists of emulating an existing method that does not suﬀer from the curse of dimensionality.",
"For instance, in the case of ﬁrst-order transport equations, one can show in some cases that NNs are capable of emulating the method of characteristics, which then also yields approximation results that are free of the curse of dimensionality [LP21].",
38 5 Optimization of deep neural networks We recall from Subsections 1.3 and 1.2.1 that the standard algorithm to solve the empirical risk minimization problem over the hypothesis set of NNs is stochastic gradient descent.,
"This method would be guaranteed to converge to a global minimum of the objective if the empirical risk were convex, viewed as a function of the NN parameters.",
"However, this function is severely nonconvex, may exhibit (higher-order) saddle points, seriously suboptimal local minima, and wide ﬂat areas where the gradient is very small.",
"On the other hand, in applications, excellent performance of SGD is observed.",
This indicates that the trajectory of the optimization routine somehow misses suboptimal critical points and other areas that may lead to slow convergence.,
"Clearly, the classical theory does not explain this performance.",
Below we describe some exemplary novel approaches that give partial explanations of this success.,
"In the ﬂavor of this article, the aim of this section is to present some selected ideas rather than giving an overview of the literature.",
"To give at least some detail about the underlying ideas and to keep the length of this section reasonable, a selection of results had to be made and some ground-breaking results had to be omitted.",
"5.1 Loss landscape analysis Given a NN Φ(·, θ) and training data s ∈Zm the function θ 7→r(θ) := bRs(Φ(·, θ)) describes, in a natural way, through its graph, a high-dimensional surface.",
This surface may have regions associated with lower values of bRs which resemble valleys of a landscape if they are surrounded by regions of higher values.,
The analysis of the topography of this surface is called loss landscape analysis.,
Below we shall discuss a couple of approaches that yield deep insights into the shape of this landscape.,
Index Loss No negative curvature at globally minimal risk.,
Critical points with high risk are unstable.,
0 0.25 0.5 Figure 5.1: Sketch of the distribution of critical points of the Hamiltonian of a spin glass model.,
Spin glass interpretation: One of the ﬁrst discoveries about the shape of the loss landscape comes from deep results in statistical physics.,
The Hamiltonian of the spin glass model is a random function on the (n −1)-dimensional sphere of radius √n.,
"Making certain simplifying assumptions, it was shown in [CHM+15] that the loss of a NN with random inputs can be considered as the Hamiltonian of a spin glass model, where the inputs of the model are the parameters of the NN.",
"This connection has far-reaching implications for the loss land- scape of NNs because of the following surprising property of the Hamiltonian of spin glass models: Consider the set of critical points of this set, and associate to each point an index that denotes the percentage of the eigenvalues of the Hessian at that point which are negative.",
This index corresponds to the relative number of directions in which the loss landscape has negative curvature.,
"Then with high probability, a picture like we see in Figure 5.1 emerges [AAˇC13].",
"More precisely, the further away from the optimal loss we are, the more unstable the critical points become.",
"Conversely, if one ﬁnds oneself in a local minimum, it is reasonable to assume that the loss is close to the global minimum.",
"While some of the assumptions establishing the connection between the spin glass model and NNs are unrealistic in practice [CLA15], the theoretical distribution of critical points as in Figure 5.1 is visible in many practical applications [DPG+14].",
Paths and level sets: Another line of research is to understand the loss landscape by analyzing paths through the parameter space.,
"In particular, the existence of paths in parameter space, such that the associated empirical risks are monotone along the path.",
"Surely, should there exist a path of nonincreasing empirical risk from every point to the global minimum, then we can be certain that no non-global minima exist, since no 39 such path can escape a minimum.",
An even stronger result holds.,
"In fact, the existence of such paths shows that the loss landscape has connected level sets [FB17, VBB19].",
A crucial ingredient of the analysis of such paths are linear substructures.,
"Consider a biasless two-layer NN Φ of the form Rd ∋x 7→Φ(x, θ) := n X j=1 θ(2) j ϱ  ⟨θ(1) j , x 1  ⟩  , (5.1) where θ(1) j ∈Rd+1 for j ∈[n], θ(2) ∈Rn, ϱ is a Lipschitz continuous activation function, and we augment the vector x by a constant 1 in the last coordinate as outlined in Remark 1.5.",
"If we consider θ(1) to be ﬁxed, then it is clear that the space eFθ(1) := {Φ(·, θ): θ = (θ(1), θ(2)), θ(2) ∈Rn} is a linear space.",
"If the risk23 is convex, as is the case for the widely used quadratic or logistic loss, then this implies that θ(2) 7→r  (θ(1), θ(2))  is a convex map and hence, for every parameter set P ⊂Rn this map assumes its maximum on ∂P.",
"Therefore, within the vast parameter space, there are many paths traveling along which does not increase the risk above the risk of the start and end points.",
"This idea was, for example, used in [FB17] in a way similar to the following simple sketch: Assume that, for two parameters θ and θmin there exists a linear subspace of NNs eFˆθ(1) such that there are paths γ1 and γ2 connecting Φ(·, θ) and Φ(·, θmin) to eFˆθ(1) respectively.",
"Further assume that the paths are such that along γ1 and γ2 the risk does not signiﬁcantly exceed max{r(θ), r(θmin)}.",
Figure 5.2 shows a visualization of these paths.,
"In this case, a path from θ to θmin not signiﬁcantly exceeding r(θ) along the way is found by concatenating the paths γ1, a path along eFˆθ(1), and γ2.",
"By the previous discussion, we know that only γ1 and γ2 determine the extent to which the combined path exceeds r(θ) along its way.",
"Hence, we need to ask about the existence of eFˆθ(1) that facilitates the construction of appropriate γ1 and γ2.",
"Φ(·, θmin) Φ(·, θ) e Fˆθ(1) Φ(·, θ∗) γ1 Figure 5.2: Construction of a path from an initial point θ to the global minimum θmin that does not have sig- niﬁcantly higher risk than the initial point along the way.",
We depict here the landscape as a function of the neural network realizations instead of their parametrizations so that this landscape is convex.,
"To understand why a good choice of eFˆθ(1), so that the risk along γ1 and γ2 will not rise much higher than r(θ), is likely possible, we set24 ˆθ(1) j := ( θ(1) j for j ∈[n/2], (θ(1) min)j for j ∈[n] \ [n/2].",
"In other words, the ﬁrst half of ˆθ(1) is made from θ(1) and the second from θ(1) min.",
"If θ(1) j , j ∈[N], are realizations of random variables distributed uniformly on the d-dimensional unit sphere, then by invoking standard covering bounds of spheres (e.g., [Ver18, Corollary 4.2.13]), we expect that, for ε > 0 and a suﬃciently large number of neurons n, the vectors (θ(1) j )n/2 j=1 already ε-approximate all vectors (θ(1) j )n j=1.",
"Replacing all vectors (θ(1) j )n j=1 by their nearest neighbor in (θ(1) j )n/2 j=1 can be done with a linear path in the parameter space, and, given that r is locally Lipschitz continuous and ∥θ(2)∥1 is bounded, this operation will not increase the risk by more than O(ε).",
We denote the vector resulting from this replacement procedure by θ(1) ∗.,
"Since for all j ∈[n] \ [n/2] we now have that ϱ  ⟨(θ(1) ∗)j,  · 1  ⟩  ∈  ϱ  ⟨(θ(1) ∗)k,  · 1  ⟩  : k ∈[n/2]  , 23As most statements in this subsection are valid for the empirical risk r(θ) = b Rs(Φ(·, θ)) as well as the risk r(θ) = R(Φ(·, θ)), given a suitable distribution of Z, we will just call r the risk.",
24We assume w.l.o.g.,
that n is a multiple of 2.,
"40 there exists a vector θ(2) ∗ with (θ(2) ∗)j = 0, j ∈[n] \ [n/2], so that Φ(·, (θ(1) ∗, θ(2))) = Φ(·, (θ(1) ∗, λθ(2) ∗ + (1 −λ)θ(2))), λ ∈[0, 1].",
"In particular, this path does not change the risk between (θ(1) ∗, θ(2)) and (θ(1) ∗, θ(2) ∗).",
"Now, since (θ(2) ∗)j = 0 for j ∈[n] \ [n/2], the realization Φ(·, (θ(1) ∗, θ(2) ∗)) is computed by a sub-network consisting of the ﬁrst n/2 hidden neurons and we can replace the parameters corresponding to the other neurons without any eﬀect on the realization function.",
"Speciﬁcally, it holds that Φ(·, (θ(1) ∗, θ(2) ∗)) = Φ(·, (λˆθ(1) + (1 −λ)θ(1) ∗, θ(2) ∗)), λ ∈[0, 1], yielding a path of constant risk between (θ(1) ∗, θ(2) ∗) and (ˆθ(1), θ(2) ∗).",
Connecting these paths completes the construction of γ1 and shows that the risk along γ1 does not exceed that at θ by more than O(ε).,
"Of course, γ2 can be constructed in the same way.",
The entire construction is depicted in Figure 5.2.,
"Overall, this derivation shows that for suﬃciently wide NNs (appropriately randomly initialized) it is very likely possible to almost connect a random parameter value to the global minimum with a path which along the way does not need to climb much higher than the initial risk.",
"In [VBB19], a similar approach is taken and the convexity in the last layer is used.",
"However, the authors invoke the concept of intrinsic dimension to elegantly solve the non-linearity of r((θ(1), θ(2))) with respect to θ(1).",
"Additionally, [SS16] constructs a path of decreasing risk from random initializations.",
"The idea here is that if one starts at a point of suﬃciently high risk, one can always ﬁnd a path to the global optimum with strictly decreasing risk.",
"The intriguing insight behind this result is that if the initialization is suﬃciently bad, i.e., worse than that of a NN outputting only zero, then there exist two operations that inﬂuence the risk directly.",
"Multiplying the last layer with a number smaller than one will decrease the risk, whereas the opposite will increase it.",
"Using this tuning mechanism, any given potentially non-monotone path from the initialization to the global minimum can be modiﬁed so that it is strictly monotonically decreasing.",
"In a similar spirit, [NH17] shows that if a deep NN has a layer with more neurons than training data points, then under certain assumptions the training data will typically be mapped to linearly independent points in that layer.",
"Of course, this layer could then be composed with a linear map that maps the linearly independent points to any desirable output, in particular one that achieves vanishing empirical risk, see also Proposition 1.1.",
"As for two-layer NNs, the previous discussion on linear paths immediately shows that in this situation a monotone path to the global minimum exists.",
"5.2 Lazy training and provable convergence of stochastic gradient descent When training highly overparametrized NNs, one often observes that the parameters of the NNs barely change during training.",
"In Figure 5.3, we show the relative distance that the parameters travel through the parameter space during the training of NNs of varying numbers of neurons per layer.",
"The eﬀect described above has been observed repeatedly and theoretically explained, see, e.g., [DZPS18, LL18, AZLS19, DLL+19, ZCZG20].",
"In Subsection 2.1, we have already seen a high-level overview and, in particular, the function space perspective of this phenomenon in the inﬁnite width limit.",
Below we present a short and highly simpliﬁed derivation of this eﬀect and show how it leads to provable convergence of gradient descent for suﬃciently overparametrized deep NNs.,
A simple learning model: We consider again the simple NN model of (5.1) with a smooth activation function ϱ which is not aﬃne linear.,
"For the quadratic loss and training data s = ((x(i), y(i)))m i=1 ∈(Rd ×R)m, where xi ̸= xj for all i ̸= j, the empirical risk is given by r(θ) = bRs(θ) = 1 m m X i=1 (Φ(x(i), θ) −y(i))2.",
"Let us further assume that Θ(1) j ∼N(0, 1/n)d+1, j ∈[n], and Θ(2) j ∼N(0, 1/n), j ∈[n], are independent random variables.",
"41 Figure 5.3: Four networks with architecture ((1, n, n, 1), ϱR) and n ∈{20, 100, 500, 2500} neurons per hidden layer were trained by gradient descent to ﬁt four points that are shown in the middle ﬁgure as black dots.",
We depict on the left the relative Euclidean distance of the parameters from the initialization through the training process.,
"In the middle, we show the ﬁnal trained NNs.",
On the right we show the behavior of the training error.,
"A peculiar kernel: Next, we would like to understand how the gradient ∇θr(Θ) looks like with high probability over the initialization Θ = (Θ(1), Θ(2)).",
"Similar to (2.3), we have by restricting the gradient to θ(2) and applying the chain rule that ∥∇θr(Θ)∥2 2 ≥ 4 m2 m X i=1 ∇θ(2)Φ(x(i), Θ)(Φ(x(i), Θ) −y(i)) 2 2 = 4 m2  (Φ(x(i), Θ) −y(i))m i=1 T ¯KΘ(Φ(x(j), Θ) −y(j))m j=1, (5.2) where ¯KΘ is a random Rm×m-valued kernel given by ( ¯KΘ)i,j :=  ∇θ(2)Φ(x(i), Θ) T ∇θ(2)Φ(x(j), Θ), i, j ∈[m].",
This kernel is closely related to the neural tangent kernel in (2.4) evaluated at the features (x(i))m i=1 and the random initialization Θ.,
"It is a slightly simpliﬁed version thereof, as in (2.4) the gradient is taken with respect to the full vector θ.",
This can also be regarded as the kernel associated with a random features model [RR+07].,
"Note that for our two-layer NN we have that  ∇θ(2)Φ(x, Θ)  k = ϱ  Θ(1) k , x 1   , x ∈Rd, k ∈[n].",
"Thus, we can write ¯KΘ as the following sum of (random) rank one matrices: ¯KΘ = n X k=1 vkvT k with vk =  ϱ  Θ(1) k ,  x(i) 1  m i=1 ∈Rm, k ∈[n].",
(5.3) The kernel ¯KΘ are symmetric and positive semi-deﬁnite by construction.,
"It is positive deﬁnite if it is non-singular, i.e., if at least m of the n vectors vk, k ∈[n], are linearly independent.",
"Proposition 1.1 shows that for n = m the probability of that event is not zero, say δ, and is therefore at least 1 −(1 −δ)⌊n/m⌋for arbitrary n. In other words, the probability increases rapidly with n. It is also clear from (5.3) that E[ ¯KΘ] scales linearly with n. From this intuitive derivation, we conclude that for suﬃciently large n, with high probability ¯KΘ is a positive deﬁnite kernel with smallest eigenvalue λmin( ¯KΘ) scaling linearly with n. The properties of ¯KΘ, in particular its positive deﬁniteness, have been studied much more rigorously as already described in Subsection 2.1.",
"42 Control of the gradient: Applying the expected behavior of the smallest eigenvalue λmin( ¯KΘ) of ¯KΘ to (5.2), we conclude that with high probability ∥∇θr(Θ)∥2 2 ≥ 4 m2 λmin( ¯KΘ)∥(Φ(x(i), Θ) −y(i))m i=1∥2 2 ≳n mr(Θ).",
"(5.4) To understand what will happen when applying gradient descent, we ﬁrst need to understand how the situation changes in a neighborhood of Θ.",
"We ﬁx x ∈Rd and observe that by the mean value theorem for all ¯θ ∈B1(0) we have ∇θΦ(x, Θ) −∇θΦ(x, Θ + ¯θ) 2 2 ≲ sup ˆθ∈B1(0) ∇2 θΦ(x, Θ + ˆθ) 2 op, (5.5) where ∥∇2 θΦ(x, Θ + ˆθ)∥op denotes the operator norm of the Hessian of Φ(x, ·) at Θ + ˆθ.",
"From inspection of (5.1), it is not hard to see that for all i, j ∈[n] and k, ℓ∈[d + 1] E ""∂2Φ(x, Θ) ∂θ(2) i ∂θ(2) j 2 # = 0, E "" ∂2Φ(x, Θ) ∂θ(2) i ∂(θ(1) j )k 2 # ≲δi,j, and E "" ∂2Φ(x, Θ) ∂(θ(1) i )k∂(θ(1) j )ℓ 2 # ≲δi,j n , where δi,j = 0 if i ̸= j and δi,i = 1 for all i, j ∈[n].",
"For suﬃciently large n, we have that ∇2 θΦ(x, Θ) is in expectation approximately a block band matrix with band-width d + 1.",
"Therefore, we conclude that E  ∥∇2 θΦ(x, Θ)∥2 op  ≲1.",
"Hence, we obtain by concentration of Gaussian random variables that with high probability ∥∇2 θΦ(x, Θ)∥2 op ≲1.",
"By the block-banded form of ∇2 θΦ(x, Θ) we have that, even after perturbation of Θ by a vector ˆθ with norm bounded by 1, the term ∥∇2 θΦ(x, Θ + ˆθ)∥2 op is bounded, which yields that the right-hand side of (5.5) is bounded with high probability.",
"Using (5.5), we can extend (5.4), which holds with high probability, to a neighborhood of Θ by the following argument: Let ¯θ ∈B1(0), then ∥∇θr(Θ + ¯θ)∥2 2 ≥ 4 m2 m X i=1 ∇θ(2)Φ(x(i), Θ + ¯θ)(Φ(x(i), Θ + ¯θ) −y(i)) 2 2 = (5.5) 4 m2 m X i=1 (∇θ(2)Φ(x(i), Θ) + O(1))(Φ(x(i), Θ + ¯θ) −y(i)) 2 2 ≳ (∗) 1 m2 (λmin( ¯KΘ) + O(1))∥(Φ(x(i), Θ + ¯θ) −y(i))m i=1∥2 2 ≳n mr(Θ + ¯θ), (5.6) where the estimate marked by (∗) uses the positive deﬁniteness of ¯KΘ again and only holds for suﬃciently large n, so that the O(1) term is negligible.",
"We conclude that, with high probability over the initialization Θ, on a ball of ﬁxed radius around Θ the squared Euclidean norm of the gradient of the empirical risk is lower bounded by n m times the empirical risk.",
"Exponential convergence of gradient descent: For suﬃciently small step sizes η, the observation in the previous paragraph yields the following convergence rate for gradient descent as in Algorithm 1, speciﬁcally (1.5), with m′ = m and Θ(0) = Θ: If ∥Θ(k) −Θ∥≤1 for all k ∈[K + 1], then25 r(Θ(K+1)) ≈r(Θ(K)) −η∥∇θr(Θ(K))∥2 2 ≤  1 −cηn m  r(Θ(K)) ≲  1 −cηn m K , (5.7) for c ∈(0, ∞) so that ∥∇θr(Θ(k))∥2 2 ≥cn m r(Θ(k)) for all k ∈[K].",
25Note that the step-size η needs to be small enough to facilitate the approximation step in (5.7).,
"Hence, we cannot simply put η = m/(cn) in (5.7) and have convergence after one step.",
43 Let us assume without proof that the estimate (5.6) could be extended to an equivalence.,
"In other words, we assume that we additionally have that ∥∇θr(Θ + ¯θ)∥2 2 ≲n mr(Θ + ¯θ).",
"This, of course, could be shown with similar tools as were used for the lower bound.",
Then we have that ∥Θ(k) −Θ∥2 ≤1 for all k ≲ p m/(η2n).,
"Setting t = p m/(η2n) and using the limit deﬁnition of the exponential function, i.e., limt→∞(1−x/t)t = e−x, yields for suﬃciently small η that (5.7) is bounded by e−c√ n/m.",
"We conclude that, with high probability over the initialization, gradient descent converges with an exponential rate to an arbitrary small empirical risk if the width n is suﬃciently large.",
"In addition, the iterates of the descent algorithm even stay in a small ﬁxed neighborhood of the initialization during training.",
"Because the parameters only move very little, this type of training has also been coined lazy training [COB19].",
"Similar ideas as above, have led to groundbreaking convergence results of SGD for overparametrized NNs in much more complex and general settings, see, e.g., [DZPS18, LL18, AZLS19].",
"In the inﬁnite width limit, NN training is practically equivalent to kernel regression, see Subsection 2.1.",
If we look at Figure 5.3 we see that the most overparametrized NN interpolates the data like a kernel-based interpolator would.,
"In a sense, which was also highlighted in [COB19], this shows that, while overparametrized NNs in the lazy training regime have very nice properties, they essentially act like linear methods.",
"6 Tangible eﬀects of special architectures In this section, we describe results that isolate the eﬀects of certain aspects of NN architectures.",
"As we have discussed in Subsection 1.3, typically only either the depth or the number of parameters are used to study theoretical aspects of NNs.",
We have seen instances of this throughout Sections 3 and 4.,
"Moreover, also in Section 5, we saw that wider NNs enjoy certain very favorable properties from an optimization point of view.",
"Below, we introduce certain specialized NN architectures.",
"We start with one of the most widely used types of NNs, the convolutional neural network (CNN).",
In Subsection 6.2 we introduce skip connections and in Subsection 6.3 we discuss a speciﬁc class of CNNs equipped with an encoder-decoder structure that are frequently used in image processing techniques.,
We introduce the batch normalization block in Subsection 6.4.,
"Then, we discuss sparsely connected NNs that typically result as an extraction from fully connected NNs in Subsection 6.5.",
"Finally, we brieﬂy comment on recurrent neural networks in Subsection 6.6.",
"As we have noted repeatedly throughout this manuscript, it is impossible to give a full account of the literature in a short introductory article.",
"In this section, this issue is especially severe since the number of special architectures studied in practice is enormous.",
"Therefore, we had to omit many very inﬂuential and widely used neural network architectures.",
"Among those are graph neural networks, which handle data from non- Euclidean input spaces.",
"We refer to the survey articles [BBL+17, WPC+21] for a discussion.",
"Another highly successful type of architectures are (variational) autoencoders [AHS85, HZ94].",
These are neural networks with a bottleneck that enforce a more eﬃcient representation of the data.,
"Similarly, generative adversarial networks [GPAM+14] which are composed of two neural networks, one generator and one discriminator, could not be discussed here.",
Another widely used component of architectures used in practice is the so-called dropout layer.,
This layer functions through removing some neurons randomly during training.,
This procedure empirically prevents overﬁtting.,
An in-detail discussion of the mathematical analysis behind this eﬀect is beyond the scope of this manuscript.,
"We refer to [WZZ+13, SHK+14, HV17, MAV18] instead.",
"Finally, the very successful attention mechanism [BCB15, VSP+17], that is the basis of transformer neural networks, had to be omitted.",
"Before we start describing certain eﬀects of special NN architectures, a word of warning is required.",
"The special building blocks, which will be presented below, have been developed based on a speciﬁc need in applications and are used and combined in a very ﬂexible way.",
"To describe these tools theoretically without completely inﬂating the notational load, some simplifying assumptions need to be made.",
It is very likely that the simpliﬁed building blocks do not accurately reﬂect the practical applications of these tools in all use cases.,
"44 6.1 Convolutional neural networks Especially for very high-dimensional inputs where the input dimensions are spatially related, fully connected NNs seem to require unnecessarily many parameters.",
"For example, in image classiﬁcation problems, neigh- boring pixels very often share information and the spatial proximity should be reﬂected in the architecture.",
"Based on this observation, it appears reasonable to have NNs that have local receptive ﬁelds in the sense that they collect information jointly from spatially close inputs.",
"In addition, in image processing, we are not necessarily interested in a universal hypothesis set.",
"A good classiﬁer is invariant under many operations, such as translation or rotation of images.",
It seems reasonable to hard-code such invariances into the architecture.,
These two principles suggest that the receptive ﬁeld of a NN should be the same on diﬀerent translated patches of the input.,
"In this sense, parameters of the architecture can be reused.",
"Together, these arguments make up the three fundamental principles of convolutional NNs: local receptive ﬁelds, parameter sharing, and equivariant representations, as introduced in [LBD+89].",
We will provide a mathematical formulation of convolutional NNs below and then revisit these concepts.,
"A convolutional NN corresponds to multiple convolutional blocks, which are special types of layers.",
"For a group G, which typically is either [d] ∼= Z/(dZ) or [d]2 ∼= (Z/(dZ))2 for d ∈N, depending on whether we are performing one-dimensional or two-dimensional convolutions, the convolution of two vectors a, b ∈RG is deﬁned as (a ∗b)i = X j∈G ajbj−1i, i ∈G.",
"Now we can deﬁne a convolutional block as follows: Let eG be a subgroup of G, let p : G →eG be a so-called pooling-operator, and let C ∈N denote the number of channels.",
"Then, for a series of kernels κi ∈RG, i ∈[C], the output of a convolutional block is given by RG ∋x 7→x′ := (p(x ∗κi))C i=1 ∈(R e G)C. (6.1) A typical example of a pooling operator is for G = (Z/(2dZ))2 and eG = (Z/(dZ))2 the 2 × 2 subsampling operator p : RG →R e G, x 7→(x2i−1,2j−1)d i,j=1.",
Popular alternatives are average pooling or max pooling.,
These operations then either pass the average or the maximum over patches of similar size.,
The convolutional kernels correspond to the aforementioned receptive ﬁelds.,
"They can be thought of as local if they have small supports, i.e., few nonzero entries.",
"As explained earlier, a convolutional NN is built by stacking multiple convolutional blocks after another26.",
"At some point, the output can be ﬂattened, i.e., mapped to a vector and is then fed into a FC NN (see Deﬁnition 1.4).",
We depict this setup in Figure 6.1.,
"Owing to the fact that convolution is a linear operation, depending on the pooling operation, one may write a convolutional block (6.1) as a FC NN.",
"For example, if G = (Z/(2dZ))2 and the 2 × 2 subsampling pooling operator is used, then the convolutional block could be written as x 7→Wx for a block circulant matrix W ∈R(Cd2)×(2d)2.",
"Since we require W to have a special structure, we can interpret a convolutional block as a special, restricted feed-forward architecture.",
"After these considerations, it is natural to ask how the restriction of a NN to a pure convolutional structure, i.e., consisting only of convolutional blocks, will aﬀect the resulting hypothesis set.",
The ﬁrst natural question is whether the set of such NNs is still universal in the sense of Theorem 1.15.,
The answer to this question depends strongly on the type of pooling and convolution that is allowed.,
"If the convolution is performed with padding, then the answer is yes [OS19, Zho20b].",
"On the other hand, for circular convolutions and without pooling, universality does not hold, but the set of translation equivariant functions can be universally approximated [Yar18b, PV20].",
"Furthermore, [Yar18b] illuminates the eﬀect of subsample pooling by showing that, if no pooling is applied, then universality cannot be achieved, whereas if pooling is applied 26We assume that the deﬁnition of a convolutional block is suitably extended to input data in the Cartesian product (RG)C. For instance, one can take an aﬃne linear combination of C mappings as in (6.1) acting on each coordinate.",
"Moreover, one may also interject an activation function between the blocks.",
45 Convolution Pooling Convolution Pooling Fully connected NN Figure 6.1: Illustration of a convolutional neural network with two-dimensional convolutional blocks and 2 × 2 subsampling as pooling operation.,
then universality is possible.,
The eﬀect of subsampling in CNNs from the viewpoint of approximation theory is further discussed in [Zho20a].,
The role of other types of pooling in enhancing invariances of the hypothesis set will be discussed in Subsection 7.1 below.,
6.2 Residual neural networks Let us ﬁrst illustrate a potential obstacle when training deep NNs.,
Consider for L ∈N the product operation RL ∋x 7→π(x) = L Y ℓ=1 xℓ.,
"It is clear that ∂ ∂xk π(x) = L Y ℓ̸=k xℓ, x ∈RL.",
"Therefore, for suﬃciently large L, we expect that ∂π ∂xk will be exponentially small, if |xℓ| < λ < 1 for all ℓ∈[L] or exponentially large, if |xℓ| > λ > 1 for all ℓ∈[L].",
"The output of a general NN, considered as a directed graph, is found by repeatedly multiplying the input with parameters in every layer along the paths that lead from the input to the output neuron.",
"Due to the aforementioned phenomenon, it is often observed that training NNs suﬀers from either the exploding or the vanishing gradient problem, which may prevent lower layers from training at all.",
The presence of an activation function is likely to exacerbate this eﬀect.,
The exploding or vanishing gradient problem seems to be a serious obstacle towards eﬃcient training of deep NNs.,
"In addition to the vanishing and exploding gradient problems, there is an empirically observed degradation problem [HZRS16].",
This phrase describes the phenomenon that FC NNs seem to achieve lower accuracy on both the training and test data when increasing their depth.,
"From an approximation theoretic perspective, deep NNs should always be superior to shallow NNs.",
The reason for this is that NNs with two layers can either exactly represent the identity map or approximate it arbitrarily well.,
"Concretely, for the ReLU activation function ϱR we have that x = ϱR(x + b) −b for x ∈Rd with xi > −bi, where b ∈Rd.",
"In addition, for any activation function ϱ which is continuously diﬀerentiable on a neighborhood of some point λ ∈R with ϱ′(λ) ̸= 0 one can approximate the identity arbitrary well, see (1.8).",
"Because of this, extending a NN architecture by one layer can only enlarge the associated hypothesis set.",
"Therefore, one may expect that the degradation problem is more associated with the optimization aspect of learning.",
This problem is addressed by a small change to the architecture of a feed-forward NN in [HZRS16].,
46 idR3 idR3 idR3 idR3 Figure 6.2: Illustration of a neural network with residual blocks.,
"Instead of deﬁning a FC NN Φ as in (1.1), one can insert a residual block in the ℓ-th layer by redeﬁning27 ¯Φ(ℓ)(x, θ) = ϱ(Φ(ℓ)(x, θ)) + ¯Φ(ℓ−1)(x, θ), (6.2) where we assume that Nℓ= Nℓ−1.",
Such a block can be viewed as the sum of a regular FC NN and the identity which is referred to as skip connection or residual connection.,
A sketch of a NN with residual blocks is shown in Figure 6.2.,
Inserting a residual block in all layers leads to a so-called residual NN.,
"A prominent approach to analyze residual NNs is by establishing a connection with optimal control problems and dynamical systems [E17, TvG18, EHL19, LLS19, RH19, LML+20].",
"Concretely, if each layer of a NN Φ is of the form (6.2), then we have that ¯Φ(ℓ) −¯Φ(ℓ−1) = ϱ(Φ(ℓ)) =: h(ℓ, Φ(ℓ)), where we abbreviate ¯Φ(ℓ) = ¯Φ(ℓ)(x, θ) and set ¯Φ(0) = x.",
"Hence, (¯Φ(ℓ))L−1 ℓ=0 corresponds to an Euler discretization of the ODE ˙φ(t) = h(t, φ(t)), φ(0) = x, where t ∈[0, L −1] and h is an appropriate function.",
"Using this relationship, deep residual NNs can be studied in the framework of the well-established theory of dynamical systems, where strong mathematical guarantees can be derived.",
"6.3 Framelets and U-Nets One of the most prominent application areas of deep NNs are inverse problems, particularly those in the ﬁeld of imaging science, see also Subsection 8.1.",
"A speciﬁc architectural design of CNNs, namely so-called U-nets introduced in [RFB15], seems to perform best for this range of problems.",
We depict a sketch of a U-net in Figure 6.3.,
"However, a theoretical understanding of the success of this architecture was lacking.",
"Recently, an innovative approach called deep convolutional framelets was suggested in [YHC18], which we now brieﬂy explain.",
"The core idea is to take a frame-theoretic viewpoint, see, e.g., [CKP12], and regard the forward pass of a CNN as a decomposition in terms of a frame (in the sense of a generalized basis).",
A similar approach will be taken in Subsection 7.2 for understanding the learned kernels using sparse coding.,
"However, based on the analysis and synthesis operators of the corresponding frame, the usage of deep convolutional framelets naturally leads to a theoretical understanding of encoder-decoder architectures, such as U-nets.",
"Let us describe this approach for one-dimensional convolutions on the group G := Z/(dZ) with kernels deﬁned on the subgroup H := Z/(nZ), where d, n ∈N with n < d, see also Subsection 6.1.",
"We deﬁne the convolution between u ∈RG and v ∈RH by zero-padding v, i.e., g ∗◦v := g ∗¯v, where ¯v ∈RG is deﬁned by ¯vi = vi for i ∈H and ¯vi = 0 else.",
"As an important tool, we consider the Hankel matrix Hn(x) = (xi+j)i∈G,j∈H ∈Rd×n associated with x ∈RG.",
"As one key property, matrix-vector multiplications with Hankel matrices are translated to convolutions via28 ⟨e(i), Hn(x)v⟩= X j∈H xi+jvj = ⟨x, e(i) ∗◦v⟩, i ∈G, (6.3) 27One can also skip multiple layers, e.g., in [HZRS16] two or three layers skipped, use a simple transformation instead of the identity [SGS15], or randomly drop layers [HSL+16].",
28Here and in the following we naturally identify elements in RG and RH with the corresponding vectors in Rd and Rn.,
47 Figure 6.3: Illustration of a simpliﬁed U-net neural network.,
"Down-arrows stand for pooling, up arrows for deconvolution or upsampling, right arrows for convolution or fully connected steps.",
Dashed lines are skip connections.,
"where e(i) := 1{i} ∈RG and v ∈RH, see [YGLD17].",
"Further, we can recover the k-th coordinate of x by the Frobenius inner product between Hn(x) and the Hankel matrix associated with e(k), i.e., 1 nTr  Hn(e(k))T Hn(x)  = 1 n X j∈H X i∈G e(k) i+jxi+j = 1 n|H|xk = xk.",
"(6.4) This allows us to construct global and local bases as follows: Let p, q ∈N, let U = u1 · · · up  ∈Rd×p, V = v1 · · · vq  ∈Rn×q, eU = ˜u1 · · · ˜up  ∈Rd×p, and eV = ˜v1 · · · ˜vq  ∈Rn×q, and assume that Hn(x) = eUU T Hn(x)V eV T .",
"(6.5) For p ≥d and q ≥n, this is, for instance, satisﬁed if U and V constitute frames with eU and eV being their respective dual frames, i.e., eUU T = Id and V eV T = In.",
"As a special case, one can consider orthonormal bases U = eU and V = eV with p = d and q = n. In the case p = q = r ≤n, where r is the rank of Hn(x), one can establish (6.5) by choosing the left and right singular vectors of Hn(x) as U = eU and V = eV , respectively.",
"The identity in (6.5), in turn, ensures the following decomposition: x = 1 n p X i=1 q X j=1 ⟨x, ui ∗◦vj⟩˜ui ∗◦˜vj.",
"(6.6) Observing that the vector vj ∈RH interacts locally with x ∈RG due to the fact that H ⊂G, whereas ui ∈RG acts on the entire vector x, we refer to (vj)q j=1 as local and (ui)p i=1 as global bases.",
"In the context of CNNs, vi can be interpreted as local convolutional kernel and ui as pooling operation29.",
"The proof of (6.6) 29Note that ⟨x, ui ∗◦vj⟩can also be interpreted as ⟨ui, vj ⋆x⟩, where ⋆denotes the cross-correlation between the zero-padded vj and x.",
"This is in line with software implementations for deep learning applications, e.g., TensorFlow and PyTorch, where typically cross-correlations are used instead of convolutions.",
"48 follows directly from properties (6.3), (6.4), and (6.5) as xk = 1 nTr  Hn(e(k))T Hn(x)  = 1 nTr  Hn(e(k))T eUU T Hn(x)V eV T  = 1 n p X i=1 q X j=1 ⟨ui, Hn(x)vj⟩⟨˜ui, Hn(e(k))˜vj⟩.",
"The decomposition in (6.6) can now be interpreted as a composition of an encoder and a decoder, x 7→C = (⟨x, ui ∗◦vj⟩)i∈[p],j∈[q] and C 7→1 n p X i=1 q X j=1 Ci,j ˜ui ∗◦˜vj, (6.7) which relates it to CNNs equipped with an encoder-decoder structure such as U-nets, see Figure 6.3.",
"Generalizing this approach to multiple channels, it is possible to stack such encoders and decoders which leads to a layered version of (6.6).",
"In [YHC18] it is shown that one can make an informed decision on the number of layers based on the rank of Hn(x), i.e., the complexity of the input features x.",
"Moreover, also an activation function such as the ReLU or bias vectors can be included.",
"The key question one can then ask is how the kernels can be chosen to obtain sparse coeﬃcients C in (6.7) and a decomposition such as (6.6), i.e., perfect reconstruction.",
"If U and V are chosen as the left and right singular vectors of Hn(x), one obtains a very sparse, however input-dependent, representation in (6.6) due to the fact that Ci,j = ⟨x, ui ∗◦vj⟩= ⟨ui, Hn(x)vj⟩= 0, i ̸= j.",
"Finally, using the framework of deep convolutional framelets, theoretical reasons for including skip connections can be derived, since they aid to obtain a perfect reconstruction.",
6.4 Batch normalization Batch normalization is a building block of NNs that was invented in [IS15] with the goal to reduce so-called internal covariance shift.,
"In essence, this phrase describes the (undesirable) situation where during training each layer receives inputs with diﬀerent distribution.",
"A batch normalization block is deﬁned as follows: For points b = (y(i))m i=1 ∈(Rn)m and β, γ ∈R, we deﬁne BN(β,γ) b (y) := γ y −µb σb + β, y ∈Rn, with µb = 1 m m X i=1 y(i) and σ2 b = 1 m m X i=1 (y(i) −µb)2, (6.8) where all operations are to be understood componentwise, see Figure 6.4.",
Such a batch normalization block can be added into a NN architecture.,
Then b is the output of the previous layer over a batch or the whole training data30.,
"Furthermore, the parameters β, γ are variable and can be learned during training.",
"Note that, if one sets β = µb and γ = σb, then BN(β,γ) b (y) = y for all y ∈Rn.",
"Therefore, a batch normalization block does not negatively aﬀect the expressivity of the architecture.",
"On the other hand, batch normalization does have a tangible eﬀect on the optimization aspects of deep learning.",
"Indeed, in [STIM18, Theorem 4.1], the following observation was made: 30In practice, one typically uses a moving average to estimate the mean µ and the standard deviation σ of the output of the previous layer over the whole training data by only using batches.",
49 µb σb β γ by = y−µb σb z = γby + β Figure 6.4: A batch normalization block after a fully connected neural network.,
"The parameters µb, σb are the mean and the standard deviation of the output of the fully connected network computed over a batch s, i.e., a set of inputs.",
"The parameters β, γ are learnable parts of the batch normalization block.",
Proposition 6.1 (Smoothening eﬀect of batch normalization).,
"Let m ∈N with m ≥2 and for every β, γ ∈R deﬁne B(β,γ) : Rm →Rm by B(β,γ)(b) = (BN(β,γ) b (y(1)), .",
", BN(β,γ) b (y(m))), b = (y(i))m i=1 ∈Rm, where BN(β,γ) b is given as in (6.8).",
"Let β, γ ∈R and let r: Rm →R be a diﬀerentiable function.",
"Then it holds for every b ∈Rm that ∥∇(r ◦B(β,γ))(b)∥2 2 = γ2 σ2 b  ∥∇r(b)∥2 −1 m⟨1, ∇r(b)⟩2 −1 m⟨B(0,1)(b), ∇r(b)⟩2 , where 1 = (1, .",
", 1) ∈Rm and σ2 b is given as in (6.8).",
"For multi-dimensional y(i) ∈Rn, i ∈[m], the same statement holds for all components as, by deﬁnition, the batch normalization block acts componentwise.",
"Proposition 6.1 follows from a convenient representation of the Jacobian of the mapping B(β,γ), given by ∂B(β,γ)(b) ∂b = γ σb  Im −1 m11T −1 mB(0,1)(b)(B(0,1)(b))T  , b ∈Rm, and the fact that { 1 √m, 1 √mB(0,1)(b)} constitutes an orthonormal set.",
"Choosing r to mimic the empirical risk of a learning task, Proposition 6.1 shows that, in certain situations— for instance, if γ is smaller than σb or if m is not too large—a batch normalization block can considerably reduce the magnitude of the derivative of the empirical risk with respect to the input of the batch normalization block.",
"By the chain rule, this implies that also the derivative of the empirical risk with respect to NN parameters inﬂuencing the input of the batch normalization block is reduced.",
"Interestingly, a similar result holds for second derivatives [STIM18, Theorem 4.2] if r is twice diﬀerentiable.",
One can conclude that adding a batch normalization block increases the smoothness of the optimization problem.,
"Since the parameters β and γ were introduced, including a batch normalization block also increases the dimension of the optimization problem by two.",
"6.5 Sparse neural networks and pruning For deep FC NNs, the number of trainable parameters usually scales like the square of the number of neurons.",
"For reasons of computational complexity and memory eﬃciency, it appears sensible to seek for techniques to reduce the number of parameters or extract sparse subnetworks (see Figure 6.5) without aﬀecting the output 50 Figure 6.5: A neural network with sparse connections.",
of a NN much.,
"One way to do this is by pruning [LDS89, HMD16].",
"Here, certain parameters of a NN are removed after training.",
"This is done, for example, by setting these parameters to zero.",
"In this context, the lottery ticket hypothesis was formulated in [FC18].",
"It states: “A randomly-initialized, dense NN contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original NN after training for at most the same number of iterations”.",
In [RWK+20] a similar hy- pothesis was made and empirically studied.,
"There, it is claimed that, for a suﬃciently overparametrized NN, there exists a subnetwork that matches the performance of the large NN after training without being trained itself, i.e., already at initialization.",
"Under certain simplifying assumptions, the existence of favorable subnetworks is quite easy to prove.",
We can use a technique that was previously indirectly used in Subsection 4.2—the Carath´eodory Lemma.,
"This result states the following: Let n ∈N, C ∈(0, ∞), and let (H, ∥· ∥) be a Hilbert space.",
"Let F ⊂H with supf∈F ∥f∥≤C and let g ∈H be in the convex hull of F. Then there exist fi ∈F, i ∈[n], and c ∈[0, 1]n with ∥c∥1 = 1 such that g − n X i=1 cifi ≤C √n, see, e.g., [Ver18, Theorem 0.0.2].",
Proposition 6.2 (Carath´eodory pruning).,
"Let d, n ∈N, with n ≥100 and let µ be a probability measure on the unit ball B1(0) ⊂Rd.",
"Let a = ((d, n, 1), ϱR) be the architecture of a two-layer ReLU network and let θ ∈RP ((d,n,1)) be corresponding parameters such that Φa(·, θ) = n X i=1 w(2) i ϱR(⟨(w(1) i , ·⟩+ b(1) i )), where (w(1) i , b(1) i ) ∈Rd × R, i ∈[n], and w(2) ∈Rn.",
Assume that for every i ∈[n] it holds that ∥w(1) i ∥2 ≤1/2 and b(1) i ≤1/2.,
"Then there exists a parameter ˜θ ∈RP ((d,n,1)) with at least 99% of its entries being zero such that ∥Φa(·, θ) −Φa(·, ˜θ)∥L2(µ) ≤15∥w(2)∥1 √n .",
"Speciﬁcally, there exists an index set I ⊂[n] with |I| ≤n/100 such that ˜θ satisﬁes that ew(2) i = 0, if i /∈I, and ( ew(1) i ,˜b(1) i ) = ( (w(1) i , b(1) i ), if i ∈I, (0, 0), if i /∈I.",
The result is clear if w(2) = 0.,
"Otherwise, deﬁne fi := ∥w(2)∥1ϱR(⟨w(1) i , ·⟩+ b(1) i ), i ∈[n], and observe that Φa(·, θ) is in the convex hull of {fi}n i=1 ∪{−fi}n i=1.",
"Moreover, by the Cauchy–Schwarz inequality, it holds that ∥fi∥L2(µ) ≤∥w(2)∥1∥fi∥L∞(B1(0)) ≤∥w(2)∥1.",
"We conclude with the Carath´eodory Lemma that there exists I ⊂[n] with |I| = ⌊n/100⌋≥n/200 and ci ∈[−1, 1], i ∈I, such that Φa(·, θ) − X i∈I cifi L2(µ) ≤∥w(2)∥1 p |I| ≤ √ 200∥w(2)∥1 √n , 51 which yields the result.",
"Proposition 6.2 shows that certain, very wide NNs can be approximated very well by sparse subnetworks where only the output weight matrix needs to be changed.",
"The argument of Proposition 6.2 is inspired by [BK18], where a much more reﬁned result is shown for deep NNs.",
6.6 Recurrent neural networks Figure 6.6: Sketch of a recurrent neu- ral network.,
Cycles in the computa- tional graph incorporate the sequen- tial structure of the input and output.,
"Recurrent NNs are NNs where the underlying graph is allowed to exhibit cycles as in Figure 6.6, see [Hop82, RHW86, Elm90, Jor90].",
"Previously, we had excluded cyclic computational graphs.",
"For a feed- forward NN, the computation of internal states is naturally performed step by step through the layers.",
"Since the output of a layer does not aﬀect previous layers, the order in which the computations of the NN are performed corresponds to the order of the layers.",
"For recurrent NNs, the concept of layers does not exist, and the order of operations is much more delicate.",
"Therefore, one considers time steps.",
"In each time step, all possible computations of the graph are applied to the current state of the NN.",
This yields a new internal state.,
"Given that time steps arise naturally from the deﬁnition of recurrent NNs, this NN type is typically used for sequential data.",
"If the input to a recurrent NN is a sequence, then every input determines the internal state of the recurrent NN for the following inputs.",
"Therefore, one can claim that these NNs exhibit a memory.",
"This fact is extremely desirable in natural language processing, which is why recurrent NNs are widely used in this application.",
"Recurrent NNs can be trained similarly to regular feed-forward NNs by an algorithm called backpropagation through time [MP69, Wer88, WZ95].",
This procedure essentially unfolds the recurrent structure yielding a classical NN structure.,
"However, the algorithm may lead to very deep structures.",
"Due to the vanishing and exploding gradient problem discussed earlier, very deep NNs are often hard to train.",
"Because of this, special recurrent structures were introduced that include gates which prohibit too many recurrent steps; these include the widely used LSTMs [HS97].",
The application area of recurrent NNs is typically quite diﬀerent from that of regular NNs since they are specialized on sequential data.,
"Therefore, it is hard to quantify the eﬀect of a recurrent connection on a fully connected NN.",
"However, it is certainly true that with recurrent connections certain computations can be performed much more eﬃciently than with feed-forward NN structures.",
"A particularly interesting construction can be found in [BF19, Theorem 4.4], where it is shown that a ﬁxed size, recurrent NN with ReLU activation function, can approximate the function x 7→x2 to any desired accuracy.",
The reason for this eﬃcient representation can be seen when considering the self-referential deﬁnition of the approximant to x −x2 shown in Figure 3.2.,
"On the other hand, with feed-forward NNs, it transpires from Theorem 3.3 that the approximation error of ﬁxed-sized ReLU NNs for any non-aﬃne function is greater than a positive lower bound.",
7 Describing the features a deep neural network learns This section presents two viewpoints which help in understanding the nature of features that can be described by NNs.,
"Section 7.1 summarizes aspects of the so-called scattering transform which constitutes a speciﬁc NN architecture that can be shown to satisfy desirable properties, such as translation and deformation invariance.",
Section 7.2 relates NN features to the current paradigm of sparse coding.,
7.1 Invariances and the scattering transform One of the ﬁrst theoretical contributions to the understanding of the mathematical properties of CNNs is [Mal12].,
"The approach taken in that work is to consider speciﬁc CNN architectures with ﬁxed parameters 52 that result in a stand-alone feature descriptor whose output may be fed into a subsequent classiﬁer (for example, a kernel support vector machine or a trainable FC NN).",
"From an abstract point of view, a feature descriptor is a function Ψ mapping from a signal space, such as L2(Rd) or the space of piecewise smooth functions, to a feature space.",
"In an ideal world, such a classiﬁer should “factor” out invariances that are irrelevant to a subsequent classiﬁcation problem while preserving all other information of the signal.",
"A very simple example of a classiﬁer which is invariant under translations is the Fourier modulus Ψ: L2(Rd) →L2(Rd), u 7→|ˆu|.",
"This follows from the fact that a translation of a signal u results in a modulation of its Fourier transform, i.e., \ u(· −τ)(ω) = e−2πi⟨τ,ω⟩ˆu(ω), τ, ω ∈Rd.",
"Furthermore, in most cases (for example, if u is a generic compactly supported function [GKR20]), u can be reconstructed up to a translation from its Fourier modulus [GKR20] and an energy conservation property of the form ∥Ψ(u)∥L2 = ∥u∥L2 holds true.",
"Translation invariance is, for example, typically exhibited by image classiﬁers, where the label of an image does not change if it is translated.",
In practical problems many more invariances arise.,
Providing an analogous representation that factors out general invariances would lead to a signiﬁcant reduction in the problem dimensionality and constitutes an extremely promising route towards dealing with the very high dimensionality that is commonly encountered in practical problems [Mal16].,
"This program is carried out in [Mal12] for additional invariances with respect to deformations u 7→uτ := u(· −τ(·)), where τ : Rd →Rd is a smooth mapping.",
"Such transformations may occur in practice, for instance, as image warpings.",
"In particular, a feature descriptor Ψ is designed that, with a suitable norm ∥· ∥on the image of Ψ, (a) is Lipschitz continuous with respect to deformations in the sense that ∥Ψ(u) −Ψ(uτ)∥≲K(τ, ∇τ, ∇2τ) holds for some K that only mildly depends on τ and essentially grows linearly in ∇τ and ∇2τ, (b) is almost (i.e., up to a small and controllable error) invariant under translations of the input data, and (c) contains all relevant information on the input data in the sense that an energy conservation property ∥Ψ(u)∥≈∥u∥L2 holds true.",
"Observe that, while the action of translations only represents a d-parameter group, the action of deforma- tions/warpings represents an inﬁnite-dimensional group.",
"Hence, a deformation invariant feature descriptor represents a big potential for dimensionality reduction.",
"Roughly speaking, the feature descriptor Ψ of [Mal12] (also coined the scattering transform) is deﬁned by collecting features that are computed by iteratively applying a wavelet transform followed by a pointwise modulus non-linearity and a subsequent low-pass ﬁltering step, i.e., |||u ∗ψj1| ∗ψj2 ∗.",
"| ∗ψjℓ| ∗ϕJ, where ψj refers to a wavelet at scale j and ϕJ refers to a scaling function at scale J.",
The collection of all these so-called scattering coeﬃcients can then be shown to satisfy the properties in (a)–(c) above in a suitable (asymptotic) sense.,
The proof of this result relies on a subtle interplay between a “deformation covariance” property of the wavelet transform and a “regularizing” property of the operation of convolution with the modulus of a wavelet.,
"We remark that similar results can be shown also for diﬀerent systems, such as Gabor frames [WGB17, CL19].",
"7.2 Hierarchical sparse representations The previous approach modeled the learned features by a speciﬁc dictionary, namely wavelets.",
It is well known that one of the striking properties of wavelets is to provide sparse representations for functions belonging to certain function classes.,
"More generally, we speak of sparse representations with respect to a representation system.",
"For a vector x ∈Rd, a sparsifying representation system D ∈Rd×p—also called dictionary—is such that x = Dφ with the coeﬃcients φ ∈Rp being sparse in the sense that ∥φ∥0 := | supp(φ)| = |{i ∈[p]: φi ̸= 0}| is small compared to p. A similar deﬁnition can be made for signals in inﬁnite-dimensional spaces.",
"Taking 53 sparse representations into account, the theory of sparse coding provides an approach to a theoretical understanding of the features a deep NN learns.",
"One common method in image processing is the utilization of not the entire image but overlapping patches of it, coined patch-based image processing.",
"Thus of particular interest are local dictionaries which sparsify those patches, but presumably not the global image.",
"This led to the introduction of the convolutional sparse coding model (CSC model), which links such local and global behaviors.",
"Let us describe this model for one-dimensional convolutions on the group G := Z/(dZ) with kernels supported on the subgroup H := Z/(nZ), where d, n ∈N with n < d, see also Subsection 6.1.",
"The corresponding CSC model is based on a decomposition of a global signal x ∈(RG)c with c ∈N channels as xi = C X j=1 κi,j ∗φj, i ∈[c], (7.1) where φ ∈(RG)C is supposed to be a sparse representation with C ∈N channels and κi,j ∈RG, i ∈[c], j ∈[C], are local kernels with supp(κi,j) ⊂H.",
"Let us consider a patch ((xi)g+h)i∈[c],h∈H of n adjacent entries, starting at position g ∈G, in each channel of x.",
"The condition on the support of the kernels κi,j and the representation in (7.1) imply that this patch is only aﬀected by a stripe of at most (2n −1) entries in each channel of φ.",
"The local, patch-based sparsity of the representation φ can thus be appropriately measured via ∥φ∥(n) 0,∞:= max g∈G ∥((φj)g+k)j∈[C],k∈[2n−1]∥0, see [PSE17].",
"Furthermore, note that we can naturally identify x and φ with vectors in Rdc and RdC and write x = Dφ, where D ∈Rdc×dC is a matrix consisting of circulant blocks, typically referred to as a convolutional dictionary.",
"The relation between the CSC model and deep NNs is revealed by applying the CSC model in a layer-wise fashion [PRE17, SPRE18, PRSE18].",
"To see this, let C0 ∈N and for every ℓ∈[L] let Cℓ, kℓ∈N and let D(ℓ) ∈RdCℓ−1×dCℓbe a convolutional dictionary with kernels supported on Z/(nℓZ).",
"A signal x = φ(0) ∈RdC0 is said to belong to the corresponding multi-layered CSC model (ML-CSC model) if there exist coeﬃcients φ(ℓ) ∈RdCℓwith φ(ℓ−1) = D(ℓ)φ(ℓ) and ∥φ(ℓ)∥(nℓ) 0,∞≤kℓ, ℓ∈[L].",
"(7.2) We now consider the problem of reconstructing the sparse coeﬃcients (φ(ℓ))L ℓ=1 from a noisy signal ˜x := x + ν, where the noise ν ∈RdC0 is assumed to have small ℓ2-norm and x is assumed to follow the ML-CSC model in (7.2).",
"In general, this problem is NP-hard.",
"However, under suitable conditions on the ML-CSC model, it can be approximately solved, for instance, by a layered thresholding algorithm.",
"More precisely, for D ∈Rdc×dC and b ∈RdC, we deﬁne a soft-thresholding operator by TD,b(x) := ϱR(DT x −b) −ϱR(−DT x −b), x ∈Rdc, (7.3) where ϱR(x) = max{0, x} is applied componentwise.",
"If x = Dφ as in (7.1), we obtain φ ≈TD,b(x) roughly under the following conditions: The distance of φ and ψ := DT x = DT Dφ can be bounded using the local sparsity of φ and the mutual coherence and locality of the kernels of the convolutional dictionary D. For a suitable threshold b, the mapping ψ 7→ϱR(ψ −b) −ϱR(−ψ −b) further recovers the support of φ by nullifying entries of ψ with ψi ≤|bi|.",
"Utilizing the soft-thresholding operator (7.3) iteratively for corresponding vectors b(ℓ) ∈RdCℓ, ℓ∈[L], then suggests the following approximations: φ(ℓ) ≈(TD(ℓ),b(ℓ) ◦· · · ◦TD(1),b(1))(˜x), ℓ∈[L].",
The resemblance with the realization of a CNN with ReLU activation function is evident.,
"The transposed dictionary (D(ℓ))T can be regarded as modeling the learned convolutional kernels, the threshold b(ℓ) models the bias vector, and the soft-thresholding operator TD(ℓ),b(ℓ) mimics the application of a convolutional block with a ReLU non-linearity in the ℓ-th layer.",
"54 Using this model, a theoretical understanding of CNNs from the perspective of sparse coding is now at hand.",
This novel perspective gives a precise mathematical meaning of the kernels in a CNN as sparsifying dictionaries of an ML-CSC model.,
"Moreover, the forward pass of a CNN can be understood as a layered thresholding algorithm for decomposing a noisy signal ˜x.",
"The results derived are then of the following ﬂavor: Given a suitable reconstruction procedure such as thresholding or ℓ1-minimization, the sparse coeﬃcients (φ(ℓ))L ℓ=1 of a signal x following a ML-CSC model can be stably recovered from the noisy signal ˜x under certain hypotheses on the ingredients of the ML-CSC model.",
8 Eﬀectiveness in natural sciences The theoretical insights of the previous sections do not always accurately describe the performance of NNs in applications.,
"Indeed, there often exists a considerable gap between the predictions of approximation theory and the practical performance of NNs [AD20].",
"In this section, we consider concrete applications which have been very successfully solved with deep- learning-based methods.",
In Section 8.1 we present an overview of deep-learning-based algorithms applied to inverse problems.,
"Section 8.2 then continues by describing how NNs can be used as a numerical ansatz for solving PDEs, highlighting their use in the solution of the multi-electron Schr¨odinger equation.",
"8.1 Deep neural networks meet inverse problems The area of inverse problems, predominantly in imaging, was presumably the ﬁrst class of mathematical methods embracing deep learning with overwhelming success.",
"Let us consider a forward operator K : Y →X with X, Y being Hilbert spaces and the associated inverse problem of ﬁnding y ∈Y such that Ky = x for given features x ∈X.",
"The classical model-based approach to regularization aims to approximate K by invertible operators, and is hence strongly based on functional analytic principles.",
"Today, such approaches take well-posedness of the approximation, convergence properties, as well as the structure of regularized solutions into account.",
"The last item allows to incorporate prior information of the original solution such as regularity, sharpness of edges, or—in the case of sparse regularization [JMS17]—a sparse coeﬃcient sequence with respect to a prescribed representation system.",
"Such approaches are typically realized in a variational setting and hence aim to minimize functionals of the form ∥Ky −x∥2 + αR(y), where α ∈(0, ∞) is a regularization parameter, R: Y →[0, ∞) a regularization term, and ∥· ∥denotes the norm on Y.",
"As said, the regularization term aims to model structural information about the desired solution.",
"However, one main hurdle in this approach is the problem that typically solution classes such as images from computed tomography cannot be modeled accurately enough to, for instance, allow reconstruction under the constraint of a signiﬁcant amount of missing features.",
"This has opened the door to data-driven approaches, and recently, deep NNs.",
Solvers of inverse problems which are based on deep learning techniques can be roughly categorized into three classes: 1.,
"Supervised approaches: The most straightforward approach is to train a NN Φ(·, θ): X →Y end-to-end, i.e., to completely learn the map from data x to the solution y.",
"More advanced approaches in this direction incorporate information about the operator K into the NN such as in [A¨O17, GOW19, MLE21].",
Yet another type of approaches aims to combine deep NNs with classical model-based approaches.,
"The ﬁrst suggestion in this realm was to start by applying a standard solver, followed by a deep NN Φ(·, θ): Y →Y which serves as a denoiser for speciﬁc reconstruction artifacts, e.g., [JMFU17].",
This was followed by more sophisticated methods such as plug-and-play frameworks for coupling inversion and denoising [REM17].,
"Semi-supervised approaches: These type of approaches aim to encode the regularization by a deep NN Φ(·, θ): Y →[0, ∞).",
The underlying idea is often to require stronger regularization on solutions y(i) 55 that are more prone to artifacts or other eﬀects of the instability of the problem.,
On solutions where typically few artifacts are observed less regularization can be used.,
"Therefore, the learning algorithm only requires a set of labels (y(i))m i=1 as well as a method to assess how hard the inverse problem for this label would be.",
"In this sense, the algorithm can be considered semi-supervised.",
"This idea was followed, for example, in [L¨OS18, LSAH20].",
"Taking a Bayesian viewpoint, one can also learn prior distributions as deep NNs, which was done in [BZAJ20].",
"Unsupervised approaches: One highlight of what we might coin unsupervised approaches in our problem setting is the introduction of deep image priors in [DKMB20, UVL18].",
"The key idea is to parametrize the solutions y as the output of a NN Φ(ξ, ·): P →Y with parameters in a suitable space P, applied to a ﬁxed input ξ.",
"Then, for given features x, one tries to solve minθ∈P ∥KΦ(ξ, θ) −x∥2 in order to obtain parameters ˆθ ∈P that yield a solution candidate y = Φ(ξ, ˆθ).",
Here often early stopping is applied in the training of the network parameters.,
"As can be seen, one key conceptual question is how to “take the best out of both worlds”, in the sense of optimally combining classical (model-based) methods—in particular the forward operator K—with deep learning.",
"This is certainly sensitively linked to all characteristics of the particular application at hand, such as availability and accuracy of training data, properties of the forward operator, or requirements for the solution.",
And each of the three classes of hybrid solvers follows a diﬀerent strategy.,
Let us now discuss advantages and disadvantages of methods from the three categories with a particular focus on a mathematical foundation.,
"Supervised approaches suﬀer on the one hand from the problem that often ground-truth data is not available or only in a very distorted form, leading to the fact that synthetic data constitutes a signiﬁcant part of the training data.",
"Thus the learned NN will mainly perform as well as the algorithm which generated the data, but not signiﬁcantly improve it—only from an eﬃciency viewpoint.",
"On the other hand, the inversion is often highly ill-posed, i.e., the inversion map has a large Lipschitz constant, which negatively aﬀects the generalization ability of the NN.",
"Improved approaches incorporate knowledge about the forward operator K as discussed, which helps to circumvent this issue.",
One signiﬁcant advantage of semi-supervised approaches is that the underlying mathematical model of the inverse problem is merely augmented by the neural network-based regularization.,
"Assuming that the learned regularizer satisﬁes natural assumptions, convergence proofs or stability estimates for the resulting regularized methods are still available.",
"Finally, unsupervised approaches have the advantage that the regularization is then fully due to the speciﬁc architecture of the deep NN.",
"This makes these methods slightly easier to understand theoretically, although, for instance, the deep prior approach in its full generality is still lacking a profound mathematical analysis.",
"8.2 PDE-based models Besides applications in image processing and artiﬁcial intelligence, deep learning methods have recently strongly impacted the ﬁeld of numerical analysis.",
"In particular, regarding the numerical solution of high- dimensional PDEs.",
These PDEs are widely used as a model for complex processes and their numerical solution presents one of the biggest challenges in scientiﬁc computing.,
We mention three exemplary problem classes: 1.,
"Black–Scholes model: The Nobel award-winning theory of Fischer Black, Robert Merton, and Myron Scholes proposes a linear PDE model for the determination of a fair price of a (complex) ﬁnancial derivative.",
The dimensionality of the model corresponds to the number of ﬁnancial assets which is typically quite large.,
"The classical linear model, which can be solved eﬃciently via Monte Carlo methods is quite limited.",
"In order to take into account more realistic phenomena such as default risk, the PDE that models a fair price becomes nonlinear, and much more challenging to solve.",
In particular (with the notable exception of Multilevel Picard algorithms [EHJK19]) no general algorithm exists that provably scales well with the dimension.,
Schr¨odinger equation: The electronic Schr¨odinger equation describes the stationary nonrelativistic behavior of a quantum mechanical electron system in the electric ﬁeld generated by the nuclei of a molecule.,
"Its numerical solution is required to obtain stable molecular conﬁgurations, compute vibrational spectra, or obtain forces governing molecular dynamics.",
"If the number of electrons is large, this is again a high-dimensional problem and to date there exist no satisfactory algorithms for its solution: It is well known that diﬀerent gold standard methods may produce completely diﬀerent energy predictions, for example, when applied to large delocalized molecules, rendering these methods useless for those problems.",
Hamilton–Jacobi–Bellman equation: The Hamilton–Jacobi–Bellman (HJB) equation models the value function of (deterministic or stochastic) optimal control problems.,
The underlying dimensionality of the model corresponds to the dimension of the space of states to be controlled and tends to be rather high in realistic applications.,
"The high dimensionality, together with the fact that HJB equations typically tend to be fully nonlinear with non-smooth solutions, renders the numerical solution of HJB equations extremely challenging and no general algorithms exist for this problem.",
"Due to the favorable approximation results of NNs for high-dimensional functions (see especially Subsec- tion 4.3), it might not come as a surprise that a NN ansatz has proven to be quite successful in solving the aforementioned PDE models.",
A pioneering work in this direction is [HJE18] which uses the backwards SDE reformulation of semilinear parabolic PDEs to reformulate the evaluation of such a PDE at a speciﬁc point as an optimization problem that can be solved by the deep learning paradigm.,
"The resulting algorithm proves quite successful in the high-dimensional regime and, for instance, enables the eﬃcient modeling of complex ﬁnancial derivatives including nonlinear eﬀects such as default risk.",
Another approach speciﬁcally tailored to the numerical solution of HJB equations is [NZGK21].,
"In this work, one uses the Pontryagin principle to generate samples of the PDE solution along solutions of the corresponding boundary value problem.",
"Other numerical approaches include the Deep Ritz Method [EY18], where a Dirichlet energy is minimized over a set of NNs, or so-called Physics Informed Neural Networks [RPK19], where typically the PDE residual is minimized along with some natural constraints, for instance, to enforce boundary conditions.",
Deep-learning-based methods arguably work best if they are combined with domain knowledge to inspire NN architecture choices.,
"We would like to illustrate this interplay at the hand of a speciﬁc and extremely relevant example: the electronic Schr¨odinger equation (under the Born–Oppenheimer approximation) which amounts to ﬁnding the smallest nonzero eigenvalue of the eigenvalue problem HRψ = λψψ, (8.1) for ψ: R3×n →R, where the Hamiltonian (HRψ)(r) = − n X i=1 1 2(∆riψ)(r) −   n X i=1 p X j=1 Zj ∥ri −Rj∥2 − p−1 X i=1 p X j=i+1 ZiZj ∥Ri −Rj∥2 − n−1 X i=1 n X j=i+1 1 ∥ri −rj∥2  ψ(r) describes the kinetic energy (ﬁrst term) as well as Coulomb attraction force between electrons and nuclei (second and third term) and the Coulomb repulsion force between diﬀerent electrons (third term).",
"Here, the coordinates R = R1 .",
"Rp  ∈R3×p refer to the positions of the nuclei, (Zi)p i=1 ∈Np denote the atomic numbers of the nuclei, and the coordinates r = r1, .",
", rn  ∈R3×n refer to the positions of the electrons.",
"The associated eigenfunction ψ describes the so-called wavefunction which can be interpreted in the sense that |ψ(r)|2/∥ψ∥2 L2 describes the joint probability density of the n electrons to be located at r. The smallest solution λψ of (8.1) describes the ground state energy associated with the nuclear coordinates R. It is of particular interest to know the ground state energy for all nuclear coordinates, the so-called potential energy surface whose gradient determines the forces governing the dynamic motions of the nuclei.",
The numerical solution of (8.1) is complicated by the Pauli principle which states that the wave function ψ must be antisymmetric in all coordinates representing electrons of equal spin.,
"To state it, we need to clarify that every electron is not only deﬁned by its location but also by its spin which may be positive or negative.",
"57 Depending on whether two electrons have the same spin or not, their interaction changes massively.",
"This is reﬂected by the Pauli principle that we already mentioned: Suppose that electrons i and j have equal spin, then the wave function must satisfy Pi,jψ = −ψ, (8.2) where Pi,j denotes the operator that swaps ri and rj, i.e., (Pi,jψ)(r) = ψ(r1, .",
"In particular, no two electrons with the same spin can occupy the same location.",
"The challenges associated with solving the Schr¨odinger equation inspired the following famous quote by Paul Dirac [Dir29]: “The fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the diﬃculty lies only in the fact that application of these laws leads to equations that are too complex to be solved.” We now describe how deep learning methods might help to mitigate this claim to a certain extent.",
Let X be a random variable with density |ψ(r)|2/∥ψ∥2 L2.,
"Using the Rayleigh–Ritz principle, ﬁnding the minimal nonzero eigenvalue of (8.1) can be reformulated as minimizing the Rayleigh quotient R R3×n ψ(r)(HRψ)(r) dr ∥ψ∥2 L2 = E (HRψ)(X) ψ(X)  (8.3) over all ψ’s satisfying the Pauli principle, see [SO12].",
Since this represents a minimization problem it can in principle be solved via a NN ansatz by generating training data distributed according to X using MCMC sampling31.,
"Since the wave function ψ will be parametrized as a NN, the minimization of (8.3) will require the computation of the gradient of (8.3) with respect to the NN parameters (the method in [PSMF20] even requires second order derivatives) which, at ﬁrst sight, might seem to require the computation of third order derivatives.",
"However, due to the Hermitian structure of the Hamiltonian one does not need to compute the derivative of the Laplacian of ψ, see, for example, [HSN20, Equation (8)].",
"Compared to the other PDE problems we have discussed, an additional complication arises from the need to incorporate structural properties and invariances such as the Pauli principle.",
"Furthermore, empirical evidence shows that it is also necessary to hard code the so-called cusp conditions which describe the asymptotic behavior of nearby electrons and electrons close to a nucleus into the NN architecture.",
"A ﬁrst attempt in this direction has been made in [HZE19] and signiﬁcantly improved NN architectures have been developed in [HSN20, PSMF20, SRG+21] opening the possibility of accurate ab initio computations for previously intractable molecules.",
The mathematical properties of this exciting line of work remain largely unexplored.,
"We brieﬂy describe the main ideas behind the NN architecture of [HSN20, SRG+21].",
"Standard numerical approaches (notably the Multireference Hartree Fock Method, see [SO12]) use a low rank approach to minimize (8.3).",
Such a low rank approach would approximate ψ by sums of products of one electron orbitals Qn i=1 ϕi(ri) but clearly this does not satisfy the Pauli principle (8.2).,
"In order to ensure the Pauli principle, one constructs so-called Slater determinants from one electron orbitals with equal spin.",
"More precisely, suppose that the ﬁrst n+ electrons with coordinates r1, .",
", rn+ have positive spin and the last n −n+ electrons have negative spin.",
"Then any function of the form det  (ϕi(rj))n+ i,j=1  · det  (ϕi(rj))n i,j=n++1  (8.4) satisﬁes (8.2) and is typically called a Slater determinant.",
"While the Pauli principle establishes an (non- classical) interaction between electrons of equal spin, the so-called exchange correlation, electrons with opposite spins are uncorrelated in the representation (8.4).",
"In particular, (8.4) ignores interactions between electrons that arise through Coulomb forces, implying that no nontrivial wavefunction can be accurately represented by a single Slater determinant.",
"To capture physical interactions between diﬀerent electrons, one needs to use sums of Slater determinants as an ansatz.",
"However, it turns out that the number of such determinants that are needed to guarantee a given accuracy scales very badly with the system size n (to the 31Observe that for such sampling methods one can just use the unnormalized density |ψ(r)|2 and thus avoid the computation of the normalization ∥ψ∥2 L2.",
"58 best of our knowledge the best currently known approximation results are contained in [Yse10], where an n-independent error rate is shown, however the implicit constant in this rate depends at least exponentially on the system size n).",
We would like to highlight the approach of [HSN20] whose main idea is to use NNs to incorporate interactions into Slater determinants of the form (8.4) using what is called the backﬂow trick [RMD+06].,
"The basic building blocks would now consist of functions of the form det  (ϕi(rj)Ψj(r, θj))n+ i,j=1  · det  (ϕi(rj)Ψj(r, θj))n i,j=n++1  , (8.5) where Ψk(·, θk), k ∈[n], are NNs.",
"If these are arbitrary NNs, it is easy to see that the Pauli principle (8.2) will not be satisﬁed.",
"However, if we require the NNs to be symmetric, for example, in the sense that for i, j, s ∈[n+] it holds that Pi,jΨk(·, θk) =      Ψk(·, θk), if k /∈{i, j}, Ψi(·, θi), if k = j, Ψj(·, θj), if k = i, (8.6) and analogous conditions hold for i, j, k ∈[n] \ [n+], the expression (8.5) does actually satisfy (8.2).",
The construction of such symmetric NNs can be achieved by using a modiﬁcation of the so-called SchNet Architecture [SKS+17] which can be considered as a speciﬁc residual NN.,
We describe a simpliﬁed construction which is inspired by [HZE19] and used in a slightly more complex form in [SRG+21].,
"We restrict ourselves to the case of positive spin (e.g., the ﬁrst n+ coordinates), the case of negative spin being handled in the same way.",
"Let Υ(·, θ+ emb) be a univariate NN (with possibly multivariate output) and denote Embk(r, θ+ emb) := n+ X i=1 Υ(∥rk −ri∥2, θ+ emb), k ∈[n+], the k-th embedding layer.",
"For k ∈[n+], we can now deﬁne Ψk (r, θk) = Ψk  r, (θk,fc, θ+ emb)  = Γk   Embk(r, θ+ emb), (rn++1, .",
", rn)  , θk,fc  , where Γk(·, θk,fc) denotes a standard FC NN with input dimension equal to the output dimension of Ψ+ plus the dimension of negative spin electrons.",
"The networks Ψk, k ∈[n] \ [n+], are deﬁned analogously using diﬀerent parameters θ− emb for the embeddings.",
"It is straightforward to check that the NNs Ψk, k ∈[n], satisfy (8.6) so that the backﬂow determinants (8.5) satisfy the Pauli principle (8.2).",
"In [HSN20] the backﬂow determinants (8.5) are further augmented by a multiplicative correction term, the so-called Jastrow factor which is also represented by a speciﬁc symmetric NN, as well as a correction term that ensures the validity of the cusp conditions.",
The results of [HSN20] show that this ansatz (namely using linear combinations of backﬂow determinants (8.5) instead of plain Slater determinants (8.4)) is vastly more eﬃcient in terms of number of determinants needed to obtain chemical accuracy.,
The full architecture provides a general purpose NN architecture to represent complicated wave functions.,
"A distinct advantage of this approach is that some parameters (for example, embedding layers) may be shared across diﬀerent nuclear geometries R ∈R3×p which allows for the eﬃcient computation of potential energy surfaces [SRG+21], see Figure 8.1.",
"Finally, we would like to highlight the customized NN design that incorporates physical invariances, domain knowledge (for example, in the form of cusp conditions), and existing numerical methods, all of which are required for the method to reach its full potential.",
Acknowledgment The research of JB was supported by the Austrian Science Fund (FWF) under grant I3403-N32.,
"GK acknowledges support from DFG-SPP 1798 Grants KU 1446/21-2 and KU 1446/27-2, DFG-SFB/TR 109 Grant C09, BMBF Grant MaGriDo, and NSF-Simons Foundation Grant SIMONS 81420.",
The authors would 59 Figure 8.1: By sharing layers across diﬀerent nuclear geometries one can eﬃciently compute diﬀerent geometries in one single training step [SRG+21].,
Left: Potential energy surface of H10 chain computed by the deep-learning-based algorithm from [SRG+21].,
The lowest energy is achieved when pairs of H atoms enter into a covalent bond to form ﬁve H2 molecules.,
Right: The method of [SRG+21] is capable of accurately computing forces between nuclei which allows for molecular dynamics simulations from ﬁrst principles.,
"like to thank H´ector Andrade Loarca, Dennis Elbr¨achter, Adalbert Fono, Pavol Harar, Lukas Liehr, Duc Anh Nguyen, Mariia Seleznova, and Frieder Simon for their helpful feedback on an early version of this article.",
"In particular, Dennis Elbr¨achter was providing help for several theoretical results.",
"References [AAˇC13] Antonio Auﬃnger, G´erard Ben Arous, and Jiˇr´ı ˇCern`y, Random matrices and complexity of spin glasses, Communications on Pure and Applied Mathematics 66 (2013), no.",
"2, 165–201.",
"[AB99] Martin Anthony and Peter L Bartlett, Neural network learning: Theoretical foundations, Cambridge University Press, 1999.",
"[ACGH19] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu, A convergence analysis of gradient descent for deep linear neural networks, International Conference on Learning Representations, 2019.",
"[ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan, On the optimization of deep networks: Implicit acceleration by overparameterization, International Conference on Machine Learning, 2018, pp.",
"[AD20] Ben Adcock and Nick Dexter, The gap between theory and practice in function approximation with deep neural networks, 2020, arXiv preprint arXiv:2001.07523.",
"[ADH+19] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang, On exact computation with an inﬁnitely wide neural net, Advances in Neural Information Processing Systems, 2019, pp.",
"[AGNZ18] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang, Stronger generalization bounds for deep nets via a compression approach, International Conference on Machine Learning, 2018, pp.",
"[AHNB+20] Yasmine S Al-Hamdani, P´eter R Nagy, Dennis Barton, Mih´aly K´allay, Jan Gerit Brandenburg, and Alexandre Tkatchenko, Interactions between large molecules: Puzzle for reference quantum- mechanical methods, 2020, arXiv preprint arXiv:2009.08927.",
"60 [AHS85] David H Ackley, Geoﬀrey E Hinton, and Terrence J Sejnowski, A learning algorithm for Boltzmann machines, Cognitive Science 9 (1985), no.",
"1, 147–169.",
"[AHW96] Peter Auer, Mark Herbster, and Manfred K Warmuth, Exponentially many local minima for single neurons, Advances in Neural Information Processing Systems, 1996, p. 316–322.",
"[AM¨OS19] Simon Arridge, Peter Maass, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Solving inverse problems using data-driven models, Acta Numerica 28 (2019), 1–174.",
"[A¨O17] Jonas Adler and Ozan ¨Oktem, Solving ill-posed inverse problems using iterative deep neural networks, Inverse Problems 33 (2017), no.",
"12, 124007.",
"[AZLS19] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via over-parameterization, International Conference on Machine Learning, 2019, pp.",
"[Bar92] Andrew R Barron, Neural net approximation, Yale Workshop on Adaptive and Learning Systems, vol.",
"1, 1992, pp.",
"[Bar93] , Universal approximation bounds for superpositions of a sigmoidal function, IEEE Transactions on Information Theory 39 (1993), no.",
"3, 930–945.",
"[Bar98] Peter L Bartlett, The sample complexity of pattern classiﬁcation with neural networks: the size of the weights is more important than the size of the network, IEEE Transactions on Information Theory 44 (1998), no.",
"2, 525–536.",
"[BBC17] Alfred Bourely, John Patrick Boueri, and Krzysztof Choromonski, Sparse neural networks topologies, 2017, arXiv preprint arXiv:1706.05683.",
"[BBC+19] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse, Dota 2 with large scale deep reinforcement learning, 2019, arXiv preprint arXiv:1912.06680.",
"[BBG+21] Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen, Solving the kolmogorov pde by means of deep learning, Journal of Scientiﬁc Computing 88 (2021), no.",
"[BBL03] Olivier Bousquet, St´ephane Boucheron, and G´abor Lugosi, Introduction to statistical learning theory, Summer School on Machine Learning, 2003, pp.",
"[BBL+17] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst, Geometric deep learning: going beyond euclidean data, IEEE Signal Processing Magazine 34 (2017), no.",
"[BBM05] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson, Local Rademacher complexities, The Annals of Statistics 33 (2005), no.",
"4, 1497–1537.",
"[BCB15] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, Neural machine translation by jointly learning to align and translate, International Conference on Learning Representations, 2015.",
"[BDG20] Julius Berner, Markus Dablander, and Philipp Grohs, Numerically solving parametric families of high-dimensional Kolmogorov partial diﬀerential equations via deep learning, Advances in Neural Information Processing Systems, 2020, pp.",
16615–16627.,
"[BE02] Olivier Bousquet and Andr´e Elisseeﬀ, Stability and generalization, Journal of Machine Learning Research 2 (2002), no.",
"Mar, 499–526.",
"61 [BEG19] Julius Berner, Dennis Elbr¨achter, and Philipp Grohs, How degenerate is the parametrization of neural networks with the ReLU activation function?, Advances in Neural Information Processing Systems, 2019, pp.",
"[Bel52] Richard Bellman, On the theory of dynamic programming, Proceedings of the National Academy of Sciences 38 (1952), no.",
"[BF19] Jan Bohn and Michael Feischl, Recurrent neural networks as optimal mesh reﬁnement strategies, 2019, arXiv preprint arXiv:1909.04275.",
"[BFT17] Peter L Bartlett, Dylan J Foster, and Matus Telgarsky, Spectrally-normalized margin bounds for neural networks, Advances in Neural Information Processing Systems, 2017, pp.",
"[BGJ20] Julius Berner, Philipp Grohs, and Arnulf Jentzen, Analysis of the generalization error: Empirical risk minimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in the numerical approximation of black–scholes partial diﬀerential equations, SIAM Journal on Mathematics of Data Science 2 (2020), no.",
"3, 631–657.",
"[BH89] Eric B Baum and David Haussler, What size net gives valid generalization?, Neural Computation 1 (1989), no.",
"1, 151–160.",
"[BHLM19] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian, Nearly-tight VC- dimension and pseudodimension bounds for piecewise linear neural networks, Journal of Machine Learning Research 20 (2019), 63–1.",
"[BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal, Reconciling modern machine- learning practice and the classical bias–variance trade-oﬀ, Proceedings of the National Academy of Sciences 116 (2019), no.",
"32, 15849–15854.",
"[BHX20] Mikhail Belkin, Daniel Hsu, and Ji Xu, Two models of double descent for weak features, SIAM Journal on Mathematics of Data Science 2 (2020), no.",
"4, 1167–1180.",
"[BK18] Andrew R Barron and Jason M Klusowski, Approximation and estimation for high-dimensional deep learning networks, 2018, arXiv preprint arXiv:1809.03090.",
"[BLLT20] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler, Benign overﬁtting in linear regression, Proceedings of the National Academy of Sciences 117 (2020), no.",
"48, 30063–30070.",
"[BMM98] Peter L Bartlett, Vitaly Maiorov, and Ron Meir, Almost linear VC-dimension bounds for piecewise polynomial networks, Neural Computation 10 (1998), no.",
"8, 2159–2173.",
"[BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal, To understand deep learning we need to understand kernel learning, International Conference on Machine Learning, 2018, pp.",
"[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeﬀrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei, Language models are few-shot learners, Advances in Neural Information Processing Systems, 2020, pp.",
"[BR89] Avrim Blum and Ronald L Rivest, Training a 3-node neural network is NP-complete, Advances in Neural Information Processing Systems, 1989, pp.",
"62 [BRT19] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov, Does data interpolation con- tradict statistical optimality?, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[BSW14] Pierre Baldi, Peter Sadowski, and Daniel Whiteson, Searching for exotic particles in high-energy physics with deep learning, Nature Communications 5 (2014), no.",
"[BZAJ20] Riccardo Barbano, Chen Zhang, Simon Arridge, and Bangti Jin, Quantifying model uncertainty in inverse problems via bayesian deep gradient descent, 2020, arXiv preprint arXiv:2007.09971.",
"[Can98] Emmanuel J Cand`es, Ridgelets: Theory and applications, Ph.D. thesis, Stanford University, 1998.",
"[CB20] Lenaic Chizat and Francis Bach, Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss, Conference on Learning Theory, 2020, pp.",
"[CHM+15] Anna Choromanska, Mikael Henaﬀ, Michael Mathieu, G´erard Ben Arous, and Yann LeCun, The loss surfaces of multilayer networks, International Conference on Artiﬁcial Intelligence and Statistics, 2015, pp.",
"[CJLZ19] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao, Eﬃcient approximation of deep ReLU networks for functions on low dimensional manifolds, Advances in Neural Information Processing Systems, 2019, pp.",
"[CK20] Alexander Cloninger and Timo Klock, ReLU nets adapt to intrinsic dimensionality beyond the target domain, 2020, arXiv preprint arXiv:2008.02545.",
"[CKP12] Peter G Casazza, Gitta Kutyniok, and Friedrich Philipp, Introduction to ﬁnite frame theory, Finite Frames: Theory and Applications, Birkh¨auser Boston, 2012, pp.",
"[CL19] Wojciech Czaja and Weilin Li, Analysis of time-frequency scattering transforms, Applied and Computational Harmonic Analysis 47 (2019), no.",
"1, 149–171.",
"[CLA15] Anna Choromanska, Yann LeCun, and G´erard Ben Arous, Open problem: The landscape of the loss surfaces of multilayer networks, Conference on Learning Theory, 2015, pp.",
"[CLM94] Charles K Chui, Xin Li, and Hrushikesh N Mhaskar, Neural networks for localized approximation, Mathematics of Computation 63 (1994), no.",
"208, 607–623.",
"[CM18] Charles K Chui and Hrushikesh N Mhaskar, Deep nets for local manifold learning, Frontiers in Applied Mathematics and Statistics 4 (2018), 12.",
"[CMBK20] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi, Multiple descent: Design your own generalization curve, 2020, arXiv preprint arXiv:2008.01036.",
"[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in diﬀerentiable program- ming, Advances in Neural Information Processing Systems, 2019, pp.",
"[CPV20] Andrei Caragea, Philipp Petersen, and Felix Voigtlaender, Neural network approximation and estimation of classiﬁers with classiﬁcation boundary in a Barron class, 2020, arXiv preprint arXiv:2011.09363.",
"[CS02] Felipe Cucker and Steve Smale, On the mathematical foundations of learning, Bulletin of the American Mathematical Society 39 (2002), no.",
"[CvMG+14] Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, Learning phrase representations using rnn encoder–decoder for statistical machine translation, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014, pp.",
"63 [Cyb89] George Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of Control, Signals and Systems 2 (1989), no.",
"4, 303–314.",
"[CZ07] Felipe Cucker and Ding-Xuan Zhou, Learning theory: an approximation theory viewpoint, vol.",
"24, Cambridge University Press, 2007.",
"[DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: A large-scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp.",
"[DeV98] Ronald A DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51–150.",
"[DGL96] Luc Devroye, L´aszl´o Gy¨orﬁ, and G´abor Lugosi, A probabilistic theory of pattern recognition, Springer, 1996.",
"[DHL18] Simon S Du, Wei Hu, and Jason D Lee, Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced, Advances in Neural Information Processing Systems, 2018, pp.",
"[DHP20] Ronald DeVore, Boris Hanin, and Guergana Petrova, Neural network approximation, 2020, arXiv preprint arXiv:2012.14501.",
"[Dir29] Paul Adrien Maurice Dirac, Quantum mechanics of many-electron systems, Proceedings of the Royal Society of London.",
"Series A, Containing Papers of a Mathematical and Physical Character 123 (1929), no.",
"792, 714–733.",
"[DKMB20] S¨oren Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer, Regularization by ar- chitecture: A deep prior approach for inverse problems, Journal of Mathematical Imaging and Vision 62 (2020), no.",
"3, 456–470.",
"[DLL+19] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient descent ﬁnds global minima of deep neural networks, International Conference on Machine Learning, 2019, pp.",
"[Don69] William F Donoghue, Distributions and fourier transforms, Pure and Applied Mathematics, Academic Press, 1969.",
"[DPG+14] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio, Identifying and attacking the saddle point problem in high-dimensional non- convex optimization, Advances in Neural Information Processing Systems, 2014, pp.",
"[DR17] Gintare Karolina Dziugaite and Daniel M Roy, Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data, Conference on Uncertainty in Artiﬁcial Intelligence, 2017.",
"[Dre62] Stuart Dreyfus, The numerical solution of variational problems, Journal of Mathematical Analysis and Applications 5 (1962), no.",
"[Dud67] Richard M Dudley, The sizes of compact subsets of hilbert space and continuity of Gaussian processes, Journal of Functional Analysis 1 (1967), no.",
"3, 290–330.",
"[Dud14] , Uniform central limit theorems, vol.",
"142, Cambridge University Press, 2014.",
"[DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent provably optimizes over-parameterized neural networks, International Conference on Learning Representations, 2018.",
"[E17] Weinan E, A proposal on machine learning via dynamical systems, Communications in Mathe- matics and Statistics 5 (2017), no.",
"64 [EGJS18] Dennis Elbr¨achter, Philipp Grohs, Arnulf Jentzen, and Christoph Schwab, DNN expression rate analysis of high-dimensional PDEs: Application to option pricing, 2018, arXiv preprint arXiv:1809.07669.",
"[EHJK19] Weinan E, Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse, On multilevel picard numerical approximations for high-dimensional nonlinear parabolic partial diﬀerential equations and high-dimensional nonlinear backward stochastic diﬀerential equations, Journal of Scientiﬁc Computing 79 (2019), no.",
"3, 1534–1571.",
"[EHL19] Weinan E, Jiequn Han, and Qianxiao Li, A mean-ﬁeld optimal control formulation of deep learning, Research in the Mathematical Sciences 6 (2019), no.",
"[Elm90] Jeﬀrey L Elman, Finding structure in time, Cognitive Science 14 (1990), no.",
"2, 179–211.",
"[EMW19a] Weinan E, Chao Ma, and Lei Wu, Barron spaces and the compositional function spaces for neural network models, 2019, arXiv preprint arXiv:1906.08039.",
"[EMW19b] , A priori estimates of the population risk for two-layer neural networks, Communications in Mathematical Sciences 17 (2019), no.",
"5, 1407–1425.",
"[EMWW20] Weinan E, Chao Ma, Stephan Wojtowytsch, and Lei Wu, Towards a mathematical understanding of neural network-based machine learning: what we know and what we don’t, 2020, arXiv preprint arXiv:2009.10713.",
"[EPGB19] Dennis Elbr¨achter, Dmytro Perekrestenko, Philipp Grohs, and Helmut B¨olcskei, Deep neural network approximation theory, 2019, arXiv preprint arXiv:1901.02220.",
"[ES16] Ronen Eldan and Ohad Shamir, The power of depth for feedforward neural networks, Conference on Learning Theory, vol.",
"49, 2016, pp.",
"[EW20a] Weinan E and Stephan Wojtowytsch, On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics, 2020, arXiv preprint arXiv:2007.15623.",
"[EW20b] , A priori estimates for classiﬁcation problems using neural networks, 2020, arXiv preprint arXiv:2009.13500.",
"[EW20c] , Representation formulas and pointwise properties for Barron functions, 2020, arXiv preprint arXiv:2006.05982.",
"[EY18] Weinan E and Bing Yu, The deep ritz method: a deep learning-based numerical algorithm for solving variational problems, Communications in Mathematics and Statistics 6 (2018), no.",
"[FB17] Daniel C Freeman and Joan Bruna, Topology and geometry of half-rectiﬁed network optimization, International Conference on Learning Representations, 2017.",
"[FC18] Jonathan Frankle and Michael Carbin, The lottery ticket hypothesis: Finding sparse, trainable neural networks, International Conference on Learning Representations, 2018.",
"[FHH+17] Felix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E Dahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld, Prediction errors of molecular machine learning models lower than hybrid DFT error, Journal of Chemical Theory and Computation 13 (2017), no.",
"11, 5255–5264.",
"[Fun89] Ken-Ichi Funahashi, On the approximate realization of continuous mappings by neural networks, Neural Networks 2 (1989), no.",
"3, 183–192.",
"65 [GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep learning, MIT Press, 2016.",
"[G´er17] Aurelien G´eron, Hands-on machine learning with scikit-learn and tensorﬂow: Concepts, tools, and techniques to build intelligent systems, O’Reilly Media, 2017.",
"[GH21] Philipp Grohs and Lukas Herrmann, Deep neural network approximation for high-dimensional parabolic Hamilton-Jacobi-Bellman equations, 2021, arXiv preprint arXiv:2103.05744.",
"[GH22] , Deep neural network approximation for high-dimensional elliptic PDEs with boundary conditions, IMA Journal of Numerical Analysis 42 (2022), no.",
"3, 2055–2082.",
"[GHJVW20] Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger, A proof that artiﬁcial neural networks overcome the curse of dimensionality in the numerical approximation of Black-Scholes partial diﬀerential equations, Memoirs of the American Mathematical Society (2020).",
"[GHJY15] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points—online stochastic gradient for tensor decomposition, Conference on Learning Theory, 2015, pp.",
"[GJS+20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St´ephane d’Ascoli, Giulio Biroli, Cl´ement Hongler, and Matthieu Wyart, Scaling description of generalization with number of parameters in deep learning, Journal of Statistical Mechanics: Theory and Experiment (2020), no.",
"[GKP20] Ingo G¨uhring, Gitta Kutyniok, and Philipp Petersen, Error bounds for approximations with deep ReLU neural networks in W s,p norms, Analysis and Applications 18 (2020), no.",
"05, 803–859.",
"[GKR20] Philipp Grohs, Sarah Koppensteiner, and Martin Rathmair, Phase retrieval: Uniqueness and stability, SIAM Review 62 (2020), no.",
"2, 301–350.",
"[GL13] Saeed Ghadimi and Guanghui Lan, Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming, SIAM Journal on Optimization 23 (2013), no.",
"4, 2341–2368.",
"[GLSS18a] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nathan Srebro, Characterizing implicit bias in terms of optimization geometry, International Conference on Machine Learning, 2018, pp.",
"[GLSS18b] , Implicit bias of gradient descent on linear convolutional networks, Advances in Neural Information Processing Systems, 2018, pp.",
"[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Linearized two- layers neural networks in high dimension, The Annals of Statistics 49 (2021), no.",
"2, 1029–1054.",
"[GOW19] Davis Gilton, Greg Ongie, and Rebecca Willett, Neumann networks for linear inverse problems in imaging, IEEE Transactions on Computational Imaging 6 (2019), 328–343.",
"[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets, Advances in Neural Information Processing Systems, 2014, pp.",
"[GRK20] Ingo G¨uhring, Mones Raslan, and Gitta Kutyniok, Expressivity of deep neural networks, 2020, arXiv preprint arXiv:2007.04759.",
"[GRS18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir, Size-independent sample complexity of neural networks, Conference On Learning Theory, 2018, pp.",
"[GS20] Lukas Gonon and Christoph Schwab, Deep ReLU network expression rates for option prices in high-dimensional, exponential L´evy models, 2020, ETH Zurich SAM Research Report.",
"66 [GV21] Philipp Grohs and Felix Voigtlaender, Proof of the theory-to-practice gap in deep learning via sampling complexity bounds for neural network approximation spaces, 2021, arXiv preprint arXiv:2104.02746.",
"[GW08] Andreas Griewank and Andrea Walther, Evaluating derivatives: principles and techniques of algorithmic diﬀerentiation, SIAM, 2008.",
"[GZ84] Evarist Gin´e and Joel Zinn, Some limit theorems for empirical processes, The Annals of Probability (1984), 929–989.",
"[Han19] Boris Hanin, Universal function approximation by deep neural nets with bounded width and ReLU activations, Mathematics 7 (2019), no.",
"[Hau95] David Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik- chervonenkis dimension, Journal of Combinatorial Theory, Series A 2 (1995), no.",
"69, 217–232.",
"[HH19] Catherine F Higham and Desmond J Higham, Deep learning: An introduction for applied mathematicians, SIAM Review 61 (2019), no.",
"4, 860–891.",
"[HHJ15] Martin Hairer, Martin Hutzenthaler, and Arnulf Jentzen, Loss of regularity for Kolmogorov equations, The Annals of Probability 43 (2015), no.",
"2, 468–527.",
"[HJE18] Jiequn Han, Arnulf Jentzen, and Weinan E, Solving high-dimensional partial diﬀerential equations using deep learning, Proceedings of the National Academy of Sciences 115 (2018), no.",
"34, 8505– 8510.",
"[HJKN20] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen, A proof that recti- ﬁed deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations, SN Partial Diﬀerential Equations and Applications 1 (2020), no.",
"[HLXZ20] Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng, ReLU deep neural networks and linear ﬁnite elements, Journal of Computational Mathematics 38 (2020), no.",
"3, 502–527.",
"[HMD16] Song Han, Huizi Mao, and William J Dally, Deep compression: Compressing deep neural network with pruning, trained quantization and Huﬀman coding, International Conference on Learning Representations, 2016.",
"[HMRT19] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani, Surprises in high- dimensional ridgeless least squares interpolation, 2019, arXiv preprint arXiv:1903.08560.",
"[Hoe63] Wassily Hoeﬀding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (1963), no.",
"301, 13–30.",
"[Hop82] John J Hopﬁeld, Neural networks and physical systems with emergent collective computational abilities, Proceedings of the National Academy of Sciences 79 (1982), no.",
"8, 2554–2558.",
"[HR19] Boris Hanin and David Rolnick, Deep ReLU networks have surprisingly few activation patterns, Advances in Neural Information Processing Systems, 2019, pp.",
"[HRS16] Moritz Hardt, Ben Recht, and Yoram Singer, Train faster, generalize better: Stability of stochastic gradient descent, International Conference on Machine Learning, 2016, pp.",
"[HS97] Sepp Hochreiter and J¨urgen Schmidhuber, Long short-term memory, Neural Computation 9 (1997), no.",
"8, 1735–1780.",
"[HS17] Boris Hanin and Mark Sellke, Approximating continuous functions by ReLU nets of minimal width, 2017, arXiv preprint arXiv:1710.11278.",
"67 [HSL+16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger, Deep networks with stochastic depth, European Conference on Computer Vision, 2016, pp.",
"[HSN20] Jan Hermann, Zeno Sch¨atzle, and Frank No´e, Deep-neural-network solution of the electronic Schr¨odinger equation, Nature Chemistry 12 (2020), no.",
"10, 891–897.",
"[HSW89] Kurt Hornik, Maxwell Stinchcombe, and Halbert White, Multilayer feedforward networks are universal approximators, Neural Networks 2 (1989), no.",
"5, 359–366.",
"[HTF01] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statistical learning: Data mining, inference, and prediction, Springer Series in Statistics, Springer, 2001.",
"[HV17] Benjamin D Haeﬀele and Ren´e Vidal, Global optimality in neural network training, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.",
"[HvdG19] Peter Hinz and Sara van de Geer, A framework for the construction of upper bounds on the number of aﬃne linear regions of ReLU feed-forward neural networks, IEEE Transactions on Information Theory 65 (2019), 7304–7324.",
"[HZ94] Geoﬀrey E Hinton and Richard S Zemel, Autoencoders, minimum description length, and helmholtz free energy, Advances in Neural Information Processing Systems 6 (1994), 3–10.",
"[HZE19] Jiequn Han, Linfeng Zhang, and Weinan E, Solving many-electron Schr¨odinger equation using deep neural networks, Journal of Computational Physics 399 (2019), 108929.",
"[HZRS15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Delving deep into rectiﬁers: Sur- passing human-level performance on imagenet classiﬁcation, Proceedings of IEEE International Conference on Computer Vision, 2015, pp.",
"[HZRS16] , Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.",
"[IS15] Sergey Ioﬀe and Christian Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, International Conference on Machine Learning, 2015, pp.",
"[JGH18] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler, Neural tangent kernel: Convergence and generalization in neural networks, Advances in Neural Information Processing Systems, 2018, pp.",
"[JKMB19] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio, Predicting the generaliza- tion gap in deep networks with margin distributions, International Conference on Learning Representations, 2019.",
"[JKNvW20] Arnulf Jentzen, Benno Kuckuck, Ariel Neufeld, and Philippe von Wurstemberger, Strong error analysis for stochastic gradient descent optimization algorithms, IMA Journal of Numerical Analysis 41 (2020), no.",
"1, 455–492.",
"[JMFU17] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser, Deep convolu- tional neural network for inverse problems in imaging, IEEE Transactions on Image Processing 26 (2017), no.",
"9, 4509–4522.",
"[JMS17] Bangti Jin, Peter Maaß, and Otmar Scherzer, Sparsity regularization in inverse problems, Inverse Problems 33 (2017), no.",
"[JNM+20] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio, Fan- tastic generalization measures and where to ﬁnd them, International Conference on Learning Representations, 2020.",
"68 [Jor90] Michael I Jordan, Attractor dynamics and parallelism in a connectionist sequential machine, Artiﬁcial neural networks: concept learning, IEEE Press, 1990, pp.",
"[JT19a] Ziwei Ji and Matus Telgarsky, Gradient descent aligns the layers of deep linear networks, International Conference on Learning Representations, 2019.",
"[JT19b] , A reﬁned primal-dual analysis of the implicit bias, 2019, arXiv preprint arXiv:1906.04540.",
"[JT20] , Directional convergence and alignment in deep learning, Advances in Neural Information Processing Systems, 2020, pp.",
17176–17186.,
"[Jud90] Stephen J Judd, Neural network design and the complexity of learning, MIT Press, 1990.",
"[Kel60] Henry J Kelley, Gradient theory of optimal ﬂight paths, Ars Journal 30 (1960), no.",
"10, 947–954.",
"[KH09] Alex Krizhevsky and Geoﬀrey Hinton, Learning multiple layers of features from tiny images, Tech.",
"report, University of Toronto, 2009.",
"[KL18] Sham M Kakade and Jason D Lee, Provably correct automatic subdiﬀerentiation for qualiﬁed programs, Advances in Neural Information Processing Systems, 2018, pp.",
"[KL20] Patrick Kidger and Terry Lyons, Universal approximation with deep narrow networks, Conference on Learning Theory, 2020, pp.",
"[KM97] Marek Karpinski and Angus Macintyre, Polynomial bounds for VC dimension of sigmoidal and general Pfaﬃan neural networks, Journal of Computer and System Sciences 54 (1997), no.",
"1, 169–176.",
"[KMN+17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang, On large-batch training for deep learning: Generalization gap and sharp minima, International Conference on Learning Representations, 2017.",
"[KPRS19] Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider, A theoretical analysis of deep neural networks and parametric PDEs, 2019, arXiv preprint arXiv:1904.00377.",
"[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton, Imagenet classiﬁcation with deep convolutional neural networks, Advances in Neural Information Processing Systems, 2012, pp.",
"[KW52] Jack Kiefer and Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function, The Annals of Mathematical Statistics 23 (1952), no.",
"3, 462–466.",
"[LBBH98] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (1998), no.",
"11, 2278–2324.",
"[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel, Backpropagation applied to handwritten zip code recognition, Neural Computation 1 (1989), no.",
"4, 541–551.",
"[LBH15] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton, Deep learning, Nature 521 (2015), no.",
"7553, 436–444.",
"[LBN+18] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-Dickstein, Deep neural networks as Gaussian processes, International Conference on Learning Representations, 2018.",
"[LC19] Guillaume Lample and Fran¸cois Charton, Deep learning for symbolic mathematics, International Conference on Learning Representations, 2019.",
"69 [LD21] Licong Lin and Edgar Dobriban, What causes the test error?",
"Going beyond bias-variance via ANOVA, Journal of Machine Learning Research 22 (2021), no.",
"[LDS89] Yann LeCun, John S Denker, and Sara A Solla, Optimal brain damage, Advances in Neural Information Processing Systems, 1989, pp.",
"[Lew43] Kurt Lewin, Psychology and the process of group living, The Journal of Social Psychology 17 (1943), no.",
"1, 113–131.",
"[Li21] Weilin Li, Generalization error of minimum weighted norm and kernel interpolation, SIAM Journal on Mathematics of Data Science 3 (2021), no.",
"1, 414–438.",
"[Lin70] Seppo Linnainmaa, Alogritmin kumulatiivinen py¨oristysvirhe yksitt¨aisten py¨oristysvirheiden Taylor-kehitelm¨an¨a, Master’s thesis, University of Helsinki, 1970.",
"[LL18] Yuanzhi Li and Yingyu Liang, Learning overparameterized neural networks via stochastic gradient descent on structured data, Advances in Neural Information Processing Systems, 2018, pp.",
"[LL19] Kaifeng Lyu and Jian Li, Gradient descent maximizes the margin of homogeneous neural networks, International Conference on Learning Representations, 2019.",
"[LLPS93] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken, Multilayer feedforward networks with a nonpolynomial activation function can approximate any function, Neural Networks 6 (1993), no.",
"6, 861–867.",
"[LLS19] Qianxiao Li, Ting Lin, and Zuowei Shen, Deep learning via dynamical systems: An approximation perspective, 2019, arXiv preprint arXiv:1912.10382.",
"[LML+20] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying, A mean ﬁeld analysis of deep ResNet and beyond: Towards provably optimization via overparameterization from depth, International Conference on Machine Learning, 2020, pp.",
"[L¨OS18] Sebastian Lunz, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Adversarial regularizers in inverse problems, Advances in Neural Information Processing Systems, 2018, pp.",
"[LP21] Fabian Laakmann and Philipp Petersen, Eﬃcient approximation of solutions of parametric linear transport equations by ReLU DNNs, Advances in Computational Mathematics 47 (2021), no.",
"[LPRS19] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes, Fisher–Rao metric, geometry, and complexity of neural networks, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[LR20] Tengyuan Liang and Alexander Rakhlin, Just interpolate: Kernel “ridgeless” regression can generalize, The Annals of Statistics 48 (2020), no.",
"3, 1329–1347.",
"[LRZ20] Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai, On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels, Conference on Learning Theory, 2020, pp.",
"[LS17] Shiyu Liang and R Srikant, Why deep neural networks for function approximation?, International Conference on Learning Representations, 2017.",
"[LSAH20] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, NETT: Solving inverse problems with deep neural networks, Inverse Problems 36 (2020), no.",
"70 [LSJR16] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht, Gradient descent only converges to minimizers, Conference on Learning Theory, 2016, pp.",
"[LT91] Michel Ledoux and Michel Talagrand, Probability in Banach spaces: Isoperimetry and processes, vol.",
"23, Springer Science & Business Media, 1991.",
"[LTY19] Bo Li, Shanshan Tang, and Haijun Yu, Better approximations of high dimensional smooth functions by deep neural networks with rectiﬁed power units, Communications in Computational Physics 27 (2019), no.",
"2, 379–411.",
"[LXS+20] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jeﬀrey Pennington, Wide neural networks of any depth evolve as linear models under gradient descent, Journal of Statistical Mechanics: Theory and Experiment 2020 (2020), no.",
"12, 124002.",
"[Mal12] St´ephane Mallat, Group invariant scattering, Communications on Pure and Applied Mathematics 65 (2012), no.",
"10, 1331–1398.",
"[Mal16] , Understanding deep convolutional networks, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2016), no.",
"2065, 20150203.",
"[MAV18] Poorya Mianjy, Raman Arora, and Rene Vidal, On the implicit bias of dropout, International Conference on Machine Learning, 2018, pp.",
"[McA99] David A McAllester, Pac-bayesian model averaging, Conference on Learning Theory, 1999, pp.",
"[McD89] Colin McDiarmid, On the method of bounded diﬀerences, Surveys in Combinatorics 141 (1989), no.",
"1, 148–188.",
"[Men14] Shahar Mendelson, Learning without concentration, Conference on Learning Theory, 2014, pp.",
"[Mha96] Hrushikesh N Mhaskar, Neural networks for optimal approximation of smooth and analytic functions, Neural Computation 8 (1996), no.",
"1, 164–177.",
"[MHR+18] Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin Ghahramani, Gaussian process behaviour in wide deep neural networks, International Conference on Learning Representations, 2018.",
"[MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller, Playing atari with deep reinforcement learning, 2013, arXiv preprint arXiv:1312.5602.",
"[MLE21] Vishal Monga, Yuelong Li, and Yonina C Eldar, Algorithm unrolling: Interpretable, eﬃcient deep learning for signal and image processing, IEEE Signal Processing Magazine 38 (2021), no.",
"[MM19] Song Mei and Andrea Montanari, The generalization error of random features regression: Precise asymptotics and double descent curve, 2019, arXiv preprint arXiv:1908.05355.",
"[MOPS20] Carlo Marcati, Joost Opschoor, Philipp Petersen, and Christoph Schwab, Exponential ReLU neural network approximation rates for point and edge singularities, 2020, ETH Zurich SAM Research Report.",
"[MP43] Warren S McCulloch and Walter Pitts, A logical calculus of the ideas immanent in nervous activity, The Bulletin of Mathematical Biophysics 5 (1943), no.",
"4, 115–133.",
"71 [MP69] Marvin Minsky and Seymour A Papert, Perceptrons, MIT Press, 1969.",
"[MP99] Vitaly Maiorov and Allan Pinkus, Lower bounds for approximation by MLP neural networks, Neurocomputing 25 (1999), no.",
"1-3, 81–91.",
"[MPCB14] Guido Mont´ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio, On the number of linear regions of deep neural networks, Advances in Neural Information Processing Systems, 2014, pp.",
"[MSL+15] Junshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik, Deep neural nets as a method for quantitative structure–activity relationships, Journal of chemical information and modeling 55 (2015), no.",
"2, 263–274.",
"[MV03] Shahar Mendelson and Roman Vershynin, Entropy and the combinatorial dimension, Inventiones mathematicae 152 (2003), no.",
"[MVSS20] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai, Harmless interpolation of noisy data in regression, IEEE Journal on Selected Areas in Information Theory 1 (2020), no.",
"[MZ20] Andrea Montanari and Yiqiao Zhong, The interpolation phase transition in neural networks: Memorization and generalization under lazy training, 2020, arXiv preprint arXiv:2007.12826.",
"[NBMS17] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro, Exploring generalization in deep learning, Advances in Neural Information Processing Systems, 2017, pp.",
"[NBS18] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro, A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks, International Conference on Learning Representations, 2018.",
"[NH17] Quynh Nguyen and Matthias Hein, The loss surface of deep and wide neural networks, Interna- tional Conference on Machine Learning, 2017, pp.",
"[NI20] Ryumei Nakada and Masaaki Imaizumi, Adaptive approximation and generalization of deep neural network with intrinsic dimensionality, Journal of Machine Learning Research 21 (2020), no.",
"[NJLS09] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro, Robust stochastic approximation approach to stochastic programming, SIAM Journal on Optimization 19 (2009), no.",
"4, 1574–1609.",
"[NK19] Vaishnavh Nagarajan and J Zico Kolter, Uniform convergence may be unable to explain general- ization in deep learning, Advances in Neural Information Processing Systems, 2019, pp.",
11615– 11626.,
"[NKB+20] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever, Deep double descent: Where bigger models and more data hurt, International Conference on Learning Representations, 2020.",
"[NLG+19] Mor Shpigel Nacson, Jason D Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry, Convergence of gradient descent on separable data, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[NTS14] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro, In search of the real inductive bias: On the role of implicit regularization in deep learning, 2014, arXiv preprint arXiv:1412.6614.",
"72 [NTS15] , Norm-based capacity control in neural networks, Conference on Learning Theory, 2015, pp.",
"[NW09] Erich Novak and Henryk Wo´zniakowski, Approximation of inﬁnitely diﬀerentiable multivariate functions is intractable, Journal of Complexity 25 (2009), no.",
"4, 398–404.",
"[NY83] Arkadi Semenovich Nemirovsky and David Borisovich Yudin, Problem complexity and method eﬃciency in optimization, Wiley-Interscience Series in Discrete Mathematics, Wiley, 1983.",
"[NZGK21] Tenavi Nakamura-Zimmerer, Qi Gong, and Wei Kang, Adaptive deep learning for high- dimensional Hamilton–Jacobi–Bellman Equations, SIAM Journal on Scientiﬁc Computing 43 (2021), no.",
"2, A1221–A1247.",
"[OF96] Bruno A Olshausen and David J Field, Sparse coding of natural images produces localized, oriented, bandpass receptive ﬁelds, Nature 381 (1996), no.",
"[OM98] Genevieve B Orr and Klaus-Robert M¨uller, Neural networks: tricks of the trade, Springer, 1998.",
"[OPS20] Joost Opschoor, Philipp Petersen, and Christoph Schwab, Deep ReLU networks and high-order ﬁnite element methods, Analysis and Applications (2020), no.",
"[OS19] Kenta Oono and Taiji Suzuki, Approximation and non-parametric estimation of ResNet-type convolutional neural networks, International Conference on Machine Learning, 2019, pp.",
4922– 4931.,
"[PGZ+18] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and JeﬀDean, Eﬃcient neural architecture search via parameters sharing, International Conference on Machine Learning, 2018, pp.",
4095– 4104.,
"[PKL+17] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh N Mhaskar, Theory of deep learning III: explaining the non-overﬁtting puzzle, 2017, arXiv preprint arXiv:1801.00173.",
"[PLR+16] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli, Exponential expressivity in deep neural networks through transient chaos, Advances in Neural Information Processing Systems, 2016, pp.",
"[PMR+17] Tomaso Poggio, Hrushikesh N Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao, Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review, International Journal of Automation and Computing 14 (2017), no.",
"5, 503–519.",
"[PP92] Etienne Pardoux and Shige Peng, Backward stochastic diﬀerential equations and quasilin- ear parabolic partial diﬀerential equations, Stochastic partial diﬀerential equations and their applications, Springer, 1992, pp.",
"[PRE17] Vardan Papyan, Yaniv Romano, and Michael Elad, Convolutional neural networks analyzed via convolutional sparse coding, Journal of Machine Learning Research 18 (2017), no.",
"1, 2887–2938.",
"[PRMN04] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi, General conditions for predictivity in learning theory, Nature 428 (2004), no.",
"6981, 419–422.",
"[PRSE18] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad, Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks, IEEE Signal Processing Magazine 35 (2018), no.",
"[PRV20] Philipp Petersen, Mones Raslan, and Felix Voigtlaender, Topological properties of the set of functions generated by neural networks of ﬁxed size, Foundations of Computational Mathematics (2020), 1–70.",
"73 [PSE17] Vardan Papyan, Jeremias Sulam, and Michael Elad, Working locally thinking globally: Theoretical guarantees for convolutional sparse coding, IEEE Transactions on Signal Processing 65 (2017), no.",
"21, 5687–5701.",
"[PSMF20] David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes, Ab initio solution of the many-electron schr¨odinger equation with deep neural networks, Physical Review Research 2 (2020), no.",
"[PV18] Philipp Petersen and Felix Voigtlaender, Optimal approximation of piecewise smooth functions using deep ReLU neural networks, Neural Networks 108 (2018), 296–330.",
"[PV20] , Equivalence of approximation by convolutional neural networks and fully-connected networks, Proceedings of the American Mathematical Society 148 (2020), no.",
"4, 1567–1581.",
"[REM17] Yaniv Romano, Michael Elad, and Peyman Milanfar, The little engine that could: Regularization by denoising (red), SIAM Journal on Imaging Sciences 10 (2017), no.",
"4, 1804–1844.",
"[RFB15] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical image computing and computer-assisted intervention, 2015, pp.",
"[RH19] Lars Ruthotto and Eldad Haber, Deep neural networks motivated by partial diﬀerential equations, Journal of Mathematical Imaging and Vision (2019), 1–13.",
"[RHW86] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams, Learning representations by back-propagating errors, Nature 323 (1986), no.",
"6088, 533–536.",
"[RM51] Herbert Robbins and Sutton Monro, A stochastic approximation method, The Annals of Mathe- matical Statistics (1951), 400–407.",
"[RMD+06] P L´opez R´ıos, Ao Ma, Neil D Drummond, Michael D Towler, and Richard J Needs, Inhomoge- neous backﬂow transformations in quantum Monte Carlo calculations, Physical Review E 74 (2006), no.",
"[Ros58] Frank Rosenblatt, The perceptron: a probabilistic model for information storage and organization in the brain, Psychological review 65 (1958), no.",
"[RPK+17] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein, On the expressive power of deep neural networks, International Conference on Machine Learning, 2017, pp.",
"[RPK19] Maziar Raissi, Paris Perdikaris, and George E Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations, Journal of Computational Physics 378 (2019), 686–707.",
"[RR+07] Ali Rahimi, Benjamin Recht, et al., Random features for large-scale kernel machines, Advances in Neural Information Processing Systems, 2007, pp.",
"[Rud06] Walter Rudin, Real and complex analysis, McGraw-Hill Series in Higher Mathematics, Tata McGraw-Hill, 2006.",
"[RWK+20] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari, What’s hidden in a randomly weighted neural network?, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp.",
11893–11902.,
"[Sak99] Akito Sakurai, Tight bounds for the VC-dimension of piecewise polynomial networks, Advances in Neural Information Processing Systems, 1999, pp.",
"74 [SCC18] Uri Shaham, Alexander Cloninger, and Ronald R Coifman, Provable approximation properties for deep neural networks, Applied and Computational Harmonic Analysis 44 (2018), no.",
"3, 537–557.",
"[Sch15] J¨urgen Schmidhuber, Deep learning in neural networks: An overview, Neural Networks 61 (2015), 85–117.",
"[SDR14] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski, Lectures on stochastic pro- gramming: modeling and theory, SIAM, 2014.",
"[SEJ+20] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, and Alex Bridgland, Improved protein structure prediction using potentials from deep learning, Nature 577 (2020), no.",
"7792, 706–710.",
"[SGHK18] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli, Analysing mathematical reasoning abilities of neural models, International Conference on Learning Representations, 2018.",
"[SGS15] Rupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber, Training very deep networks, Advances in Neural Information Processing Systems, 2015, pp.",
"[SH19] Johannes Schmidt-Hieber, Deep ReLU network approximation of functions on a manifold, 2019, arXiv preprint arXiv:1908.00695.",
"[She20] Zuowei Shen, Deep network approximation characterized by number of neurons, Communications in Computational Physics 28 (2020), no.",
"5, 1768–1811.",
"[SHK+14] Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, Dropout: a simple way to prevent neural networks from overﬁtting, Journal of Machine Learning Research 15 (2014), no.",
"1, 1929–1958.",
"[SHM+16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot, Mas- tering the game of go with deep neural networks and tree search, Nature 529 (2016), no.",
"7587, 484–489.",
"[SHN+18] Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro, The implicit bias of gradient descent on separable data, 2018.",
"[ˇS´ım02] Jiˇr´ı ˇS´ıma, Training a single sigmoidal neuron is hard, Neural Computation 14 (2002), no.",
"11, 2709–2728.",
"[SKS+17] Kristof T Sch¨utt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M¨uller, Schnet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions, Advances in Neural Information Processing Systems, 2017, pp.",
"[SLJ+15] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp.",
"[SO12] Attila Szabo and Neil S Ostlund, Modern quantum chemistry: introduction to advanced electronic structure theory, Courier Corporation, 2012.",
"[SPRE18] Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad, Multilayer convolutional sparse modeling: Pursuit and dictionary learning, IEEE Transactions on Signal Processing 66 (2018), no.",
"15, 4090–4104.",
"75 [SRG+21] Michael Scherbela, Rafael Reisenhofer, Leon Gerard, Philipp Marquetand, and Philipp Grohs, Solving the electronic Schr¨odinger equation for multiple nuclear geometries with weight-sharing deep neural networks, 2021, arXiv preprint arXiv:2105.08351.",
"[SS16] Itay Safran and Ohad Shamir, On the quality of the initial basin in overspeciﬁed neural networks, International Conference on Machine Learning, 2016, pp.",
"[SS17] , Depth-width tradeoﬀs in approximating natural functions with neural networks, Interna- tional Conference on Machine Learning, 2017, pp.",
"[SS18] , Spurious local minima are common in two-layer ReLU neural networks, International Conference on Machine Learning, 2018, pp.",
"[SSBD14] Shai Shalev-Shwartz and Shai Ben-David, Understanding machine learning: From theory to algorithms, Cambridge University Press, 2014.",
"[SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton, Mastering the game of go without human knowledge, Nature 550 (2017), no.",
"7676, 354–359.",
"[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan, Stochastic convex optimization, Conference on Learning Theory, 2009.",
"[STIM18] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, How does batch normalization help optimization?, Advances in Neural Information Processing Systems, 2018, pp.",
"[SZ19] Christoph Schwab and Jakob Zech, Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in uq, Analysis and Applications 17 (2019), no.",
"[Tal94] Michel Talagrand, Sharper bounds for Gaussian and empirical processes, The Annals of Proba- bility (1994), 28–76.",
"[Tel15] Matus Telgarsky, Representation beneﬁts of deep feedforward networks, 2015, arXiv preprint arXiv:1509.08101.",
"[TvG18] Matthew Thorpe and Yves van Gennip, Deep limits of residual neural networks, 2018, arXiv preprint arXiv:1810.11741.",
"[UVL18] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, Deep image prior, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.",
"[Vap99] Vladimir Vapnik, An overview of statistical learning theory, IEEE Transactions on Neural Networks 10 (1999), no.",
"5, 988–999.",
"[Vap13] , The nature of statistical learning theory, Springer science & business media, 2013.",
"[VBB19] Luca Venturi, Afonso S Bandeira, and Joan Bruna, Spurious valleys in one-hidden-layer neural network optimization landscapes, Journal of Machine Learning Research 20 (2019), no.",
"[VBC+19] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, and Petko Georgiev, Grandmaster level in StarCraft II using multi-agent reinforcement learning, Nature 575 (2019), no.",
"7782, 350–354.",
"76 [VC71] Vladimir Vapnik and Alexey Chervonenkis, On the uniform convergence of relative frequencies of events to their probabilities, Theory of Probability & Its Applications 16 (1971), no.",
"2, 264–280.",
"[vdVW97] Aad W van der Vaart and Jon A Wellner, Weak convergence and empirical processes with applications to statistics, Journal of the Royal Statistical Society-Series A Statistics in Society 160 (1997), no.",
"3, 596–608.",
"[Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications in data science, vol.",
"47, Cambridge University Press, 2018.",
"[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in Neural Information Processing Systems, 2017, pp.",
"[Wer88] Paul J Werbos, Generalization of backpropagation with application to a recurrent gas market model, Neural Networks 1 (1988), no.",
"4, 339–356.",
"[WGB17] Thomas Wiatowski, Philipp Grohs, and Helmut B¨olcskei, Energy propagation in deep convolu- tional neural networks, IEEE Transactions on Information Theory 64 (2017), no.",
"7, 4819–4842.",
"[Whi34] Hassler Whitney, Analytic extensions of diﬀerentiable functions deﬁned in closed sets, Transac- tions of the American Mathematical Society 36 (1934), no.",
"[WPC+21] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip, A comprehensive survey on graph neural networks, IEEE Transactions on Neural Networks and Learning Systems 32 (2021), no.",
"[WZ95] Ronald J Williams and David Zipser, Gradient-based learning algorithms for recurrent, Back- propagation: Theory, Architectures, and Applications 433 (1995), 17.",
"[WZZ+13] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus, Regularization of neural networks using dropconnect, International Conference on Machine Learning, 2013, pp.",
"[XM12] Huan Xu and Shie Mannor, Robustness and generalization, Machine learning 86 (2012), no.",
"3, 391–423.",
"[Yan19] Greg Yang, Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation, 2019, arXiv preprint arXiv:1902.04760.",
"[Yar17] Dmitry Yarotsky, Error bounds for approximations with deep ReLU networks, Neural Networks 94 (2017), 103–114.",
"[Yar18a] , Optimal approximation of continuous functions by very deep ReLU networks, Conference on Learning Theory, 2018, pp.",
"[Yar18b] , Universal approximations of invariant maps by neural networks, 2018, arXiv preprint arXiv:1804.10306.",
"[Yar21] , Elementary superexpressive activations, 2021, arXiv preprint arXiv:2102.10911.",
"[YGLD17] Rujie Yin, Tingran Gao, Yue M Lu, and Ingrid Daubechies, A tale of two bases: Local-nonlocal regularization on image patches with convolution framelets, SIAM Journal on Imaging Sciences 10 (2017), no.",
"2, 711–750.",
"[YHC18] Jong Chul Ye, Yoseob Han, and Eunju Cha, Deep convolutional framelets: A general deep learning framework for inverse problems, SIAM Journal on Imaging Sciences 11 (2018), no.",
"2, 991–1048.",
"77 [YHPC18] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria, Recent trends in deep learning based natural language processing, IEEE Computational Intelligence Magazine 13 (2018), no.",
"[Yse10] Harry Yserentant, Regularity and approximability of electronic wave functions, Springer, 2010.",
"[YZ20] Dmitry Yarotsky and Anton Zhevnerchuk, The phase diagram of approximation rates for deep neural networks, Advances in Neural Information Processing Systems, vol.",
"[ZAP16] Hao Zhou, Jose M Alvarez, and Fatih Porikli, Less is more: Towards compact CNNs, European Conference on Computer Vision, 2016, pp.",
"[Zas75] Thomas Zaslavsky, Facing up to arrangements: Face-count formulas for partitions of space by hyperplanes: Face-count formulas for partitions of space by hyperplanes, Memoirs of the American Mathematical Society, American Mathematical Society, 1975.",
"[ZBH+17] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals, Under- standing deep learning requires rethinking generalization, International Conference on Learning Representations, 2017.",
"[ZBH+20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C Mozer, and Yoram Singer, Identity crisis: Memorization and generalization under extreme overparameterization, International Conference on Learning Representations, 2020.",
"[ZBS19] Chiyuan Zhang, Samy Bengio, and Yoram Singer, Are all layers created equal?, 2019, arXiv preprint arXiv:1902.01996.",
"[ZCZG20] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Gradient descent optimizes over- parameterized deep ReLU networks, Machine Learning 109 (2020), no.",
"3, 467–492.",
"[Zho20a] Ding-Xuan Zhou, Theory of deep convolutional neural networks: Downsampling, Neural Networks 124 (2020), 319–327.",
"[Zho20b] , Universality of deep convolutional neural networks, Applied and Computational Har- monic Analysis 48 (2020), no.",
"2, 787–794.",
"[ZKS+18] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael Rabbat, Pascal Vincent, Naﬁssa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C Lawrence Zitnick, Michael P Recht, Daniel K Sodickson, and Yvonne W Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, 2018, arXiv preprint arXiv:1811.08839.",
"[ZL17] Barret Zoph and Quoc V Le, Neural architecture search with reinforcement learning, International Conference on Learning Representations, 2017.",
"Deep Learning and Computational Physics (Lecture Notes) Deep Ray, Orazio Pinti and Assad A. Oberai1 1Department of Aerospace and Mechanical Engineering, University of Southern California, Los Angeles, California, USA arXiv:2301.00942v1 [cs.LG] 3 Jan 2023 Contents Preface 3 1 Introduction 4 1.1 Computational physics .",
4 1.2 Machine learning .,
5 1.2.1 Examples of ML .,
5 1.2.2 Types of ML algorithms based leaning task .,
"5 1.3 Artiﬁcial Intelligence, Machine Learning and Deep Learning .",
6 1.4 Machine learning and computational physics .,
7 2 Introduction to deep neural networks 9 2.1 MLP architecture .,
9 2.2 Activation functions .,
10 2.2.1 Linear activation .,
12 2.2.2 Rectiﬁed linear unit (ReLU) .,
12 2.2.3 Leaky ReLU .,
12 2.2.4 Logistic function .,
12 2.2.5 Tanh .,
13 2.2.6 Sine .,
13 2.3 Expressivity of a network .,
13 2.3.1 Universal approximation results .,
"15 2.4 Training, validation and testing of neural networks .",
15 2.5 Generalizability .,
17 2.5.1 Regularization .,
17 2.6 Gradient descent .,
18 2.7 Some advanced optimization algorithms .,
20 2.7.1 Momentum methods .,
20 2.7.2 Adam .,
21 2.7.3 Stochastic optimization .,
21 2.8 Calculating gradients using back-propagation .,
23 2.9 Regression versus classiﬁcation .,
25 3 Residual neural networks 27 3.1 Vanishing gradients in deep networks .,
27 3.2 ResNets .,
28 3.3 Connections with ODEs .,
29 3.4 Neural ODEs .,
30 1 4 Solving PDEs with MLPs 33 4.1 Finite diﬀerence method .,
34 4.2 Spectral collocation method .,
35 4.3 Physics-informed neural networks (PINNs) .,
38 4.4 Extending PINNs to a more general PDE .,
40 4.5 Error analysis for PINNs .,
41 4.6 Data assimilation using PINNs .,
42 5 Convolutional Neural Networks 44 5.1 Functions and images .,
44 5.2 Convolutions of functions .,
44 5.2.1 Example 1 .,
45 5.2.2 Example 2 .,
46 5.3 Discrete convolutions .,
46 5.4 Connection to ﬁnite diﬀerences .,
48 5.5 Convolution layers .,
48 5.5.1 Average and Max Pooling .,
50 5.5.2 Convolution for inputs with multiple channels .,
51 5.6 Convolution Neural Network (CNN) .,
51 5.7 Transpose convolution layers .,
53 5.8 Image-to-image transformations .,
56 6 Operator Networks 57 6.1 The problem with PINNs .,
57 6.2 Parametrized PDEs .,
57 6.3 Operators .,
58 6.4 Deep Operator Network (DeepONet) Architecture .,
59 6.5 Training DeepONets .,
61 6.6 Error Analysis for DeepONets .,
62 6.7 Physics-Informed DeepONets .,
63 6.8 Fourier Neural Operators - Architecture .,
64 6.9 Discretization of the Fourier Neural Operator .,
66 6.10 The Use of Fourier Transforms .,
67 7 Probabilistic Deep Learning 70 7.1 Key elements of Probability Theory .,
70 7.2 Random Variables .,
72 7.2.1 Cumulative distribution function .,
72 7.2.2 Probability density function .,
74 7.2.3 Examples of Important RVs .,
75 7.2.4 Expectation and variance of RVs .,
76 7.2.5 Pair of RVs .,
77 7.3 Unsupervised probabilistic deep learning algorithms .,
79 7.3.1 GANs .,
79 7.4 Supervised probabilistic deep learning algorithms .,
82 7.4.1 Conditional GANs .,
83 2 Preface These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California.,
They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics.,
The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial diﬀerential equations to select topics in deep learning.,
These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals.,
"First, they use concepts from computational physics to develop an understanding of deep learning algorithms.",
"Not surprisingly, many concepts in deep learning can be connected to similar concepts in computational physics, and one can utilize this connection to better understand these algorithms.",
"Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics.",
"Thus, they oﬀer someone who is interested in modeling a physical phenomena with a complementary set of tools.",
3 Chapter 1 Introduction This course deals with topics that lie at the interface of computational physics and machine learning.,
"Before we can appreciate the need to combine both these important concepts, we need to understand what each of them mean on their own.",
1.1 Computational physics Computational physics plays a fundamental role in solving many problems in ﬁelds of science and engineering.,
"To gain an understanding of this concept, we brieﬂy outline the key steps involved in solving a physical problem: 1.",
Consider a physical phenomena and collect measurements of some observable of interest.,
"For example, the measurements of the water height and wave direction obtain from ocean bouys, when studying oceanic waves.",
"Based on the observations, postulate a physical law.",
"For instance, you observe that the mass of ﬂuid is a closed-system is conserved for all time.",
Write down a mathematical description of the law.,
"This could make use of ordinary diﬀerential equations (ODEs), partial diﬀerential equations (PDEs), integral equations, etc.",
"Once the mathematical model is framed, solve for the solution of the system.",
There are two ways to obatin this: (a) In certain situations an exact analytical form of the solution can be obtained.,
"For instance one could solve ODEs/PDEs using separation of variables, Laplace transforms, Fourier transforms or integration factors.",
"(b) In most scenarios, exact expressions of the solution cannot be obtained and must be suitable approximated using a numerical algorithm.",
"For instance, one could use forward or backward Euler, mid-point rule, or Runge-Kutta schemes for solving systems of ODEs; or one could use ﬁnite diﬀerence/volume/element methods methods for solving PDEs.",
"Once the algorithm to evaluate the solution (exactly or approximately) is designed, use it to validate the mathematical model, i.e., see if the predictions are in tune with the data collected.",
All these steps broadly describe what computational physics entails.,
"4 1.2 Machine learning Unlike computational physics, machine learning (ML) does not require the postulation of a physical law.",
The general steps involved are: 1.,
"Collect data by observing the physical phenomena, by real-time measurements of some observable or by using an numerical solver approximating the phenomenon.",
"Train a suitable algorithm using the collected data, with the aim of discovering a pattern or relation between the various samples.",
See Section 1.2.1 for some concrete examples.,
"Once trained, use the ML algorithm to make future predictions, and validate it with additional collected data.",
1.2.1 Examples of ML 1.,
"Regression algorithms: Given the set of pairwise data {(xi, yi) : 1 ≤i ≤N} which corresponds to some unknown function y = f(x), ﬁt a polynomial (or any other basis) to this data set in order to approximate f. For instance, ﬁnd the coeﬃcients a, b of the linear ﬁt ˜f(x; a, b) = ax + b to minimize the error E(a, b) = N X i=1 |yi −˜f(xi)|2.",
"If (a∗, b∗) = arg mina,b E(a, b), then we can consider ˜f∗(x) := ˜f(x; a∗, b∗) to be the approxi- mation of f(x) (see Figure 1.1(a)).",
"Decision trees: We are given a dataset from a sample population, containing the features: age, income.",
"Furthermore, the data is divided into two groups; an individual in Group A owns a house while an individual in Group B does not.",
"Then, given a features of a new data point, we would like to predict the probability of this new individual owning a house.",
Decision trees can be used to solve this classiﬁcation problem.,
The way a typical decision tree works is by making cuts the maximize the group-based separation for the samples in the dataset (see Figure 1.1(b)).,
"Then, based on these cuts, the algorithm determines the probability of belonging to a particular class/group for a new point.",
"Clustering algorithms: Given a set of data with a number of features per sample, ﬁnd cluster/patterns in the data (see Figure 1.1(c)).",
"1.2.2 Types of ML algorithms based leaning task Broadly speaking, there are four types of ML algorithms: 1.",
"Supervised learning: Given the data S = {(xi, yi) : 1 ≤i ≤N}, predict ˆy for some new ˆx, such that (ˆx, ˆy) /∈S.",
"For instance, given a set of images and image labels (e.g.",
"dog, cat, cow, etc), train a classiﬁcation ML algorithm to learn the relation between images and labels, and use it to predict the label of a new image.",
"Unsupervised learning: Given the data S = {xi ∈Ωx : 1 ≤i ≤N}, ﬁnd a relation among diﬀerent regions of Ωx.",
"For instance, ﬁnd clusters in the dataset, or ﬁnd an expression for the probability distribution px(x) governing the spread of this data and generate new samples from it.",
5 (a) Linear regression (b) Decision tree (c) Clustering Figure 1.1: Examples of ML 3.,
Semi-supervised learning: This family of methods falls between the supervised and unsupervised learning families.,
They typically make use of a combination of labelled and unlabelled data for training.,
"For example let’s say we are given 10,000 images that are unlabeled and only 50 images that are labeled.",
Can we use this dataset to develop an image classiﬁcation algorithm?,
Re-inforcement learning: The methods belonging to this family learn driven by rewards or penalties for decisions taken.,
"Thus, a suitable path/policy is learned to maximize the reward.",
These kinds of methods are used to train algorithms to play chess or Go.,
"In this course, we will primarily focus on the ﬁrst two types of ML algorithms.",
"1.3 Artiﬁcial Intelligence, Machine Learning and Deep Learning At times, the terms Artiﬁcial Intelligence (AI), ML and Deep Learning (DL) are used inter- changeably.",
"In reality, these are three related but diﬀerent concepts.",
This can be understood by looking at the Venn diagram in Figure 1.2.,
"Figure 1.2: The relation between AI, ML and DL AI refers to a system with human-like intelligence.",
"While ML is a key component of an AI system, there are other ingredients involved.",
A self-driving car is a prototypical example of AI.,
6 Let’s take a closer look at the design of such a system (see Figure 1.3).,
A car is mounted with a camera which takes lives images/video of the road ahead.,
"These frames are then passed to an ML algorithm which performs a semantic segmentation, i.e., segments out diﬀerent regions of the frame and classiﬁes the type of object (car, tree, road, sky, etc) in each segment.",
"Once this segmentation is done, it is passed to a decision system that decides the next action of the car should be based on this segmented image.",
This information then passes though a control module that actually controls the mechanical actions of the car.,
"This entire process is mimics what a real driver would do, and is thus artiﬁcial intelligence.",
"On the other hand, machine learning (ML) are the components of this system that are trained using data.",
That is they learn through data.,
"In the example above, the Semantic Segmenter is one such system.",
"There are many ML algorithms that can perform this task using data, and we will learn some in this course.",
The Decision System could also be an ML component - where the appropriate decision to be made is learnt from prior data.,
"However, it could be non-ML.",
Perhaps a rules based expert system.,
Figure 1.3: Schematic of AI system for a self-driving car.,
Some illustration taken from [32].,
DL is a subset of ML algorithms.,
"The simplest form of a DL architecture, known as a feed- forward network.",
It comprises a number of layers of non-linear transformations.,
The non-linear transformations are applied (component-wise) to an aﬃne transformation of an intermediate output.,
This architecture is loosely motivated by how signals are transmitted by the central nervous in living organisms.,
We will study the DL architecture in greater detail in Chapter 2.,
"1.4 Machine learning and computational physics Now that we have a better understanding of computational physics and ML, the next obvious question would be “why do we need to look at a combination of the two?” We list down a few motivations below: • For complex patterns of “physical” data, ML provides an alternate route to representing mathematical laws.",
Consider a physical process that contains two important components.,
"Of these, one is well understood and has a trusted mathematical model, and the other is poorly understood and does not have a mathematical description.",
"In this scenario, one may use computational physics for the ﬁrst component and ML for the second.",
A concrete example of this would be a system governed by conservation of energy and a complex constitutive model.,
"For the former we may have a well understood mathematical model, while for the latter we may have to rely on ML to develope a model.",
• ML in general is very data hungry.,
But the knowledge of physics can help restrict the 7 manifold on which the input and solution/predictions lie.,
"With such constraints, we can reduce the amount of data required to train the ML algorithm.",
"• Tools for analyzing computational physics (functional analysis, numerical analysis, notions of convergence to exact solutions, probabilistic frameworks) carry over to ML.",
Applying these tools to ML helps us better understand and design better ML algorithms.,
We brieﬂy summarize the various topics that will be covered in this course: • Deep Neural Networks (MLPs) and their convergence.,
• Resnets and their connections with non-linear ODEs (Neural ODEs).,
• Recurnets and their connections with nonlinear ODEs.,
• Convolutional neural networks and their connection to PDEs.,
• Stochastic gradient descent and how it is related to ODEs.,
• Deep Learning algorithms for solving PDEs.,
• Deep Learning algorithms for approximation operators.,
• Generative adversarial algorithms and their connection to computational physics.,
"8 Chapter 2 Introduction to deep neural networks In this chapter, we will take a closer look at the simplest network architecture that is available known as multilayer perceptron (MLP).",
"2.1 MLP architecture Let us deﬁne our objective as the approximation of a function f : x ∈Rd 7→y ∈RD using an MLP, which we denote as F. Computing units of an MLP, called artiﬁcial neurons, are stacked in a number of consecutive layers.",
"The zeroth layer of F is called the source layer, which is not a computing layer but is only responsible for providing an input (of dimension d) to the network.",
"The last layer of F is known as the output layer, which outputs the network’s prediction (of dimension D).",
Every other layer in between is known as a hidden layer.,
The number of neurons in a layer deﬁnes the width of that layer.,
A schematic of an MLP with 2 hidden layers is shown in Figure 2.1. x(l) i = σ(W (l) i j x(l−1) j +b(l) i ) x(0) x(1) x(2) x(3) Act.,
"Source Layer Hidden Layer 1 Hidden Layer 2 Output Layer Figure 2.1: MLP with 2 hidden layers To understand the operations occurring inside an MLP, let us deﬁne some notations.",
"We consider a network with L hidden layers, with the width of layer (l) denoted as Hl for l = 0, 1, ..., L+1.",
"Note that for consistency with the function f that we are trying to approximate, we must have H0 = d and HL+1 = D. Let us denote the output vector for l-th layer by x(l) ∈RHl, which will serve as the input to the next layer.",
We set x(0) = x ∈Rd which will be the input signal provided by the input layer.,
"In each layer l, 1 ≤l ≤L + 1, the i-th neuron performs an 9 aﬃne transformation on that layers input x(l−1) followed by a non-linear transformation x(l) i = σ  W (l) ij x(l−1) j | {z } Einstein sum +b(l) i  , 1 ≤i ≤Hl, 1 ≤j ≤Hl−1 (2.1) where W (l) ij and b(l) i are respectively known as the weights and bias associated with i-th neuron of layer l, while the function σ(.)",
"is known as the activation function, and plays a pivotal role in helping the network to represent non-linear complex functions.",
"If we set W (l) ∈RHl−1×Hl to be the weight matrix for layer l and b(l) ∈RHl to be the bias vector for layer l, then we can re-write the action of the whole layer as x(l) = σ  A(l)(x(l−1))  , A(l)(x(l−1)) = W (l)x(l−1) + b(l) (2.2) where the activation function is applied component-wise.",
"Thus, the action of the whole network F : Rd 7→RD can be mathematically seen as a composition of alternating aﬃne transformations and component-wise activations F(x) = A(L+1) ◦σ ◦A(L) ◦σ ◦A(L−1) ◦· · · ◦σ ◦A(1)(x).",
(2.3) We make a few remarks here: 1.,
"For simplicity of the representation, we assume that the same activation function is used across all layers of the network.",
"However, this is not a strict rule.",
"In fact, there is recent evidence that suggests that alternating activation function from layer to layer leads to better neural networks [33].",
"At times, there might be an output function O instead of an activation function at the end of the output layer, which is typically used to reformulate the output into a suitable form.",
We will see examples of such functions later in the course.,
"We will use the term depth of the network to denote the number of computing layers in the MLP, i.e.",
"the number of hidden layers and the output layer, which would be L + 1 as per the notations used above.",
"The parameters of the network is all the weights and biases, which we will represent as θ = {W (l), b(l)}L+1 l=1 ∈RNθ where Nθ denotes the total number of parameters of the network.",
"The network F(x; θ) represents a family of parameterized functions, where θ needs to suitably chosen such that the network approximates the target function f(x) at the input x.",
Question 2.1.1.,
Prove that Nθ = PL+1 l=1 (Hl−1 + 1)Hl.,
2.2 Activation functions The activation function is perhaps the most important component of an MLP.,
"A large number of activations are available in literature, each with its own advantages and disadvantages.",
Let us take a look at a few of these options (also see Figure 2.2).,
10 ξ σ(ξ) (a) Linear ξ σ(ξ) (b) ReLU α = 0.1 ξ σ(ξ) (c) Leaky ReLU 1 ξ σ(ξ) (d) Logistic 1 −1 ξ σ(ξ) (e) Tanh 1 −1 ξ σ(ξ) (f) Sine Figure 2.2: Examples of activation functions 11 2.2.1 Linear activation The simplest activation corresponds to σ(ξ) = ξ.,
"Some features of this function are • The function is inﬁnitely smooth, but all derivatives beyond the second derivative are zero.",
"• The range of the function is (−∞, ∞).",
• Using the linear activation function (in all layers) will reduce the entire network to a single aﬃne transformation of the input x.,
"In other words, the network will be nothing more that a linear approximation of the target function f, which is not useful if f is highly non-linear.",
"2.2.2 Rectiﬁed linear unit (ReLU) This function is piecewise linear and deﬁned as σ(ξ) = max{0, ξ} = ( ξ, if ξ ≥0 0, if ξ < 0 (2.4) This is one of the most popular activation functions used in practice.",
"Some features of this function are: • The function is continuous, while its derivative will be piecewise constant with a jump ξ = 0.",
The second derivative will be a dirac function concentrated at ξ = 0.,
"In other words, the higher-order derivates (greater than 1) are not well-deﬁned.",
"• The range of the function is [0, ∞).",
2.2.3 Leaky ReLU The ReLU activation leads to a null output from a neuron if the aﬃne transformation of the neuron is negative.,
"This can lead to the phenomena of dying neurons [15] while training a neural network, where neurons drops out completely from the network and no longer contribute to the ﬁnal prediction.",
"To overcome this challenge, a leaky version ReLU was designed σ(ξ; α) = ( ξ, if ξ ≥0 αξ, if ξ < 0 (2.5) where α becomes a network hyper-parameter.",
Some features of this function are: • The derivates of Leaky ReLU behave in the same way as those for ReLU.,
"• The range of the function is (−∞, ∞).",
2.2.4 Logistic function The Logistic or Sigmoid activation function is given by σ(ξ) = 1 1 + e−ξ (2.6) and has the following properties • The function is inﬁnitely smooth and monotonic.,
"• The range of the function is (0, 1), i.e., the function is bounded.",
Such a function is useful in representing probabilities.,
"• Since the derivative quickly decays to zero away from ξ = 0, this activation function can lead to slow convergence of the network while training.",
12 2.2.5 Tanh The tanh function is can be seen as a symmetric extension of the logistic function σ(ξ) = eξ −e−ξ eξ + e−ξ (2.7) and has the following properties • The function is inﬁnitely smooth and monotonic.,
"• The range of the function is (−1, 1), i.e., the function is bounded.",
"Note that it maps zeros input to zero, while pushing positive (negative) inputs to +1 (-1).",
"• Similar to the logistic function, the derivative of tanh quickly decays to zero away from ξ = 0 and can thus lead to slow convergence while training networks.",
"2.2.6 Sine Recently, the sine function, i.e., σ(ξ) = sin(ξ) has been proposed as an eﬃcient activation function [27].",
It has the best features of all the activation function discussed above: • The function is inﬁnitely smooth.,
"• The range of the function is (−1, 1), i.e., the function is bounded.",
• None of the derivatives of this function decay to zero.,
Question 2.2.1.,
"Can you think of an MLP architecture with the sine activation function, which leads to an approximation very similar to a Fourier series expansion?",
2.3 Expressivity of a network Let us try to understand the eﬀects of Nθ increases.,
"To see this, let us consider a simple example using the ReLU activation function, i.e., σ(ξ) = max{ξ, 0}.",
"We set d = D = 1, L = 1 and the parameters W (1) = 2 1  , b(1) = −2 0  , W (2) =  1 1  , b(2) = 0. as shown in Figure 2.3(a).",
"Then the various layer outputs are x(1) 1 = max{2x(0) 1 −2, 0}, x(1) 2 = max{x(0) 1 , 0}, x(2) 1 = max{2x(0) 1 −2, 0} + max{x(0) 1 , 0}.",
"Notice that while the the output x(1) of the hidden layer (see Figures 2.3(b) and (c)) have only one corner/kink, the ﬁnal output ends up having two kinks (see Figures 2.3(d)).",
"We generalize this formulation to a bigger network with L hidden layers each of width H. Then one can expect that x(1) i , 1 ≤i ≤H will have a single kink, with the location and angle of the kink depending on the weights and bias associated with each neuron of the hidden layer.",
"The vector x(1) is passed to the next hidden layer, where each neuron will combine the single kinks and give an output with possibly H kinks.",
"Once again, the location and angles of the H kinks in the output from each neuron of the second hidden layer will be diﬀerent.",
"The location of the kinks will be diﬀerent because each neuron is allowed a diﬀerent bias, and therefore can induce a diﬀerent shift.",
"Continuing this argument, one can expect the number of kinks to increase as H, H2, H3 as it passes through the various hidden layers with width H. In general the total number of kinks can grow as HL.",
"In other words, the networks have the ability to become more expressive as the depth (and width) of the network is increased.",
"13 w = 2 b = −2 w = 1 b = 0 w = 1 b = 0 w = 1 (0) (1) (2) (a) MLP with L = 1,W = 2 x(0) 1 x(1) 1 0 4 4 One kink (b) x(1) 1 vs x(0) 1 x(0) 1 x(1) 2 0 4 4 One kink (c) x(1) 2 vs x(0) 1 x(0) 1 x(2) 1 0 4 4 Two kinks (d) x(2) 1 vs x(0) 1 Figure 2.3: Examples to understand the expressivity of neural networks 14 2.3.1 Universal approximation results To quantify the expressivity of networks in a mathematically rigorous manner, we look at some results about the approximation properties of MLPs.",
"For these results, we assume K ⊂Rd is a closed and bounded set.",
"Theorem 2.3.1 (Pinkus, 1999 [23]).",
"Let f : K →R, i.e., D = 1, be a continuous function.",
"Then given an ϵ > 0, there exists an MLP with a single hidden layer (L = 1), arbitrary width H and a non-polynomial continuous activation σ such that max x∈K |F(x; θ) −f(x)| ≤ϵ.",
"Theorem 2.3.2 (Kidger, 2020 [9]).",
Let f : K →RD be a continuous vector-valued function.,
"Then given an ϵ > 0, there exists an MLP with arbitrary number of hidden layers L, each having width H ≥d + D + 2, a continuous activation σ (with some additional mild conditions), such that max x∈K ∥F(x; θ) −f(x)∥≤ϵ.",
"Theorem 2.3.3 (Yarotsky, 2021 [33]).",
"Let f : K →R be a function with two continuous derivates, i.e., f ∈C2(K).",
Consider an MLP with ReLU activations and H ≥2d + 10.,
Then there exists a network with this conﬁguration such that the error converges as max x∈K |F(x; θ) −f(x)| ≤C(Nθ)−4 where C is a constant depending on the number of network parameters.,
"Numerical results like those mentioned above help demystify the “black-box” nature of neural network, and serve as useful practical guidelines when designing network architectures.",
"2.4 Training, validation and testing of neural networks Now that we have a better understanding of the architecture of MLPs, we would now like to discuss how the parameters of these networks are set to approximate some target function.",
We restrict our discussions to the framework of supervised learning.,
"Let us assume that we are given a dataset of pairwise samples S = {(xi, yi) : 1 ≤i ≤N} corresponding to a target function f : x 7→y.",
"We wish to approximate this function using the neural network F(x; θ, Θ) where θ are the network parameters deﬁned before, while Θ corresponds to the hyper-parameters of the network such as the depth L + 1, width H, type of activation function σ, etc.",
The strategy to design a robust network involves three steps: 1.,
Find the optimal values of θ (for a ﬁxed Θ) in the training phase.,
Find the optimal values of Θ in the validation phase.,
Test the performance of the network on unseen data on the testing phase.,
"To accomplish these three tasks, it is ﬁrst customary to split the dataset S into three distinct parts: a training set with Ntrain samples, a validation set with Nval samples and test set with Ntest samples, with N = Ntrain + Nval + Ntest.",
"Typically, one uses around 60% of the samples as training samples, 20% as validation samples and the remaining 20% for testing.",
15 Splitting the dataset is necessary as neural networks are heavily over-parameterized functions.,
The large number of degrees of freedom available to model the data can lead to over-ﬁtting the data.,
This happens when the error or noise present in the data drives the behavior of the network more than the underlying input-output relation itself.,
"Thus, a part of the data is used to determine θ, and another part to determine the hyper-parameters Θ.",
"The remainder of the data is kept aside for testing the performance of the trained network on unseen data, i.e., the network’s ability to generalize well.",
"Now let us discuss how this split is used during the three phases in further details: Training: Training the network makes use of the training set Strain to solve the following optimization problem: Find θ∗= arg min θ Πtrain(θ), where Πtrain(θ) = 1 Ntrain Ntrain X i=1 (xi,yi)∈Strain ∥yi −F(xi; θ, Θ)∥2 for some ﬁxed Θ.",
The optimal θ∗is obtained using a suitable gradient based algorithm (will be discussed later).,
The function Πtrain is referred to as the loss function.,
In the example above we have used the mean-squared loss function.,
Later we will consider other types of loss functions.,
"Validation: Validation of the network involves using the validation set Sval to solve the following optimization problem: Find Θ∗= arg min Θ Πval(Θ), where Πval(Θ) = 1 Nval Nval X i=1 (xi,yi)∈Sval ∥yi −F(xi; θ∗, Θ)∥2.",
The optimal Θ∗is obtained using a techniques such as (random or tensor) grid search.,
"Testing: Once the ""best"" network is obtained, characterized by θ∗and Θ∗, it is evaluated on the test set Stest to estimate the networks performance on data not used during the ﬁrst two phases.",
"Πtest = 1 Ntest Ntest X i=1 (xi,yi)∈Stest ∥yi −F(xi; θ∗, Θ∗)∥2.",
This testing error is also known as the (approximate) generalizing error of the network.,
Let’s see an example to better understand how such a network is obtained Example 2.4.1.,
"Let us consider an MLP where all hyper-parameters are ﬁxed except for the following ﬂexible choices σ ∈{ReLU, tanh}, L ∈{10, 20}.",
We use the following algorithm 1.,
"For each possible σ, L pair: (a) Find θ∗= arg minθ Πtrain(θ) (b) With this θ∗, evaluate Πval(Θ) 2.",
Select Θ∗to be the one that gave the smallest value of Πval(Θ).,
"Finally, report Πtest for this Θ∗and the corresponding θ∗.",
"16 2.5 Generalizability If we train a network that has a small value of Πtrain and Πval, does it ensure that Πtest will be small?",
"This question is addressed by studying the generalizability of the trained network, i.e., it capability to perform well on data not seen while training/validating the network.",
"If the network is trained to overﬁt the training data, the network will typically lead to poor predictions on test data.",
"Typically, if Strain, Sval and Stest are chosen from the same distribution of data, then a small value of Πtrain, Πval can lead to small values of Πtest.",
"Let us look at the commonly used technique to avoid data overﬁtting, called regularization.",
"2.5.1 Regularization Neural networks, especially MLPs, are almost always over-parametrized, i.e., Nθ ≫N where N is the number of training samples.",
"This would lead to a highly non-linear network model, for which the loss function Π(θ) (where we omit the subscript ""train"" for brevity) can have a landscape with many local minimas (see Figure 2.4(a)).",
Then how do we determine which minima leads to a better generalization?,
"To nudge the choice of θ∗in a more favorable direction, a regularization technique can be employed.",
(a) Loss function landscape (b) Network sensitivity Figure 2.4: The eﬀect of regularization on the loss function.,
We have assumed a scalar θ for easier illustration.,
"The simplest method of regularization involves augmenting a penalty term to the loss function: Π(θ) −→Π(θ) + α∥θ∥, α ≥0 where α is a regularization hyper-parameter, and ∥θ∥is a suitable norm of the network parameters θ.",
This augmentation can change the landscape of Π(θ) as illustrated in Figure 2.4(a).,
"In other words, such a regularization encourages the selection of a minima corresponding to smaller values of the parameters θ.",
It is not obvious why a smaller value of θ would be a better choice.,
"To see why this is better, consider the intermediate network output x(1) 1 = σ(W (1) 1j x(0) j + b(1) 1 ), which gives ∂x(1) 1 ∂x(0) 1 = σ′(W (1) 1j x(0) j + b(1) 1 )W (1) 11 ∝W (1) 11 .",
"17 Since this derivate scales with W (1) 11 , this implies that |∂F(x) ∂x(0) 1 | scales with W (1) 11 as well.",
"If |W (1) 11 | ≫1, then network would be very sensitive to even small changes in the input x(0) 1 , i.e., the network would be ill-posed.",
"As illustrated in Figure 2.4(b), using a proper regularization would help avoid over ﬁtting.",
Let us consider some common types of regularization: • l2 regularization: Here we use the l2 norm in the regularization term ∥θ∥= ∥θ∥2 = Nθ X i=1 θ2 i !1/2 .,
"• l1 regularization: Here we use the l1 norm in the regularization term ∥θ∥= ∥θ∥1 = Nθ X i=1 |θi|, which promotes the sparsity of θ.",
2.6 Gradient descent Recall that we wish to solve the minimization problem θ∗= arg min Π(θ) in the training phase.,
"This minimization problem can be solved using gradient descent (GD), also known as steepest descent.",
Consider the Taylor expansion about θ0 Π(θ0 + ∆θ) = Π(θ0) + ∂Π ∂θ (θ0) · ∆θ + ∂2Π ∂θiθj (ˆθ)∆θi∆θj for some ˆθ in a small neighbourhood of θ0.,
"When |∆θ| is small and assuming ∂2Π ∂θiθj is bounded, we can neglect the second order term and just consider the approximation Π(θ0 + ∆θ) ≈Π(θ0) + ∂Π ∂θ (θ0) · ∆θ.",
"In order to lower the value of the loss function as much as possible compared to its evaluation at θ0, i.e.",
"minimize ∆Π = Π(θ0 + ∆θ) −Π(θ0), we need to choose the step ∆θ in the opposite direction of the gradient, i.e.",
": ∆θ = −η∂Π ∂θ (θ0) with the step-size η ≥0, also known as the learning-rate.",
This is yet another hyper-parameter that we need to tune during the validation phase.,
"This is the crux of the GD algorithm, and can be summarized as follows: 1.",
Initialize k = 0 and θ0 2.,
"While |Π(θk)| > ϵ1, do (a) Evaluate ∂Π ∂θ (θk) (b) Update θk+1 = θk −η ∂Π ∂θ (θk) (c) Increment k = k + 1 18 Convergence: Assume that Π(θ) is convex and diﬀerentiable, and its gradient is Lipschitz continuous with Lipschitz constant K. Then for a η ≤1/K , the GD updates converges as ∥θ∗−θk∥2 ≤C k .",
"However, in most scenarios Π(θ) is not convex.",
"If there is more than one minima, then what kind of minima does GD like to pick?",
"To answer this, consider the loss function for a scalar θ as shown in Figure 2.5, which has two valleys.",
Let’s assume that the proﬁle of Π(θ) in the each valley can be approximated by a (centered) parabola Π(θ) ≈1 2aθ2 where a > 0 is the curvature of each valley.,
Note that the curvature of the left valley is much smaller than the curvature of the right valley.,
Let’s pick a constant learning rate η and a starting value θ0 in either of the valleys.,
"Then, ∂Π ∂θ (θ0) = aθ0 and the new point after a GD update will be θ1 = θ0(1 −aη).",
"Similarly, it is easy to see that all subsequent iterates write θk+1 = θk(1 −aη).",
"For convergence, we need θk+1 θk < 1 =⇒|1 −aη| < 1.",
"Since a > 0 in the valleys, we will need the following condition on the learning rate −1 < 1 −aη =⇒aη < 2.",
"If we ﬁx η, then for convergence we need the local curvature to satisfy a < 2/η.",
"In other words, GD will prefer to converge to a minima with a ﬂat/small curvature, i.e., it will prefer the minima in the left valley.",
"If the starting point is in the right valley, there is a chance that we will keep overshooting the right minima and bounce oﬀthe opposite wall till the GD algorithm slingshots θk outside the valley.",
After this it will enter the left valley with a smaller curvature and gradually move towards its minima.,
Figure 2.5: GD prefers ﬂatter minimas.,
"While it is clear that GD prefers ﬂat minima, what is not clear is why are ﬂat minima better.",
"There is empirical evidence that the parameter values obtained at ﬂat minima tend to generalize better, and therefore are to be preferred.",
19 2.7 Some advanced optimization algorithms We discussed how GD can be used to solve the optimization problem involved in training neural networks.,
Let us look at a few advanced and popular optimization techniques motivated by GD.,
"In general, the update formula for most optimization algorithms make use of the following formula [θk+1]i = [θk]i −[ηk]i[gk]i, 1 ≤i ≤Nθ, (2.8) where [ηk]i is the component-wise learning rate and the vector-valued function g depends/approximates the gradient.",
Note that the notation [.,
]i is used to denote the i-th component of the vector.,
"Also note that the learning rate is allowed to depend on the iteration number k. The GD method makes use of [ηk]i = η, gk = ∂Π ∂θ (θk).",
An issue with the GD method is that the convergence to the minima can be quite slow if η is not suitably chosen.,
"For instance, consider the objective function landscape shown in Figure 2.6, which has sharper gradients along the [θ]2 direction compared to the [θ]1 direction.",
"If we start from a point, such as the one shown in the ﬁgure, then if η is too large (but still within the stable bounds) the updates will keep zig-zagging its way towards the minima.",
"Ideally, for the particular situation shown in Figure 2.6, we would like the steps to take longer strides along the [θ]1 compared to the [θ]2 direction, thus reaching the minima faster.",
Figure 2.6: Zig-zagging updates with GD.,
Let us look at two popular methods that are able to overcome some of the issues faced by GD.,
"2.7.1 Momentum methods Momentum methods make use of the history of the gradient, instead of just the gradient at the previous step.",
"The formula for the update is given by [ηk]i = η, gk = β1gk−1 + (1 −β1)∂Π ∂θ (θk), g−1 = 0 20 where gk is a weighted moving average of the gradient.",
This weighting is expected to smoothen out the zig-zagging seen in Figure 2.6 by cancelling out the components of gradient along the [θ]2 direction and move more smoothly towards the minima.,
A commonly used value for β1 is 0.9.,
"2.7.2 Adam The Adam optimization was introduced by Kingma and Ba [10], which makes use of the history of the gradient as well the second moment (which is a measure of the magnitude) of the gradient.",
"For an initial learning rate η, the updates are given by gk = β1gk−1 + (1 −β1)∂Π ∂θ (θk) [Gk]i = β2[Gk−1]i + (1 −β2)  ∂Π ∂θi (θk) 2 [ηk]i = η p [Gk]i + ϵ (2.9) where gk and Gk are the weighted running averages of the gradients and the square of the gradients, respectively.",
"The recommended values for the hyper-parameters are β1 = 0.9, β2 = 0.999 and ϵ = 10−8.",
Note that the learning rate for each component is diﬀerent.,
"In particular, the larger the magnitude of the gradient for a component the smaller is its learning rate.",
"Referring back to the example in Figure 2.6, this would mean a smaller learning rate for θ2 in comparison to θ1, and therefore will help alleviate the zig-zag path of the optimization algorithm.",
Remark 2.7.1.,
The Adam algorithm also has additional correction steps for gk and Gk to improve the eﬃciency of the algorithm.,
See [10] for details.,
"2.7.3 Stochastic optimization We note that the training loss can be rewritten as Π(θ) = 1 Ntrain Ntrain X i=1 Πi(θ), Πi(θ) = ∥yi −F(xi; θ, Θ)∥2 Thus, the gradient of the loss function is ∂Π ∂θ (θ) = 1 Ntrain Ntrain X i=1 ∂Πi ∂θ (θ) However, taking the summation of gradients can be very expensive since Ntrain is typically very large, Ntrain ∼106.",
"One easy way to circumvent this problem is to use the following update formula (shown here for the GD method) θk+1 = θk −ηk ∂Πi ∂θ (θk), (2.10) where i is randomly chosen for each update step k. This is known as stochastic gradient descent.",
"Remarkably, this modiﬁed algorithm does converge assuming that Πi(θ) is convex and diﬀerentiable, and ηk ∼1/ √ k [19].",
"To illustrate why ηk needs to decay, consider the toy function(s) for θ ∈R2 Π1(θ) = ([θ]1 −1)2 + ([θ]2 −1)2, Π2(θ) = ([θ]1 + 1)2 + 0.5([θ]2 −1)2, Π3(θ) = 0.7([θ]1 + 1)2 + 0.5([θ]2 + 1)2, Π4(θ) = 0.7([θ]1 −1)2 + 1 2([θ]2 + 1)2, Π(θ) = 1 4 (Π1(θ) + Π2(θ) + Π3(θ) + Π4(θ)) .",
"(2.11) 21 The contour plots of these functions in shown in Figure 2.7(a), where the black contour plots corresponds to Π(θ).",
"Note that the θ∗= (0, 0) is the unique minima for Π(θ).",
"We consider solving with the SGD algorithm with a constant learning rate ηk = 0.4 and a decaying learning rate ηk = 0.4/ √ k. Starting with θ0 = (−1.0, 2.0) and randomly selecting i ∈1, 2, 3, 4 for each step k, we run the algorithm for 10,000 iterations.",
The ﬁrst 10 steps with each learning rate is plotted in Figure 2.7(a).,
"We can clearly see that without any decay in the learning rate, the SGD algorithm keeps overshooting the minima.",
"In fact, this behaviour continues for all future iterations as can be seen in Figure 2.7(b) where the norm of the updates does not decay (we expect it to decay to |θ∗| = 0).",
"On the other hand, we quickly move closer to θ∗if the learning rate decays as 1/ √ k. The reason for reducing the step size as we approach closer to the minima is that far away from the minima for Π the gradient vector for Π and all the individual Πi’s align quite well.",
"However, as we approach closer to the minima for Π this is not the case and therefore one is required to take smaller steps so as not be thrown oﬀto a region far away from the minima.",
(a) Function contours and paths (b) Norm of updates Figure 2.7: SGD algorithm with and without a decay in the learning rate.,
"In practice, stochastic optimization algorithms are not used for the following reasons: 1.",
"Although the loss function decays with the number of iterations, it ﬂuctuates in a chaotic manner close the the minima and never manages to reach the minima.",
"While handling all samples at once can be computationally expensive, handling a single sample at a time severly under-utilizes the computational and memory resources.",
"However, a compromise can be made by using mini-batch optimization.",
"In this strategy, the dataset of Ntrain samples is split into Nbatch disjoint subsets known as mini-batches.",
"Each mini-batch contains Ntrain = Ntrain/Nbatch samples, which also refered to as the batch-size.",
"Thus, the gradient of the loss function can be approximated by ∂Π ∂θ (θ) = 1 Ntrain Ntrain X i=1 ∂Πi ∂θ (θ) ≈ 1 Ntrain X i∈batch(j) ∂Πi ∂θ (θ).",
"(2.12) 22 Note that taking Nbatch = 1 leads to the original optimization algorithms, while take Nbatch = Ntrain gives the stochastic gradient descent algorithm.",
One typically chooses a batch-size to maximize the amount of data that can be loaded into the RAM at one time.,
We deﬁne an epoch as one full pass through all samples (or mini-batches) of the full training set.,
The following describes the mini-batch stochastic optimization algorithm: 1.,
"For epoch = 1, ..., J (a) Randomly shuﬄe the full training set (b) Create Nbatch mini-batches (c) For i = 1, · · · , Nbatch i.",
Evaluate the batch gradient using (2.12).,
"Update θ using this gradient and your favorite optimization algorithm (gradient descent, momentum, or Adam).",
Remark 2.7.2.,
There is an interesting study [31] that suggests that stochastic gradient descent might actually help in selecting minima that generalize better.,
In that study the authors prove that SGD prefers minima whose curvature is more homogeneous.,
"That is, the distribution of the curvature of each of the components of the loss function is sharp and centered about a small value.",
This is contrast to minima where the overall curvature might be small; however the distribution of the curvature of each component of loss function is more spread out.,
Then they go on to show (empirically) that the more homogeneous minima tend to generalize better than their heterogeneous counterparts.,
2.8 Calculating gradients using back-propagation The ﬁnal piece of the training algorithm that we need to understand is how the gradients are actually evaluated while training the network.,
"Recall the output x(l+1) of layer l + 1 is given by Aﬃne transform: ξ(l+1) i = W (l+1) ij x(l) j + b(l+1) i , 1 ≤i ≤Hl+1 (2.13) Non-linear transform: x(l+1) i = σ  ξ(l+1) i  , 1 ≤i ≤Hl+1.",
"(2.14) Given a training sample (x, y), set x(0) = x.",
The value of the loss/objective function (for this particular sample) can be evaluated using the forward pass: 1.,
"For l = 1, ..., L + 1 (a) Evaluate ξ(l) using (2.13).",
(b) Evaluate x(l) using (2.14).,
"Evaluate the loss function for the given sample Π(θ) = ∥y −F(x; θ, Θ)∥2.",
This operation can be written succinctly in the form of a computational graph as shown in Figure 2.8.,
"In this ﬁgure, the lower portion of the graph represents the evaluation of the loss function Π.",
We would of course need to repeat this step for all samples in the training set (or a mini-batch for stochastic optimization).,
"For simplicity, we restrict the discussion to the evaluation of the loss and its gradient for a single sample.",
"23 In order to update the network parameters, we need ∂Π ∂θ , or more precisely ∂Π ∂W (l) , ∂Π ∂b(l) for 1 ≤l ≤L + 1.",
We will derive expressions for these derivatives by ﬁrst deriving expressions for ∂Π ∂ξ(l) and ∂Π ∂x(l) .,
From the computational graph it is easy to see how each hidden variable in the network is transformed to the next.,
"Recognizing this, and applying the chain rule repeatedly yields ∂Π ∂ξ(l) = ∂Π ∂x(L+1) · ∂x(L+1) ∂ξ(L+1) · ∂ξ(L+1) ∂x(L) · · · ∂x(l+1) ∂ξ(l+1) · ∂ξ(l+1) ∂x(l) · ∂x(l) ∂ξ(l) .",
"(2.15) In order to evaluate this expression we need to evaluate the following terms: ∂Π ∂x(L+1) = −2(y −x(L+1))T (2.16) ∂ξ(l+1) ∂x(l) = W (l+1) (2.17) ∂x(l) ∂ξ(l) = S(l) ≡diag[σ′(ξ(l) 1 ), · · · , σ′(ξ(l) Hl)], (2.18) where the last two relations hold for any network layer l, Hl is the width of that particular layer, and σ′ denotes the derivative of the activation with respect to its argument.",
"Using these relations in (2.15), we arrive at, ∂Π ∂ξ(l) = ∂Π ∂x(L+1) · S(L+1) · W (L+1) · · · S(l+1) · W (l+1) · S(l).",
"(2.19) Taking the transpose, and recognizing that Σ(l) is diagonal and therefore symmetric, we ﬁnally arrive at ∂Π ∂ξ(l) = S(l)W (l+1)T S(l+1) · · · W (L+1)T S(L+1)[−2(y −x(L+1))].",
(2.20) This evaluation can also be represented as a computational graph.,
"In fact, as shown in Figure 2.8, it can be appended to the original graph, where this part of the computation appear in the upper row of the graph.",
Note that we are now traversing in the backward direction.,
Hence the name back propagation.,
P A(1) s A(l+1) s s W(1) W(l+1) S(L+1) 𝒙(𝟎) 𝝃(𝟏) 𝒙(𝟏) 𝒙(𝒍) 𝒙(𝒍&𝟏) 𝒙(𝑳&𝟏) 𝝃(𝒍) 𝝃(𝒍&𝟏) 𝝃(𝑳&𝟏) 𝝏𝚷 𝝏𝝃(𝟏) 𝝏𝚷 𝝏𝒙(𝟎) 𝝏𝚷 𝝏𝒙(𝒍) 𝝏𝚷 𝝏𝒙(𝒍&𝟏) 𝝏𝚷 𝝏𝒙(𝑳&𝟏) 𝝏𝚷 𝝏𝝃(𝒍) 𝝏𝚷 𝝏𝝃(𝒍&𝟏) 𝝏𝚷 𝝏𝝃(𝑳&𝟏) 𝝏𝚷 𝝏𝒙(𝟏) S(1) S(l) S(l+1) s Figure 2.8: Computational graph for computing the loss function and its derivatives with respect to hidden/latent vectors.,
The ﬁnal step is to evaluate an explicit expression for ∂Π ∂W (l) .,
"This can be done by recognizing, ∂Π ∂W (l) = ∂Π ∂ξ(l) · ∂ξ(l) ∂W (l) = ∂Π ∂ξ(l) ⊗x(l−1), (2.21) 24 where [x ⊗y]ij = xiyj is the outer product.",
"Thus, in order to evaluate ∂Π ∂W (l) we need x(l−1) which is evaluted during the forward phase and ∂Π ∂ξ(l) which is evaluated during back propagation.",
Question 2.8.1.,
Can you derive a similar set of expressions and the corresponding algorithm to evaluate ∂Π ∂b(l) ?,
Question 2.8.2.,
Can you derive an explicit expression for ∂x(L+1) ∂x(0) .,
That is the an expression for the derivative of the output of the network with respect to its input?,
This is a very useful quantity that ﬁnds use in algorithms like physics informed neural networks and Wasserstein generative adversarial networks.,
"2.9 Regression versus classiﬁcation Till now, given the labelled dataset S = {(xi, yi) : 1 ≤i ≤N}, we have considered two types of losses • The mean square error (MSE) Π(θ) = 1 Ntrain Ntrain X i=1 ∥yi −F(xi; θ, Θ)∥2.",
"• The mean absolute error (MAE) Π(θ) = 1 Ntrain Ntrain X i=1 ∥yi −F(xi; θ, Θ)∥.",
Neural networks with the above losses can be used to solve various regression problems where the underlying function is highly nonlinear and the inputs/outputs are multi-dimensional.,
Example 2.9.1.,
"Given the house/apartment features such as the zip code, the number of bedrooms/bathrooms, carpet area, age of construction, etc, predict the outcomes such as the market selling price, or the number of days on the market.",
"Now let us consider some examples of classiﬁcation problems, where the output of the network typically lies in a discrete ﬁnite set.",
Example 2.9.2.,
"Given the symptoms and blood markers of patients with COVID-19, predict whether they will need to be admitted to ICU.",
"So the input and output for this problem would be x = [pulse rate, temperature, SPO2, procalcitonin, ...] y = [p1, p2] where p1 is the probability of being admitted to the ICU, while p2 is the probability of not being admitted.",
"Note that 0 ≤p1, p2 ≤1 and p1 + p2 = 1.",
Example 2.9.3.,
"Given a set of images of animals, predict whether the animal is a dog, cat or bird.",
"In this case, the input and output should be x = the image y = [p1, p2, p2] where p1, p2, p3 is the probability of being a dog, cat or bird, respectively.",
"25 Since the output for the classiﬁcation problem corresponds to probabilities, we need to make a few changes to the network 1.",
"Make use of an output function at the end of the output layer that suitably transforms the output vector into the desired form, i.e, a vector of probabilities.",
This is typically done using the softmax function x(L+1) i = exp (ξ(L+1) i ) PC j=1 exp (ξ(L+1) j ) where C is the number of classes (and also the output dimension).,
"Verify that with this transformation, the components of the x(L+1) form a convex combination, i.e., x(L+1) i ∈[0, 1] and PC i=1 x(L+1) i = 1.",
The output labels for the various samples need to be one-hot encoded.,
"In other words, for the sample (x, y), the output label y should have dimension D = C, and whose component is 1 only for the component signifying the class x belongs to, otherwise 0.",
"For instance, in Example 2.9.3 y =      [1, 0, 0]⊤ if x is a dog, [0, 1, 0]⊤ if x is a cat, [0, 0, 1]⊤ if x is a pig.",
"Although the MSE or MSA can still be used as the loss function, it is preferable to use the cross-entropy loss function Π(θ) = 1 Ntrain Ntrain X i=1 C X c=1 −yci log(Fc(xi; θ)), (2.22) where yci is the c-th component of the true label for the i-th sample.",
The loss function in (2.22) treats yc and Fc as probability distributions and measures the discrepancy between the two.,
It can be shown to be related to the Kullback-Liebler divergence between the two distributions.,
"Compared to MSE, this loss function severely penalizes strongly conﬁdent incorrect predictions.",
This is demonstrated in Example 2.9.4 Example 2.9.4.,
"Let us consider a binary classiﬁcation problem, i.e., C = 2.",
"For a given x, let y = [0, 1] and let the prediction be F = [p, 1 −p].",
"Clearly, a small value of p is preferred.",
Therefore any reasonable cost function should penalize large values of p. Now let us evaluate the error using various loss functions • MSE Loss = (0 −p)2 + (1 −1 + p)2 = 2p2.,
• Cross-entropy Loss = −(0 log(p) + 1 log(1 −p) = −log(1 −p).,
"Note that both losses penalize large values of p. Also when p = 0, both losses are zero.",
"However, as p →1 (which would lead the wrong prediction), the MSE loss →2, while the cross-entropy loss →∞.",
"That is, it strongly penalizes incorrect conﬁdent predictions.",
26 Chapter 3 Residual neural networks Residual networks (or ResNets) were introduced by He et al.,
[8] in 2015.,
"In this chapter, we will discuss what these networks are, why they were introduced and their relation to ODEs.",
"3.1 Vanishing gradients in deep networks While training neural networks, the gradients ∂Π ∂W (l) , ∂Π ∂b(l) might become very small.",
"For instance, consider a very deep network, say L ≥20.",
"If ∂Π ∂W (l) ≪1 for l ≤¯l, then the contribution of ﬁrst ¯l layers of the network will be negligible, as the inﬂuence of their weights on the loss function is small.",
"Because of this depth cut-oﬀ, the beneﬁt in terms of expressivity of deep networks is lost.",
So why does this happen?,
Recall from Section 2.8 that ∂Π ∂W (l) = ∂Π ∂ξ(l) ⊗x(l−1) and ∂Π ∂ξ(l) = Σ(l) L+1 Y m=l+1 (W (m)T Σ(m)) ∂Π ∂ξ(L+1) .,
"(3.1) For any matrix, A, let τ(A) denote the largest singular value.",
Then we can bound | ∂Π ∂ξ(l) | by | ∂Π ∂ξ(l) | ≤τ(Σ(l)) L+1 Y m=l+1 (τ(W (m))τ(Σ(m)))| ∂Π ∂ξ(L+1) |.,
"(3.2) Recall that Σ(m) ≡diag[σ′(ξ(m) 1 ), · · · , σ′(ξ(m) Hl )], where σ′ denotes the derivative of σ with respect to its argument.",
For ReLU its value is either 0 or 1.,
Therefore τ(Σ(m))) = 1.,
"Also, for stability we would want τ(W (m)) < 1.",
Otherwise the output of the network can become unbounded.,
In practise this is enforced by the regularization term.,
"Using this in the equation above we have | ∂Π ∂ξ(l) | ≤ L+1 Y m=l+1 (τ(W (m)))| ∂Π ∂ξ(L+1) |, (3.3) where each term in the product is a scalar less than 1.",
"As the number of terms increases, that is L −l ≫1, this product can, and does, become very small.",
"This typically happens when 27 L −l ≈20, in which case | ∂Π ∂ξ(l) |, and therefore | ∂Π ∂W (l) |, become very small.",
This issue is called the problem of vanishing gradients.,
It manifests itself in deep networks where the weights in the inner layers (say L −l > 20) do not contribute to the network.,
"In [8], the authors demonstrate that taking a deeper network can actually lead to an increase in training and validation error (see Figure 3.1).",
"Thus, beyond a certain point, increasing the depth of a network can be counterproductive.",
Based on our previous discussion on vanishing gradients we know why this is the case.,
"Given this, we would like to come up with a network architecture that addresses the problem of vanishing gradients by ensuring ∂Π ∂ξ(L+1) ≈ ∂Π ∂ξ(1) .",
"This means requiring that when the weights of the network approach small values, the network should approach the identity mapping, and not the null mapping.",
This is the core idea behind a ResNet architecture.,
"Figure 3.1: Training error (left) and test error (right) on CIFAR-10 data with “plain"" deep networks (taken from [8]).",
3.2 ResNets Figure 3.2: ResNet of depth 6 with skip connections.,
Consider an MLP with depth 6 (as shown in Figure 3.2) with a ﬁxed width H for each hidden layer.,
"We add skip connections between the hidden layers in the following manner x(l) i = σ(W (l) ij x(l−1) j + b(l) i ) + x(l−1) i , 2 ≤l ≤L.",
(3.4) We can make the following observations: 1.,
"If all weights (and biases) were null, then x(5) = x(1), which in turn would imply ∂Π ∂x(1) = ∂Π ∂x(5) , 28 P A(1) s A(l+1) s s W(1) W(l+1) S(L+1) 𝒙(𝟎) 𝝃(𝟏) 𝒙(𝟏) 𝒙(𝒍) 𝒙(𝒍&𝟏) 𝒙(𝑳&𝟏) 𝝃(𝒍) 𝝃(𝒍&𝟏) 𝝃(𝑳&𝟏) 𝝏𝚷 𝝏𝝃(𝟏) 𝝏𝚷 𝝏𝒙(𝟎) 𝝏𝚷 𝝏𝒙(𝒍) 𝝏𝚷 𝝏𝒙(𝒍&𝟏) 𝝏𝚷 𝝏𝒙(𝑳&𝟏) 𝝏𝚷 𝝏𝝃(𝒍) 𝝏𝚷 𝝏𝝃(𝒍&𝟏) 𝝏𝚷 𝝏𝝃(𝑳&𝟏) 𝝏𝚷 𝝏𝒙(𝟏) S(1) S(l) S(l+1) I I I I I I s Figure 3.3: Computational graph for forward and backpropagation in a Resnet.",
"i.e., we will not have the issue of vanishing gradients.",
The computational graph for forward and back-propagation of a ResNet is shown in Figure 3.3.,
"Looking at this graph, it is clear that the expression for ∂x(l+1) ∂x(l) now involves traversing two branches and adding their sum.",
"Therefore, we have ∂Π ∂ξ(l) = Σ(l) L+1 Y m=l+1 (I + W (m)T Σ(m)) ∂Π ∂ξ(L+1) .",
"(3.5) Now, if we assume that |W (m)| ≪1 via regularization, we have ∂Π ∂ξ(l) = Σ(l) I + L+1 X m=l+1 W (m) T Σ(m) + higher order terms  ∂Π ∂ξ(L+1) .",
"(3.6) In the expression above, even if the individual matrices have small entries, their sum need not approach a zero matrix.",
"This implies that we can create a ﬁnite (and signiﬁcant) change between the gradients near the input and output layers, while still requiring the weights to be small (via regularization).",
Remark 3.2.1.,
"The above analysis can be extended to cases when H is not ﬁxed, but the analysis is not as clean.",
See [8] on how we can do this.,
"3.3 Connections with ODEs Let us ﬁrst consider the special case of a ResNet with d = D = H. Recall the relation (3.4), which we can rewrite as x(l) −x(l−1) ∆t = 1 ∆tσ(W (l)x(l−1) + b(l)) = 1 ∆tσ(ξ(l)) (3.7) 29 for some scalar ∆t, where we note that ξ(l) is a function of x(l−1) parameterized by θ(l) = [W (l), b(l)].",
"Thus, we can further rewrite (3.7) as x(l) −x(l−1) ∆t = V (x(l−1); θ(l)).",
"(3.8) Now consider a ﬁrst-order system of (possibly non-linear) ODEs, where given x(0) and ˙x ≡dx dt = V (x, t) (3.9) we want to ﬁnd x(T).",
"In order to solve this numerically, we can uniformly divide the temporal domain with a time-step ∆t and temporal nodes t(l) = l∆t, 0 ≤l ≤L + 1, where (L + 1)∆t = T. Deﬁne the discrete solution as x(l) = x(l∆t).",
"Then, given x(l−1), we can use a time-integrator to approximate the solution x(l).",
"We can consider a method motivated by the forward Euler integrator, where the the LHS of (3.9) is approximated by LHS ≈x(l) −x(l−1) ∆t .",
while the RHS is approximated using a parameter θ(l) as RHS ≈V (x(l−1); t(l)) = V (x(l−1); θ(l)).,
where we are allowing the parameters to be diﬀerent at each time-step.,
"Putting these two together, we get exactly the relation of the ResNet given in (3.8).",
"In other words, a ResNet is nothing but a descritization of a non-linear system of ODEs.",
We make some comments to further strengthen this connection.,
"• In a fully trained ResNet we are given x(0) and the weights of a network, and we predict x(L+1).",
"• In a system of ODEs, we are given x(0) and V (x, t), and we predict x(T).",
"• Training the ResNet means determining the parameters θ of the network so that x(L+1) is as close as possible to yi when x(0) = xi, for i = 1, · · · , Ntrain.",
"• When viewed from the analogous ODE point of view, training means determining the right hand side V (x, t) by requiring x(T) to be as close as possible to yi when x(0) = xi, for i = 1, · · · , Ntrain.",
"• In a ResNet we are looking for ""one"" V (x, t) that will map xi to yi, for all 1 ≤i ≤Ntrain.",
"3.4 Neural ODEs Motivated by the connection between ResNets and ODEs, neural ODEs were proposed in [4].",
"Consider a system of ODEs given by dx dt = V (x, t) (3.10) Given x(0), we wish to ﬁnd x(T).",
"In [4], the RHS, i.e., V (x, t), is deﬁned using a feed-forward neural network with parameters θ (see Figure 3.4).",
"The input to the network is (x, t) while the output is V (x, t) (having the same dimension as x).",
"With this description, the system (3.10) is solved using a suitable time-marching scheme, such as forward Euler, Runge-Kutta, etc.",
Figure 3.4: Feed-forward neural network used to model the right hand side in a Neural ODE.,
The number of dependent variables = d −1.,
Figure 3.5: Analogy between regression problems and Neural ODEs.,
So how do we use this network to solve a regression problem?,
"Assume that you are given the labelled training data S = {(xi, yi) : 1 ≤i ≤Ntrain}.",
Here both xi and yi are assumed to have the same dimension d −1.,
"The key idea is to think of xi as points in the d −1-dimensional space that represent the initial state of the system, and to think of yi as points that represent the ﬁnal state.",
Then the regression problem becomes ﬁnding the RHS of (3.10) that will map the initial points to the ﬁnal points with minimal amount of error.,
"In other words, ﬁnd the parameters θ such that Π(θ) = 1 N N X i=1 |xi(T; θ) −yi|2 is minimized.",
"Here, xi(T; θ) denotes the solution (at time t = T) to (3.10) with x(0) = xi and the RHS represented by a feed-forward neural network V (x, t; θ).",
Note that yi is the output value that is measured.,
There is a relatively straightforward way of extending this approach to the case when xi and yi have diﬀerent dimensions.,
"In summary, in Neural ODEs one transforms a regression problem to one of ﬁnding the nonlinear, time-dependent RHS of a system of ODEs.",
"Let us list the advantages and diﬀerences when comparing Neural ODEs to ResNets: • If we interpret the number of time-steps in the Neural ODE as the number of hidden 31 layers L in a ResNet, then the computational cost for both methods is O(L).",
This is the cost associated with performing one forward propagation and one backward propagation.,
"However the memory cost (the cost associated with storing the weights of each layer), is diﬀerent.",
"For the neural ODE all the weights are associated with the feed-forward network used to represent the function V (x, t; θ).",
Thus the number of weights are independent of the number of time-steps used to solve the ODE.,
"On the other hand, for a ResNet the number of weights increases linearly with the number of layers, therefore the cost of storing them scales as O(L).",
"• In Neural ODEs, we can take the limit ∆t →0 and study the convergence, since this will not change the size of the network used to represent the RHS.",
"However, this is not computationally feasible to do for ResNets, where ∆t →0 corresponds to the network depth L →∞!",
"• ResNet uses a forward Euler type method, but in a Neural ODE one can use any time- integrator.",
"Especially, other higher-order explicit time-integrator like the Runge-Kutta methods that converge to the “exact” solution at a faster rate.",
32 Chapter 4 Solving PDEs with MLPs A number of numerical methods exist to solve PDEs.,
Some of these are: • Finite diﬀerence methods • Finite volume methods • Finite element methods • Spectral Galerkin and collocation methods • Deep neural networks!,
"To better appreciate some of these methods, especially deep neural networks, let us consider a simple model problem describing the scalar advection-diﬀusion problem in one-dimension: Find ﬁnd u(x) in the interval x ∈(0, l) such that adu dx −κd2u dx2 = f(x), x ∈(0, ℓ) u(0) = 0 u(ℓ) = 1 (4.1) where a denotes the advective velocity, κ is the diﬀusion coeﬃcient while f(x) is the source.",
"Such equations are used to model many physical phenomena, such as the transport of pollutant by ﬂuids, or modelling the ﬂow of electrons through semiconductors.",
"The multi-dimensional version of this problem will take the form a · ∇u(s) −κ∆u(s) = f(s), s ∈Ω u(s) = g(s), s ∈∂Ω (4.2) Note that the model problem is a linear PDE (ODE in the one-dimensional case).",
Replacing the velocity a by u leads to the viscous Burgers equation.,
The solution to (4.1) for f ≡0 can be analytically written as u(x) = 1 −exp(ax/κ) 1 −exp(aℓ/κ) where aℓ/κ is known as the Peclet number (Pe) and measures the ratio of the strength of advection to the strength of diﬀusion.,
We plot the solution for varying values of a and κ in Figure 4.1.,
"Note that for small Pe, the solution is essentially a straight line.",
"But as Pe increases, the solution starts to bend and forming a steeper boundary layer near the right boundary.",
The thickness of this boundary layer is given by δ ≈Pe × l. We will now consider a few methods to numerically solve this toy problem.,
33 (a) Fixed κ (b) Fixed a Figure 4.1: Exact solution of (4.1) with ℓ= 1.,
4.1 Finite diﬀerence method The key steps of a ﬁnite diﬀerence scheme are as follows: 1.,
"Discretize the domain into a grid of points, with the goal being to ﬁnd the solution at these points.",
Approximate the derivates with ﬁnite diﬀerence approximations at these points.,
This leads to a system of (linear or non-linear) algebraic equations.,
Solve this system using a suitable algorithm to ﬁnd the solution.,
Applying these steps to (4.1) leads to: 1.,
"Discretize the domain into N + 1 points, with xi = ih, 0 ≤i ≤N where h = ℓ/N.",
We wish to solve for u(xi) = ui.,
We also know from the boundary conditions that u0 = 0 and uN = 1.,
Use the approximations du dx(xi) = ui+1 −ui−1 2h + O(h2) d2u dx2 (xi) = ui+1 −2ui + ui−1 h2 + O(h2) Note that both the approximation used above are second order accurate.,
"They are “central diﬀerence” approximations, as they weigh points on either side of the i-th point with the same magnitude.",
"It is worth mentioning that in the limit of large Peclet number, a central diﬀerence approximation of the advective term is not ideal since it leads to numerical instability.",
"In such a case, an “upwind” approximation is preferred.",
"Applying 34 the approximations to the PDE at xi, 1 ≤i ≤N −1 aui+1 −ui−1 2h −κui+1 −2ui + ui−1 h2 = fi ⇐⇒ui+1  a 2h −κ h2  | {z } γ +ui 2κ h2  | {z } β +ui−1  −a 2h −κ h2  | {z } α = fi Looking at each node where the solution is unknown (recall that u0 = 0 and uN = 1 are known), βu1 + γu2 = −αu0 + f1 αui−1 + βui + γui+1 = fi, ∀2 ≤i ≤N −2 αuN−2 + βuN−1 = −γuN + fN−1 (4.3) Combining all the N −1 equations in (4.3), we get the following linear system Ku = f (4.4) where the tridiagonal matrix K and the other vectors in (4.4) are deﬁned as K =   β γ 0 α ... ... ... ... γ 0 α β   ∈R(N−1)×(N−1), u =  u1 u2 · · · uN−2 uN−1 ⊤∈RN−1, f =  −αu0 + f1 f2 f3 · · · fN−2 fN−1 −γuN + fN−1 ⊤∈RN−1 3.",
Solve u = K−1f.,
"Note that: • In practice, we never actually invert K as it is computationally expensive.",
We instead use smart numerical algorithms to solve the system (4.4).,
"For instance, one can use the Thomas tridiagonal algorithm for this particular system, which is a simpliﬁed version of Gaussian elimination.",
• We only obtain an approximation ui ≈u(xi).,
"To reduce the approximation error, we could reduce the mesh size h. Alternatively, we could use higher-order ﬁnite diﬀerence approximations which would lead to a ""wider stencil"" to approximate the derivates at each point.",
• We can think of each point where we “apply” the PDE as a collocation point.,
This idea of applying the PDE at collocation points is shared by the next method we consider.,
It is also shared by the method with uses MLPs to solve PDEs.,
4.2 Spectral collocation method Spectral collocation methods seek a solution written as an expansion in terms of a set of smooth and global basis functions.,
"The basis functions are chosen a priori, whereas the coeﬃcients of the expansion are unknowns, and are computed by requiring that the numerical solution of the PDE is exact at a set of so-called collocation points.",
"More speciﬁcally, this approach involves the following steps.",
Select a set of global basis functions with the following properties: (a) It forms complete basis in the space of functions being considered.,
(b) Is smooth enough so that derivatives can be evaluated.,
(c) Easy to evaluate.,
(d) Derivatives that are easy to evaluate.,
"For instance, one can use the Chebyshev polynomials deﬁned on ξ ∈(−1, 1), given by the following recurrence relation T0(ξ) = 1, T1(ξ) = ξ, Tn+1(ξ) = 2ξTn(ξ) −Tn−1(ξ) The ﬁrst few Chebyshev polynomials are shown in Figure 4.2.",
Note that this basis satisﬁes all the required properties listed above.,
It is easy to evaluate at any point because one can use the recurrence relation above and the values of the two lower-order polynomials to evaluate the Chebyshev polynomial of the subsequent order.,
One can also take derivatives of the recurrence relation above to evaluate a recurrence relation for derivatives of all orders.,
Figure 4.2: First few Chebyshev polynomials.,
Write the solution as a linear combination of the basis functions {φn(x)}N n=0 u(x) = N X n=0 unφn(x) (4.5) where un are the basis coeﬃcients.,
"For our toy problem (4.1) (assuming ℓ= 1), we will use the Chebyshev polynomials φn(x) = Tn(2x −1), where the argument is transformed to use these functions on the interval (0, 1).",
"Evaluate the derivates for the PDE, which for our toy problem will be du dx(x) = N X n=0 unφ′ n(x) = N X n=0 un2T ′ n(2x −1) d2u dx2 (x) = N X n=0 unφ′′ n(x) = N X n=0 un4T ′′ n(2x −1) (4.6) 36 4.",
Use the boundary conditions of the PDE.,
"For the speciﬁc case of (4.1), u(0) = 0 =⇒ N X n=0 unφn(0) = N X n=0 unTn(−1) = 0, u(1) = 1 =⇒ N X n=0 unφn(1) = N X n=0 unTn(1) = 1 (4.7) which leads to 2 linear equations for N + 1 coeﬃcients.",
"We then consider a set of (suitably chosen) nodes xi, 1 ≤i ≤N −1 in the interior of the domain, i.e.",
"the collocation points, and use the derivatives found in step 3. in the PDE evaluated at these N −1 nodes a N X n=0 unφ′ n(xi) −κ N X n=0 unφ′′ n(xi) = f(xi) =⇒ N X n=0 un  2aT ′ n(2xi −1) −4κT ′′ n(2xi −1)  = f(xi) (4.8) This leads to an additional N −1 equations for the N + 1 coeﬃcients.",
"Combining (4.7) and (4.8) leads to the following linear system Ku = f (4.9) where K ∈R(N+1)×(N+1), u =  u0 u2 · · · uN−1 uN ⊤∈RN+1, f =  0 f(x1) f(x1) · · · f(xN−2) f(xN−1) 1 ⊤∈RN+1 5.",
Solve u = K−1f.,
"We need to choose the collocation/quadrature points xi properly, so that K has desirable properties that make the linear system (4.9) easier to solve.",
"These include invertibility, positive- deﬁniteness, sparseness, etc.",
Remark 4.2.1.,
The method is called a collocation method as the PDE is evaluated at the speciﬁc collocation/quadrature points xi.,
Remark 4.2.2.,
"When working with a non-linear PDE, we will end up with a non-linear systems of algebraic equations for the coeﬃcients u0, ..., uN, which is typically solved by Newton’s method.",
Let us look at a least-square variant for ﬁnding the coeﬃcients of the expansion of the spectral methods.,
"As done earlier, we still represent the solution using (4.5) and compute its derivates.",
"Then, the coeﬃcients u are found by minimizing the following loss function Π(u) = Πint(u) + λΠbc(u) Πbc(u) = N X n=0 unφn(0) −0 2 + N X n=0 unφn(1) −1 2 Πint(u) = 1 Ntrain Ntrain X i=1 a N X n=0 unφ′ n(xi) −κ N X n=0 unφ′′ n(xi) −f(xi) 2 (4.10) 37 This can be solved using any of the gradient-based methods we have seen in Chapter 2.",
This approach is especially useful when treating non-linear PDEs.,
"In fact, in those cases it is not be possible to write a linear system in terms of the coeﬃcients such as (4.9).",
A few things to note here • λ is a parameter used to scale the interior loss and boundary loss diﬀerently.,
• The number of interior point xi can be chosen independently of the number of basis functions.,
"In other words, Ntrain does not have to be the same as N. • We will see in the next section how this variant of the spectral method is very similar to how deep neural networks are used to solve PDEs.",
4.3 Physics-informed neural networks (PINNs) The idea of using neural networks to solve partial diﬀerential equations was introduced in 1990- 2000s by Lagaris et al.,
"With the renewed interest in using machine learning tools in solving PDEs, this idea was rediscovered in 2019 by Raissi et al.",
"[24], and was given the term PINNs (physics-informed neural networks).",
"The basic idea of PINNs is similar to regression, except that the loss function Π(θ) contains derivate operators arising in the PDE being considered.",
"We outline the main steps below for a one-dimensional scalar PDE, which can easily be extended to multi-dimensional systems of PDEs.",
We recommend that the reader thinks about the similarities and diﬀerences between this method and spectral collocation method described in the previous section.,
Select a neural network as a function representation of the PDE solution: u = F(x; θ).,
"(4.11) Some crucial properties required by this representation are: (a) Do we have completeness with the representation, i.e., can we accurately approximate the necessary class of function using the representation?",
"The answer is yes, because of the universal approximation theorems of neural networks (see Section 2.3.1).",
(b) Is the representation smooth?,
"The answer is yes if the activation function is smooth, such as tanh, sin, etc.",
Note that we cannot use ReLU since it does not enough number of smooth derivatives.,
(c) Is it easy to evaluate?,
"The answer is yes, due to a quick forward propagation pass.",
(d) Is it easy to evaluate derivates?,
"The answer is yes, due to back-propagation.",
This will be discussed in detail below.,
"Given the representation (4.11), we need to ﬁnd θ such that the PDE is satisﬁed in some suitable form.",
"Compare this with spectral collocation approximation given by (4.5), where we need to determine the coeﬃcients un.",
"Note that while the dependence on the coeﬃcients un in (4.5) is linear, the dependence on θ in (4.11) can be highly non-linear.",
Next we want to ﬁnd the derivatives of the representation.,
Consider the computational graph of the network as shown in Figure 4.3.,
It comprises alternate steps of aﬃne transformations and component-wise nonlinear transformation.,
The derivative of the output with respect to the input can be evaluated by back-propagation.,
The graph in Figure 4.3 is obtained by 38 simply setting Π = x(L+1) in the graph shown in Figure 2.8.,
"Further, once we recognize that ∂x(L+1) ∂x(L+1) = 1, the identity matrix, we can easily read from this graph that ∂x(L+1) ∂x(0) = W (L+1)S(L+1)W (L)S(L) · · · W (2)S(2)W (1)S(1) Hence, the evaluation of du dx requires the extention of the original graph with a backward branch used to evaluate the derivative of the activation function for each component of the vectors ξ(l) (see Figure 4.3).",
The second derivative d2u dx2 is evaluated by performing back-propagation of the extended graph.,
"To evaluate higher order derivatives, the graph will need to be extended further in a similar manner.",
"This is what happens behind the scenes in Pytorch when a call to ""autograd"" is made.",
A(1) s A(l+1) s s W(1) W(l+1) S(L+1) 𝒙(𝟎) 𝝃(𝟏) 𝒙(𝟏) 𝒙(𝒍) 𝒙(𝒍&𝟏) 𝒙(𝑳&𝟏) 𝝃(𝒍) 𝝃(𝒍&𝟏) 𝝃(𝑳&𝟏) 𝝏𝒙(𝑳#𝟏) 𝝏𝝃(𝟎) 𝝏𝒙(𝑳#𝟏) 𝝏𝒙(𝟎) 𝝏𝒙(𝑳#𝟏) 𝝏𝒙(𝒍) 𝝏𝒙(𝑳#𝟏) 𝝏𝒙(𝒍#𝟏) 𝝏𝒙(𝑳#𝟏) 𝝏𝒙(𝑳#𝟏) = 1 𝝏𝒙(𝑳#𝟏) 𝝏𝝃(𝒍) 𝝏𝒙(𝑳#𝟏) 𝝏𝝃(𝒍#𝟏) 𝝏𝒙(𝑳#𝟏) 𝝏𝝃(𝑳#𝟏) 𝝏𝒙(𝑳#𝟏) 𝝏𝒙(𝟏) S(1) S(l) S(l+1) s Figure 4.3: Extended graph to evaluate derivatives with respect to network input.,
Insert the functional representation of the solution (4.11) into the PDE to ﬁnd the parame- ters θ.,
"To do this, we ﬁrst deﬁne a set of points S = {xi : 1 ≤i ≤Ntrain} used to train the network, analogous to the set of collocation points in the spectral collocation methods.",
"Thereafter, we need to deﬁne the loss function (specialized to our toy problem (4.1)) Π(θ) = Πint(θ) + λbΠb(θ), Πb(θ) = (F(0; θ) −0)2 + (F(1; θ) −1)2 , Πint(θ) = 1 Ntrain Ntrain X i=1  aF′(xi; θ) −κF′′(xi; θ)n −f(xi) 2 .",
"(4.12) After training the network, i.e.",
"solving the minimization problem θ∗= arg min θ Π(θ), the solution writes u∗(x) = F(x; θ∗).",
"Note that this is exactly what is done for the least squares variant of the spectral collocation method, where the coeﬃcients un are solved by minimizing a similar loss.",
"We make a few remarks: • When we are able to ﬁnd θ∗for which Π(θ∗) = 0, this implies Πint(θ∗) = 0 and Πb(θ∗) = 0.",
"In other words, the PDE residuals are zero at the collocation points.",
This will lead to a good solution as long as the collocation points cover the domain well.,
"• There are various ways to improve the accuracy of PINNs, such as 39 – Increasing the number of collocation points.",
– Changing the hyper-parameter λb weighting the boundary loss.,
– Increasing the size of the network.,
"That is, increasing Nθ.",
"• The boundary conditions (BCs) of a diﬀerential equation carry fundamental physical properties of the phenomena we are trying to describe, and it is paramount that those are satisﬁed by our numerical solution.",
"In the framework of PINNs, BCs are enforced as a soft constrained via the penalization term Πb(θ).",
"Hence, the hyper-parameter λb plays a crucial role in the training of the network, as it balances the interplay between the two loss terms during the minimization process.",
"If the gradients of the diﬀerent loss terms are not adequately scaled, the convergence to a solution that satisﬁes both the BCs and the PDE itself can be extremely slow.",
This is particularly exacerbated for stiﬀPDEs.,
"To address this issue, diﬀerent self-adaptive techniques to tune the value of λb along the training have been proposed [29, 16, 3].",
"4.4 Extending PINNs to a more general PDE Consider a general PDE: Find the solution u : Ω⊂Rd →RD such that L(u(x)) = f(x), x ∈Ω B(u(x)) = g(x), x ∈∂Ω (4.13) where L is the diﬀerential operator, f is the known forcing term, B is the boundary operator, and g is the non-homogeneous part of boundary condition (also prescribed).",
"As an example, we can consider the three-dimensional incompressible Navier-Stokes equation solving for the velocity ﬁeld v = [v1, v2, v3] and pressure p on Ω= ΩS × [0, T].",
"Here ΩS is the three dimensions spatial domain and [0, T] is the time interval of interest.",
"The equation is given by ∂v ∂t + v · ∇v + ∇p −µ∆v = f, ∀(s, t) ∈Ω ∇· u = 0, ∀(s, t) ∈Ω v = 0, ∀(s, t) ∈∂ΩS × [0, T] v(s, 0) = v0(s), ∀s ∈ΩS.",
(4.14) The ﬁrst equation above is the balance of linear moment.,
The second equation enforces the conservation of mass.,
The third equation is the no-slip boundary condition which is used when the boundary is rigid and ﬁxed.,
The fourth equation is the prescription of the initial velocity ﬁeld.,
"To design a PINN for (4.13), the input to the network should be the independent variables x and the output should be the solution vector u.",
"For the speciﬁc case of the Navier-Stokes system (4.14), the input to the network would be [s1, s2, s3, t] ∈R4, while the output vector would be u = [v1, v2, v3, p] ∈R4.",
The steps would be the following: 1.,
Construct the loss functions • Deﬁne the interior residual R(u) = L(u) −f.,
• Deﬁne the boundary residual Rb(u) = B(u) −g.,
• Select suitable Nv collocation points in the interior of the domain and Nb points on the domain boundary to evaluate the residuals.,
"These could be chosen as based on quadrature rules, such as Gaussian, Lobatto, Uniform, Random, etc.",
40 Then the loss function is Π(θ) = Πint(θ) + λbΠb(θ) Πint(θ) = 1 Nv Nv X i=1 |R(F(xi; θ)|2 Πb(θ) = 1 Nb Nb X i=1 |Rb(F(xi; θ)|2 2.,
"Train the network: ﬁnd θ∗= arg min θ Π(θ), and set the solution as u∗= F(x; θ∗) We make some remarks here: • It is implicitly assumed that a weight regularization term is also added to the loss Π(θ).",
• Is u∗(x) the exact solution to the PDE?,
The answer is No!,
"– Firstly, Π(θ∗) may not be zero.",
"– Even if Π(θ∗) is identically zero, it only means that the residuals vanishes at the collocation points.",
"However, that does not guarantee that the residuals will vanish everywhere in the domain.",
"For that Nv, Nb →∞.",
"– Also, with a ﬁxed network (Nθ ﬁxed) we cannot represent all functions.",
"For that, we will need Nθ →∞.",
"• In practice, we only compute Π(θ∗).",
Is the solution error ∥e∥= ∥u∗−u∥related to this loss value?,
"And if it is, can we say that this error will be small as long as the loss is small?",
This is what we try to answer in next section.,
"4.5 Error analysis for PINNs In order to evaluate the error e = u∗−u, we need to know the exact solution u which is not available in general.",
"We consider a way of overcoming this issue, by restricting our discussion to linear PDEs i.e., L and B are linear operators.",
"Note that if u is the exact solution, then L(e) = L(u∗−u) = L(u∗) −L(u) = L(u∗) −f = R(u∗) (4.15) and B(e) = B(u∗−u) = B(u∗) −B(u) = B(u∗) −g = Rb(u∗) (4.16) Thus, (4.15) (4.16) lead to a PDE for e driven by the residuals of the MLP solution, L(e) = R(u∗), in Ω B(e) = Rb(u∗), on Ω (4.17) If the residuals of u∗were zero, then e = 0.",
"Unfortunately, these residuals are not zero.",
The most that we can say is that they are small at the collocation points.,
"However, from the theory of stability of well-posed PDEs, we have ∥e∥L2(Ω) ≤C1  ∥R(u∗)∥L2(Ω) + ∥Rb(u∗)∥L2(∂Ω)  (4.18) 41 where C1 is a stability constant that depends on the PDE, the domain Ω, etc.",
This is a condition that hols for all well-posed PDEs.,
"It says that if the terms driving the PDE are small, then the solution to the PDE will also be small.",
This equation tells us that we can control the error if we can control the residuals for the MLP solution.,
"However, in practise we know and control Πint, Πb and not ∥R(u∗)∥2 L2(Ω), ∥Rb(u∗)∥2 L2(∂Ω).",
"The question then becomes, are these quantities related.",
"This is answered in the analysis below, ∥R(u∗)∥L2(Ω) = mΩΠint(θ∗)1/2 + ∥R(u∗)∥L2(Ω) −mΩΠint(θ∗)1/2 ≤mΩΠint(θ∗)1/2 + ∥R(u∗)∥L2(Ω) −mΩΠint(θ∗)1/2 ≤mΩΠint(θ∗)1/2 + C2(Nv)−α (4.19) where mΩis the measure of the domain Ω, C2 and α > 0 will depend on the type of quadrature points chosen.",
In the equation above the ﬁrst line is obtained by adding and subtracting the term mΩΠint(θ∗)1/2.,
The second line is obtained by using the triangle inequality.,
The third line is a statement of error in approximating an integral with a ﬁnite sum of values evaluated at quadrature points.,
"Similarly for the boundary residual ∥Rb(u∗)∥L2(∂Ω) ≤m∂ΩΠb(θ∗)1/2 + C3(Nb)−β (4.20) where m∂Ωis the measure of ∂Ω, C3 and β > 0 will depend on the type of boundary quadrature points.",
"Using (4.18), (4.19) and (4.20), we get ∥e∥L2(Ω) ≤C1   mΩΠint(θ∗)1/2 + m∂ΩΠb(θ∗)1/2 | {z } reduced by Nθ↑ + C2(Nv)−α + C3(Nb)−β | {z } reduced by Nv,Nb↑    (4.21) This equation tells us that it is possible to control the error in the PINNs solution by reducing the loss functions (by increasing Nθ) and by increasing the number of interior and boundary collocation points.",
"For further details about this analysis, the reader is referred to [18].",
4.6 Data assimilation using PINNs The problem of data assimilation is often encountered in the science and engineering.,
"In this problem, we are able to make a few sparse measurements of a quantity, and using these we wish to evaluate it everywhere on a ﬁne grid.",
We are also given a physical principal (in the form of a PDE) that the variable of interest must adhere to.,
"Let us assume that we are given a set of sparse measurements of some quantity u on the domain Ω ui = u(xi), xi ∈Ω, 1 ≤i ≤M Furthermore, we are given that u satisﬁes some constraint R(u) = 0 on Ω.",
"Then, data assimilation corresponds to using this information to ﬁnd the value of u at any x ∈Ω.",
We can solve this problem using PINNs.,
"First, we represent u using a neural network F(x, θ).",
"Next, we deﬁne a loss function Π(θ) = λI M M X i=1 (ui −F(xi, θ))2 | {z } data matching + 1 Nv M+Nv X i=M+1 |R(F(xi, θ))|2 | {z } physical constraint + λ∥θ∥2 | {z } smoothness 42 where xi, M + 1 ≤i ≤M + Nv are some collocation points chosen to evaluate the residual, while λI, λ are hyper-parameters.",
"Then we train the network by ﬁnding θ∗= arg min θ Π(θ), and set the PINNs solution as u∗= F(x; θ∗).",
"43 Chapter 5 Convolutional Neural Networks In the previous chapters, we have seen how to construct neural networks using fully-connected layers.",
"We will now look at a diﬀerent class of layers, called convolution layers, which are very useful when handling inputs which are images.",
"These tasks include classifying images into categories, performing semantic segmentation on images, and transforming images from one type to another.",
"5.1 Functions and images Consider a function u(x) deﬁned on x ∈[a, b] × [c, d] ⊂R2.",
"Then we can visualize the discretized version of this function as an image U ∈RN1×N2, where U[i, j] = u(ih, jh), 1 ≤i ≤N1, 1 ≤j ≤N2, h = pixel size.",
(5.1) Note that the image in (5.1) deﬁnes a grayscale image where the value of u at each pixel is just the intensity.,
"If we work with color images, then it would be a three-dimensional tensor, with the third dimension corresponding to the red, blue and green channels.",
"In other words, U ∈RN1×N2×3.",
"If we want to use a fully-connected neural network (MLP) which takes as input a colored 2D image of size 100 × 100, then the input dimension after unravelling the entire image as a single vector would be 3 × 104, which is very large.",
This would in turn lead to very large connected layers which is not computationally feasible.,
"Secondly, when unravelling the image, we lose all spatial context of the initial image.",
"Finally, one would expect that local operations, such as edge detection, would be the same in any region of the image.",
Consider the weights for a fully connected layer.,
"These would be represented by the matrix Wij, where the i index represent the output of the linear transform and the j index represents the input.",
"If the operation was the same for every output index, we would apply the same operation for every i and therefore not need the matrix.",
"To address all these issues, we can use the convolution operator on functions.",
5.2 Convolutions of functions The convolution operator maps functions to functions.,
"Consider the function u(x), x ∈Rd, and a suﬃciently smooth kernel function g(x) which decays as |x| →∞.",
Then the convolution operator is given by u(x) = Z Rd g(y −x)u(y)dy (5.2) 44 We can interpret the convolution operator as sampling u by varying x.,
"For example, in 1D, let u(x) and g(x) be as shown in Figure 5.1, and u(x) = Z R g(y −x)u(y)dy.",
Consider a point x0.,
Then g(y −x0) shifts the kernel to the location x0 which will sample the function u in the orange shaded region.,
"Similarly, for another point x1, g(y −x1) shifts the kernel to the location x1 which will sample the function u in the green shaded region.",
"So as the kernel moves, it samples u in diﬀerent windows.",
Note that the same operation is applied regardless of the value of x.,
Lets now consider a few typical kernel functions.,
(a) Kernel g(x) (b) Sampling Figure 5.1: Sampling with shifted kernel in 1D.,
"5.2.1 Example 1 A popular choice is the Gaussian kernel g(ξ) = ρ(|ξ|), where for any scalar r, ρ(r) = exp(−r2/2σ2) p (2πσ2)d .",
which is used as a smoothing/blurring ﬁlter.,
Here d is the dimension while σ denotes the spread.,
This kernel in 1D is shown in 5.1(a) for σ = 0.2.,
"Some important properties of this kernel are: • It is isotropic, as it only depends on the magnitude of ξ.",
• The integral over the whole domain is unity.,
"• It is parameterized by σ, which represents a frequency cut-oﬀ, as scales ﬁner than σ are ﬁltered out by the convolution.",
This smoothing eﬀect is demonstrated in Figure 5.2 45 Figure 5.2: 1D convolution with Gaussian kernel.,
5.2.2 Example 2 Let us consider another example of a kernel that would produce the derivative of a smooth version of u.,
"In 2D, we want this to look like u(x) = ∂ ∂x1 Z ∞ −∞ Z ∞ −∞ ρ(|y −x|)u(y)dy1dy2  = Z ∞ −∞ Z ∞ −∞ ∂ρ(|y −x|) ∂x1 u(y)dy1dy2 = Z ∞ −∞ Z ∞ −∞  −∂ρ(|y −x|) ∂y1  | {z } required kernel u(y)dy1dy2 (5.3) This kernel is shown in both 1D and 2D in Figure 5.3.",
Note that the action of this kernel look like a smoothed ﬁnite diﬀerence operation.,
That is the region to the left of the center of the kernel is weighted by a negative value and the region to the right is weighted by a positive value.,
"5.3 Discrete convolutions To evaluate the discrete convolution in 2D, consider (5.2) and discretize it using quadrature.",
"Then, we will have U[i, j] = N1 X m=1 N2 X n=1 g[m −i, n −j]U[m, n] (5.4) where we have absorbed the measure h2 into the deﬁnition of the kernel.",
"As in the continuous case, we will assume that g vanishes after a certain distance g[p, q] = 0, |p|, |q| > ¯N (measure of width of the kernel).",
"46 (a) 1D (b) 2D, with a derivative along the 1-direction Figure 5.3: Derivative kernel.",
The blue curve denotes the Gaussian kernel and the orange curve denotes the derivative.,
"Thus, the limits of the sum are reduced by exclusing all the pixels over which the convolution will be zero, U[i, j] = i+ ¯ N X m=i−¯ N j+ ¯ N X n=j−¯ N g[m −i, n −j]U[m, n].",
(5.5) Let m′ = m −i and n′ = n −j.,
"Then U[i, j] = ¯ N X m′=−¯ N ¯ N X n′=−¯ N g[m′, n′]U[i + m′, j + n′].",
(5.6) This is precisely how a convolution is applied in deep learning.,
"Thus, the convolution is entirely determined by g[m, n], |m|, |n| ≤¯N, which become the weights of the convolution layer, with the number of weight being ( ¯N + 1)2.",
"Let’s consider some examples: • A smoothing kernel would be 1 8   1 4 1 1 4 1 3 1 1 4 1 1 4  ≈Gaussian kernel with some σ • Kernels that lead to the derivative along the x-direction and y-direction are given by   0 0 0 −1 0 1 0 0 0   and   0 1 0 0 0 0 0 −1 0   • Similarly, the second derivatives anlong the x and y-directions are given by kernels of the form   0 0 0 −1 2 −1 0 0 0   and   0 −1 0 0 2 0 0 −1 0   47 • While the Laplacian is given by the kernel of the type   0 −1 0 −1 4 −1 0 −1 0   Remark 5.3.1.",
We can have diﬀerent ¯N in diﬀerent directions.,
"That is, we can have kernels with diﬀerent widths along each direction.",
5.4 Connection to ﬁnite diﬀerences There is a very strong connection between the concept of convolution and the stencil of a ﬁnite diﬀerence scheme.,
This is made clear in the discussion below.,
"Say some function u(x1, x2) is represented on a ﬁnite grid, where the grid points are indexed by (i, j) with a grid size h. Then we use the notation U[i, j] = u(xi 1, xj 2).",
"Using Taylor series expansion about (i, j) U[i + 1, j] −U[i −1, j] 2h = u(xi+1 1 , xj 2) −u(xi−1 1 , xj 2) 2h = ∂ ∂x1 u(xi 1, xj 2) + O(h2).",
The operation above is identical to the operation of a discrete convolution with weights given by   0 0 0 −1 0 1 0 0 0  1 2h ≈∂u ∂x1 .,
Thus we may say that this convolution operation approximates a derivative along the 1-direction.,
"Similarly, we can show that U[i + 1, j] −2U[i, j] + U[i −1, j] h2 = ∂ ∂x2 1 u(xi 1, xj 2) + O(h2).",
"and thus the convolution with the kernel given by   0 0 0 1 −2 1 0 0 0  1 h2 ≈∂u ∂x2 1 , approximates the computation of the second derivative along the 1-direction.",
"5.5 Convolution layers The key things to remember are: • Each convolution layer consists of several discrete convolutions, each with its own kernel.",
"• The weights of the kernel, which determine its action (smoothing, ﬁrst derivative, second derivative etc.",
"), are learnable parameters and are determined when training the network.",
Thus the way to think about the learning process is that the network learns the operations (convolutions) that are appropriate for its task.,
"The task can be a classiﬁcation problem, for example.",
48 Let us assume we have an N1×N2 image as an input.,
"Then, we will have multiple convolutions in a convolution layer, each of which will generate a diﬀerent image, as shown in Figure 5.4(a).",
The trainable parameters of this layer are the weights of each convolution kernel.,
"Assuming the width of the kernels is ¯N in each direction, and there are P kernels, then the number of trainable weights will be P × (2 ¯N + 1)2.",
Next let us consider the size of the output image after applying a single kernel operation.,
Note that we will not be able to apply the kernel on the boundary pixels since there are no pixel-values available beyond the image boundary (see Figure 5.4(b)).,
"Thus, we will have to skip ¯N pixels at each boundary when applying the kernel, leading to an output image of shape (N1 −¯N + 1) × (N2 −¯N + 1).",
One way to overcome this is by padding the image with ¯N pixels with zero value on each edge.,
"Now we can apply the kernel on the boundary pixels and the output image will be the same size as the input image, as can be seen in Figure 5.4(c).",
Another feature of convolutions is known as the stride which determines the number of pixels by which the kernel is shifted as we move over the image.,
"In the examples above, the stride was 1 in both directions.",
"In practice, we can choose a stride > 1 which will further shrink the size of the output image.",
"For instance, if stride was taken as S in each direction (with zero-padding applied), then the output image size would reduce by a factor of S in each direction (see Figure 5.4(d)).",
(a) Action of a convolution layer (b) Convolution without padding (c) Convolution with zero-padding (d) Convolution with zero-padding and stride 2 Figure 5.4: Action of a convolution layer/kernel.,
"49 5.5.1 Average and Max Pooling Pooling operations are generally used to reduce the size of an image, and allowing you to step through diﬀerent scales of the image.",
"If applied on an image of size N × N over patches of size S × S, the new image will have dimensions N S × N S , where S is the stride of the pooling operation.",
This is shown in Figure 5.5 for S = 2.,
"Note it is typical to select the patch of pixels over which the max or average is computed to be (S × S), where S is the stride.",
This is true for Figure 5.5 (b) but not for 5.5 (a).,
"Also, we show in Figure 5.6 how pooling allows us to move through various scales of the image, where the image gets coarser as more pooling operations are applied.",
Note that pooling operations do not have any trainable parameters.,
The pooling operation has strong analog in similar operators that are used when designing multigrid preconditioners for solving linear systems of algebraic equations.,
(a) Stride 1 (b) Stride 2 Figure 5.5: Max pooling applied to an image over patches of size (2 × 2).,
(a) Original (b) After 1 pooling op.,
(c) After 2 pooling op.,
(d) After 3 pooling op.,
(e) After 4 pooling op.,
(f) After 5 pooling op.,
Figure 5.6: Max pooling applied repeatedly to an image over patches of size (2 × 2) with stride 2.,
"50 5.5.2 Convolution for inputs with multiple channels Assume that the input to a convolution layer is of size N1 × N2 × C, where C is the number of channels in the input image.",
Then a convolution layer apply P convolutions on this input and give an output of size M1 × M2 × P. Note that both the spatial resolution as well as the number of channels of the output image might be diﬀerent from the input image.,
"Furthermore, if a single convolution in the layer uses a kernel of width k = 2 ¯N + 1, then the kernel will be of the shape k × k × C, i.e., the kernel will have k × k weights for each of the C input channels of the input image.",
Each convolution will act on the input to give an output image of shape M1 × M2 × 1.,
The output of each convolution are stacked together to give the ﬁnal output of the convolution layer.,
"This can be written as, ¯U[i, j, k] = ¯ N X m=−¯ N ¯ N X n=−¯ N C X c=1 gk[m, n, c]U[i + m, j + n, c], 1 ≤i ≤M1, 1 ≤j ≤M2, 1 ≤k ≤P, where gk is the kernel of the k-th convolution in the layer.",
"Note that the total number of trainable parameters will be (2 ¯N + 1) × (2 ¯N + 1) × P × C. This is the type of convolutional layer most frequently encountered in a convolutional neural network, which is described in the following section.",
5.6 Convolution Neural Network (CNN) Now let’s put all the elements together to form the full network.,
Consider an image classiﬁcation problem.,
"Then the CNN will be given by y = F(x; θ) where x ∈RN1×N2×N3 is the input image with N3 channels, while y ∈RC is the probability vector whose i-th component of denotes the probability that the input image belongs the i-th class among a total of C classes.",
The y are typically one-hot encoded.,
The possible architecture of this network is shown in Figure 5.7.,
"This consists of a number of convolution layers followed by pooling layers, which will reduce the spatial resolution of the input image while increasing the number of channels.",
"The output of the ﬁnal pooling layer is ﬂattened to form a vector, which is then passed though a number of fully connected layers with some activation function (say ReLU).",
"The ﬁnal fully connected layer reduced the size of the vector to C (which is taken to be 10 in the Figure), which is then passed through a softmax function to generate the predicted probability vector y.",
"Since we are solving a classiﬁcation problem, the loss function is taken to be the cross-entropy function Π(θ) = − Ntrain X i=1 C X c=1 h y(i) c log  Fc(x(i); θ) i .",
We train the CNN by trying to ﬁnd θ∗= arg min θ Π(θ) with the ﬁnal network being y = F(x; θ∗).,
We make some important remarks: 1.,
"The convolution operation is also a linear operation on the input, as is the case for a fully connected layer.",
"The only diﬀerence is that in a fully-connected layer, the weights connect every pixel in the output to every pixel of the input, while in a convolution layer the weights connect one pixel of the output only to a patch of pixels in the input.",
"Furthermore, the same weights are applied on each patch of the input.",
"In the CNN shown in Figure 5.7, the convolution layers can be interpreted as encoding information about the input image, while the fully connected layers can be interpreted as 51 Figure 5.7: Example of a CNN architecture for an image classiﬁcation problem, for 10 classes.",
using this information to solve the classiﬁcation problem.,
"This is why in the literature, convolution layers are said to perform feature selection.",
Further the part of the network leading up to the “ﬂattened” vector is sometimes referred to as the encoder.,
"Sometime, activation is also applied to the output of the convolution layer along with a bias x(l+1)[i, j, k] = σ( X m,n,c gk[m, n, c]x(l)[i + m, j + n, c] + bk) where a single bias bk is used for a given output channel k. 4.",
In the example above we considered an image classiﬁcation problem.,
"That is, the network was a transform from an image to a class label.",
We can think of other similar cases.,
"For example, when the network is a transform from an image to a real number.",
This might have several useful applications in computational physics.,
Consider the case where you want to create an enstrophy calculator.,
"That is a network that will take as input images of the velocity components of a ﬂuid deﬁned on a grid, and produce as output the integral of the square of the vorticity (called the enstrophy) over the entire domain.",
Another example would be a network that takes as input the components of the displacement of an elastic solid and produces as output the total strain energy stored within the solid.,
"The architecture we have considered allows us to transform images to vectors, which is useful in problems involving image classiﬁcation, image analysis, etc.",
"However, there is another architecture that does the opposite, i.e., maps vectors into images.",
This is useful in applications involving image synthesis.,
"Finally, by putting these two architecture together, we can transform an image to a vector and back to another image.",
Such image-to-image transformations are useful in applications such as image semantic segmentation.,
These ideas are described in the following Sections.,
It is worth taking a moment to analyze how convolution layers act on images and why they are so useful.,
"When dealing with images input in the context of deep learning, a ﬁrst naive approach could be to ﬂatten the image and feed it to a regular fully connected MLP.",
"52 However, this would lead to diﬀerent problems.",
"In fact, for regular images, the size of the ﬂatten input would be extremely large.",
"In that case, we would have 2 possibilities when deﬁning the architecture of the network: (a) We can size the ﬁrst layers of the network to have a width comparable to the (large) dimension of the input.",
(b) We can have a sharp decrease in the width of the second hidden layer.,
Either of the strategies would lead to issues.,
"In the ﬁrst case, the size of the network would be too large.",
"Hence, there would be too many trainable parameters which would require an unrealistic amount of data to train the network.",
"In the second case, the compression of information happening between the ﬁrst two layers would be too aggressive, making it very hard for the network to learn how to extract the right features across diﬀerent images.",
"Moreover, important spatial relationship among pixels (like edges, shapes, etc.)",
are lost by ﬂattening an image.,
"Ideally we would like to leverage these relations as much as possible, since they carry important spatial information.",
Convolution layers can solve both issues.,
"They allow the input to be a 2D image, while drastically decreasing the number of learnable parameters needed for the feature extraction task.",
"In fact, kernels introduce a limited number of parameters compared to a classic fully connected layer.",
"Since the same kernel is applied at diﬀerent pixel locations in an image, .i.e.",
"parameter sharing, they utilize the computational resources in an eﬃcient and smart manner.",
5.7 Transpose convolution layers We have seen how convolution and pooling layers can be used to scale down images.,
"We now consider layers that do the opposite, i.e, scale up images.",
"To understand what this operation would look like, let us look at a few examples 1.",
"Consider a 1D image of size 4 Input =  u1, u2, u3, u4  Consider a kernel of size 3 × 1 k =  x, y, z  Consider a convolution layer with the kernel k, stride 1 and zero-padding layer of size 1.",
"Then, the output of the layer acting on the input is Output =  yu1 + zu2, xu1 + yu2 + zu3, xu2 + yu3 + zu4, xu3 + yu4  The steps involved in convolution operator are: pad, dot-product, stride.",
Note that using padding and stride 1 have ensured the output has the same size as the input.,
"Consider another convolution with the same kernel k, zero-padding layer 1 but stride 2.",
"Then, the output of the layer acting on the same input as earlier is Output =  yu1 + zu2, xu2 + yu3 + zu4  Note that the size of the output has reduced by a factor of 2.",
"In other words, increasing the stride has allowed us to downsample the input.",
The question we want to ask is whether we can perform an upsampling in a similar way?,
This can indeed be done by transposing every operation in a convolution layer.,
"53 • Instead of using a dot-product (inner-product), we will use an outer-product.",
• Instead of skipping pixels in the input (stride) we will skip pixels in the output.,
"• Instead of padding, we will need to crop the output.",
Let us now see an example of a transpose convolution layer.,
"Consider a 1D input image of size 2 × 1 Input =  u1, u2  and a kernel of size 3 × 1 k =  x, y, z  .",
"if we perform the outer-product of the input with k, we will get Outer product = u1x u1y u1z u2x u2y u2z  .",
"If we use a stride of 2, we will need to shift the rows of the outer-product by 2 Striding = u1x u1y u1z 0 0 0 0 u2x u2y u2z  .",
"After striding is performed we need to add the entries in each column and crop the vector to get the output Output = Crop(  u1x, u1y, u1z + u2x, u2y, u2z  ) =  u1x, u1y, u1z + u2x, u2y  where we have cropped out the last few elements (by convention) to get an output which has 2 times the size of the input.",
We consider transpose convolution in 2D applied on a 2D image of size (2 × 2).,
The kernel is taken to be of shape (3 × 3) with stride 2 and padding (cropping).,
"The action of this transpose convolution is shown in Figure 5.8(a), where we ﬁrst obtain an image of size (5 × 5) which is then cropped to give an output of size (4 × 4).",
"Note that the output pixels get an unequal contribution from the various patches (indicated by numbers in the Figure), which leads to checker-boarding, which is undesirable.",
Checker-boarding refers to pixel-to-pixel variations in the values of the output image.,
"One way to avoid checker-boarding, is by ensuring that the ﬁlter size is an integer multiple of the stride.",
Let us repeat the previous example but with a (2 × 2) kernel.,
"The operation is illustrated in Figure 5.8(b)In this case, we do not need to pad (crop) and each output pixel has an equal contribution.",
We make some remarks: 1.,
"Transpose convolution layers are also called fractionally-strided layers, because for every one step in the input, we take greater than one step in the output.",
This is the opposite of what happens in a convolution layer.,
"In a convolution layer, we take step greater than one in the input image, for step of uint one in the output image.",
Transpose convolutions are a tool of choice for upscaling through learnable parameters.,
"Upscaling is typically done with a reduction in the number of channels, which is once again the opposite of what is done in convolution layers.",
"54 (a) kernel size (3 × 3), checker-boarding (b) kernel size (2 × 2), no checker-boarding Figure 5.8: Example of a transpose convolution operation.",
The cells marked with red X’s in the output are cropped out.,
The numbers denote the number of patches that contributed to the value in the output pixel.,
55 5.8 Image-to-image transformations Image-to-image to transformations can be seen analogous to function-to-function transformations.,
"These types of networks are typically used in computer vision, super-resolution, style transfer, and also in computational physics where we (say) map the source (RHS) ﬁeld to the solution of the PDE.",
"We will discuss a particular type of network for such transformations, which is known as U-Nets [26].",
"In a U-Net (see Figure 5.9), there a down is a downward branch which takes an input image and downscales the images using a number of convolution layers and pooling operations.",
"As we go down this branch, the number of channels typically increase.",
"After we reach the coarsest level, we have an upward branch that scales up the image and reduced the number of channels using transpose convolution type operations, to ﬁnally give the output image.",
"In addition to these branches, the U-Net also makes use of skip connections that combines information at a particular scale in the downward branch to the information in the upward branch at the same scale.",
These connections are similar to what are used in ResNets.,
"In the upscaling branch of the U-net, if you consider the activation at one point, you will see they come from two diﬀerent sources.",
"One of these is the from the same spatial scale in the down-scaling branch of the U-net, and the other is from the coarser scales of the upscaling branch of the U-net.",
Remark 5.8.1.,
The U-net architecture shares many common features with the V-cycle that is typically used in multigrid preconditioners.,
Remark 5.8.2.,
We can also think of a the U-net as an encode-decoder network with the additional feature of including skip connections.,
Figure 5.9: Example of a U-Net taken from [26].,
56 Chapter 6 Operator Networks 6.1 The problem with PINNs Recall that a typical MLP y = F(x; θ) is a function that takes as input x ∈Rd and gives an output y ∈Rd with trainable weights θ.,
"Also, as we discussed in Chapter 4, a PINN is a network of the form u(x) = F(x; θ) taking as input the independent variable x of the underlying PDE and giving the solution u(x) (of the PDE) as output.",
The network is trained by minimizing the weighted sum of the PDE and boundary residual.,
"However, this is just one instance of the solution of the PDE for some given boundary condition and source term.",
"For instance, if we consider the PDE ∇· (κ∇u) = f(x), x ∈Ω= [0, 1] × [0, 1] u(x) = g(x), x ∈∂Ω (6.1) and train a PINN F(x; θ) to minimize the loss function Π(θ) = 1 Nv Nv X i=1 |∇· (κ∇F(xi; θ)) −f(xi)|2 + λb Nb Nb X i=1 |F(xi; θ) −g(xi)|2 Then, if θ∗= arg min θ Π(θ), the PINN solving (6.1) will be u(x) = F(x; θ∗).",
"However, if we change f or g in (6.1), we have no reason to believe that the same trained network would work.",
"In fact, we would need to retrain the network (with perhaps the same architecture) for the new f and g. This can be quite cumbersome to do, and we would ideally like to avoid it.",
"In this chapter, we will see ways by which we can overcome this issue.",
6.2 Parametrized PDEs Assume the the source term f in (6.1) is given as a parametric function f(x; α).,
"For instance, we could have f(x1, x2; α) = 4αx1(1 −x1)x2(1 −x2) Then we could train a PINN that accommodates for the parametrization by considering a network that takes as input both x and α, i.e., F(x, α; θ).",
"This is shown in Figure 6.1 This network can be trained by minimizing the loss function Π(θ) = 1 Na Na X j=1 "" 1 Nv Nv X i=1 |∇· (κ∇F(xi, αj; θ)) −f(xi, αj)|2 + λb Nb Nb X i=1 |F(xi, αj; θ) −g(xi)|2 # 57 Note that we have to also consider collocation points for the parameter α while constructing the loss function.",
"If θ∗= arg min θ Π(θ), then the solution to the parameterized PDE would be u(x, α) = F(x, α; θ∗).",
"Further, for any new value of α = ˆα we could ﬁnd the solution by evaluating F(x, ˆα; θ∗).",
We could use the same approach if there was a way of parameterizing the functions κ(x) and g(x).,
"𝑥"" 𝛼 ℱ 𝑢 Figure 6.1: Schematic of a PINN with a parameterized input.",
"However, what if we wanted the solution for an arbitrary, non-parametric f?",
"In order to do this, we need to ﬁnd a way to approximate operators that map functions to functions.",
6.3 Operators Consider a class of functions a(y) ∈A such that a : ΩY →RD.,
"The functions in this class might have certain properties, such as a ∈C(ΩY ) or a ∈L2(ΩY ).",
"Also consider the operator N : A 7→C(ΩX), with u(x) = N(a)(x) for x ∈ΩX.",
Let us see some examples of operators N. 1.,
"Consider the PDE ∇· (κ∇u) = f, x ∈Ω u = g, x ∈∂Ω (6.2) For this PDE, ΩX = ΩY = Ωand the operator N maps the RHS f to the solution (temperature) u of the PDE.",
"That is, u = N(f)(x).",
The input and the output to the operator are related by the equation above where it is assumed that κ and g are given and ﬁxed.,
"Consider the PDE ∇· (κ∇u) = f, x ∈Ω u = g, x ∈∂Ω (6.3) which is the same as the previous PDE but we are assuming that the conductivity ﬁeld κ might change for the model, instead of the RHS.",
"Then, ΩX = ΩY = Ωand the operator N maps the conductivity κ to the solution u of the PDE.",
"That is, u = N(κ)(x).",
The input and the output to the operator are related by the equation above where it is assumed that f and g are given and ﬁxed.,
"Once again, consider the same PDE but with conductivity and the boundary condition being allowed to change ∇· (κ∇u) = f(x), x ∈Ω u(x) = g(x), x ∈∂Ω (6.4) Then, the operator N maps the boundary condition g and the conductivity κ to the solution u of the PDE.",
"That is, u = N(κ, g)(x).",
"In this case the input to the operator are two functions (g, κ) and the output is a single function.",
"Therefore ΩX = Ω, while ΩY = Ω× ∂Ω.",
The input and the output are related through the solution to the PDE above where it is assumed that f is given and ﬁxed.,
"Now consider the equations of linear isotropic elasticity posed on a three-dimensional domain Ω⊂R3, ∇·  λ(∇· u)I + 2µ∇S(u)  = f(x), x ∈Ω u(x) = g(x), x ∈∂Ω.",
(6.5) Consider the operator deﬁned by u(x) = N(f)(x).,
"Here the input function, f : Ω→R3, and the output function u : Ω→R3.",
"The two are related by the equations above where λ, µ, g are given and ﬁxed.",
"Now, consider a diﬀerent PDE.",
"In particular, the advection-diﬀusion-reaction equation, ∂u ∂t + a · ∇u −κ∇2u + u(1 −u) = f, (x, t) ∈Ω× (0, T] u(x, t) = g(x, t), (x, t) ∈∂Ω× (0, T] u(x, 0) = u0(x), x ∈Ω.",
"(6.6) We want to ﬁnd the operator N maps the initial condition u0 to the solution u at the ﬁnal time T, i.e., u(x, T) = N(u0)(x).",
In this case ΩX = ΩY = Ω.,
"Further the input and the output functions are related to each other via the solution of the PDE above with a, κ, f, g given and ﬁxed.",
Remark 6.3.1.,
It is often useful to determine whether an operator is linear or non-linear.,
This is because if it is linear it can be well approximated by another linear operator.,
"In the cases considered above the operators in examples 1 and 4 were linear whereas those in examples 2,3, and 5 were nonlinear.",
We are interested in networks that approximate the operator N. We will see how we can do this in the next section.,
These types of networks are often referred to as Operator Networks.,
They are two popular versions of these networks.,
"One is referred to as a Deep Operator Network, or a DeepONet, and the other is referred to as a Fourier Neural Operator.",
We describe the DeepONet in the next section.,
"6.4 Deep Operator Network (DeepONet) Architecture Operator networks were ﬁrst proposed by Chen and Chen [5], where they considered only shallow networks with a single hidden layer.",
This idea was rediscovered and extended to deep architectures more recently in [14] and were called DeepONets.,
A standard DeepONet comprises two neural networks.,
"We describe below its construction to approximate an operator N : A →U, where A is a set of functions of the form a : ΩY ⊂Rd →R while U consists of functions of the form u : ΩX ⊂RD →R.",
"Furthermore, we assume that point-wise evaluations of both class of functions is possible.",
The architecture for the DeepONet for this operator is illustrated in Figure 6.2.,
"It is explained below: 59 • Fix M distinct sensor points y(1), ..., y(M) in ΩY .",
"• Sample a function a ∈A at these sensor points to get the vector a = [a(y(1)), ..., a(y(M))]⊤∈ RM.",
"• Supply a as the input to a sub-network, called the branch net B(.",
"; θB) : RM →Rp, whose output would be the vector β = [β1(a), ..., βp(a)]⊤∈Rp.",
Here θB are the trainable parameters of the branch net.,
"The dimension of the output of the branch is relatively small, say p ≈100.",
"• Supply x as an input to a second sub-network, called the trunk net T (.",
"; θT ) : RD →Rp, whose output would be the vector τ = [τ1(x), ..., τp(x)]⊤∈Rp.",
Here θT are the trainable parameters of the trunk net.,
"• Take a dot product of the outputs of the branch and trunk nets to get the ﬁnal output of the DeepONet e N(., .",
"; θ) : RD × RM →R which will approximate the value of u(x) u(x) ≈e N(x, a; θ) = p X k=1 βk(a)τk(x).",
"(6.7) where the trainable parameters of the DeepONet will be the combined parameters of the branch and trunk nets, i.e., θ = [θT , θM].",
"Figure 6.2: Schematic of a DeepONet In the above construction, once the DeepONet is trained (we will discuss the training in the following section), it will approximate the underlying operator N, and allow us to approximate the value of any N(a)(x) for any a ∈A and any x ∈ΩX.",
"Note that in the construction of the DeepONet, the M sensor points need to be pre-deﬁned and cannot change during the training and evaluation phases.",
We can make the following observations regarding the DeepONet architecture: 1.,
The expression in (6.7) has the form of representing the solution as the sum of a series of coeﬃcients and functions.,
"The coeﬃcients are determined by the branch network, while the 60 functions are determined by the trunk network.",
In that sense the DeepONet construction is similar to that of what is used in the spectral method or the ﬁnite element method.,
There is a critical diﬀerence though.,
"In these methods, the basis functions are pre-determined and selected by the user.",
"However, in the DeepONet these functions are determined by the trunk network and their ﬁnal form depends on the data used to train the DeepONet.",
"Architecture of the branch sub-network: When points for sampling the input function are chosen randomly, the appropriate architecture for the branch network comprises fully connected layers.",
"Further recognizing that the dimension of the input to this network can be rather large N1 ≈104, while the output is typically small p ≈102, this network can be thought of as an encoder.",
"When points for sampling the input function are chosen on a uniform grid, the appropriate architecture for the branch network comprises convolutional layer layers.",
"In that case this network maps an image of large dimension (N1 ≈104) to a latent vector of small dimension, p ≈102.",
Thus it is best represented by a convolutional neural network.,
"Broadly speaking, there are two ways of improving the experssivity of the DeepONet.",
"These involve increase the number of network parameters in the branch and trunk sub-networks, and increasing the dimension p of the latent vectors formed by these sub-networks.",
"6.5 Training DeepONets Training a DeepONet is typically supervised, and requires pairwise data.",
The following are the main steps involved: 1.,
"Select N1 representative function a(i), 1 ≤i ≤N1 from the set A.",
"Evaluate the values of these functions at the M sensor points, i.e., a(i) j = a(i)(y(j)) for 1 ≤j ≤M.",
"This gives us the vectors a(i) = [a(i)(y(1)), ..., a(i)(y(M))]⊤∈RM for each 1 ≤i ≤N1.",
"For each a(i), determine (numerically or analytically) the corresponding functions u(i) given by the operator N. 3.",
"Sample the function u(i) at N2 points in ΩX, i.e., u(i)(x(k)) for 1 ≤k ≤N2.",
"Construct the training set S = n a(i), x(k), u(i)(x(k))  : 1 ≤i ≤N1, 1 ≤k ≤N2 o which will have N1 × N2 samples.",
"Deﬁne the loss function Π(θ) = 1 N1N2 N1 X i=1 N2 X k=1 | e N(x(k), a(i); θ) −u(i)(x(k))|2.",
Training the DeepONet corresponds to ﬁnding θ∗= arg min θ Π(θ).,
"Once trained, then given any new a ∈A samples at the M sensor points (which gives the vector a ∈RM), and a new point x ∈ΩX, we can evaluate the corresponding prediction u∗(x) = e N(x, a; θ∗).",
61 Remark 6.5.1.,
We need not choose the same N2 points across all i in the training set.,
"In fact, these can be chosen randomly leading to a more diverse dataset.",
Remark 6.5.2.,
The DeepONet can be easily extended to the case where the input comprises multiple functions.,
"In this case, the trunk network remains the same, however the branch network now has multiple vectors as input.",
"The case corresponding to two input functions, a(y) and b(y), which when sampled yield the vectors, a and b, is shown in Figure 6.3. 𝒂 𝒃 𝒙 Branch 𝜷 Trunk 𝝉 𝒖 Dot Product Figure 6.3: Schematic of a DeepONet with two input functions.",
Remark 6.5.3.,
The DeepONet can be easily extended to the case where the output comprises multiple functions (say D such functions).,
"In this case, the output of the branch and trunk network leads to D vectors each with dimension p. The solution is then obtained by taking the dot product of each one of these vectors.",
The case corresponding to two output functions u1(x) and u2(x) is shown in Figure 6.3. 𝒂 𝒙 Branch 𝜷𝟐 Trunk 𝝉𝟏 𝒖𝟏 Dot Product 𝜷𝟏 𝝉𝟐 𝒖𝟐 Dot Product Figure 6.4: Schematic of a DeepONet with two output functions.,
6.6 Error Analysis for DeepONets There is a universal approximation theorem for a shallow version of DeepONets by Chen and Chen [5] 62 Theorem 6.6.1.,
"Suppose ΩX and ΩY are compact sets in RD (or more generally a Banach space) and Rd, respectively.",
"Let N be a nonlinear, continuous operator mapping V ⊂C(ΩY ) into C(ΩX).",
"Then given ϵ > 0, there exists a DeepONet with M sensors and a single hidden layer of width P in the branch and trunk nets such that max x∈ΩX a∈A | e N(x, a; θ) −N(a)(x)| < ϵ for a large enough P and M. This result has been extended to a deeper version of the network in [14], and generalized further by removing the compactness assumptions on the spaces [12].",
"Recently in [20], the authors have developed an estimate of the error in a DeepONet that clearly pinpoints the diﬀerent sources of error in a DeepONet.",
"This estimate states, the error measured in the L2(Ω) norm is bounded as max a∈A ∥e N(x, a; θ) −N(a)(x)∥L2(Ω) ≤C  ϵh + √ϵt + ϵs + M−α1 + (N2)−α2 (6.8) where ϵh is the error made by the numerical solver used to generate the approximate target solutions u(i) in the training set (as compared to the exact solutions), ϵt is the ﬁnal training error/loss attained, while ϵs bounds the distance of any a ∈A from the set of functions {a(i)}N1 i=1 used to construct the construct the training set, i.e., an estimate of how well the training samples covers the input space A.",
"Further, since the input function is evaluated at M ﬁnite sensor nodes, while the output is evaluated at N2 output nodes, this will lead to an additional discretization (or quadrature) error which is given by the last two terms in (6.8).",
Note that this is similar to the error estimates obtained for PINNs in (4.21).,
"6.7 Physics-Informed DeepONets Recall that DeepONets approximates u(x) = N(a)(x) ≈e N(x, a; θ).",
Assume that the pair a and u satisfy a PDE.,
"For example, ∇· (κu) = f in Ω u = g on ∂Ω (6.9) where κ and g are prescribed.",
"To construct the operator N that maps f to u, we need to solve the PDE externally.",
"However, in addition to this, we can also use a PINN-type loss function and add that to the total loss.",
This is the idea of Physic-Informed DeepONets proposed in [30].,
"So for the above model PDE, the loss additional physics-based loss would would be, Πp(θ) = 1 ¯N1 1 ¯N2 ¯ N1 X i=1 ¯ N2 X k=1 ∇x ·  κ∇x e N(x(k), f (i); θ)  −f(i)(x(k)) 2 .",
"(6.10) This is in addition to the standard data-driven loss function which, for this example is given by Πd(θ) = 1 N1N2 N1 X i=1 N2 X k=1 | e N(x(k), f (i); θ) −u(i)(x(k))|2.",
"(6.11) The total loss function is a weighted sum of these two terms: Π(θ) = Πd(θ) + λΠp(θ), (6.12) where λ is a hyper-parameter.",
A few comments are in order: 63 1.,
The output sensor points used in the physics-based loss function are usually distinct from the output sensor points used in the data-driven loss term.,
"The former represent the locations at which we wish to minimize the residual of the PDE, while the latter represent the points at which the solution is available to us through external means.",
"The total number of the output sensor points in the physics-based portion of the loss function is denoted by ¯N2, whereas in the data-driven loss function it is denoted by N2.",
The set of input functions used to construct the physics-based loss function is usually distinct from the set of input functions used to construct the data-driven loss function.,
"The former set represents the functions for which we wish to minimize the residual of the PDE, while the latter set represents the collection of input functions for which the solution is available to us through external means.",
"The total number of functions in the set used to construct the the physics-based portion of the loss function is denoted by ¯N1, whereas the total number of functions in the set used to construct the data-driven portion of the loss function is denoted by N −1.",
"As earlier, we train the network by ﬁnding θ∗= arg min θ Π(θ) and approximate the solution for a new a by u∗(x) = e N(x, a; θ∗).",
The advantages of adding the extra physics-based loss are: 1.,
It reduces the demand on the amount of data in the data-driven loss term.,
What is means is that we don’t have to generate as many solutions of the PDE for training the DeepONet.,
It makes the network more robust in that it becomes more likely to produce accurate solutions for the type of input functions not included in the training set for the data-driven loss term.,
6.8 Fourier Neural Operators - Architecture We now introduce and discuss Fourier Nerual Operators (FNOs) [13].,
"We discuss their architecture in this section, and then discuss other aspects in the following section.",
"Our approach in developing the architecture for a FNO will be to begin with the architecture of a typical feed-forward MLP that maps a scalar to another scalar, and systematically extend it so that the extended version maps a scalar valued function to another scalar valued function.",
A(1) s A(l+1) s s 𝒙(𝟎) 𝝃(𝟏) 𝒙(𝟏) 𝒙(𝒍) 𝒙(𝒍&𝟏) 𝒙(𝑳&𝟏) 𝝃(𝒍) 𝝃(𝒍&𝟏) 𝝃(𝑳&𝟏) s Figure 6.5: Computational graph for a feed-forward MLP.,
In Figure 6.5 we have plotted the computational graph of an MLP.,
We are focused only on the forward part (not the back-propagation) part of this network.,
"For simplcity, we assume that the input x(0) = x is a scalar and the output x(L) = y is also a scalar.",
Further all the other hidden variables (with the exception of ξ(L+1)) are vectors with H components.,
"That is, the width of each layer is H. The ﬁrst step in this process will be to replace the input and the output with functions.",
The input will now be the function a(x) : Ω7→R1.,
Similarly the output is the function u(x) : Ω7→R1.,
This leads us to the computational graph shown in Figure 6.6.,
64 A(1) s A(l+1) s s 𝑎(𝒙) 𝒗(𝟏) 𝒖(𝟏) 𝒖(𝒍) 𝒖(𝒍%𝟏) 𝒗(𝒍) 𝒗(𝒍%𝟏) 𝒗(𝑳%𝟏) s 𝑢(𝒙) Figure 6.6: Computational graph for a feed-forward Fourier Neural Operator (FNO) network.,
The next step is consider the variables in the hidden layers.,
"In the MLP, these were all vectors with H components.",
"In the FNO, these will be functions with H components.",
"That is, v(1), · · · , v(L), u(1), · · · , u(L) : Ω7→RH.",
"(6.13) As shown in Figure 6.6, v(n) and u(n) are the counterparts of ξ(n) and x(n), respectively.",
"Further since ξ(L+1) was a scalar, we will set v(L+1) to be a scalar valued function.",
"We are now done with extending the input, the output and the variables in the hidden layers from vectors to functions.",
"Next, we need to extend the operators that transform vectors to vectors within an MLP to those that transform functions to functions within an FNO.",
"We begin with the operator A(1), which in an MLP is an aﬃne map from a vector with one component to a vector with H components.",
"Its straightforward extension to functions is, v(1)(x) = A(1)(a)(x), (6.14) where v(1) i (x) = W (1) i a(x) + b(1) i , i = 1, · · · , H. (6.15) Here W (1) i and b(1) i are the weights and biases associated with this layer.",
"Similarly, in an MLP the operator A(L+1) is an aﬃne map from a vector with H components to a vector with 1 component.",
"It’s straightforward extension to functions is, v(L+1)(x) = A(L+1)(u(L))(x), (6.16) where v(L+1)(x) = W (L+1) i u(L) i (x) + b(L+1), i = 1, · · · , H. (6.17) Here W (L+1) i and b(L+1) are the weights and the bias associated with this layer.",
Next we describe the action of the activation on input functions.,
It is a simple extension of the activation function applied to the point-wise values of the input function.,
"That is, u(n)(x) = σ(v(n))(x), (6.18) where u(n) i (x) = σ(v(n) i (x)), i = 1, · · · , H. (6.19) Finally it remains to extend the operators A(n), n = 2, · · · , L to functions.",
"These are deﬁned as, v(n+1)(x) = A(n+1)(u(n))(x), (6.20) where v(n+1) i (x) = W (n+1) ij u(n) j (x) + b(n+1) i (6.21) + Z Ω κ(n+1) ij (y −x)u(n) j (y)dy, i = 1, · · · , H. (6.22) 65 In the equation above the summation over the dummy index j (from 1 to H) is implied.",
The new term that appears in this equation is a convolution.,
It is motivated by the observation that a large class of linear operators can be represented as convolutions.,
An example is the so-called Green’s operator which maps the right hand side (also called the forcing function) of a linear PDE to its solution.,
The functions κ(n+1) ij (z) are called the kernels of the convolution.,
We note that there are H2 of these functions in each layer.,
It is instructive to examine a speciﬁc case of a convolution.,
"Let us consider Ω= [0, L1]×[0, L2], where we denote the two coordinates by either x1 and x2, or y1 or y2.",
"In this case we may write the convolution as, vi(x1, x2) = Z L1 0 Z L2 0 κij(y1 −x1, y2 −x2)uj(y1, y2) dy2dy1, i = 1, · · · , H. (6.23) In the equation above, we have dropped the superscripts since they are not relevant to the discussion.",
Remark 6.8.1.,
We may interpret the FNO as a sequence of an aﬃne transform and convo- lution followed by a point-wise nonlinear activation.,
This combination of linear and nonlinear (activation) operations allows us to approximate nonlinear operator using this architecture.,
Remark 6.8.2.,
It is instructive to list all the trainable entities in a FNO.,
"First we list all the trainable parameters: W (1) i , W (2) ij , · · · , W (L) ij , W (L+1) i ; b(1) i , b(2) i , · · · , b(L) i , b(L+1).",
"(6.24) Thereafter, all the trainable kernel functions κ(n) ij (z), n = 2, · · · , L. (6.25) The neural operator introduced in this section acts directly on functions and transforms them into functions.",
"However, when implementing this operator on a computer the functions have to be represented discretely.",
This is described in the following section.,
"6.9 Discretization of the Fourier Neural Operator The functions that appear in the neural operator described in the previous section are: a, v(1), u(1), · · · , v(L), u(L), v(L+1), u.",
(6.26) Each of these functions is deﬁned on the domain Ω.,
"We discretize this domain with N uniformly distributed points, and represent each function using its values on these points.",
"As an example, in two dimensions, with Ω= [0, L1] × [0, L2], we represent the function a(x1, x2) as, a[m, n] = a(x1m, x2n), m = 1 · · · , N1, n = 1 · · · , N2.",
(6.27) where x1m = (m −1) × L1 N1 −1 (6.28) x1n = (n −1) × L2 N2 −1.,
(6.29) The same representation will be used for all other functions.,
66 We now have to consider the discrete version of all operations on these functions as well.,
"This is described below for the special case of Ω= [0, L1] × [0, L2].",
We begin with the operator A(1).,
"The discretized version is v(1)[m, n] = A(1)(a)[m, n], (6.30) where v(1) i [m, n] = W (1) i a[m, n] + b(1) i , i = 1, · · · , H. (6.31) Similarly, the discretized version of the operator A(L+1) is, v(L+1)[m, n] = A(L+1)(u(L))[m, n], (6.32) where v(L+1)[m, n] = W (L+1) i u(L) i [m, n] + b(L+1), i = 1, · · · , H. (6.33) Next we describe the action of the activation function on discretized input functions.",
"It is given by u(n)[m, n] = σ(v(n))[m, n], (6.34) where u(n) i [m, n] = σ(v(n) i [m, n]), i = 1, · · · , H. (6.35) Finally it remains to develop the discrete version of the operators A(n), n = 2, · · · , L. These are deﬁned as, v(p+1)[m, n] = A(p+1)(u(p))[m, n], (6.36) where v(p+1) i [m, n] = W (p+1) ij u(p) j [m, n] + b(p+1) i (6.37) + N1 X r=1 N2 X s=1 κ(p+1) ij [r −m, s −n]u(p) j [r, s]h1h2, i = 1, · · · , H, (6.38) where h1 = L1 N1−1 and h2 = L2 N2−1.",
Note that the integral in the convolution is now replaced by a sum over all the grid points.,
"Computing this integral for each value of i and m, n involves O(N1N2H) ﬂops.",
"And since this needs to be done for H diﬀerent values of i, N1 values of M, and N2 values of j, the total cost of discretizing the convolution operation is O(N2 1 N2 2 H) = O(N2H2), where N = N1 × N2.",
The factor of N2 in this cost is not acceptable and makes the implementation of this algorithm impractical.,
In the following section we describe how the use of Fourier Transforms (forward and inverse) overcomes this bottleneck and leads to a practical algorithm.,
"This is also the reason that this algorithm is referred to as a “Fourier Neural Operator.""",
"6.10 The Use of Fourier Transforms Consider a periodic function u(x2, x2) deﬁne on Ω≡[0, L1]×[0, L2].",
"If this function is suﬃciently smooth it may be approximated by a truncated Fourier series, u(x1, x2) ≈ N1/2 X m=−N1/2 N2/2 X n=−N2/2 ˆu[m, n]e2πi( mx1 L1 + nx2 L2 ).",
"(6.39) Here N1 and N2 are even integers, the coeﬃcients ˆu[m, n] are the Fourier coeﬃcients and i = √−1.",
We note that while the function u is real-valued the coeﬃcients are complex-valued.,
"However, 67 since u is real-valued, they obey the rule ˆu[−m, −n] = ˆu∗[m, n], where (.",
)∗denotes the complex- conjugate of a complex number.,
"The approximation can be made more accurate by increasing N1 and N2, and as these numbers tend to inﬁnity, we recover the equality.",
"The relation above is often referred to as the inverse Fourier transform, since it maps the Fourier coeﬃcients to the function in the physical space.",
The forward Fourier transform (which maps the function in the physical space to the Fourier coeﬃcients) can be obtained from the relation above by 1.,
"Multiplying both sides by e−2πi( rx1 L1 + sx2 L2 ), where r and s are integers.",
Integrating both sides over Ω.,
"Recognizing that the integral R Ωe2πi( (m−r)x1 L1 + (n−s)x2 L2 )dx1dx2 is non-zero only when m = r and n = s, and in that case it evaluates to L1L2.",
"These steps yield the ﬁnal relation: ˆu[r, s] = 1 L1L2 Z L1 0 Z L2 0 u(x1, x2)e−2πi( rx1 L1 + sx2 L2 )dx1dx2.",
(6.40) We now describe how Fourier transforms can be used to evaluate the convolution eﬃciently.,
To do this we consider the special case of 2D convolution in (6.23).,
"We begin with substituting uj(y1, y2) = PN1/2 m=−N1/2 PN2/2 n=−N2/2 ˆuj[m, n]e2πi( my1 L1 + ny2 L2 ) in this equation to get, vi(x1, x2) = Z L1 0 Z L2 0 κij(y1 −x1, y2 −x2) X m,n ˆuj[m, n]e2πi( my1 L1 + ny2 L2 ) dy2dy1 = X m,n ˆuj[m, n] Z L1 0 Z L2 0 κij(y1 −x1, y2 −x2)e2πi( my1 L1 + ny2 L2 ) dy2dy1 = X m,n ˆuj[m, n] Z L1−x1 −x1 Z L2−x2 −x2 κij(z1, z2)e2πi( m(z1+x1) L1 + n(z2+x2) L2 ) dz2dz1 = X m,n ˆuj[m, n]e2πi( mx1 L1 + nx2 L2 ) Z L1 0 Z L2 0 κij(z1, z2)e2πi( mz1 L1 + nz2 L2 ) dz2dz1 = L1L2 X m,n ˆuj[m, n]ˆκij[−m, −n]e2πi( mx1 L1 + nx2 L2 ).",
"(6.41) In the development above, in going from the ﬁrst to the second line we have taken the summation outside the integral and recognized that the coeﬃcients ˆuj[m, n] do not depend on y1 and y2.",
In going from the second to the third line we have introduced the variables z1 = y1 −x1 and z2 = y2 −x2.,
"In going from the third to the fourth line we have made use of the fact that the functions κij(z1, z2) are periodic.",
Finally in going from the fourth to the ﬁfth line we have made use of the deﬁnition of the Fourier Transform (6.40).,
This ﬁnal relation tells us that the convolution can be computed by: 1.,
Computing the Fourier Transform of uj.,
Computing the Fourier Transform of κij.,
Computing the product of the coeﬃcients of these two transforms.,
Computing the inverse Fourier Transform of the product.,
"68 Next, we account for the fact that we will only work with the discrete forms of the functions uj and κij.",
This means that we evaluate the inverse Fourier transform (6.39) at a ﬁnite set of grid points.,
"Further, it means that we have to approximate the integral in the Fourier transform (6.40).",
"This alternate form is given by ˆu[r, s] = h1h2 L1L2 N1 X m=1 N2 X n=1 u[m, n]e−2πi( rx1m L1 + sx2n L2 ).",
"(6.42) Here h1 = L1 N1 and h2 = L2 N2 , x1m = (m −1)h1 and x2n = (n −1)h2.",
The ﬁnal observation is that the evaluating the sums in (6.39) and (6.42) require O(N2) operations.,
This would make the evaluation of the convolution via the Fourier method impractical except for when N is very small.,
"However, the use of Fast Fourier Transform (FFT) reduces this cost to O(N log N).",
Thus the cost of implementing the convolution reduces to O(N log NH2).,
This makes the implementation of Fourier Neural Operators practical.,
"69 Chapter 7 Probabilistic Deep Learning So far, we have considered regression and classiﬁcation problems, where for a given input x we need to compute a single output y.",
"However, this may not be enough for many problems of interest.",
"In fact, there may be many y’s for a given x.",
"For example, 1. y and x might be measured with some random noise.",
2. y and x might be inherently stochastic.,
"For instance, y could be the pressure measured in a turbulent ﬂow at some point x in space.",
3. inverse problems can have multiple solutions.,
"For instance, the forward/direct problems would be determining the temperature ﬁeld given the head conductivity, while the inverse problem could be determining the conductivity ﬁeld given the (possibly noisy) temperature ﬁeld.",
"Thus, we need to formulate a probabilistic framework to use deep learning algorithms to solve such problems.",
"Recall, that our deterministic model was given by y = F(x; θ).",
"In the probabilistic setup, y, x and θ are treated as random variables.",
Before we can work with random variables we need to understand some key elements of the theory of probability that are necessary in deﬁning random variables.,
7.1 Key elements of Probability Theory A random experiment is described by a procedure and a set of one or more observa- tions/measurements.,
"For example, 1.",
Observe a switch and determine whether it is on or oﬀ.,
Toss a coin 3 times and note the sequence of heads H or tails T. 3.,
Toss a coin 3 times and count the number of times H appears.,
Note that this is the same experiment as earlier but the measurement is diﬀerent.,
"Spin a spinner, and measure the ﬁnal angle in radians.",
The outcome is the results of the experiment that cannot be broken down into smaller parts.,
"The sample space, denoted by S, is the set of all possible outcomes of an experiment.",
"For each of the four random experiments observed above, we have 1.",
"S = {on, oﬀ}.",
"S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH}.",
"S = {0, 1, 2, 3}.",
"S = {ψ : ψ ∈(0, 2π]}.",
"Note that there is a fundamental diﬀerence between the ﬁrst 3 experiments, where S is discrete and countable, and the last experiment where the S is uncountable.",
"An event is a collection of outcomes, i.e., a subset of S. Typically the outcomes in an event satisfy a condition.",
Let’s see some examples for the above experiments 1.,
"A = {on} or A = {on, oﬀ} = S .",
"A are all outcomes with at least 2 H, i.e., A = {THH, HTH, HHT, HHH}.",
"A are all outcomes with at least 2 H, i.e., A = {2, 3}.",
"If we deﬁne B to be all outcomes with 4 H, then no outcome would satisfy this condition, i.e., B = ∅the null set.",
"A are all outcomes with ψ > π/4, i.e, A = {ψ : ψ ∈(π/4, 2π]}.",
An event class E is a collection of all event (sets) over which probabilities can be deﬁned.,
"When S is countable, E is all subsets of S. When S is not countable, E is the Borel ﬁeld (or Borel algebra), which is the collection of all open and closed sets in S. The probability law is a rule that assigns a probability to all sets in an event class E .",
"We list the axioms of probability, which are the requirements of a probability law.",
Consider a sample space S for an experiment and the corresponding event class E .,
"Let P : E 7→[0, 1] satisfy 1.",
P[A] ≥0 for all A ∈E .,
"If A1, A2, ... are events such that Ai ∩Aj = ∅for all i ̸= j, i.e., the events are mutually exclusive, then P[ ∞ [ i=1 Ai] = ∞ X i=1 P[Ai].",
Any assignment P that satisﬁes the above conditions is said to be a valid probability law.,
Note that probability is like mass.,
"It is non-negative (axiom 1), conserved (total mass is always 1, axiom 2), and for distinct points the total mass is obtained by adding individual masses (axiom 3).",
"If S is countable, then it is suﬃcient to deﬁne a probability law for all elements of S, i.e., for all elementary outcomes, while making sure that the probabilities are non-negative and add up to 1 (the ﬁrst two axioms).",
Let us try to assign probability laws for the ﬁrst three examples which have a countable S using these criteria.,
"For some p ∈[0, 1], deﬁne P[on] = p; P[oﬀ] = 1 −p.",
"For a fair die with no memory, P[ai] = 1/8, where ai ∈S, i = 1, · · · , 8.",
"For a fair die with no memory, P[0] = 1/8, P[1] = 3/8, P[2] = 3/8, P[3] = 1/8.",
Remark 7.1.1.,
"As an exercise, verify that the axioms are satisﬁed for each of these cases.",
"71 For a continuous sample space, it is suﬃcient to deﬁne a probability law for all open and closed intervals, while ensuring axioms 1 and 2.",
"Let us consider the fourth example which has an uncountable S. If the spinner is completely unbiased, then the probability is uniformly distributed.",
"Then for b ≥a, we say that P[(a, b]] = (b −a)/(2π).",
Note that the probability of singleton sets in a continuous sample space is zero (for any distribution).,
Remark 7.1.2.,
"From this point on, whenever we talk about the sample space S, we will im- plicitly assume that we are referring to the triplet (S, E , P).",
"This triplet is also known as a ""measure space"".",
7.2 Random Variables A random variable X is a function deﬁned from S to the real line with the property that the set Ab = {ξ ∈S : X(ξ) ≤b} belongs to E for all b ∈R.,
"Note that in the measure theoretic language, we are requiring X to be a measurable function.",
"Also note that according to the deﬁnition of Ab, we are enforcing the requirement that we should be able to evaluate P[Ab] for all b ∈R.",
Let us deﬁne random variables (RVs) for the above examples: 1.,
"For S = {on, oﬀ} with P[on] = p, P[oﬀ] = 1 −p, deﬁne the RV X(ξ) = ( 0 if ξ = oﬀ 1 if ξ = on.",
(7.1) This is also known as a Bernoulli Random Variable.,
"For S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH} with P[ai] = 1/8 for all ai ∈S, deﬁne X(ξ) = Number of heads in ξ.",
(7.2) Note that this is the random event that was described in Experiment 3.,
This random event is already a random variable.,
"For the spinner experiment with S = {ψ : ψ ∈(0, 2π]} with P[(a, b]] = (b−a)/(2π), deﬁne X(ψ) = ψ 2π.",
"(7.3) If X is deﬁned on a discrete sample space, it is called a discrete random variable, while if it is deﬁned on a continuous sample space, it is called a continuous random variable.",
"As described above, a random variable inherits its probabilistic interpretation from the measure space used to deﬁne it.",
In the following sections we deﬁne the probabilistic interpretation of a random variable.,
"7.2.1 Cumulative distribution function The cumulative distribution function (cdf) of a random variable X is given by FX(x) = P[ξ : X(ξ) ≤x] which deﬁnes a probability on R of X taking values in the interval (−∞, x].",
Let us deﬁne the cdf for the above examples: 1.,
"For the Bernoulli RV deﬁned by (7.1) 72 • if x < 0, then FX(x) = P[∅] = 0 • if 0 ≤x < 1, then FX(x) = P[{oﬀ}] = 1 −p • if x ≥1, then FX(x) = P[{on, oﬀ}] = 1 The full cdf is shown in Figure 7.1(a).",
"For the RV deﬁned by (7.2) • if x < 0, then FX(x) = P[∅] = 0 • if 0 ≤x < 1, then FX(x) = P[#ofH = 0] = P[{TTT}] = 1/8 • if 1 ≤x < 2, then FX(x) = P[#ofH = 0, 1] = 1/8 + 3/8 = 4/8 • if 3 ≤x < 3, then FX(x) = P[#ofH = 0, 1, 2] = 1/8 + 3/8 + 3/8 = 7/8 • if x ≥3, the FX(x) = P[#ofH = 0, 1, 2, 3] = 1 The full cdf is shown in Figure 7.1(b).",
This random variable is the same as Example 2.,
"For the spinner experiment with the RV deﬁned by (7.3) FX(x) = P[ψ : X(ψ) ≤x] = P[{ψ : ψ ≤2πx}] • if x < 0, then FX(x) = P[∅] = 0 • if 0 ≤x < 1, then FX(x) = P[{ψ ∈(0, 2πx]}] = 2πx 2π = x • if x ≥1, then FX(x) = P[{ψ ≤2π}] = 1 The full cdf is shown in Figure 7.1(c).",
(a) Bernoulli (b) 3 coin tosses (c) Spinner Figure 7.1: Examples of cumulative distribution functions Let us discuss some properties of FX: 1.,
0 ≤FX(x) ≤1.,
2. limx→∞FX(x) = 1.,
3. limx→−∞FX(x) = 0.,
FX is monotonically increasing.,
The cdf is always continuous from the right FX(x) = lim h→0+ Fx(x + h).,
Note that the FX for discrete RV (see Figure 7.1) are discontinuous at ﬁnitely many x.,
"In fact, the cdf for discrete RVs can be written as a ﬁnite sum of the form FX(x) = K X k=1 pkH(x −xk), K X k=1 pk = 1, where pk is the probability mass and H is the Heaviside function H(x) = ( 1 if x > 0 0 if x ≤0 .",
Remark 7.2.1.,
"Once we have the FX we can calculate the probability that X will take values in ""any"" interval in R, i.e., we can compute P[a < X ≤b].",
Note that FX(b) = P[X ≤b] = P[(X ≤a) ∪(a < X ≤b)] = P[X ≤a] + P[a < X ≤b] (mutually exclusive events) = FX(a) + P[a < X ≤b].,
"Thus, P[a < X ≤b] = FX(b) −FX(a).",
7.2.2 Probability density function We deﬁne the probability density function (pdf).,
"For a continuous FX, it is deﬁned as fX(x) = d dxFX(x) (7.4) which enjoys the following properties inherited from the cdf: 1. fX(x) ≥0, ∀x ∈R, since FX is monotonically increasing.",
2. limx→−∞fX(x) = limx→∞fX(x) = 0.,
"Integrating (7.4) from (−∞, x] gives us Z x −∞ fX(y)dy = FX(x) − lim x→−∞FX(x) = FX(x).",
Also P[a < X ≤b] = FX(b) −FX(a) = Z b −∞ fX(y)dy − Z a −∞ fX(y)dy = Z b a fX(y)dy.,
"Thus, the integral of a pdf in an interval gives the ""probability mass"" which is the probability that the RV lies in that interval.",
"This is the reason why the pdf is called a ""density"".",
"Furthermore, Z ∞ −∞ fX(y)dy = lim x→∞FX(x) − lim x→−∞FX(x) = 1.",
"For a very small h > 0, we have the interpretation P[a < X ≤a + h] = Z a+h a fX(y)dy ≈hfX(a).",
"Note that as h →0+, P[a < X ≤a + h] →0.",
"That is, for a continuous RV the probability of attaining a single value is zero.",
"7.2.3 Examples of Important RVs Let us look at some important random variables and the associated cdf, pdf (also see Figure 7.2): 1.",
"Uniform RV: for some interval (a, b], the pdf is given by fX(x) = ( 1 b−a if x ∈(a, b] 0 other wise , while the cdf is given by FX(x) =      0 if x < a x−a b−a if x ∈(a, b] 1 if x > b .",
Exponential RV: used to model lifetime of devices/humans after a critical event.,
"In this case, X represents the time to failure and P[X > x] = e−λx where λ > 0 is a model parameter which denotes the rate of failure.",
"Thus, FX(x) = P[X ≤x] = 1 −P[X > x] = 1 −e−λx, and fX(x) = d dxFX(x) = λe−λx.",
"Gaussian RV: used to model natural things like height, weight, etc.",
"In fact, through the Central Limit Theorem, this is also the distribution given by an aggregate of many RVs.",
"The pdf is given by fX(x) = 1 √ 2πσe−1 2( x−µ σ ) 2 which is parameterized by the mean µ which denotes the center of this distributions, and the variance σ2 which denotes its spread.",
"The corresponding cdf is given by FX(x) = 1 2  1 + erf x −µ σ √ 2  , erf(x) = 2 √π Z x 0 e−t2dt.",
In probabilistic Machine Learning one makes extensive use of uniform and Gaussian random variables.,
"75 (a) Uniform RV (a = −1, b = 1) (b) Exponential RV (λ = 0.8) (c) Gaussian RV Figure 7.2: Continuous random variables 7.2.4 Expectation and variance of RVs Given a RV X with pdf fX, we can calculate its expected value or expectation or mean as µX := E[X] = Z ∞ −∞ xfX(x)dx.",
"The expectation has the following properties: • Note that if a pdf is symmetric about x = m, then E[X] = m. To see this, note that (m −x)fX(x) will be anti-symmetric about m. Thus 0 = Z ∞ −∞ (m−x)fX(x)dx = m Z ∞ −∞ fX(x)dx− Z ∞ −∞ xfX(x)dx =⇒ Z ∞ −∞ xfX(x)dx = m. Using this property, we can easily say the mean for a uniform RV is (a + b)/2, while for a Gaussian RV it is µ.",
• E[c] = c for a constant c. • We can calculate the expected value of functions of RVs as E[g(X)] = Z ∞ −∞ g(x)fX(x)dx.,
"• The expectation is linear, i.e., E[g(X) + ch(X)] = E[g(X)] + cE[h(X)].",
The variance of a RV measures its variation about the mean.,
It is evaluated as VAR[X] = Z ∞ −∞ (x −µX)2fX(x)dx.,
"Furthermore, we denote the standard deviation as σX := STD[X] = p VAR[X].",
76 For a uniform RV VAR[X] = Z b a  x −b + a 2 2 1 b −adx = (b −a)2 12 .,
"For a Gaussian RV, we ﬁrst use the property of the pdf to write Z ∞ −∞ e−1 2( x−µ σ ) 2 dx = √ 2πσ.",
Taking a derivative with respect to σ on both sides lead to Z ∞ −∞ e−1 2( x−µ σ ) 2 (x −µ)2σ−3dx = √ 2π which after a bit of algebra gives us VAR[X] = Z ∞ −∞ (x −µ)2 1 √ 2πσe−1 2( x−µ σ ) 2 dx = σ2.,
7.2.5 Pair of RVs In probabilistic ML we deal with multiple random variables.,
"For example, the input, the output and the weights might all be RVs.",
Thus we need to extend concepts from a single RV to a vector of RVs.,
We do this in this section by ﬁrst considering a pair of RVs.,
Most of the concepts deﬁned for a pair of RVs carry forward to a vector of RVs.,
"A pair of RVs is a mapping from the measure space, with event class E , of the form X : E →R2, where the mapping can be discrete or continuous.",
"We will sometimes use the notation X = (X, Y ).",
"For example, we can spin the spinner twice and measure ψ1 ∈(0, 2π], ψ2 ∈(0, 2π].",
"In this case, we can deﬁne the two RVs X(ψ1) = ψ1 2π, Y (ψ2) = ψ2 2π.",
Events for X are sets in R2.,
"To compute probability of events, we need to deﬁne the joint cdf FXY : R2 →R, where FXY (x, y) = P[X ≤x, Y ≤y] = P[ξ ∈S : X(ξ) ≤x, Y (ξ) ≤y].",
"Analogous to single RVs • Joint cdfs are non-increasing functions of x, y.",
"In other words, for x ≥x′ and y ≥y′ FXY (x, y) ≥FXY (x′, y′).",
"• limx→−∞F(x, y) = 0, limy→−∞F(x, y) = 0, limx,y→∞F(x, y) = 1.",
"• We can calculate P[x1 < X ≤x2, y1 < Y ≤y2] = FXY (x2, y2) + FXY (x1, y1) −FXY (x1, y2) −FXY (x2, y1).",
"For X, Y jointly continuous, we can deﬁne the joint pdf as fXY (x, y) = ∂2FXY (x, y) ∂x∂y which enjoys the following properties 77 • fXY (x, y) = 0 as x →±∞or y →±∞.",
"• FXY (x, y) = R x −∞ R y −∞fXY (r, s)drds.",
"• R ∞ −∞ R ∞ −∞fXY (r, s)drds = 1.",
"• P[x1 < X ≤x2, y1 < Y ≤y2] = R x2 x1 R y2 y1 fXY (x, y)dxdy.",
"• P[X ∈B] = R R B fXY (x, y)dxdy.",
Let us look at some important joint random variables : 1.,
"Joint uniform RV: for some region (a, b] × (c, d], the joint pdf is given by fXY (x, y) = ( 1 (b−a)(d−c) if (x, y) ∈(a, b] × (c, d] 0 otherwise .",
"Joint Gaussian RV: the joint pdf is given by fXY (x, y) = 1 p (2π)2det(Σ) exp  −1 2(x −µ)⊤Σ−1(x −µ)  where x = (x, y), µ = (µx, µy) is the mean, and Σ is called the covariance matrix Σ =  σ2 x ρσxσy ρσxσy σ2 y  .",
The covariance matrix is symmetric and positive deﬁnite.,
"We can deﬁne the marginal PDF of the RV X, which is the pdf of X assuming Y attains all possible values fX(x) = Z ∞ −∞ fXY (x, y)dy.",
"Similarly, the marginal of Y is fY (y) = Z ∞ −∞ fXY (x, y)dx.",
"The RVs X and Y are said to be independent if fXY (x, y) = fX(x)fY (y).",
Question 7.2.1.,
Show that the joint uniform RVs with joint pdf (7.5) are independent.,
"Consider the function g(X), which can be scalar-, vector-, or tensor-valued, then its expected value is given by E[g(X)] = Z ∞ −∞ Z ∞ −∞ g(x)fXY (x, y)dxdy as long as the integral is deﬁned.",
"For instance: • For g(X) = X, we have E[g(X)] = R ∞ −∞ R ∞ −∞xfXY (x, y)dxdy.",
"• For g(X) = X, we have a vector valued expectation E[g(X)] = [E[X], E[Y ]].",
"• For g(X) = X + Y , we have E[g(X)] = E[X] + E[Y ].",
78 The covariance of X is given by COV[X] = E[(X −E[X]) ⊗(X −E[X])] where COV[X]11 = E[(X −E[X])2] = VAR[X] COV[X]22 = E[(Y −E[Y ])2] = VAR[Y ] COV[X]12 = COV[X]21 = E[(X −E[X])(Y −E[Y ])].,
X and Y are said to be uncorrelated if COV[X]12 = 0.,
"Furthermore, COV[X]12 = 0 for independent RVs.",
Caution: COV[X]12 = 0 does not imply the RVs are independent!,
"Finally, we are interested in looking at the pdf of Y when we know X = ˆx.",
"A good guess would be fXY (ˆx, y).",
"However, this need not be a pdf as it need not integrate to unity over y.",
"This leads us to the conditional pdf of Y when we know X = ˆx, fY |X(y|ˆx) = fXY (ˆx, y) R ∞ −∞fXY (ˆx, y)dy = fXY (ˆx, y) fX(ˆx) .",
"Similarly, we can write the conditional pdf of X given Y = ˆy as fX|Y (x|ˆy) = fXY (x, ˆy) fY (ˆy) .",
We note that the extension of a regression problem to probabilistic framework leads us to determining the conditional distribution of the output given an instance of the input.,
This will be discussed in Section 7.4.,
7.3 Unsupervised probabilistic deep learning algorithms We begin with a vector of RVs X with NX components with a pdf given by fX.,
"Let’s assume that we are given a dataset of samples {xi} sampled from the density fX, which we denote by xi ∼fX.",
"For instance, these samples could correspond to RGB images of cars, with a resolution of 512 × 512.",
"Note that this would mean that the samples would lie in a space with dimension NX = 512 × 512 × 3, which is quite large!",
"We can treat each pixel of the images as a RV, taking values given by the pixel intensities (across all 3 channels).",
"Thus, these images can be seen as samples of a NX-dimensional RV with some unknown density fX.",
"Also, because of the inherent structure of the objects (i.e.",
"the cars) in these images, the various components of the multidimensional RV can be expected to be highly correlated, leading to a non-trivial form of fX.",
This correlation also implies that it might be possible to reduce the dimension of X from NX to a smaller number and thus make the representation simpler.,
"We are interested in using the given ﬁnite set of samples {xi} to learning the density fX of the data, and generating new samples from the learned distribution.",
Such methods are known as generative algorithms.,
"Although a number of generative algorithms are available, we focus on a speciﬁc type of deep learning algorithm known as Generative Adversarial Networks, or GANs for short.",
7.3.1 GANs GANs were ﬁrst proposed by Goodfellow et al.,
[6] in 2014.,
"Since then, many variants of GANs have been proposed which diﬀer based on the network architecture and the objective function 79 used to train the GAN.",
We begin by describing the abstract problem setup followed by the architecture and training procedure of a GAN.,
Consider the dataset S = {xi ∈ΩX ⊂RNX : 1 ≤i ≤Ntrain}.,
"We assume the samples are realizations of some RV X with density fX, i.e., xi ∼fX.",
We want to train a GAN to learn fX and generate new samples from it.,
"A GAN typically comprises two sub-networks, a generator and a discriminator (or critic).",
The generator is a network of the form g(.,
"; θg) : ΩZ →ΩX, g : z 7→x (7.6) where θg are the trainable parameters and z ∈ΩZ ⊂RNZ is the realization of a RV Z following a simple distribution, such as an uncorrelated multivariate Gaussian with density fZ(z) = 1 p (2π)2det(Σ) exp  −1 2(z −µ)⊤Σ−1(z −µ)  with µ = 0, Σ = I.",
"Typically, NZ ≪NX with Z known as the latent variable of the GAN.",
The architecture of the generator will depend on the size/shape of x.,
"If x is a vector, then g can be an MLP with input dimension NZ and output dimension NX.",
"If x is an image, say of shape H × W × 3, then the generator architecture will have a few fully connected layers, followed by a reshape into a coarse image with many channels, which is pushed through a number of transpose convolution channels that gradually increase the spatial resolution and compress the number of channels to ﬁnally scale up to the shape H × W × 3.",
"This is also known as a decoder architecture, similar to the upward branch of a U-Net (see Figure 5.9.)",
"In either case, for a ﬁxed θg, the generator g transforms the RV Z to another RV, Xg = g(Z; θg) with density fg X, which corresponds to the latent density fZ pushed-forward by g. We want to choose θg such that fg X is close to the unknown target distribution fX.",
The critic network is of the form d(.,
; θd) : ΩX →R (7.7) with the trainable parameters θd.,
"Once again, the critic architecture will depend on the shape of x.",
If x is a vector then d can be an MLP with input dimension NX and output dimension 1.,
"If x is an image, then the critic architecture will have a few convolution layers, followed be a ﬂattening layer and a number of fully connected layers.",
This is similar to the CNN architecture shown in Figure 5.7 but with a scalar output and without an output function.,
Figure 7.3: Schematic of a GAN The schematic of the GAN along with the inputs and outputs of the sub-networks is shown in Figure 7.3.,
The generator and critic play adversarial roles.,
The critic is trained to distinguish 80 between true samples coming from fX and fake samples generated by g with the density fg X.,
"The generator on the other hand is trained to fool the critic by trying to generate realistic samples, i.e., samples similar to those sampled from fX.",
"We deﬁne the objective function describing a Wasserstein GAN (WGAN) [2], which has better robustness and convergence properties compared to the original GAN.",
"The objective function is given by Π(θg, θd) = 1 Ntrain Ntrain X i=1 d(xi; θd) | {z } critic value on real samples − 1 Ntrain Ntrain X i=1 d(g(zi; θg); θd) | {z } critic value on fake samples (7.8) where xi ∈S are samples from the true target distribution fX, while zi ∼fZ are passed through g to generate the fake samples.",
"To distinguish between true and fake samples, the critic attains large positive values when evaluated on real samples and large negative values on fake generated samples.",
"Thus, critic is trained to maximize objective function.",
"In other words, we want to solve the problem θ∗ d(θg) = arg max θd Π(θg, θd) for any θg.",
(7.9) Note that the optimal parameters of the critic will depend on θg.,
"Now to fool the critic, the generator g tries to minimize the objective function, θ∗ g = arg min θg Π(θg, θ∗ d).",
"(7.10) Thus, training the WGAN corresponds to solving a minmax optimization problem.",
We note that the critic and the generator are working in an adversarial manner.,
"That is, while the former is trying to maximize the objective function, the latter is trying to minimize it.",
Hence the name generative adversarial network.,
"In practice, we need to add a stabilizing term to the critic loss.",
"So the critic is trained to maximize Πc(θg, θd) = Π(θg, θd) −λ ¯N ¯ N X i=1  ∂d ∂ˆx(ˆxi; θd) −1 2 (7.11) where ˆxi = αxi + (1 −α)g(zi; θg) and α is sampled from a uniform RV in (0, 1).",
"The additional term in (7.11) is known as a gradient penalty term and is used to constraining the (norm of) gradient of the critic d with respect to its input to be close to 1, and thus be 1-Lipschitz function.",
"For further details on this term, we direct the interested readers to [7].",
"The iterative Algorithm 1 is used train g and d simultaneously, which is also called alternating steepest descent, where ηd and ηg are the learning rates for the critic and the generator, respectively.",
Note that we take K > 1 optimization steps for the critic followed by a single optimization step for the generator.,
This is because we want to solve the inner maximization problem ﬁrst so that the critic is able to distinguish between real and fake samples.,
"Although taking a very large K would lead to a more accurate solve of the minmax problem, it would also make the training algorithm computationally intractable for moderately sized networks.",
"Thus, K is typically chosen between 4 to 6 in practice.",
"The minmax problem is a hard optimization problem to solve, and convergence is usually reached after training for many epochs.",
"Alternatively, the critic optimization steps can be done over mini-batches of the training data, with many mini-batches taken per epoch, leading to a similar number of optimization steps for a relatively small number of epochs.",
"As the iterations go on, d becomes better at detecting fake samples and g becomes better at creating samples that can fool the critic.",
"81 Algorithm 1: Algorithm to train a GAN Input: θ0 d, θ0 g, K, N_epochs, ηd, ηg for n = 1, ..., N_epochs do ˆθd ←θ(n−1) d for k = 1, ..., K do Maximization update: ˆθd ←ˆθd + ηd ∂Πc ∂θd (θ(n−1) g , ˆθd) end θ(n) d ←ˆθd Minimization update: θn g ←θ(n−1) g −ηg ∂Π ∂θg (θ(n−1) g , θ(n) d ) end Under the assumption of inﬁnite capacity (Nθg, Nθd →∞), inﬁnite data (Ntrain →∞) and a perfect optimizer, we can prove that the generated distribution fg X converges weakly to the target distribution fX [2].",
"This is equivalent to saying EZ[ℓ(g(Z; θ∗ g))] −→EX[ℓ(X)], (7.12) for every continuous, bounded function ℓon ΩX, i.e., ℓ∈Cb(ΩX).",
"Once the GAN is trained, we can use the optimized g to generate new samples from fg X ≈fX by ﬁrst sampling z ∼fZ, and then passing it through the generator to get the sample x = g(z; θ∗ g).",
"Furthermore, due to the weak convergence described above, the statistics (mean, variance, etc) of the generated samples will convergence to the true statistics associated with fX.",
Remark 7.3.1.,
We make a few important remarks here: 1.,
"Once the GAN is trained, we typically only retain the generator and don’t need the critic.",
The primary role of training the critic is to obtain a suitable g that can generate realistic samples.,
"The reason the term ""Wasserstein"" appears in the name WGAN is because one can show that solving the minmax problem is equivalent to minimizing the Wasserstein-1 distance between fg X and fX [2, 28].",
The Wasserstein-1 distance is a popular metric used to measure discrepancies between two probability measures.,
"Since the dimension NZ of the latent variable is typically much smaller than the dimension NX of samples in ΩX, the trained generator also provides a low dimensional representation of high-dimensional data, which can be very useful in several downstream tasks [21, 22].",
"7.4 Supervised probabilistic deep learning algorithms Recall the deterministic problem where given the labelled/pairwise dataset S = {(xi, yi) : xi ∈ΩX ⊂RNX, y ∈ΩY ⊂RNY }Ntrain i=1 (7.13) 82 we want to ﬁnd y for a new x not appearing in S. We have seen in the previous chapters how neural networks can be used to solve such a regression (or classiﬁcation) problem.",
Now let us consider the probabilistic version of this problem.,
"We assume that x and y are modelled using RVs X and Y , respectively.",
"Further, let the paired samples in (7.13) be drawn from the unknown joint distribution fXY .",
"Then given a realization X = ˆx, we wish to use S to determine the conditional distribution fY |X(y|ˆx) and generate samples from it.",
"There are several popular approaches to solve this probabilistic problem, such as Bayesian neural networks, variational inference, dropouts or deep Boltzman machines.",
But we will focus on an extension of GANs which also addresses these type of problems.,
7.4.1 Conditional GANs Conditional GANs were ﬁrst proposed in [17] to learn conditional distributions.,
"We will discuss a special variant of these models known as conditional Wasserstein GANS (cWGANs) which were developed in [1], and used to solve a number of physics-based (inverse) problems in [25].",
Figure 7.4: Schematic of a conditional GAN The schematic of a conditional GAN is depicted in Figure 7.4.,
The generator is a network of the form g(.,
"; θg) : ΩZ × ΩX →ΩY , g : (z, x) 7→y (7.14) where z ∼fZ is the latent variable.",
"Note that unlike a GAN, the generator in a conditional GAN also takes as input x.",
"For a given value of X = ˆx, sampling z ∼fZ will generate many samples of y from some induced conditional distribution fg Y |X(y|ˆx).",
The goal is to prescribe the parameters θg such that fg Y |X(y|ˆx) approximates the true conditional fY |X(y|ˆx) for (almost) every value of ˆx.,
The critic is a network of the form d(.,
"; θd) : ΩX × ΩY →R (7.15) which is trained to distinguish between paired samples (x, y) generated from the true joint distribution fXY and the fake pairs (x, ˆy) where ˆy is generated by g given (real) x.",
"83 The objective function for a cWGAN is given by Π(θg, θd) = 1 Ntrain Ntrain X i=1 d(xi, yi; θd) | {z } critic value on real pairs − 1 Ntrain Ntrain X i=1 d(xi, g(zi, xi; θg); θd) | {z } critic value on fake pairs .",
"(7.16) As earlier, the critic is trained to maximize the objective function (given by (7.9)) while the generator is trained to minimize it (given by (7.10)).",
"Further, a stabilizating gradient penalty term needs to be included when optimizing the critic (see [25]).",
The generator and critic are trained using the alternating steepest descent algorithm described for GANs.,
"Under the assumption of inﬁnite capacity (Nθg, Nθd →∞), inﬁnite data (Ntrain →∞) and a perfect optimizer, we can prove [1] that the generated conditional distribution fg Y |X(y|ˆx) converges in a weak sense to the target condition distribution fY |X(y|ˆx) (on average) for a given X = ˆx.",
"84 Bibliography [1] J. Adler and O. Öktem, Deep bayesian inversion.",
"https://arxiv.org/abs/1811.05910, 2018.",
"[2] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial net- works, in Proceedings of the 34th International Conference on Machine Learning, D. Precup and Y. W. Teh, eds., vol.",
"70 of Proceedings of Machine Learning Research, International Convention Centre, Sydney, Australia, 06–11 Aug 2017, PMLR, pp.",
"[3] R. Bischof and M. Kraus, Multi-objective loss balancing for physics-informed deep learning.",
"http://rgdoi.net/10.13140/RG.2.2.20057.24169, 2021.",
"[4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary diﬀerential equations.",
"https://arxiv.org/abs/1806.07366, 2018.",
"[5] T. Chen and H. Chen, Universal approximation to nonlinear operators by neural net- works with arbitrary activation functions and its application to dynamical systems, IEEE Transactions on Neural Networks, 6 (1995), pp.",
"[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial nets, in Advances in neural information processing systems, 2014, pp.",
"[7] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved training of wasserstein gans, in Advances in neural information processing systems, 2017, pp.",
"[8] K. He, X. Zhang, S. Ren, and J.",
"Sun, Deep residual learning for image recognition, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp.",
"[9] P. Kidger and T. Lyons, Universal Approximation with Deep Narrow Networks, in Proceedings of Thirty Third Conference on Learning Theory, J. Abernethy and S. Agarwal, eds., vol.",
"125 of Proceedings of Machine Learning Research, PMLR, 09–12 Jul 2020, pp.",
2306– 2327.,
"[10] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization.",
https://arxiv.,
"org/abs/1412.6980v9, 2017.",
"[11] I. Lagaris, A. Likas, and D. Papageorgiou, Neural-network methods for boundary value problems with irregular boundaries, IEEE Transactions on Neural Networks, 11 (2000), pp.",
"85 [12] S. Lanthaler, S. Mishra, and G. E. Karniadakis, Error estimates for DeepONets: a deep learning framework in inﬁnite dimensions, Transactions of Mathematics and Its Applications, 6 (2022).",
"[13] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, Fourier neural operator for parametric partial diﬀerential equations.",
"https://arxiv.org/abs/2010.08895, 2020.",
"[14] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear operators via deeponet based on the universal approximation theorem of operators, Nature Machine Intelligence, 3 (2021), pp.",
"[15] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al., Rectiﬁer nonlinearities improve neural network acoustic models, in Proc.",
"[16] L. McClenny and U. Braga-Neto, Self-adaptive physics-informed neural networks using a soft attention mechanism.",
"https://arxiv.org/abs/2009.04544, 2020.",
"[17] M. Mirza and S. Osindero, Conditional generative adversarial nets.",
https://arxiv.,
"org/abs/1411.1784, 2014.",
"[18] S. Mishra and R. Molinaro, Estimates on the generalization error of physics-informed neural networks for approximating PDEs, IMA Journal of Numerical Analysis, (2022).",
"[19] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation approach to stochastic programming, SIAM Journal on Optimization, 19 (2009), pp.",
1574– 1609.,
"[20] D. Patel, D. Ray, M. R. A. Abdelmalik, T. J. R. Hughes, and A.",
"A. Oberai, Variationally mimetic operator networks.",
"https://arxiv.org/abs/2209.12871, 2022.",
[21] D. V. Patel and A.,
"A. Oberai, Gan-based priors for quantifying uncertainty in supervised learning, SIAM/ASA Journal on Uncertainty Quantiﬁcation, 9 (2021), pp.",
"[22] D. V. Patel, D. Ray, and A.",
"A. Oberai, Solution of physics-based bayesian inverse prob- lems with deep generative priors, Computer Methods in Applied Mechanics and Engineering, 400 (2022), p. 115428.",
"[23] A. Pinkus, Approximation theory of the mlp model in neural networks, Acta Numerica, 8 (1999), pp.",
"[24] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations, Journal of Computational Physics, 378 (2019), pp.",
"[25] D. Ray, H. Ramaswamy, D. V. Patel, and A.",
"A. Oberai, The eﬃcacy and gen- eralizability of conditional gans for posterior inference in physics-based inverse problems.",
"https://arxiv.org/abs/2202.07773, 2022.",
"[26] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomed- ical image segmentation, in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds., Cham, 2015, Springer International Publishing, pp.",
"86 [27] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein, Implicit neural representations with periodic activation functions.",
"https://arxiv.org/abs/ 2006.09661, 2020.",
"[28] C. Villani, Optimal Transport: Old and New, Grundlehren der mathematischen Wis- senschaften, Springer Berlin Heidelberg, 2008.",
"[29] S. Wang, Y. Teng, and P. Perdikaris, Understanding and mitigating gradient ﬂow pathologies in physics-informed neural networks, SIAM Journal on Scientiﬁc Computing, 43 (2021), pp.",
A3055–A3081.,
"[30] S. Wang, H. Wang, and P. Perdikaris, Learning the solution operator of parametric partial diﬀerential equations with physics-informed deeponets, Science Advances, 7 (2021).",
"[31] L. Wu, C. Ma, and W. E, How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective, in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds., vol.",
"31, Curran Associates, Inc., 2018.",
"[32] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, Denseaspp for semantic segmentation in street scenes, in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp.",
"[33] D. Yarotsky and A. Zhevnerchuk, The phase diagram of approximation rates for deep neural networks.",
"https://arxiv.org/abs/1906.09477, 2019.",
"A multitask deep learning model for real-time deployment in embedded systems Miquel Mart´ı∗† and Atsuto Maki† ∗Universitat Polit`ecnica de Catalunya, Barcelona, Catalonia/Spain †KTH Royal Institute of Technology, Stockholm, Sweden Email: {miquelmr,atsuto}@kth.se Abstract—We propose an approach to Multitask Learning (MTL) to make deep learning models faster and lighter for applications in which multiple tasks need to be solved simultaneously, which is particularly useful in embedded, real-time systems.",
We develop a multitask model for both Object Detection and Semantic Segmentation and analyze the challenges that appear during its training.,
"Our multitask network is 1.6x faster, lighter and uses less memory than deploying the single-task models in parallel.",
We conclude that MTL has the potential to give superior performance in exchange of a more complex training process that introduces challenges not present in single-task models.,
INTRODUCTION Deep learning models (and machine learning models in general) focus on solving one task at a time.,
"However, applications often require more than one task to be performed simultaneously and the naive solution is to deploy in parallel one model for each task.",
"Applications imposing real-time constraints require small inference times and prohibit off-board computation, forcing the deployment of deep learning models in em- bedded systems, in which not only storage and memory available are limited but also computing power.",
"This presents challenges in terms of resources, accentuated when deploying multiple models: weights storage and for- ward pass memory usage and computational complexity.",
"Multitask Learning [1] learns to solve tasks in par- allel using a shared representation of a common input, improving the generalization capabilities of the models.",
Multitask networks share a base trunk and a number of task-speciﬁc branches emerging from it.,
Only the task- speciﬁc layers are computed separately.,
"In this work, we propose using multitask models to get beneﬁts in terms of speed, memory usage and storage during deployment.",
"For studying our approach, we train and evaluate a multitask model for both semantic segmen- tation and object detection.",
"We highlight the challenges imposed by applying MTL, explain how they affect the performance of our model and show that it compares positively in terms of inference time, memory usage and model size against deploying one model per task.",
"RELATED WORK A. Multitask Learning MTL has been succesfully used in different domains, including CV [2], [3].",
Some challenges appear when applying it [1]: learning speed differences between tasks mbox_loca on_out mbox_conf_out Fig.,
Our multitask network architecture for object detection and semantic segmentation.,
"and deciding what to share according to the relatedness between tasks in the multitask architecture [4], [5].",
"B. Semantic Segmentation Semantic segmentation aims at partitioning parts of images belonging to the same semantic class, typically via pixel-wise classiﬁcation.",
Fully convolutional networks (FCN) [6] have improved both accuracy and speed for dense prediction problems by using only convolutional layers.,
Upsampling layers allow a segmentation output size equal to the input and skip connections add ﬁner details.,
"Other approaches add post-processing steps [7], learnable deconvolution layers [8] or global context [9].",
C. Object Detection Object detection aims at ﬁnding in an image all in- stances of objects and classifying them in a number of classes.,
Faster R-CNN [10] was the ﬁrst to give close to real-time performance.,
YOLO [11] avoids the generation of region proposals for increased speed.,
SSD [12] avoids fully-connected layers for speed and takes features at different levels for improved accuracy.,
"Model details Extended details on our new model architecture, datasets and training strategies can be found in [13].",
"We select a ResNet-50 [14] as our base network, FCN with a skip connection for the semantic segmentation task and SSD for objection detection due to their speed vs. accuracy trade-offs.",
1 shows our multitask model.,
"B. Datasets For the Pascal VOC dataset [15], we only use the subset of samples that have ground-truth annotations for both tasks from VOC07, VOC12 and SBD[16].",
arXiv:1711.00146v1 [cs.CV] 31 Oct 2017 TABLE I PASCAL VOC RESULTS.,
References Ours [18] [12] [19] SSD FCN Multi Color mIoU (%) - - 62.5 - 56.4 54.4 55.2 mAP (%) 74.3 70.4 - 51.3 - 51.8 52.6 Fig.,
Pascal VOC detection and segmentation samples.,
"As there is no aerial view dataset with annotations for both tasks, we interleave images from Stanford Drone Dataset [17] for object detection and the much smaller Okutama/Swiss [18] datasets for semantic segmentation in each mini-batch.",
RESULTS A. Pascal VOC We train single-task baselines with the limitations imposed by using MTL and compare those with the multitask model and reference models from the literature.,
Table I shows the results.,
Some example output images can be seen in Fig.,
We add color distortion for data augmentation.,
B. Aerial view We train single-task baselines with no limitations and compare them to our multitask model.,
"Table II shows the results in terms of accuracy and resources used when deployed on an NVIDIA GTX Titan X for the single- task models, their combination and our multitask model.",
Sample output images can be seen in Fig.,
"C. Analysis The compromises made due to the particularities of MTL and especially the lack of a strong data augmen- tation caused the ﬁnal accuracy of our multitask model to lag behind that of the single-task ones trained without these although they improved in terms of speed and usage of resources, being 1.6x faster, lighter and consuming less memory than the naive solution.",
"Compared to the single-task models trained with the same limitations, the multitask models matched or outperformed their accuracy for one of the tasks.",
Find a more detailed analysis in [13].,
V. CONCLUSION We conclude that MTL has the potential to give su- perior performance in the accuracy vs. speed over using multiple single-task models simultaneously as far as the two analyzed are concerned.,
"This is in exchange of a training process that is more complex as it requires extra choices to be made, a new tuning of the parameters that jointly works well for each task and overcoming new challenges that were not present in the training of single- task models.",
TABLE II AERIAL VIEW RESULTS.,
Base FCN SSD Multitask Naive mIoU (%) - 70.9 - 65.3 70.9 mAP (%) - - 54.3 28.2 54.3 Inf.,
Time (ms) 19 24 27 30 49 Memory (MB) 1203 1233 1525 1552 2511 Size (MB) 95 95 140 140 235 Fig.,
Aerial view detection and segmentation samples.,
"ACKNOWLEDGMENT The work was partially conducted while the ﬁrst author was at Prendinger Lab, participating in the NII Interna- tional Internship Program in Tokyo, Japan.",
"REFERENCES [1] R. Caruana, “Multitask learning,” Mach.",
"Learn., vol.",
"41–75, Jul.",
"[2] I. Kokkinos, “Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory,” CoRR, vol.",
"abs/1609.02132, 2016.",
"[3] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick, “Mask R-CNN,” CoRR, vol.",
"abs/1703.06870, 2017.",
"[4] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert, “Cross-stitch networks for multi-task learning,” 2016, pp.",
"[5] Y. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris, “Fully- adaptive feature sharing in multi-task networks with applications in person attribute classiﬁcation,” arXiv:1611.05377, 2016.",
"Long, E. Shelhamer, and T. Darrell, “Fully convolutional net- works for semantic segmentation,” June 2015.",
"[7] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic image segmentation with deep convolutional nets and fully connected crfs,” arXiv:1412.7062, 2014.",
"[8] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for semantic segmentation,” December 2015.",
"[9] W. Liu, A. Rabinovich, and A. C. Berg, “Parsenet: Looking wider to see better,” CoRR, vol.",
"abs/1506.04579, 2015.",
"[10] S. Ren, K. He, R. B. Girshick, and J.",
"Sun, “Faster R-CNN: towards real-time object detection with region proposal networks,” CoRR, vol.",
"abs/1506.01497, 2015.",
"[11] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Uniﬁed, real-time object detection,” 2016, pp.",
"[12] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg, “SSD: single shot multibox detector,” CoRR, vol.",
"abs/1512.02325, 2015.",
"[13] M. Mart´ı, “Multitask deep learning models for real-time deploy- ment in embedded systems,” Master’s thesis, KTH, Sweden, 2017.",
"[14] K. He, X. Zhang, S. Ren, and J.",
"Sun, “Deep residual learning for image recognition,” arXiv:1512.03385, 2015.",
"[15] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes challenge: A retrospective,” International Journal of Computer Vision, vol.",
"98–136, 2015.",
"[16] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik, “Semantic contours from inverse detectors,” 2011.",
"[17] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learn- ing social etiquette: Human trajectory understanding in crowded scenes.” Springer, 2016, pp.",
"[18] J. Laurmaa, “A deep learning model for scene segmentation of images captured by drones,” Master’s thesis, EPFL, Switzerland, 2016.",
"[19] J. Mahadeokar, “pynetbuilder,” https://github.com/ jay-mahadeokar/pynetbuilder, 2016.",
"DILIE: Deep Internal Learning for Image Enhancement Indra Deep Mastan and Shanmuganathan Raman Indian Institute of Technology Gandhinagar Gandhinagar, Gujarat, India {indra.mastan, shanmuga}@iitgn.ac.in Abstract We consider the generic deep image enhancement prob- lem where an input image is transformed into a perceptually better-looking image.",
Recent methods for image enhance- ment consider the problem by performing style transfer and image restoration.,
The methods mostly fall into two cate- gories: training data-based and training data-independent (deep internal learning methods).,
We perform image en- hancement in the deep internal learning framework.,
Our Deep Internal Learning for Image Enhancement framework enhances content features and style features and uses con- textual content loss for preserving image context in the en- hanced image.,
We show results on both hazy and noisy im- age enhancement.,
"To validate the results, we use structure similarity and perceptual error, which is efﬁcient in measur- ing the unrealistic deformation present in the images.",
We show that the proposed framework outperforms the relevant state-of-the-art works for image enhancement.,
Introduction Many computer vision tasks could be formulated as im- age enhancement tasks where the aim is to improve the per- ceptual quality of the image.,
"For example, an image denois- ing method enhances image features and remove noise.",
The image style transfer method enhances the content image by transferring style features from a style image.,
"Deep image enhancement is an ill-posed problem that aims to improve the perceptual quality of an image using a deep neural network [13, 28, 12, 30].",
An image could be considered as the composition of content features and style features.,
"The content features denote the objects, their structure, and their relative positions.",
Style features rep- resent the color and the texture information of the objects.,
Deep image enhancement aims to improve the quality of the content and the style features.,
The image features may get corrupted in various ways.,
"For example, bad weather conditions, camera shake, and noise, etc.",
Let us discuss an example of a deep image en- hancement task.,
Consider a hazy image denoted by I. Haze particles degrade both the content features and the style fea- tures.,
The content features are corrupted because haze par- ticles reduce the clarity of the structure of the objects.,
Style features are corrupted due to gray and blueish patterns intro- duced by haze.,
The enhancement of content and style fea- tures can draw inspiration from the image restoration and the style transfer methods.,
Image enhancement task is to improve the perceptual quality of hazy image I.,
The challenge is the haze parti- cles are non-uniformly spread over the scene.,
One strategy is to utilize the content features from I and transferring the photo-realistic features from a style image S. The inter- esting observation here is that maintaining the balance be- tween the content feature and the style feature is challenging (Fig.,
Performing deep image enhancement without using paired samples of training data was proposed as an open problem [32].,
"Here, paired-samples indicate the instances of the original image and corrupted image pairs.",
"Recent ad- vancement in deep internal learning (DIL) solves the open problem for image restoration and image synthesis tasks [25, 27].",
"We categorize DIL methods for simplicity as: im- age reconstruction models [27], layer separation models [6], and single image GAN frameworks [17, 25].",
"The deep im- age enhancement of an image (content) is also performed using a style image in the style transfer [7, 16, 10].",
We formulate a generic framework called Deep Internal Learning for Image Enhancement (DILIE).,
It does not use paired samples of training data and aims to learn features internally to perform image enhancement.,
1 shows the deep image enhancement performed for hazy and noisy im- ages.,
"The good perceptual quality of DILIE framework is due to the ability of CNN to learn good quality image statis- tics from a single image [25, 27, 6].",
We have illustrated DILIE framework in Fig.,
2 for hazy image enhancement.,
It takes the degraded image I as in- put and generates the enhanced image I∗.,
The main idea is to formulate the content feature enhancement (CFE) and the style feature enhancement (SFE) models separately for gen- 1 arXiv:2012.06469v1 [cs.CV] 11 Dec 2020 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 1: The ﬁgure shows that the deep internal learning-based DILIE framework output images with better perceptual quality.,
The style image is shown at the left corner of the content image in (a).,
The ﬁrst row shows that DILIE output image with better perceptual quality for the enhancement of the hazy image.,
The second row shows DILIE output images with better clarity for noisy image enhancement.,
eralizability.,
2 shows CFE decomposes the hazy im- age I into environmental haze layer H and haze-free image Icfe.,
SFE transfers photo-realistic features from style im- age S to Icfe.,
We describe the DILIE framework in Sec.,
CFE is modeled based on the type of corruption.,
"CFE, using the image decomposition model, is used for image de- hazing and CFE using image reconstruction for image de- noising.",
The image decomposition model performs joint optimization to separate the degraded image into clean and corrupted features.,
Image reconstruction generates a clean image with pixel-based reconstruction loss.,
Both these ap- proaches rely upon the strong image prior captured by the encoder-decoder network.,
The aim of SFE is to transform the input image (con- tent) into a visually appealing output image by transferring style features from the style image.,
"SFE is modeled based on the desired style speciﬁcation, i.e., photo-realistic style transfer [16, 31] or artistic style transfer [10].",
Note that the distortions in the style transfer output lead to a lack of photo-realism.,
We measure the deformations using percep- tual error Pieapp [22] computed between the content image and the output image.,
DILIE output images with low per- ceptual error (Table 2).,
One of the important requirements for image enhance- ment is to preserve the context of the input image.,
DILIE preserves the semantics of the input image by comparing the context vectors.,
The context vectors represent high-level content information and are computed from feature extrac- tor VGG19 [19].,
DILIE framework computes the contex- tual content loss LCL to preserve the semantics of the scene.,
2 illustrates here LCL is computed between I and Icfe to preserve the contextual content features in Icfe.,
"We propose a generic deep internal learning framework (DILIE) that addresses corruption speciﬁc image enhance- ment using image reconstruction and image decomposition, and photo-realistic feature enhancement.",
We summarize the major contributions as follows.,
• We show that utilization of contextual features improves image dehazing and outperform relevant state-of-the-art works (Table 1).,
• We show image enhancement for the challenging scenario where photos were taken in hazy weather (Fig.,
DILIE also performs enhancement of the noisy images (Fig.,
• DILIE outputs images with better visual quality and lower perceptual error (Table 2 and Fig.,
Related Work Deep Internal Learning.,
"Recent DIL approaches perform image synthesis and image restoration without using paired samples for training [27, 25].",
The aim is to learn the internal patch distribution [25] and utilize the deep image prior [27].,
"DIL is different from training data-based methods that use prior examples to supervise the image enhancement task [9, 19].",
Content Feature Enhancement.,
CFE is performed us- ing image reconstruction and image decomposition mod- els.,
The structure of the encoder-decoder network (ED) 2 Figure 2: Image Dehazing.,
The ﬁgure shows the pictorial representation of DILIE framework for the enhancement of the hazy image.,
The hazy image I is transformed into an enhanced image I∗.,
The left side shows the content feature enhancement (CFE) and the right side shows the style feature enhancement (SFE).,
"CFE performs image decomposition to output haze-free image Icfe, transmission map M and haze layer H. VGG19 network φ is used to extract features to compute contextual content loss LCL, content loss LC, and style loss LS.",
Image decomposition loss LID is a pixel-based loss (Eq.,
LCL uses contextual similarity criteria on content features (Eq.,
SFE improves style features using content loss LC and style loss LS.,
We describe DILIE framework in Sec.,
3. provides application-speciﬁc image prior implicitly [27].,
"Image reconstruction models use ED for denoising, super- resolution, and inpainting.",
"Dehazing is formulated as an image decomposition problem [6], where ED computes the image layer and haze layer separately.",
"For simplicity, image dehazing methods could be classiﬁed into classical [5, 8], supervised method using deep learning [11], and unsuper- vised methods [6].",
Style Feature Enhancement.,
The related works for style feature enhancement are discussed as follows.,
Gatys et al.,
proposed Neural style [7] for style feature enhancemnt.,
Luan et al.,
[16] improved Neural style [7] for photo- realism.,
WCT2 enhances photorealism using wavelet trans- forms [31].,
STROTSS [10] uses optimal transport for more general style transfer.,
Our Approach DILIE is a uniﬁed framework to restore the content fea- tures and synthesizes new style features for the image en- hancement task.,
Let us denote the input image by I.,
The DILIE framework is deﬁned in Eq.,
"I∗= DILIE(I, f, S, φ, α, β).",
"(1) Here, I∗is the enhanced image.",
The encoder-decoder net- work f is used for the reconstruction or decomposition of input I.,
The style image S is used to enhance the style features of image I.,
"The VGG19 network φ is used for image context learning [17] and the style features enhance- ment [7, 10].",
DILIE framework performs content feature enhancement (CFE) and style features enhancement (SFE) separately.,
α and β are the parameters used for CFE and SFE.,
CFE enhances content features by learning deep fea- tures using encoder-decoder f (Sec.,
SFE uses a style image S for photo-realistic and artistic feature enhancement (Sec.,
2 shows DILIE framework for hazy image enhance- ment.,
"CFE performs image decomposition to decompose hazy image I into content feature Icfe, haze layer H, and transmission map M. The image composition block com- bines the decomposed image features and outputs the recon- struction of I.",
"It is done to preserve relationship between Icfe, H, and M. SFE outputs the enhanced image as I∗.",
We discuss the components of the DILIE framework as fol- lows.,
Content Feature Enhancement CFE could be majorly performed in the following two ways: image reconstruction (IR) and image decomposition (ID).,
The formulation of content feature enhancement is given in Eq.,
"Icfe = CFE(I, f, φ, α).",
"(2) Here, Icfe denotes the output of the content feature en- hancement.",
The structure of the encoder-decoder network f provides an implicit image prior for the restoration of im- age features.,
"The corruption speciﬁc image prior enables diverse applications, e.g., dark channel prior for the image dehazing [6, 8] and encoder-decoder without skip connec- tions as denoising prior [27].",
The VGG network φ is used to extract the contextual features to compute the contex- 3 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 3: Hazy Image Enhancement (outdoor).,
The content image contains haze and the style images are clear images (bottom left corner).,
The style images are photo-realistic.,
Neural style [7] deforms the geometry of the objects.,
DPST [16] does not distribute image features well.,
"WCT2 [31] output contains haze corruption, as shown by white spots.",
STROTSS [10] does not preserve ﬁne image features details.,
It could be observed that DILIE (ours) output images with better visual quality (the images are best viewed after zooming).,
tual content loss for preserving the context of image I in CFE output.,
The parameter α denotes whether CFE is used to model image decomposition (α = 1) or reconstruction (α = 2).,
3.1.1 Image Decomposition Image decomposition (ID) improves the quality of images by separating image features and corrupted features.,
"For- mally, given an image I as a combination of image feature layer and environmental noise.",
"ID separate I into the image features layer Icfe and the image corruption layer D, where the separation is determined by a mask M. In the image de- hazing task, the mask is a transmission map that determines image features Icfe and airlight H. ID is deﬁned in Eq.",
"(θ∗ c, θ∗ d, θ∗ m) = arg min (θc,θd,θm) LID  I; fθc, fθd, fθm  .",
"(3) Here, LID denotes the image decomposition loss.",
"θc is the parameter of image content layer, θd is the parameter of distortion layer, and θm is the parameter of mask M. fθc, fθd, and fθm are the instances of encoder-decoder net- work.",
"zc, zd, and zm denote the inputs for the networks.",
"Formally, Eq.",
"3 models the joint optimization to compute Icfe = fθ∗c (zc), D = fθ∗ d(zd), and M = fθ∗ m(zm).",
We have shown LID in Eq.,
"LID  I; fθc, fθd, fθm  =  fθm(zm) ⊙fθc(zc)+ (1 −fθm(zm)) ⊙fθd(zd)  −I .",
"(4) Here, Eq.",
"4 shows that the layer separation is achieved by composing image I from image features Icfe = fθ∗c (zc) and corruption layer D = fθ∗ d(zd), and then minimizing pixel-wise differences.",
We will discuss the image decom- position for image dehazing task in Sec.,
The image decomposition in Eq.,
4 does not consider the context of the input image.,
"The abstract information of con- tent features represents the context of the image, i.e., objects and their relative positions.",
The feature extractor VGG19 is denoted by φ.,
It is used to extract the content features and style features [7].,
The content features are mostly present at the higher layers of feature extractor φ denoted by φC.,
The style features are mostly contained at the initial layers denoted by φS.,
The contextual content loss LCL is deﬁned between the content features of I and Icfe = fθc(zc) as given in Eq.,
"LCL  I, φ; fθc  = −log CX  φC(fθc(zc)), φC(I)  .",
"(5) Here, CX denotes the contextual similarity computed by ﬁnding for each feature φC(fθc(zc))i of the image Icfe, the contextually similar feature φC(I)j of the corrupted image I, and then sum over all the features in φC(fθc(zc)).",
We call the strategy above as the contextual similarity criterion.,
The key observation is that high-level content information (image context) is similar in both Icfe and I. LCL maxi- mizes the contextual similarity between Icfe and I to im- prove performance1.,
3.1.2 Image Reconstruction Image reconstruction model (IR) uses encoder-decoder f to reconstruct the desired image.,
IR is described in Eq.,
"6. θ∗= arg min θ LIR(I; fθr), where LIR(I; fθr) = fθ(zr) −T (I) .",
"(6) Here, LIR is the reconstruction loss, θr is the network parameters, zr is the network input, and T is the image transformation function.",
The output of CFE in image re- construction is Icfe = fθ∗ r (zr).,
The function T varies based on the application under consideration.,
"For example, T is an identity function for denoising and T is a down- sampling function for super-resolution [27].",
IR model in Eq.,
"6 performs content feature enhancement by incorporat- ing the application-speciﬁc encoder-decoder architectures 1We have used φC={conv4 2} and φS={conv1 2, conv2 2, conv3 2} in our experiments.",
4 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 4: Hazy Image Enhancement (indoor).,
The ﬁgure shows the image feature enhancement of the indoor scene.,
It could be observed that DILIE (ours) distributed image features with better perceptual quality (the images are best viewed after zooming).,
for f. The network architecture is observed to provide an implicit image prior for restoration [27].,
Style Feature Enhancement We described that CFE enhances the content features of I. SFE aims to improve style features and output the en- hanced image I∗given the CFE output Icfe.,
SFE transfer the style features to Icfe using style image S. We deﬁne SFE in Eq.,
"I∗= SFE(Icfe, S, f, φ, β).",
"(7) Here, I∗is the enhanced image and S is the reference style image.",
"β represents the type of feature enhancement, i.e., photo-realistic (β = 1) or painting style artistic (β = 2).",
The style features enhacement is performed using the content loss LC and style loss LS.,
The content loss LC is deﬁned between the content feature representations φC(Icfe) extracted from Icfe and the content feature rep- resentations φC(I∗) extracted from I∗.,
"The content loss is given by LC = L  φC(Icfe), φC(I∗)  .",
The style loss LS is computed between the style feature representations φS(S) extracted from S and the style feature representation φS(I∗) of I∗.,
"Formally, LS = L  φS(S), φS(I∗)  .",
We provide the detailed description of LC and LS in the sup- plementary material.,
SFE could be considered as photo-realistic or artistic fea- tures enhancement.,
"The photo-realistic feature enhance- ment (PE) is aimed to minimize the distortion of object boundaries and preserve photo-realism using loss LP E. In contrast, the artistic feature enhancement (AE) allows small deformations to achieve an artistic look using loss LAE.",
3.2.1 Photo-realistic Feature Enhancement The photo-realism characterization in the image is an un- solved problem [16].,
"The enhancement of the photo- realistic features is based on the observation that if the in- put image is photo-realistic, then those features could be retained with an afﬁne loss [16].",
The image with lower per- ceptual errors is observed to be more photo-realistic [22].,
The degree of photo-realism in the output I∗is measured by the perceptual error score PieAPP [22].,
"The total loss for PE is deﬁned as LP E = Lm + µ × LC + κ × LS, where µ and κ are the coefﬁcients for the content loss LC and the style loss LS.",
The afﬁne loss Lm preserves the object structure while transforming the style features.,
"More speciﬁcally, afﬁne loss uses Matting Lapla- cian MIcfe of the input Icfe [16], where MIcfe represents the grayscale matte for the content features.",
"Intuitively, the afﬁne loss function transforms the color distribution of I∗ while preserving the object structure.",
3.2.2 Artistic Feature Enhancement We described that small image feature deformation could be present in the artistic style transfer.,
"Therefore, the strategy is to match the distribution of the style and the content fea- tures and do not use the afﬁne loss to reduce deformations in I∗.",
"The total loss for AE is deﬁned as LAE = µ × LC + κ × LS, where µ and κ are the coefﬁcients for the con- tent loss LC and the style loss LS.",
We use relaxed earth mover distance (EMD) to match the image feature distribu- tion [10].,
The EMD loss preserves the distance between all the pairs of features extracted from the VGG19 φ to allow pixel value modiﬁcation for style features while preserving the structure of the objects.,
Applications We perform image enhancement of hazy and noisy im- ages.,
"Hazy Image Enhancement Pictures taken in the hazy weather may lack scene infor- mation such as contrast, colors, and object structure.",
"Haze is composed of small particles (e.g., dust) suspended in the 5 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 5: Noisy image enhancement.",
The ﬁgure shows that DILIE outputs images with better perceptual quality (see the cropped images).,
The style images are artistic images and content images contain noise with strength σ = 0.25.,
Table 1: The table shows SSIM comparison for dehazing of I-Haze and O-Haze dataset.,
DILIE outperforms other methods in comparison.,
AODNet [11] MSCNN [23] DcGAN [14] GFN [24] GCANet [3] PFFNet [20] DoubleDIP [6] DILIE (ours) I-Haze [1] 0.732 0.755 0.733 0.751 0.719 0.740 0.691 0.790 O-Haze [2] 0.539 0.650 0.681 0.671 0.645 0.669 0.643 0.705 Table 2: The table shows that DILIE (ours) performs image en- hancement with minimum perceptual error PieAPP [22].,
Neural [7] DPST [16] WCT2 [31] STROTSS [10] DILIE I-Haze [1] 3.80 3.33 3.52 2.91 2.78 O-Haze [2] 3.00 2.71 2.88 2.81 2.55 Denoising 100 5.00 4.98 4.53 4.82 4.27 gas.,
We have discussed the pictorial representation for hazy image enhancement in Sec.,
The image degrada- tion model for the hazy image is usually formulated using an atmospheric scattering model [29] as shown in Eq.,
I(p) = ˆI(p) × M(p) + H(p) × (1 −M(p)).,
"(8) Here, p is the pixel location and I is the degraded obser- vation.",
ˆI is the haze-free image and M is the transmission map.,
"Intuitively, the hazy image I could be considered as a haze layer H superimposed on the true scene content ˆI.",
"Image dehazing can be formulated as a layer decomposi- tion problem to separates the hazy image (I) into a haze-free image layer (Icfe) and a haze layer (H), where Icfe is the approximation of haze-free image ˆI.",
We have discussed the generalized image decomposition framework in Eq.,
We show its applicability for hazy image enhancement in Eq.,
"(θ∗ c, θ∗ h, θ∗ m) = arg min (θc,θh,θm) LID  I; fθc, fθh, fθm  +LCL  I, φ; fθc  .",
"(9) Here, LID is for image decomposition (Eq.",
3) and LCL is for preserving image context (Eq.,
θh represents the parameters for haze layer.,
The transmission map M = fθ∗m(zm) separates the haze-free image Icfe = fθ∗c (zc) and the atmospheric light H = fθ∗ h(zh).,
The joint framework is aimed to estimate ˆI and H preserving their relations.,
The main goal of Eq.,
9 is to separate image features and haze features based on the semantics.,
The characteristics of haze particles in I are similar.,
"Therefore, they accumulate into haze layer H. Similarly, the image features of I have similar characteristics and get separated into the haze-free image layer Icfe.",
We have discussed contextual content loss LCL given in Eq.,
5 matches the contextual similarity between features.,
LCL improves the performance of the layer decomposition framework.,
3 shows the image enhancement of outdoor images and Fig.,
4 shows the enhancement of the indoor scenes.,
The outdoor scenes mostly contain clouds and trees and the in- door images mostly contain objects present in the house- hold.,
The image dehazing removes the haze from the input image and hazy image enhancement improves the quality of image features.,
Table 1 shows that DILIE achieves a good Structural Similarity Index (SSIM) for image dehazing.,
Table 2 shows that DILIE output images with better perceptual quality for hazy image enhancement2.,
It is interesting to observe that the generalisability of DILIE (ours) allows good perfor- mance for both content feature enhancement (image dehaz- ing) and style feature enhancement (hazy image enhance- ment).,
"6 shows that if the input image contains haze par- ticles, then the haze information gets incorporated into the output even when S does not include haze information.",
"Ide- ally, the output should contain the content features from I 2We used implementation of Neural style provided in [26], Tensorﬂow implementation of DPS given in [15], contextual loss implementation in [18], STROTSS implementation in [21], and WCT2 implementation in [4].",
We have provided more visual comparisons and implementation details of our method in the supplementary material.,
6 (a) Haze-free Image ˆI (b) Hazy Image I (c) Style Image S (d) DPST [16] H: 0.455 (e) WCT2 [31] H: 0.943 (f) STROTSS [10] H: 0.876 (g) DILIE (ours) H: 0.350 Figure 6: Ablation Study.,
The ﬁgure highlights the corruption of image features due to the haze in the enhanced output images.,
The style features (color information) of the outputs get affected by haze even when the input style image does not contain haze particles.,
H denotes the relative perceptual error due to haze computed using PieAPP [22].,
DILIE output image with the minimum perceptual error.,
It could also be observed visually that DILIE output has minimum effect from the haze.,
and style features from S. The image enhancement of hazy images highlight that preserving a perceptually good bal- ance between style and content features is very challenging.,
Our CFE module removes haze features so that the ﬁnal output I∗has less inﬂuence due to bad weather conditions.,
Noisy Image Enhancement Denoising aims to recover a clean image from a noisy observation.,
The image degradation model for the noisy image is given as I = ˆI + ϵ.,
"Here, I the noisy image, ˆI is the clean content image, and ϵ is the additive noise.",
"Image denoising is formulated as image reconstruction, where an encoder-decoder f reconstructs the clear image Icfe from the noisy observation I.",
The network f provides a high impedance to noise and allows image features [27].,
We have discussed the generalized framework for image re- construction using transformation T in Eq.,
Image de- noising is performed by taking T to be identity function as given in Eq.,
"Icfe = fθ(z), where, θ∗= arg min θ ∥fθ(z) −I ∥.",
"(10) Here, the restored image Icfe = fθ(z) is the approxima- tion of ˆI.",
The reconstruction loss given in Eq.,
"10 is itera- tively minimized, and early stopping is used to get the best possible outcome before the network over-learn the noisy features.",
We make noisy image enhancement more challenging by using the style and the content images containing noise with the strength σ = 0.25.,
We show the output images in Fig.,
It could be observed that DILIE gets a better distribution of features with better clarity (see cropped images).,
We have shown a quantitative comparison in Table 2.,
It can be ob- served that DILIE outperforms other methods in compari- son.,
Ablation Study Fig.,
6 illustrates that DILIE output images with less en- vironmental noise.,
The quantitative comparison for haze corruption is described as follows.,
"Consider the hazy im- age I, haze-free image ˆI, and the style image S (Fig.",
The difference of image features between I and ˆI is due to the haze.,
"Let ST(y, z) denote the style transfer of content y using style z.",
"6 shows that when performing ST be- tween I and S, the output image is observed to have haze corruption even when S does not have haze information.",
The goal is to minimize haze corruption.,
"To quantify haze corruption, let E(w, x) denote the per- ceptual error [22] between image w and image x.",
"The rel- ative error H = ∥E  ˆI, ST(ˆI, S)  −E  ˆI, ST(I, S)  ∥with reference to haze-free image ˆI measures the deformations caused by haze in ST(I, S) by comparing ST output of the clean image ˆI and the corrupted image I using perceptual error PieAPP [22].",
6 shows that DILIE output image with minimum perceptual error H. It could also be observed visually that in WCT2 [31] output contains haze corruption.,
DPST [16] and STROTSS [10] outputs also have haze effects when looking carefully.,
DILIE has the minimum haze effect 3.,
Conclusion DILIE is a deep internal learning approach for image en- hancement.,
It is a generic framework for image restoration and image style transfer tasks for content feature enhance- ment (CFE) and style feature enhancement (SFE) models.,
The contextual content loss for image decomposition im- provised the performance of the image dehazing task.,
The interesting challenge here is that the degraded input image corrupts both style and content features.,
CFE and SFE to- gether lead to output images with a low perceptual error and good structure similarity.,
"As future work, we propose to explore image enhancement for other image degradation models such as under-water scenes and snowfall.",
3We discuss the ablation study more in the supplementary material.,
"7 References [1] Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe De Vleeschouwer.",
I-haze: a dehazing bench- mark with real hazy and haze-free indoor images.,
"In arXiv:1804.05091v1, 2018.",
"[2] Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe De Vleeschouwer.",
O-haze: a dehazing bench- mark with real hazy and haze-free outdoor images.,
"In IEEE Conference on Computer Vision and Pattern Recognition, NTIRE Workshop, NTIRE CVPR’18, 2018.",
"[3] Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Li- heng Zhang, Dongdong Hou, Lu Yuan, and Gang Hua.",
Gated context aggregation network for image dehazing and derain- ing.,
"In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1375–1383.",
"IEEE, 2019.",
[4] clovaai.,
"https://github.com/clovaai/WCT2, 2019.",
[5] Raanan Fattal.,
Single image dehazing.,
"ACM transactions on graphics (TOG), 27(3):1–9, 2008.",
"[6] Yossi Gandelsman, Assaf Shocher, and Michal Irani.",
Double-dip”: Unsupervised image decomposition via cou- pled deep-image-priors.,
"In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), volume 6, page 2, 2019.",
"[7] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.",
Im- age style transfer using convolutional neural networks.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016.",
"[8] Kaiming He, Jian Sun, and Xiaoou Tang.",
Single image haze removal using dark channel prior.,
"IEEE transactions on pat- tern analysis and machine intelligence, 33(12):2341–2353, 2010.",
"[9] Justin Johnson, Alexandre Alahi, and Li Fei-Fei.",
Perceptual losses for real-time style transfer and super-resolution.,
"In European Conference on Computer Vision, pages 694–711.",
"Springer, 2016.",
"[10] Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich.",
Style transfer by relaxed optimal transport and self-similarity.,
"In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 10051–10060, 2019.",
"[11] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan Feng.",
Aod-net: All-in-one dehazing network.,
"In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 4770–4778, 2017.",
"[12] Chongyi Li, Saeed Anwar, and Fatih Porikli.",
Underwater scene prior inspired deep underwater image and video en- hancement.,
"Pattern Recognition, 98:107038, 2020.",
"[13] Chongyi Li, Jichang Guo, Fatih Porikli, and Yanwei Pang.",
Lightennet: a convolutional neural network for weakly illu- minated image enhancement.,
"Pattern Recognition Letters, 104:15–22, 2018.",
"[14] Runde Li, Jinshan Pan, Zechao Li, and Jinhui Tang.",
Single image dehazing via conditional generative adversarial net- work.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8202–8211, 2018.",
[15] Yang Liu.,
"https://github.com/LouieYang/ deep-photo-styletransfer-tf, 2017.",
"[16] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala.",
Deep photo style transfer.,
"Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 6997–7005, 2017.",
[17] Indra Deep Mastan and Shanmuganathan Raman.,
Dcil: Deep contextual internal learning for image restoration and image retargeting.,
"In The IEEE Winter Conference on Appli- cations of Computer Vision, pages 2366–2375, 2020.",
[18] Roey Mechrez.,
"https://github.com/roimehrez/ contextualLoss, 2018.",
"[19] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor.",
The contextual loss for image transformation with non-aligned data.,
"European Conference on Computer Vision (ECCV), 2018.",
"[20] Kangfu Mei, Aiwen Jiang, Juncheng Li, and Mingwen Wang.",
Progressive feature fusion network for realistic image dehazing.,
"In Asian Conference on Computer Vision, pages 203–215.",
"Springer, 2018.",
[21] Nkolkin13.,
"https://github.com/nkolkin13/ STROTSS, 2019.",
"[22] Ekta Prashnani, Hong Cai, Yasamin Mostoﬁ, and Pradeep Sen. Pieapp: Perceptual image-error assessment through pairwise preference.",
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1808– 1817, 2018.",
"[23] Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and Ming-Hsuan Yang.",
Single image dehazing via multi- scale convolutional neural networks.,
"In European conference on computer vision, pages 154–169.",
"Springer, 2016.",
"[24] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang.",
Gated fusion network for single image dehazing.,
"In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 3253–3261, 2018.",
"[25] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani.",
Ingan: Capturing and retargeting the “dna” of a natural im- age.,
"In 2019 IEEE/CVF International Conference on Com- puter Vision (ICCV), pages 4491–4500.",
[26] Cameron Smith.,
"https://github.com/cysmith/ neural-style-tf, 2016.",
"[27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.",
Deep image prior.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446– 9454, 2018.",
"[28] Tianren Wang, Teng Zhang, and Brian C Lovell.",
Ebit: Weakly-supervised image translation with edge and bound- ary enhancement.,
"Pattern Recognition Letters, 138:534–539, 2020.",
[29] Dong Yang and Jian Sun.,
Proximal dehaze-net: A prior learning-based deep network for single image dehazing.,
"In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 702–717, 2018.",
"[30] Shibai Yin, Yibin Wang, and Yee-Hong Yang.",
A novel image-dehazing network with a parallel attention block.,
"Pat- tern Recognition, 102:107255, 2020.",
"8 [31] Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha.",
Photorealistic style transfer via wavelet transforms.,
"In International Conference on Com- puter Vision (ICCV), 2019.",
[32] Lei Zhang and Wangmeng Zuo.,
Image restoration: From sparse and low-rank priors to deep priors [lecture notes].,
"IEEE Signal Processing Magazine, 34(5):172–179, 2017.",
"Deep learning observables in computational ﬂuid dynamics Kjetil O. Lye ∗, Siddhartha Mishra †and Deep Ray ‡ December 17, 2019 Abstract Many large scale problems in computational ﬂuid dynamics such as uncertainty quantiﬁcation, Bayesian inversion, data assimilation and PDE constrained optimization are considered very chal- lenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs.",
"We propose a machine learning algorithm, based on deep artiﬁcial neu- ral networks, that predicts the underlying input parameters to observable map from a few training samples (computed realizations of this map).",
"By a judicious combination of theoretical arguments and empirical observations, we ﬁnd suitable network architectures and training hyperparameters that result in robust and eﬃcient neural network approximations of the parameters to observable map.",
"Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver.",
"Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi- Monte Carlo (QMC) methods to eﬃciently compute uncertainty propagation for nonlinear PDEs.",
"Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods.",
"Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented.",
"1 Introduction Many interesting ﬂuid ﬂows are modeled by so-called convection-diﬀusion equations [29] i.e, nonlinear partial diﬀerential equations (PDEs) of the generic form, Ut + divx(F(U)) = ν divx(D(U)∇xU), (x, t) ∈D ⊂Rds × R+, (1.1) with U ∈Rm denoting the vector of unknowns, F = (Fi)1⩽i⩽ds the ﬂux vector, D = (Dij)1⩽i,j⩽ds the diﬀusion matrix and ν a small scale parameter representing kinematic viscosity.",
"Prototypical examples for (1.1) include the compressible Euler equations of gas dynamics, shallow water equations of oceanography and the magnetohydrodynamics (MHD) equations of plasma physics.",
"These PDEs are hyperbolic systems of conservation laws i.e, special cases of (1.1) with ν = 0 and with the ﬂux Jacobian ∂U(F · n) having real eigenvalues for all normal vectors n. Another important example for (1.1) is provided by the incompressible Navier-Stokes equations, where 0 < ν << 1 and the ﬂux function is non-local on account of the divergence-free constraint on the velocity ﬁeld.",
It is well known that solutions to convection-diﬀusion equations (1.1) can be very complicated.,
These solutions might include singularities such as shock waves and contact discontinuities in the case of hy- perbolic systems of conservation laws.,
"For small values of ν, these solutions can be generically unstable, even turbulent, and contain structures with a large range of spatio-temporal scales [29].",
"∗Seminar for Applied Mathematics (SAM), D-Math ETH Z¨urich, R¨amistrasse 101, Z¨urich-8092, Switzerland †Seminar for Applied Mathematics (SAM), D-Math ETH Z¨urich, R¨amistrasse 101, Z¨urich-8092, Switzerland ‡Department of Computational & Applied Mathematics, Rice University, Houston, Texas, USA.",
1 arXiv:1903.03040v2 [physics.comp-ph] 16 Dec 2019 Numerical schemes play a key role in the study of ﬂuid ﬂows and a large variety of robust and eﬃcient numerical methods have been designed to approximate them.,
"These include (conservative) ﬁnite diﬀerence [31], ﬁnite volume [21, 24], discontinuous Galerkin (DG) ﬁnite element [24] and spectral (viscosity) methods [50].",
These methods have been extremely successful in practice and are widely used in science and engineering today.,
"The exponential increase in computational power in the last decades provides us with the opportunity to solve very challenging large scale problems in computational ﬂuid dynamics, such as uncertainty quantiﬁcation (UQ) [4, 19], (Bayesian) inverse problems [48] and real-time optimal control, design and PDE constrained (shape) optimization [6, 51].",
"In such problems, one is not always interested in computing the whole solution ﬁeld U of (1.1).",
"Rather and in analogy with experimental measurements, one is interested in computing the so-called observables (functionals or quantities of interest) for the solution U of (1.1).",
"These can be expressed in the generic form, L(U) = ∫ D×R+ ψ(x, t)g(U(x, t))dxdt.",
(1.2) Here ψ : D×R+ →R and g : Rm →R are suitable test functions.,
"Prototypical examples of such observables (functionals) are provided by body forces, such as the lift and drag in aerodynamic simulations, and by the runup height (at certain probe points) in simulations of tsunamis.",
"Moreover in practice, one is interested not just in a single value, but rather in the statistics of such observables.",
"Typical statistical quantities of interest are the mean, variance, higher-moments and prob- ability density functions (pdfs) of the observable (1.2).",
"Such statistical quantities quantify uncertainty in the solution, propagated from possible uncertainty in the underlying inputs i.e, ﬂuxes, diﬀusion co- eﬃcients, initial and boundary data.",
They might also stem from the presence of a statistical spread in design parameters such as those describing the geometry of the computational domain.,
"It is customary to represent the resulting solution ﬁeld as U = U(y), with y ∈Y ⊂Rd, a possibly very high dimensional parameter space.",
"Thus, the goal of many CFD simulations is to compute (statistics of) the so-called parameters to observable map y 7→L(U(y)).",
Calculating a single realization of this parameters to observable map might require a very expensive forward solve of (1.1) with a CFD solver and a quadrature to calculate (1.2).,
"However, in UQ, Bayesian inversion or optimal design and control, one needs to evaluate a large number of instances of this map, necessitating a very high computational cost, even on state of the art HPC systems.",
"As a concrete example, we consider a rather simple yet prototypical situation.",
We are interested in computing statistics of body forces such as lift and drag for a model two-dimensional RAE2822 airfoil [25] (see ﬁgure 2).,
"A single forward solve of the underlying compressible Euler equations on this airfoil geometry on a mesh of high resolution (see ﬁgure 2), with a state of the art high-resolution ﬁnite volume solver [43], takes approximately 7 wall-clock hours on a HPC cluster (see table 5).",
This cost scales up in three space dimensions proportionately.,
"However, a typical UQ problem such as Bayesian inversion with a state of the art MCMC algorithm, for instance the Random walk Metropolis-Hastings algorithm [48], might need upto 105 −106 such realizations, rendering even a two-dimensional Bayesian inversion infeasible !",
"This example illustrates the fact that the high-computational cost of the parameters to observable map makes problems such as forward UQ, Bayesian inversion, and optimal control and design very challenging.",
"Hence, we need numerical methods which allow ultrafast (several order of magnitude faster than state of the art) computations of the underlying parameters to observable map.",
"Machine learning, in the form of artiﬁcial neural networks (ANNs), has become extremely popular in computer science in recent years.",
"This term is applied to methods that aim to approximate functions with layers of units (neurons), connected by (aﬃne) linear operations between units and nonlinear activations within units, [20] and references therein.",
"Deep learning, i.e an artiﬁcial neural network with a large number of intermediate (hidden) layers has proven extremely successful at diverse tasks, for instance in image processing, computer vision, text and speech recognition, game intelligence and more recently in protein folding [15], see [30] and references therein for more applications of deep learning.",
A key element in deep learning is the training of tunable parameters in the underlying neural network by (approximately) minimizing suitable loss functions.,
"The resulting (non-convex) optimization problem, on a very high dimensional underlying space, is customarily solved with variants of the stochastic gradient descent method [45].",
2 Deep learning is being increasingly used in the context of numerical solution of partial diﬀerential equations.,
"Given that neural networks are very powerful universal function approximators [10, 26, 3, 36, 54], it is natural to consider the space of neural networks as an ansatz space for approximating solutions of PDEs.",
"First proposed in [28] on an underlying collocation approach, it has been successfully used recently in diﬀerent contexts in [40, 41, 35, 12, 13, 23] and references therein.",
"Given spatial and temporal locations as inputs, the (deep) neural networks, proposed by these authors, approximate the solution of the underlying PDE, by outputting function values.",
"This approach appears to work quite well for problems with high regularity (smoothness) of the underlying solutions (see [46, 23]) and/or if the solution of the underlying PDE possesses a representation formula in terms of integrals [13, 23].",
"Given their low regularity, it is unclear if solutions of (1.1), realized as functions of space and time, can be eﬃciently learned by deep neural networks [49].",
"Several papers applying deep learning techniques in the context of CFD advocate embedding deep learning modules within existing CFD codes to increase their eﬃciency [49, 42, 33].",
"In this paper, we adopt a diﬀerent approach and propose to use deep neural networks to learn and predict the input parameters to observable map.",
Our algorithm will be based on fully connected networks (multi-layer preceptrons) which output values of the observable (1.2) for diﬀerent input parameters y ∈Y ⊂Rd with d >> 1.,
"The network will be trained on data, generated from a few, say O(100), samples i.e, realizations of (1.2) with expensive CFD solvers.",
"However, the task of designing deep neural networks that will approximate the parameters to observ- able map with reasonable accuracy is far from straightforward on account of the following issues, • Approximation results for deep neural networks [54, 5, 39], stipulate a network size of O  ε −d s  for attaining an error of O(ε), with s denoting the Sobolev regularity of the underlying map.",
"However, and as explained in section 2.3, the parameters to observable map for most CFD problems is atmost Lipschitz continuous.",
"Thus, we might require unrealistically large networks for approximating the underlying maps in this context.",
"• Another relevant metric for estimating errors with deep neural network is the so-called generalization error of the trained network, (see section 2.3), that measures the errors of the network in predicting unseen data.",
The best available bounds for this error scale (inversely) with the square-root of the number of training samples.,
"However, in this context, our training samples are generated from expensive CFD simulations.",
"Hence, we might end up with large prediction errors in this data poor regime.",
"Hence, it is quite challenging to ﬁnd deep neural networks that can accurately approximate maps of low Sobolev regularity in a data poor regime.",
"Moreover, there are several hyperparameters that need to be speciﬁed in this framework, for instance size of the networks, choice of which variant of the stochastic gradient algorithm that one uses, choice of loss functions and regularizations thereof etc.",
"A priori, the prediction errors can be sensitive to these choices.",
We propose an ensemble training procedure to search the hyperparameter space systematically and identify network architectures that are eﬃcient in our context.,
"By a combination of theoretical considerations and rigorous empirical experimentation, we provide a recipe for ﬁnding appropriate deep neural networks to compute the parameters to observable map i.e, those network architectures which ensure a low generalization error, even for relatively few training samples.",
"A second aim of this paper is to employ these trained neural networks, in conjunction with (Quasi- )Monte Carlo methods to compute statistical quantities of interest, particularly the underlying probability distributions of the observable and to demonstrate that these deep learning (quasi)-Monte Carlo methods signiﬁcantly outperform the baseline algorithms.",
The rest of the paper is organized as follows: the deep learning algorithm is presented in section 2.,
The deep learning (Quasi-)Monte Carlo algorithm for forward UQ is presented in section 3 and some details for the implementation of both sets of algorithms are provided in section 4.,
"In section 5, we present numerical experiments illustrating the proposed algorithms and the results of the paper are summarized and discussed in section 6.",
"3 2 Deep learning algorithm 2.1 The problem We consider the following very general form of a parameterized convection-diﬀusion equation, ∂tU(t, x, y) + divx(F(y, U)) = ν divx(D(y, U)∇xU), ∀(t, x, y) ∈[0,T] × D(y) × Y, U(0, x, y) = U(x, y), ∀(x, y) ∈D(y) × Y, LbU(t, x, y) = Ub(t, x, y), ∀(t, x, y) ∈[0,T] × D(y) × Y (2.1) Here, Y is the parameter space and without loss of generality, we assume it to be Y = [0, 1]d, for some d ∈N.",
"The spatial domain is labeled as y →D(y) ⊂Rds and U : [0,T] × D × Y →Rm is the vector of unknowns.",
"The ﬂux vector is denoted as F = (Fi)1⩽i⩽ds : Rm × Y →Rm and D = (Dij)1⩽i,j⩽ds : Rm →Rm is the diﬀusion matrix.",
"The operator Lb is a boundary operator that imposes boundary conditions for the PDE, for instance the no-slip boundary condition for incompressible Navier-Stokes equations or characteristic boundary conditions for hyperbolic systems of conservation laws.",
"Additional conditions such as hyperbolicity for the ﬂux function F and positive-deﬁniteness of the diﬀusion matrix D might also be imposed, depending on the speciﬁc problem that is being considered.",
"We remark that the parameterized PDE (2.1) will arise in the context of (both forward and inverse) UQ, when the underlying convection-diﬀusion equation (1.1) contains uncertainties in the domain, the ﬂux and diﬀusion coeﬃcients and in the initial and boundary data.",
This input uncertainty propagates into the solution.,
"Following [4] and references therein, it is customary to model such random inputs and the resulting solution uncertainties by random ﬁelds.",
"Consequently, one can parameterize the probability space on which the random ﬁeld is deﬁned in terms of a parameter space Y ⊂Rd, for instance by expressing random ﬁelds in terms of (truncated) Karhunen-Loeve expansions.",
"By normalizing the resulting random variables, one may assume Y = [0, 1]d, with possibly a large value of the parameter dimension d. Moreover, there exists a measure, µ ∈Prob(Y), with respect to which the data from the underlying parameter space is drawn.",
"We point out that the above framework is also relevant in problems of optimal control, design and PDE constrained optimization.",
"In these problems, the parameter space Y represents the set of design or control parameters.",
"For the parameterized PDE (2.1), we aim to compute observables of the following general form, Lg(y, U) := T ∫ 0 ∫ Dy ψ(x, t)g(U(t, x, y))dxdt, for µ a.e y ∈Y.",
"(2.2) Here, ψ ∈L1 loc(Dy × (0,T)) is a test function and g ∈Cs(Rm), for s ⩾1.",
"Most interesting observables encountered in experiments, such as the lift and the drag, can be cast in this general form.",
"For ﬁxed functions ψ, g, we also deﬁne the parameters to observable map: L : y ∈Y →L(y) = Lg(y, U), (2.3) with Lg being deﬁned by (2.2).",
We also assume that there exist suitable numerical schemes for approximating the convection-diﬀusion equation (2.1) for every parameter vector y ∈Y.,
"These schemes could be of the ﬁnite diﬀerence, ﬁnite volume, DG or spectral type, depending on the problem and on the baseline CFD code.",
"Hence for any mesh parameter (grid size, time step) ∆, we are assuming that for any parameter vector y ∈Y, a high- resolution approximate solution U∆(y) ≈U(y) is available.",
"Hence, there exists an approximation to the input to observable map L of the form, L : y ∈Y →L(y) = Lg(y, U∆), (2.4) 4 with the integrals in (2.2) being approximated to high accuracy by quadratures.",
"Therefore, the original input parameters to observable map L is approximated by L∆to very high accuracy i.e, for every value of a tolerance ε > 0, there exists a ∆<< 1, such that ∥L(y) −L∆(y)∥Lp µ (Y) < ε, (2.5) for some 1 ⩽p ⩽∞and weighted norm, ∥f ∥Lp µ (Y) := ∫ Y | f (y)|pdµ(y)  1 p , for 1 ⩽p < ∞.",
The L∞ µ norm is analogously deﬁned.,
"2.2 Deep learning the parameters to observable map As stated earlier, it can be very expensive to compute the map L∆(y) for each single realization, y ∈Y, as a high-resolution CFD solver, possibly entailing a very large number of degrees of freedom, needs to be used.",
"We propose instead, to learn this map by deep neural networks.",
"This process entails the following steps, 2.2.1 Training set.",
"As is customary in supervised learning [20] and references therein, we need to generate or obtain data to train the network.",
"To this end, we select a set of parameters S = {yi}1⩽i⩽N, with each yi ∈Y.",
"The points in S can be chosen as, (i.)",
"randomly from Y, independently and identically distributed with the underlying probability distri- bution µ.",
"from a suitable set of quadrature points in Y, for instance the so-called low discrepancy sequences that arise in Quasi-Monte Carlo (QMC) quadrature algorithms [8].",
Examples of such sequences include Sobol or Halton QMC quadrature points [8].,
"We emphasize that the generation of QMC points is very cheap, particularly for Sobol or Halton sequences.",
"Moreover, these points are better spread out over the parameter space than a random selection of points and might provide more detailed information about it [8].",
"Hence, a priori QMC points might be a better choice for sampling the data.",
"One can also replace QMC points with other hierarchical algorithms such as nodes of sparse grids (Smolyak quadrature points) [7] to form the set S. Once the training parameter set S is chosen, we perform a set of high-resolution CFD simulations to obtain L∆(y), for all y ∈S.",
"As each high-resolution CFD simulation could be very expensive, we will require that N = #(S) will not be very large.",
It will typically be of at most O(100).,
Figure 1: An illustration of a (fully connected) deep neural network.,
The red neurons represent the inputs to the network and the blue neurons denote the output layer.,
They are connected by hidden layers with yellow neurons.,
Each hidden unit (neuron) is connected by aﬃne linear maps between units in diﬀerent layers and then with nonlinear (scalar) activation functions within units.,
5 2.2.2 Neural network.,
"Given an input vector, a feedforward neural network (also termed as a multi-layer perceptron), shown in ﬁgure 1, consists of layer of units (neurons) which compose of either aﬃne-linear maps between units (in successive layers) or scalar non-linear activation functions within units, culminating in an output [20].",
"In our framework, for any input vector z ∈Y, we represent an artiﬁcial neural network as, Lθ(z) = CK ⊙σ ⊙CK−1 .",
⊙σ ⊙C2 ⊙σ ⊙C1(z).,
"(2.6) Here, ⊙refers to the composition of functions and σ is a scalar (non-linear) activation function.",
A large variety of activation functions have been considered in the machine learning literature [20].,
"A very popular choice is the ReLU function, σ(z) = max(z, 0).",
"(2.7) When, z ∈Rp for some p > 1, then the output of the ReLU function in (2.7) is evaluated componentwise.",
"Moreover, for any 1 ⩽k ⩽K, we deﬁne Ckzk = Wkzk + bk, for Wk ∈Rdk+1×dk, zk ∈Rdk, bk ∈Rdk+1.",
"(2.8) For consistency of notation, we set d1 = d and dK = 1.",
"Thus in the terminology of machine learning (see also ﬁgure 1), our neural network (2.6) consists of an input layer, an output layer and (K −1) hidden layers for some 1 < K ∈N.",
The k-th hidden layer (with dk neurons) is given an input vector zk ∈Rdk and transforms it ﬁrst by an aﬃne linear map Ck (2.8) and then by a ReLU (or another) nonlinear (component wise) activation σ (2.7).,
"Although the neural network consists of composition of very elementary functions, its complexity and ability to learn very general functions arises from the interactions between large number of hidden layers [20].",
A straightforward addition shows that our network contains  d + 1 + K−1 Í k=2 dk  neurons.,
"We denote, θ = {Wk, bk}, θW = {Wk} ∀1 ⩽k ⩽K, (2.9) to be the concatenated set of (tunable) weights for our network.",
It is straightforward to check that θ ∈Θ ⊂RM with M = K−1 Õ k=1 (dk + 1)dk+1.,
"(2.10) Thus, depending on the dimensions of the input parameter vector and the number (depth) and size (width) of the hidden layers, our proposed neural network can contain a large number of weights.",
"Moreover, the neural network explicitly depends on the choice of the weight vector θ ∈Θ, justifying the notation in (2.6).",
"Although a variety of network architectures, such as convolutional neural networks or recurrent neural networks, have been proposed in the machine learning literature, [20] and references therein, we will restrict ourselves to fully connected architectures i.e, we do not a priori assume any sparsity structure for our set Θ.",
2.2.3 Loss functions and optimization.,
"For any y ∈S, we have already evaluated L∆(y) from the high-resolution CFD simulation.",
One can readily compute the output of the neural network Lθ(y) for any weight vector θ ∈Θ.,
"We deﬁne the so-called loss function or mismatch function, as J(θ) := Õ y∈S |L∆(y) −Lθ(y)|p, (2.11) for some 1 ⩽p < ∞.",
"The goal of the training process in machine learning is to ﬁnd the weight vector θ ∈Θ, for which the loss function (2.11) is minimized.",
The resulting optimization (minimization) problem might lead to 6 searching a minimum of a non-convex loss function.,
"So, it is not uncommon in machine learning [20] to regularize the minimization problem i.e we seek to ﬁnd, θ∗= arg min θ ∈Θ (J(θ) + λR(θ)) .",
"(2.12) Here, R : Θ 7→R is a regularization (penalization) term.",
A popular choice is to set R(θ) = ∥θW ∥q q for either q = 1 (to induce sparsity) or q = 2.,
The parameter 0 < λ << 1 balances the regularization term with actual loss J (2.11).,
"The above minimization problem amounts to ﬁnding a minimum of a possibly non-convex function over a subset of RM for very large M. We can approximate the solutions to this minimization problem iteratively, either by a full batch gradient descent algorithm or by a mini-batch stochastic gradient descent (SGD) algorithm.",
"A variety of SGD algorithms have been proposed in the literature and are heavily used in machine learning, see [45] for a survey.",
"A generic step in a (stochastic) gradient method is of the form: θr+1 = θr −ηr∇θ (J(θk) + λR(θk)), (2.13) with ηr being the learning rate.",
"The stochasticity arises in approximating the gradient in (2.13) by, ∇θ J(θk) ≈∇θ ©­­ « Õ y∈ˆSq |L∆(y) −Lθ(y)|pª®® ¬ , (2.14) and analogously for the gradient of the regularization term in (2.13).",
"Here ˆSq ⊂S refers to the q-th batch, with the batches being shuﬄed randomly.",
"Moreover, the SGD methods are initialized with a starting value θ0 = ¯θ ∈Θ.",
A widely used variant of the SGD method is the so-called ADAM algorithm [27].,
"For notational simplicity, we denote the (approximate, local) minimum weight vector in (2.12) as θ∗ and the underlying deep neural network Lθ∗will be our neural network surrogate for the parameters to observable map L (2.4).",
"The algorithm for computing this neural network is summarized below, Algorithm 2.1.",
Deep learning of parameters to observable map.,
"Inputs: Parameterized PDE (2.1), Observable (2.2), high-resolution numerical method for solving (2.1) and calculating (2.2).",
Goal: Find neural network Lθ∗for approximating the parameters to observable map L (2.4).,
Step 1: Choose the training set S and evaluate L∆(y) for all y ∈S by high-resolution CFD simulations.,
"Step 2: For an initial value of the weight vector θ ∈Θ, evaluate the neural network Lθ (2.6), the loss function (2.12) and its gradients to initialize the (stochastic) gradient descent algorithm.",
Step 3: Run a stochastic gradient descent algorithm of form (2.13) till an approximate local minimum θ∗of (2.12) is reached.,
The map L∗= Lθ∗is the desired neural network approximating the parameters to observable map L. 2.3 Theory 2.3.1 Approximation with deep neural networks.,
"Universal approximation theorems [3, 26, 10] prove that neural networks can approximate any continuous (in fact any measurable) function.",
"However, these universality results are not constructive.",
"More recent papers such as [54, 39] provide the following constructive approximation result: Theorem 2.2.",
"Let f ∈Ws,p  [0, 1]d, R for some 1 ⩽p ⩽∞, such that ∥f ∥W s, p ⩽1, then for every ε > 0, there exists a neural network NN( f ) of the form (2.6) with the ReLU activation function, with O  1 + log   1 ε  layers and of size (number of weights) O  ε−d s  1 + log   1 ε  such that ∥f −NN( f )∥Lp ⩽ε, 1 ⩽p ⩽∞.",
"(2.15) Given the above approximation result, it is essential to investigate regularity of the underlying pa- rameters to observeble map L in order to estimate the size of neural networks, needed to approximate them.",
"7 2.3.2 Regularity of the parameters to observable map (2.3) For simplicity, we assume that the underlying domain Dy = D ⊂BR for a. e. y ∈Y and BR is a ball of radius R. Moreover, we assume that ψ, gk ∈L∞, with gk denoting the k-th derivative of the map g in (2.2), for some k ⩾1.",
"Then, it is straightforward to obtain the following upper bound, ∥L∥W k, p(Y) ⩽C∥U∥W k, p(D×(0,T)×Y), (2.16) with the constant C depending only on the initial data, Ψ and g. Thus, regularity in the parameters to observable map is predicated on the space-time and parametric regularity of the underlying solution ﬁeld U.",
"Unfortunately, it is well-known that one cannot expect such space-time or parametric regularity for solutions of generic convection-diﬀusion equations (1.1).",
"In particular, even for the simple case of scalar one-dimensional conservation laws, at best we can show that the solution U ∈L∞((0,T); BV(D × Y)).",
"Therefore, one can show readily that ∥L∥BV(Y) ⩽C.",
(2.17) It can be rightly argued that deriving regularity estimates on L in terms of the underlying solution ﬁeld U can lead to rather pessimistic results and might overestimate possible cancellations.,
"This is indeed the case, at least for scalar conservation laws with random initial data i.e, (2.1) with m = 1, ν = 0 and F(y, U) = F(U).",
"In this case, we have the following theorem, Theorem 2.3.",
"Consider the scalar conservation law, i.e parameterized convection-diﬀusion PDE (2.1) with m = 1 and ν = 0, in domain D = Rds and time period [0,T] and assume that the initial data u0 ∈W1,∞ Y; L1(D) and support of u0 is compact.",
"Assume, furthermore that ψ ∈L∞(D × (0,T)) and g ∈W1,∞(R).",
"Then, the parameters to observable map L, deﬁned in (2.3) satisﬁes, ∥L∥W 1,∞(Y) ⩽C.",
"(2.18) for some constant C, depending on the initial data u0, ψ and g. Moreover, if the numerical solution u∆ is generated by a monotone numerical method, then the approximate parameters to observable map L∆ (2.4) satisﬁes ∥L∆∥W 1,∞(Y) ⩽C.",
(2.19) The proof of this theorem is a consequence of the L1 stability (contractivity) of the solution operator for scalar conservation laws [11].,
"In fact, for any y, y∗∈Y, a straightforward calculation using the deﬁnition (2.3) and the Lipschitz regularity of g yields, |L(y) −L(y∗)| ⩽∥ψ∥∞∥g∥Lip T ∫ 0 ∥u(t, ., y) −u(t, ., y∗)∥1dt, ⩽∥ψ∥∞∥g∥Lip T ∫ 0 ∥u0(., y) −u0(., y∗)∥1dt, by L1 −contractivity ⩽∥ψ∥∞∥g∥LipT∥u0∥W 1,∞(Y,L1(D)) ⩽C The above proof also makes it clear that bounds such as (2.18) and (2.19) will hold for systems of conservation laws and the incompressible Navier-Stokes equations as long as there is some stability of the ﬁeld with respect to the input parameter vector.",
"On the other hand, we cannot expect any such bounds on the higher parametric derivatives of L, due to the lack of diﬀerentiability of the underlying solution ﬁeld.",
"Given a bound such as (2.18) or (2.19), we can illustrate the diﬃculty of approximating the map L by considering a prototypical problem, namely that of learning the lift and the drag of the RAE2822 airfoil (see section 5.1).",
"For this problem, the underlying parameter space is six-dimensional.",
"Assuming that s = 1 in theorem 2.2 and requiring that the approximation error is at most one percent relative error i.e ε = 10−2 in (2.15), yields a neural network of size O(1012) tunable parameters and at least 6 layers.",
Such a large network is clearly unreasonable as it will be very diﬃcult to train and expensive to evaluate.,
8 2.3.3 Trained networks and generalization error It can be argued that approximation theoretic results can severely overestimate errors with trained net- works.,
"Rather, the relevant measure is the so-called generalization error i.e, once a (local) minimum θ∗ of (2.12) has been computed, we are interested in the following error, EG(θ∗) := ©­ « ∫ Y |L∆(y) −Lθ∗(y)|pdµ(y)ª® ¬ 1 p , 1 ⩽p ⩽∞.",
"(2.20) In practice, one estimates the generalization error on a so-called test set i.e, T ⊂Y, with #(T) >> N = #(S).",
"For instance, T could consist of i.i.d random points in Y, drawn from the underlying distribution µ.",
"In this case, the generalization error (2.20) can be estimated by the prediction error, EP(θ∗) := ©­ « 1 #(T) Õ y∈T |L∆(y) −Lθ∗(y)|pª® ¬ 1 p (2.21) It turns out that one can estimate the generalization error (2.20) by using tools from machine learning theory [47] as, EG(θ∗) ⩽ r U N, (2.22) with N = #(S) being the number of training samples.",
"The numerator U is typically estimated in terms of the Rademacher complexity or the Vapnik-Chervonenkis (VC) dimension of the network, see [47] and references therein for deﬁnitions and estimates.",
"Unfortunately, such bounds on U are very pessimistic in practice and over estimate the generalization error by several (tens of) orders of magnitude [2, 55, 38].",
Recent papers such as [2] provide far sharper bounds by estimating the compression i.e number of eﬀective parameters to total parameters in a network.,
"Nevertheless, even if we make a stringent requirement of U ∼O(1), we might end up with unacceptably high prediction errors of 30 −100% for the O(102 −103) training samples that we can aﬀord to generate in practice.",
"Summarizing, theoretical considerations outlined above indicate the challenges of ﬁnding deep neural networks to accurately learn maps of low regularity in a data poor regime.",
We illustrate these diﬃculties with a simple numerical example.,
2.3.4 An illustrative numerical experiment.,
"In this experiment we consider the parameter space Y = [0, 1]6 the following two maps, L1(y) = 6 Õ i=1 sin(4πyi), L2(y) = 6 Õ i=1 1 i3 sin(4πyi).",
(2.23) Both maps are inﬁnitely diﬀerentiable but with high amplitudes of the derivatives.,
We deﬁne a neural network of the form (2.6) and with the architecture speciﬁed in table 3.,
"In order to generate the training set, we select the ﬁrst 128 Sobol points in [0, 1]6 and sample the functions (2.23) at these points.",
The training is performed with ADAM algorithm and hyperparameters deﬁned in table 4 (ﬁrst line).,
"The performance of the neural network is ascertained by choosing the ﬁrst 8192 Sobol points in [0, 1]6 as the test set T. The results of evaluating the trained networks on the test set is shown in table 1 where we present the relative prediction error (2.21) as a percentage and also present the standard deviation (on the test set) of the prediction error.",
"Clearly, the errors are unacceptably high, particularly for the unscaled sum of sines L1 in (2.23).",
"Although still high, the errors reduce by an almost order of magnitude for the scaled sum of sines L2.",
This is to be expected as the derivatives for this map are smaller.,
"This example illustrates the diﬃculty to approximating (even very regular) functions, in moderate to high dimensions, by neural networks when the training set is relatively small.",
9 Map Error (Mean) in % Error (Std.),
"in % Í6 i=1 sin(4πxi) 133.95 417.83 Í6 i=1 sin(4πxi) i3 43.66 41.26 Table 1: Relative percentage Prediction errors (2.21) with p = 2, for the trained neural networks in the sum of sines experiment (2.23).",
"2.3.5 Pragmatic choice of network size We estimate network size based on the following very deep but easy to prove fact about training neural networks [55], Lemma 2.4.",
"For the map L∆and for the training set S, with #(S) = N, there exists a weight vector ˆθ ∈Θ, and a resulting neural network of the form Lθ, with Θ ⊂RM and M = O(d + N), such that the following holds, L∆(z) = Lˆθ(z), ∀z ∈S.",
"(2.24) The above lemma implies that there exists a neural network with weight vector ˆθ of size O(d + N) such that the training error deﬁned by, ET(ˆθ) := 1 #(S) Õ y∈S |L∆(y) −Lˆθ(y)|p = 0.",
"(2.25) Thus, it is reasonable to expect that a network of size O(d + N) can be trained by a gradient descent method to achieve a very low training error.",
"It is customary to monitor the generalization capacity of a trained neural network by computing a so-called validation set V ⊂Y with V ∩S = ∅, and evaluating the so-called validation loss, JV(θ) := 1 #(V) Õ y∈V ∥L∆(y) −Lθ(y)∥p p. (2.26) A low validation loss is observed to correlate with low generalization errors.",
"In order to generate the validation set, one can set aside a small proportion, say 10 −20% of the training set as the validation set.",
2.4 Hyperparameters and Ensemble training.,
"Given the theoretical challenges of determining neural networks that can provide low prediction errors in our data poor regime, a suitable choice of the hyperparameters in the training process becomes imperative.",
"To this end, we device a simple yet eﬀective ensemble training algorithm.",
"To this end, we consider the set of hyperparameters, listed in table 2.",
A few comments regarding the listed hyperparameters are in order.,
"We need to choose the number of layers and width of each layer such that the overall network size, given by (2.10) is O(N + d).",
"The exponents p, q in the loss function and regularization terms (2.12) usually take the values of 1 or 2 and the regularization parameter λ is required to be small.",
The starting value ¯θ is chosen randomly.,
"For each choice of the hyperparameters, we realize a single sample in the hyperparameter space.",
"Then, the machine learning algorithm 2.1 is run with this sample and the resulting loss function minimized.",
Further details of this procedure are provided in section 4.,
We remark that this procedure has many similarities to the active learning procedure proposed recently in [56].,
3 Uncertainty quantiﬁcation in CFD with deep learning.,
"We will employ the trained deep neural networks that approximate the parameters to observable map L∆, to eﬃciently quantify uncertainty in the underlying map.",
We are interested in computing the entire measure (probability distribution) of the observable.,
"To this end, we assume that the underlying proba- bility distribution on the input parameters to the problem (2.1) is given by µ ∈Prob(Y).",
"In the context of forward UQ, we are interested in how this initial measure is changed by the parameters to observable 10 1.",
Number of hidden layers (K −1) 2.,
Number of units in k-th layer (dk) 3.,
Exponent p in the loss function (2.11).,
Exponent q in the regularization term in (2.12) 5.,
Value of regularization parameter λ in (2.12) 6.,
Choice of optimization algorithm (optimizer) – either standard SGD or ADAM.,
Initial guess ¯θ in the SGD method (2.13).,
Table 2: Hyperparameters in the training algorithm map L (or its high-resolution numerical surrogate L∆).,
"Hence, we consider the push forward measure ˆµ∆∈Prob(R) given by ˆµ∆:= L∆#µ, ⇒ ∫ R h(z)d ˆµ∆(z) = ∫ Y h(L∆(y))dµ(y), (3.1) for any µ-measurable function h : R 7→R.",
It should be emphasized that any statistical moment of interest can be computed by integrating an appropriate test function with respect to this measure ˆµ∆.,
"For instance, letting h(w) = w in (3.1) yields the mean of the observable.",
"Similarly, the variance can be computed from the mean and letting h(w) = w2 in (3.1).",
The task of eﬃciently computing this probability distribution is signiﬁcantly harder than just estimating the mean and variance of the underlying map.,
The simplest algorithm for computing this measure ˆµ∆is to use the Monte Carlo algorithm ([8] and references therein).,
"It consists of selecting J samples i.e points {yj} with each yj ∈Y and 1 ⩽j ⩽J, that are independent and identically distributed (iid).",
"Then, the Monte Carlo approximation of the push-forward measure is given by, ˆµmc = 1 J J Õ j=1 δL∆(yj) ⇒ ∫ R h(z)d ˆµmc(z) = 1 J J Õ j=1 h  L∆(yj)  .",
(3.2) It can be shown that ˆµmc ≈ˆµ∆with an error estimate that scales (inversely) as the square root of the number of samples J.,
"Hence, the Monte Carlo algorithm can be very expensive in practice.",
3.1 Quasi-Monte Carlo (QMC) methods 3.1.1 Baseline QMC algorithm.,
"Quasi-Monte Carlo (QMC) methods are deterministic quadrature rules for computing integrals, [8] and references therein.",
"The key idea underlying QMC is to choose a (deterministic) set of points on the domain of integration, designed to achieve some measure of equidistribution.",
"Thus, the QMC algorithm is expected to approximate integrals with higher accuracy than MC methods, whose nodes are randomly chosen [8].",
"For simplicity, we set the underlying measure µ to be the scaled Lebesgue measure on Y.",
Our aim is to approximate the push forward measure ˆµ∆with the QMC algorithm.,
"To this end, we choose a set of points Jq = {yj} ⊂Y with 1 ⩽j ⩽Jq = #(Jq).",
"Then the measure ˆµ∆can be approximated by, ˆµqmc = 1 Jq Jd Õ j=1 δL∆(yj) ⇒ ∫ R h(z)d ˆµqmc(z) = 1 Jq Jq Õ j=1 h  L∆(yj)  , (3.3) for any measurable function h : Y →R.",
"We want to estimate the diﬀerence between the measures ˆµ∆and ˆµqmc and need some metric on the space of probability measures, to do so.",
One such metric is the so-called Wasserstein metric [53].,
"To deﬁne this metric, we need the following, Deﬁnition 3.1.",
"Given two measures, ν, σ ∈Prob(R), a transport plan π ∈Prob(R2) is a probability measure on the product space such that the following holds for all measurable functions F, G, ∫ R×R (F(u) + G(v))dπ(u, v) = ∫ R F(u)dν(u) + ∫ R G(v)dσ(v).",
"(3.4) 11 The set of transport plans is denoted by Π(ν, σ) ■ Then the 1-Wasserstein distance [53] between ˆµ∆and ˆµqmc is deﬁned as, W1  ˆµ∆, ˆµqmc  := inf π∈Π(ˆµ∆, ˆµqmc) ∫ R×R |u −v|dπ(u, v) (3.5) It can be proved using the Koksma-Hlawka inequality and some elementary optimal transport techniques that the error in the Wasserstein metric behaves as, W1  ˆµ∆, ˆµqmc  ∼V∆D(Jq).",
"(3.6) Here, V∆is some measure of variation of L∆.",
"It can be bounded above by V∆⩽C ∫ Y ∂d ∂y1y2 · · · yd L∆(y)dy , (3.7) the Hardy-Krause variation of the function L∆.",
This upper bound requires some regularity for the underlying map in terms of mixed partial derivatives.,
"However, it is well-known that the bound (3.7) is a large overestimate of the integration error [8].",
"The D(Jq) in (3.6) is the so-called discrepancy of the point sequence Jq, deﬁned formally in [8] and references therein.",
It measures how equally is the point sequence Jq spread out in Y.,
The whole objective in the development of quasi-Monte Carlo (QMC) methods is to design low discrepancy sequences.,
"In particular, there exists many popular QMC sequences such as Sobol, Halton or Niederreiter [8] that satisfy the following bound on discrepancy, D(Jq) ∼(log(Jq))d Jq .",
"(3.8) Thus, the integration error with QMC quadrature based on these rules behaves log-linearly with respect to the number of quadrature points.",
"For simplicity, we assume that the dimension is moderate enough to replace the logarithms in (3.6) by a suitable power leading to W1  ˆµ∆, ˆµqmc  ∼ V∆ Jq α , (3.9) for some 1/2 < α ⩽1.",
"Thus, for moderate dimensions, we see that the QMC algorithm outperforms the baseline MC algorithm.",
"Requiring that the Wasserstein distance is of size O(ε) for some tolerance ε > 0, entails choosing Jq ∼V ∆ ε 1α and leads to a cost of ˆCqmc ∼CV∆ ε 1 α , (3.10) with C being the computational cost of a single CFD forward solve.",
"This can be very expensive, partic- ularly for smaller values of α, as the cost of each forward solve is very high.",
"3.1.2 Deep learning quasi-Monte Carlo (DLQMC) In order to accelerate the baseline QMC method, we describe an algorithm that combines the QMC method with a deep neural network approximation of the underlying parameters to observable map.",
Algorithm 3.2.,
Deep learning Quasi-Monte Carlo (DLQMC).,
"Inputs: Parameterized PDE (2.1), Observables (2.2), high-resolution numerical method for solving (2.1) and calculating (2.2).",
Goal: Approximate the push-forward measure ˆµ∆.,
"12 Step 1: For JL ∈N, select JL = {yj} ⊂Y, with 1 ⩽j ⩽JL, as the ﬁrst JL points from a Quasi-Monte Carlo low-discrepancy sequence such as Sobol, Halton etc.",
"Step 2: For some N << JL, denote S = {yj} ⊂JL, with 1 ⩽j ⩽N i.e, ﬁrst N points in JL, and evaluate the map h(L∆(yj)), for all yj ∈Jd with the high-resolution CFD simulation.",
"As a matter of fact, any consecutive set of N QMC points can serve as the training set.",
"Step 3: With S as the training set, compute an optimal neural network L∗for the parameters to observable map L∆, by using algorithm 2.1.",
"Step 4: Approximate the measure ˆµ∆by ˆµ∗ qmc = 1 JL JL Õ j=1 δL∗(yj), (3.11) The complexity of the DLQMC algorithm is analyzed in the following theorem.",
Theorem 3.3.,
"For any given tolerance ε > 0 and under the assumption that the generalization error (2.20) is of size O(ε) and the baseline QMC estimator (3.3) follows the error estimate (3.9), the speedup Σdlqmc, deﬁned as the ratio of the cost of baseline QMC algorithm and the cost of the DLQMC algorithm 3.2 for approximating the push-forward measure ˆµ∆to an error of size O(ε) in the 1-Wasserstein metric, with the DLQMC algorithm 3.2, satisﬁes 1 Σdlqmc ∼N Jq + C∗ C V∗ V∆.",
"(3.12) Here, C, C∗are the computational cost of computing L∆(y) (high resolution CFD simulation) and L∗(y) (deep neural network) for any y ∈Y and V∗= V(L∗) is an estimate on the variation that arises in a QMC estimate such as (3.9).",
"We deﬁne the measure ˆµ∗= L∗#µ ∈Prob(R) and estimate, W1  ˆµ∆, ˆµ∗ qmc  ⩽W1  ˆµ∆, ˆµ∗ + W1  ˆµ∗, ˆµ∗ qmc  .",
"We claim that assuming that the generalization error EG ∼ε implies that W1   ˆµ∆, ˆµ∗ ∼ε.",
"To see this, we deﬁne a transport plan π∗∈Prob(R2) by π∗= ˆµ∆⊗ˆµ∗.",
"Then, ∫ R2 |u −v|dπ∗(u, v) = ∫ R2 |u −v|d(L∆#µ × L∗#µ)(u, v), = ∫ Y |L∆(y) −L∗(y)|dµ(y) := EG ∼ε.",
This provides an upper bound on the Wasserstein distance (3.5) and proves the claim.,
"Similarly, we use an estimate of the type (3.9) to obtain W1  ˆµ∗, ˆµ∗ qmc  ∼  V ∗ JL α .",
"Requiring this error to be O(ε) yields JL ∼V ∗ ε 1α and leads to the following estimate on the total cost of the DLQMC algorithm, ˆCdlqmc ∼CN + C∗ V∗ ε 1 α , (3.13) with C, C∗being the computational cost of a single CFD forward solve and evaluation of the deep neural network L∗, respectively.",
Dividing (3.13) with (3.10) and using Jq ∼V ∆ ε 1α leads to the speedup estimate (3.12).,
□ Remark 3.4.,
"In particular, we know that the cost C = O  ∆− 1 d+1  is very high for a small mesh size ∆.",
On the other hand the cost C∗= O(M) with M being the number of neurons (2.10) in the network.,
"This cost is expected to be much lower than C. Moreover, it is reasonable to expect that V∗≈V∆.",
"As long as we can guarantee that N << Jq, it follows from (3.12) that the speedup Σdlqmc, which measures the gain of using the deep learning based DLQMC algorithm 3.2 over the baseline Quasi-Monte Carlo method, can be quite substantial.",
"■ 13 4 Implementation of ensemble training In the following numerical experiments, we use fully connected neural networks with ReLU activation functions.",
Essential hyperparameters are listed in table 2.,
"For each experiment, we use a reference network architecture i.e number of hidden layers and width per layer, such that the total network size is O(d + N).",
This ensures consistency with the prescriptions of lemma 2.4.,
"We use two choices of the exponent p in the loss function (2.11) i.e either p = 1 or p = 2, denoted as mean absolute error (MAE) or mean square error (MSE), respectively.",
Similarly the exponent q in the regularization term (2.12) is either q = 1 or q = 2.,
"In order to keep the ensemble size tractable, we chose four diﬀerent values of the regularization parameter λ in (2.12) i.e, λ = 7.8 × 10−5, 7.8 × 10−6, 7.8 × 10−7, 0, corresponding to diﬀerent orders of magnitude for the regularization parameter.",
"For the optimizer of the loss function, we use either a standard SGD algorithm or ADAM.",
Both algorithms are used in the full batch mode.,
This is reasonable as the number of training samples are rather low in our case.,
A key hyperparameter is the starting value ¯θ in the SGD or ADAM algorithm (2.13).,
We remark that the loss function (2.12) is non-convex and we can only ensure that the gradient descent algorithms converge to a local minimum.,
"Therefore, the choice of the initial guess is quite essential as diﬀerent initial starting values might lead to convergence to local minima with very diﬀerent loss proﬁles and generalization properties.",
"Hence, it is customary in machine learning to retrain i.e, start with not one, but several starting values and run the SGD or ADAM algorithm in parallel.",
"Then, one has to select one of the resulting trained networks.",
The ideal choice would be to select the network with the best generalization error.,
"However, this quantity is not accessible with the data at hand.",
"Hence, we rely on the following surrogates for predicting the best generalization error, 1.",
Best trained network.,
"(train): We choose the network that leads to the lowest training error (2.25), once training has been terminated.",
Best validated network.,
"(val): We choose the network that leads to the lowest validation loss (2.26), once training has been terminated.",
One disadvantage of this approach is to sacriﬁce some training data for the validation set.,
Best trained network wrt mean.,
"(mean-train): We choose the network that has minimized the error in the mean of the training set i.e, Emean := 1 N N Õ j=1 L∆(yj) −1 N N Õ j=1 L∗(yj) , ∀yj ∈S.",
Best trained network wrt Wasserstein.,
"(wass-train): We choose the network that has mini- mized the error in the Wasserstein metric with respect to the training set S: Ewass := W1 1 N N Õ j=1 δL∆(yj), 1 N N Õ j=1 δL∗(yj) !",
(4.2) We note that Wasserstein metric for measures on R can be very eﬃciently computed with the Hungarian algorithm [37].,
"Another hyperparameter (property) is whether we choose randomly distributed points (Monte Carlo) or more uniformly distributed points, such as quasi-Monte Carlo (QMC) quadrature points, to select the training set S. We use both sets of points in the numerical experiments below in order to ascertain which choice is to be preferred in practice.",
Other important hyperparameters in machine learning are the learning rate in (2.13) and the number of training epochs.,
"Unless otherwise stated, we keep them ﬁxed to the values ηr == 0.01, ∀r, for both the SGD and ADAM algorithms and to 500000 epochs, respectively.",
4.1 Code All the following numerical experiments use ﬁnite volume schemes as the underlying CFD solver.,
"The machine learning and ensemble training runs are performed by a collection of Python scripts and Jupyter 14 notebooks, utilizing Keras [57] and Tensorﬂow [58] for machine learning and deep neural networks.",
"The ensemble runs over hyperparameters are parallelized as standalone processes, where each process trains the network for one choice of hyperparameters.",
"Jupyter notebooks for all the numerical experiments can be downloaded from https://github.com/kjetil-lye/learning_airfoils, under the MIT license.",
A lot of care is taken to ensure reproducability of the numerical results.,
"All plots are labelled with the git commit SHA code, that produced the plot, in the upper left corner of the plot.",
"(0,0) (1,0) SU(x) SL(x) (a) Airfoil shape (b) Primary mesh (c) Dual mesh Figure 2: The shape of the RAE airfoil and the underlying primary and voronoi dual meshes.",
"5 Numerical results In the following numerical experiments, we consider the compressible Euler equations of gas dynamics as the underlying PDE (1.1).",
"In two dimensions, these equations are Ut + Fx(U)x + Fy(U)y = 0, U = ©­­­ « ρ ρu ρv E ª®®® ¬ , Fx(U) = ©­­­ « ρu ρv2 + p ρuv (E + p)u ª®®® ¬ , Fy(U) = ©­­­ « ρv ρuv ρv2 + p (E + p)v ª®®® ¬ (5.1) where ρ, u = (u, v) and p denote the ﬂuid density, velocity and pressure, respectively.",
"The quantity E represents the total energy per unit volume E = 1 2 ρ|u|2 + p γ −1, where γ = cp/cv is the ratio of speciﬁc heats, chosen as γ = 1.4 for our simulations.",
"Additional important variables associated with the ﬂow include the speed of sound a = p γp/ρ, the Mach number M = |u|/a and the ﬂuid temperature T evaluated using the ideal gas law p = ρRT, where R is the ideal gas constant.",
5.1 Flow past an airfoil 5.1.1 Problem description.,
"We consider ﬂow past an RAE 2822 airfoil, whose shape is shown in Figure 2 (Left).",
The ﬂow past this airfoil is a benchmark problem for UQ in aerodynamics [25].,
"The upper and lower surface of the airfoil are denoted at (x, SU(x)) and (x, SL(x)), with x ∈[0, 1].",
"We consider the following parametrized (free-stream) initial conditions and perturbed airfoil shapes T∞(y) = 1 + ε1G1(y), M∞(y) = 0.729(1 + ε1G2(y)), α(y) = [2.31(1 + G3(y))]◦, p∞(y) = 1 + ε1G4(y), ˆSU(x; y) = SU(x)(1 + ε2G5(y)), ˆSL(x; y) = SL(x)(1 + ε2G6(y)), (5.2) where α is the angle of attack, the gas constant R = 1, ε1 = 0.1, ε2 = 0.2, y ∈Y = [0, 1]6 is a parameter and Gk(y) = 2yk −1 for k = 1, ..., 6.",
15 The total drag and lift experienced on the airfoil surface ˆS(y) = ˆSL(y) Ð ˆSU(y) are chosen as the observables for this experiment.,
"These are expressed as Lift = L1(y) = 1 K(y) ∫ ˆS(y) p(y)(n(y) · ψl(y))ds, Drag = L2(y) = 1 K(y) ∫ ˆS(y) p(y)(n(y) · ψd(y))ds, (5.3) where K(y) = ρ∞(y)|u∞(y)|2/2 is the free-stream kinetic energy, while ψl(y) = [−sin(α(y)), cos(α(y))] and ψd(y) = [cos(α(y)), sin(α(y))].",
Layer Width (Number of Neurons) Number of parameters Hidden Layer 1 12 84 Hidden Layer 2 12 156 Hidden Layer 3 10 130 Hidden Layer 4 12 132 Hidden Layer 5 10 130 Hidden Layer 6 12 156 Hidden Layer 7 10 130 Hidden Layer 8 10 110 Hidden Layer 9 12 132 Output Layer 1 13 1149 Table 3: Reference neural network architecture for the ﬂow past airfoil problem.,
"The domain is discretized using triangles to form the primary mesh, while the control volumes are the voronoi dual cells formed by joining the circumcenters of the triangles to the face mid-points.",
A zoomed view of the primary and dual meshes are shown in Figure 2.,
"The solutions are approximated using a second-order ﬁnite volume scheme, with the mid-point rule used as the quadrature to approximate the cell-boundary integrals.",
"The numerical ﬂux is chosen as the kinetic energy preserving entropy stable scheme [43], with the solutions reconstructed at the cell-interfaces using a linear reconstruction with the minmod limiter .",
"For details on ﬂux expression and the reconstruction procedure, we refer interested readers to [43].",
The scheme is integrated using the SSP-RK3 time integrator.,
"The contour plots for the Mach number with two diﬀerent realizations of the parameter y ∈[0, 1]6 are shown in Figure 3.",
"It should be emphasized that the stochastic perturbations in (5.2) are rather strong, representing on an average a 10 −20 percent standard deviation (over mean).",
"Hence, there is a signiﬁcant statistical spread in the resulting solutions.",
"As shown in ﬁgure 3, one of the conﬁgurations lead to a transsonic shock around the airfoil while another leads to a shock-free ﬂow around the airfoil.",
5.1.2 Generation of training data.,
"In order to generate the training, validation and test sets, we denote Qsob as the ﬁrst 1001 Sobol points on the domain [0, 1]6.",
"For each point yj ∈Qsob, the maps or (rather their numerical surrogate) L∆ 1,2(yj) is then generated from high-resolution numerical approximations U∆(y) and the corresponding lift and drag are calculated by a numerical quadrature .",
"This forms the test set T. For this numerical experiment, we take the ﬁrst 128 Sobol points to generate the training set S and the next 128 points to generate the validation set V. 5.1.3 Results of the Ensemble training procedure.",
"To set up the ensemble training procedure of the previous section, we select a fully connected network, with number of layers and size of each layer listed in table 3.",
Our network has 1149 tunable parameters and this choice is clearly consistent with the prescriptions of Lemma 2.4.,
Varying the hyperparameters as described in section 4 led to a total of 114 conﬁgurations and each was retrained 5 times with diﬀerent random initial choices of the weights and the biases.,
"Thus, we trained a total of 570 networks in parallel.",
"16 (a) Sample 1 (b) Sample 2 Figure 3: Numerical solutions (Mach number) for the ﬂow past an RAE airfoil, with two diﬀerent realizations of the parameter y.",
"For each conﬁguration, we select the retraining that minimizes the set selection criteria for the conﬁg- uration.",
"Once the network with best retraining is selected, it is evaluated on the whole test set T = Qsob and the percentage relative L2 prediction error (2.21) computed.",
"Histograms, describing the probability distribution over all hyperparameter samples for the lift and the drag are shown in ﬁgure 4.",
"From this ensemble, we choose the hyperparameter conﬁguration that led to the smallest prediction error and list them in table 4.",
"As seen from this table, there is a diﬀerence in the best performing network correspond- ing to lift and to drag.",
"However, both networks use ADAM as the optimizer and regularize with the L2-norm of the weights in the loss function (2.12).",
"We plot the training (and validation) loss, for these Obs Opt Loss L1-reg L2-reg Selection BP.",
Err mean (std) Lift ADAM MSE 0.0 7.8 × 10−6 wass-train 0.786 (0.010) Drag ADAM MAE 0.0 7.8 × 10−6 mean-train 1.847 (0.022) Table 4: The hyperparameter conﬁgurations that correspond to the best performing network (one with least mean prediction error) for the two observables of the ﬂow past airfoils.,
best performing networks with respect to the number of epochs of the ADAM algorithm in ﬁgure 5 (Top) and observe a three orders (for lift) and two orders (for drag) reduction in the underlying loss function during training.,
"From ﬁgure 5 (Bottom) and table 4, we note that the prediction errors for the best performing networks are very small i.e, less than 1% relative error for the lift and less than 2% relative error for the drag, even for the relatively small number (128) of training samples.",
This is orders of magnitude less than the sum of sines regression problem (table 1).,
These low prediction errors are particularly impressive when viewed in term of the theory presented in section 2.3.,
"A crude upper bound on the generalization error is given by q M N , with M, N being the number of parameters in the network and number of training samples respectively.",
Setting these numbers for this problem yields a relative error of approximately 300%.,
"However, our prediction errors are 100−200 times (two orders of magnitude) less, demonstrating signiﬁcant compression in the trained networks [2].",
"These results are further contextualized, when considered with reference to the computational costs, shown in table 5.",
"In this table, we present the costs of generating a single sample, the average training time (over a subset of hyperparameter conﬁgurations) and the time to evaluate a trained network.",
"Each sample is generated on the EULER high-performance cluster of ETH- 17 (a) Lift (b) Drag Figure 4: Histograms depicting distribution of the percentage relative mean L2 prediction error (2.21) (X-axis) over number of hyperparameter conﬁgurations (samples, Y-axis) for the lift and the drag in the ﬂow past airfoil problem.",
Note that the range of X-axis is diﬀerent in the plots.,
"(a) Lift, Best Performing (b) Drag, Best Performing (c) Lift, Best Performing (d) Drag, Best Performing Figure 5: Training loss (2.25) and Validation loss (2.26) (Top) and prediction errors (Ground truth vs. prediction) (Bottom) for the best performing (table 4) neural networks for the ﬂow past airfoils problem as a function of the number of epochs.",
"18 Time (in secs) Sample generation 24000 Training (Lift) 700 Evaluation (L1) 9 × 10−6 Training (Drag) 840 Evaluation (L2) 10−5 Table 5: Computational times (cost) of (single) sample generation, network training and (single) network evaluation, for the ﬂow past airfoils problem.",
Each sample was generated using a parallelized ﬁnite- volume solver on 16 Intel(R) Xeon(R) Gold 5118 @ 2.30GHz processor cores.,
The entire run took 1500 secs on the cluster and translates into a total wall clock time of 1500 × 16 = 24000 secs.,
The training and evaluation of the neural networks were all performed on a Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz machine.,
The training and evaluation times are approximations of the average runtimes over all hyperparameter conﬁgurations.,
"In this case, the training time is a very small fraction of the time to generate a single sample, let alone the whole training set whereas the computational cost of evaluating the neural networks is almost negligible as it is nine orders of magnitude less than evaluating a single training sample.",
"A priori, it is unclear if the underlying problem has some special features that makes the compression, and hence high accuracy possible.",
"In particular, learning an aﬃne map is relatively straightforward for a neural network.",
"To test this possibility, we approximate the underlying parameters to observable map, for both lift and drag, with classical linear least squares algorithm i.e, Llsq j : R6 →R be aﬃne map deﬁned by L∗,lsq j = arg min   Õ y∈S ( f (y) −Lj(y))2 | f : R6 →R is aﬃne   , j = 1, 2.",
(5.4) The resulting convex optimization problem is solved by the standard gradient descent.,
The errors with the linear least squares algorithm are shown in ﬁgure 4.,
From this ﬁgure we can see that the prediction error with the linear least squares is approximately 4% for lift and 36% for drag.,
"Hence, the best performing neural networks are approximately 5 times more eﬃcient for lift and 20 times more eﬃcient for drag, than the linear least squares algorithm.",
The gain over linear least squares implicitly measures how nonlinear the underlying map is.,
"Clearly in this example, the drag is more nonlinear than the lift and thus more diﬃcult to approximate.",
5.1.4 Network sensitivity to hyperparameters.,
"We summarize the results of the sensitivity of network performance to hyperparameters below, • Overall sensitivity.",
"The overall sensitivity to hyperparameters is depicted as histograms, represent- ing the distribution, over samples (hyperparameter conﬁgurations), shown in ﬁgure 4.",
"As seen from the ﬁgure, the prediction errors with diﬀerent hyperparameters are spread over a signiﬁcant range for each observable.",
"There are a few outliers which perform very poorly, particularly for the drag.",
"On the other hand, a large number of hyperparameter conﬁgurations concentrate around the best performing network.",
"Moreover all (most) of the hyperparameter choices led to better prediction errors for the lift (drag), than the linear least squares algorithm.",
• Choice of optimizer.,
The diﬀerence between ADAM and the standard SGD algorithm is shown in ﬁgure 6.,
"As in the Sod shock tube problem, ADAM is clearly superior to the standard SGD algorithm.",
"Moreover, there are a few outliers with the ADAM algorithm that perform very poorly for the drag.",
"We call these outliers as the bad set of ADAM and they correspond to the conﬁgurations with MSE loss function and L1 regularization, or L2 regularization, with a regularization parameter λ > 10−5, or MAE loss function without any regularization.",
"The diﬀerence between the bad set of ADAM and its complement i.e, the good set of ADAM is depicted in ﬁgure 7.",
19 • Choice of loss function.,
"The diﬀerence between L1 mean absolute error (MAE) and L2 root mean square error (MSE) i.e, exponent p = 1 or p = 2 in the loss function (2.11) is plotted in the form of a histogram over the error distributions in ﬁgure 8.",
"The diﬀerence in performance, with respect to choice of loss function is minor.",
"However, there is an outlier, based on MSE, for the drag.",
• Choice of type of regularization.,
The diﬀerence between choosing an L1 or L2 regularization term in (2.12) is shown in ﬁgure 9.,
"Again, the diﬀerences are minor but the L2 regularization seems to be slightly better, particularly for the drag.",
• Value of regularization parameter.,
The variation of network performance with respect to the value of the regularization parameter λ is shown in ﬁgure 11.,
We observe from this ﬁgure that a little amount of regularization works better than no regularization.,
"However, the variation in prediction errors wrt respect to the non-zero values of λ is minor.",
• Choice of selection criteria.,
"The distribution of prediction errors with respect to the choice of selection criteria for retraining, namely train,val,mean-train,wass-train is shown in ﬁgure 10.",
There are very minor diﬀerences between mean-train and wass-train.,
"On the other hand, both these criteria are clearly superior to the other two selection criteria.",
• Sensitivity to retrainings.,
"The variation of performance with respect to retrainings i.e, diﬀerent random initial starting values for ADAM, is provided in table 6.",
"In this table, we list the minimum, maximum, mean and standard deviation of the relative prediction error, over 5 retrainings, for the best performing networks for each observable.",
"As seen from this table, the sensitivity to retraining is rather low for the lift.",
"On the other hand, it is more signiﬁcant for the drag.",
• Variation of Network size.,
We consider the dependence of performance with respect to variation of network size by varying the width and depth for the best performing networks.,
The resulting prediction errors are shown in table 7 where a 3 × 3 matrix for the errors (rows represent width and columns depth) is tabulated.,
All the network sizes are consistent with the requirements of lemma 2.4.,
"As observed from the table, the sensitivity to network size is greater for the drag than for the lift.",
It appears that increasing the width for constant depth results in lower errors for the drag.,
"On the other hand, training clearly failed for some of the larger networks as the number of training samples appears to be not large enough to train these larger networks.",
5.1.5 Network performance with respect to number of training samples.,
All the above results were obtained with N = 128 training samples.,
"We have repeated the entire ensemble training with diﬀerent samples sizes i.e N = 32, 64, 256 also and present a summary of the results in ﬁgure 12, where we plot the mean (percentage relative) L2 prediction error with respect to the number of samples.",
"For each sample size, three diﬀerent errors are plotted, namely the minimum (maximum) prediction errors and the mean prediction error over a subset of the hyperparameter space, that omits the bad set of ADAM as the optimizer.",
"Moreover, we only consider errors with selected (optimal) retrainings in each case.",
"As seen from the ﬁgure, the prediction error decreases with sample size N as predicted by the theory.",
The decay seems of the of form  Ui N αi with α1 = 0.81 and U1 = 0.77 for the lift and α2 = 0.92 and U2 = 3.42 for the drag.,
"In both cases, the decay of error with respect to samples, is at a higher rate than that predicted by the theory in (2.22).",
"Moreover, the constants are much lower than the number of parameters in the network.",
Both facts indicate a very high degree of compressibility (at least in this possible pre-asymptotic regime) for the trained networks and explains the unexpectedly low prediction errors.,
"5.1.6 UQ with deep learning Next, we approximate the underlying probability distributions (measures) (3.1), with respect to the lift and the drag in the ﬂow past airfoils problem with the DLQMC algorithm 3.2.",
A reference measure is computed from the whole test set of 1001 samples and the corresponding histograms are shown in ﬁgure 13 to visualize the probability distributions.,
"In the same ﬁgure, we also plot corresponding histograms, 20 (a) Lift (b) Drag Figure 6: Histograms for the relative prediction errors comparing the ADAM and SGD algorithms on the ﬂow past airfoils.",
"(a) Lift (b) Drag Figure 7: Histograms of the relative prediction errors, comparing Good ADAM and bad ADAM, for the ﬂow past airfoils.",
"(a) Lift (b) Drag Figure 8: Histograms for the relative prediction errors, comparing the mean absolute error and mean square error loss functions, for the ﬂow past airfoils 21 (a) Lift (b) Drag Figure 9: Histograms for the relative prediction error, comparing L1 and L2 regularizations of the loss function, for the ﬂow past airfoils (a) Lift (b) Lift (c) Lift (d) Drag (e) Drag (f) Drag Figure 10: Network performance with respect to selection criteria for retrainings (see section 4 i.e, train,val,mean-train,wass-train for the ﬂow past airfoils problem.",
We compare wass-train with each of the other three criteria.,
Histograms for prediction error (X-axis) with number of conﬁgurations (Y-axis) are shown.,
Obs Err (min) Err (max) Err (mean) Err (std) Lift 0.786 1.336 1.004 0.187 Drag 1.847 8.016 3.841 2.332 Table 6: Sensitivity of the best performing networks (listed in table 4) to retrainings i.e starting values for ADAM for ﬂow past airfoil.,
"All errors are relative mean L2 prediction error (2.21) in percentage and we list the minimum, maximum, mean and standard deviation of the error over 5 retrainings.",
"22 Width/Depth 6 12 24 4 [1.21, 4.37] [0.83, 1.74] [1.09, 2.48] 8 [0.84, 3.66] [0.86, 2.02] [0.81, 1.69] 16 [0.80, ¬] [¬, 74.72] [¬, 5.11] Table 7: Performance of trained neural networks with respect to the variation of network size for the ﬂow past airfoils problem.",
The rows represent variation with respect to Width (number of neurons per hidden layer) and the columns represent variation with respect to Depth (number of hidden layers).,
"For each entry of the Width-Depth matrix, we tabulate the vector of relative percentage mean prediction error for the best performing networks.",
"The components of each vector represent error in [Lift, Drag] and ¬ is used to indicate that the training procedure failed for this particular conﬁguration.",
(a) Lift (b) Drag Figure 11: Variation of prediction error (Y-axis) with respect to the size of the regularization parameter λ in (2.12) (X-axis) for the ﬂow past airfoils problems.,
"The minimum and mean of prediction error (over all hyperparameter conﬁgurations) is shown (a) Lift (b) Drag Figure 12: Relative percentage mean square prediction error (2.21) (Y-axis) for the neural networks approximating the ﬂow past airfoils problem, with respect to number of training samples (X-axis).",
"For each number of training samples, we plot the mean, minimum and maximum (over hyperparameter conﬁgurations) of the prediction error.",
Only the selected (optimal) retraining is shown.,
23 computed with the best performing networks for each observable (listed in table 4) and observe very good qualitative agreement between the outputs of the DLQMC algorithm and the reference measure.,
"In order to quantify the gain in computational eﬃciency with the DLQMC algorithm over the baseline QMC algorithm in approximating probability distributions, we will compute the speedup.",
"To this end, we observe from ﬁgure 14 that the baseline QMC algorithm converges to the reference probability measure with respect to the Wasserstein distance (3.9), at a rate of α = 0.81, for both the lift and drag.",
"Next, from table 5, we see that once the training samples have been generated, the cost of training the network and performing evaluations with the trained network is essentially free.",
"Consequently, if the number of training samples is N, we compute a raw speed-up (for each observable) given by, σraw i,dlqmc := W1  ˆµi,qmc,N, ˆµi∆ W1  ˆµ∗ i,qmc, ˆµi∆ .",
"(5.5) Here for i = Lift, Drag, ˆµ∆ i is the reference measure (probability distribution) computed from the test set T, ˆµi,qmc,N is the measure (3.3) with the baseline QMC algorithm and N QMC points, and ˆµ∗ i,qmc is the measure computed with the DLQMC algorithm (3.11).",
"The real speed up is then given by, σreal i,dlqmc =  σraw i,dlqmc  1 αi , (5.6) in order to compensate for the rate of convergence of the baseline QMC algorithm.",
"In other words, our real speed up compares the costs of computing errors in the Wasserstein metric, that are of the same magnitude, with the DLQMC algorithm vis a vis the baseline QMC algorithm.",
"Henceforth, we denote σreal dlqmc as the speedup and list the speedups (for each observable) with the DLQMC algorithm for N = 128 training samples in table 8.",
We observe from this table that we obtain speedups ranging between half an order to an order of magnitude for the lift and the drag.,
The speedups for drag are slightly better than that for lift.,
"In ﬁgure 15, we plot the speedup of the DLQMC algorithm (over the baseline QMC algorithm) as a function of the number of training samples in the deep learning algorithm 2.1.",
"As seen from the ﬁgure, the best speed ups are obtained in case of 64 training samples for the lift and 128 training samples for the drag.",
This is on account of a complex interaction between the prediction error which decreases (rather fast) with the number of samples and the fact that the errors with the baseline QMC algorithm also decay with an increase in the number of samples.,
"5.1.7 Comparison with Monte Carlo algorithms We have computed a Monte Carlo approximation of the probability distributions of the lift and the drag by randomly selecting N = 128 points from the parameter domain Y = [0, 1]6 and computing the probability measure ˆµmc = 1 N ÍN j=1 δL∆(yj).",
"We compute the Wasserstein error W1(ˆµmc, ˆµ∆), with respect to the reference measure ˆµ∆and divide it with the error obtained with the DLQMC algorithm to obtain a raw speedup of the DLQMC algorithm over MC.",
"As MC errors converge as a square root of the number of samples, the real speedup of DLQMC over MC is calculated by squaring the raw speedup.",
We present this real speedup over MC in table 9 and observe from that the speedups of the DLQMC algorithm over the baseline MC algorithm are very high and amount to at least two orders of magnitude.,
This is not surprising as the baseline QMC algorithm signiﬁcantly outperforms the MC algorithm for this problem.,
"Given that the DLQMC algorithm was an order of magnitude faster than the baseline QMC algorithm, the cumulative eﬀect leads to a two orders of magnitude gain over the baseline MC algorithm.",
5.1.8 Choice of training set.,
"For the sake of comparision with our choice of Sobol points to constitute the training set S in this problem, we chose N = 128 random points in Y as the training set.",
"With this training set, we repeated the ensemble training procedure and found the best performing networks.",
The relative percentage mean L2-prediction error (2.21) (with respect to a test set of 320 random points) was computed and is presented 24 in table 10.,
"As seen from this table, the prediction errors with respect to the best performing networks are considerably (an order of magnitude) higher than the corresponding errors obtained for the QMC training points (compare with table 4).",
"Hence, at least for this problem, Sobol points are a much better choice for the training set than random points.",
An intuitive reason for this could lie in the fact that the Sobol points are better distributed in the parameter domain than random points.,
"So on an average, a network trained on them will generalize better to unseen data, than it would if trained on random points.",
(a) Lift (b) Drag Figure 13: Empirical histograms representing the probability distribution (measure) for the lift and the drag in the ﬂow past airfoils problem.,
We compare the reference histograms (computed with the test set T) and the histograms computed with the DLQMC algorithm.,
"(a) Lift (b) Drag Figure 14: Convergence of the Wasserstein distance W1  ˆµi,qmc, ˆµi∆ (Y-axis) for the of the lift and the drag for the ﬂow past airfoils, with respect to number of QMC points (X-axis) 5.2 A stochastic shock tube problem As a second numerical example, we consider a shock tube problem with the one-dimensional version of the compressible Euler equations (5.1).",
"25 Observable Speedup Lift 6.64 Drag 8.56 Table 8: Real speedups (5.6), for the lift and the drag in the ﬂow past airfoils problem, comparing DLQMC with baseline QMC algorithm.",
"Observable Speedup over MC Lift 246.02 Drag 179.54 Table 9: Real speedups, for the lift and the drag in the ﬂow past airfoils problem, comparing DLQMC with the baseline MC algorithm.",
(a) Lift (b) Drag Figure 15: Real speedups (5.6) for the DLQMC algorithm over the baseline QMC algorithm (Y-axis) with respect to number of training samples (X-axis) for the ﬂows past airfoils problem.,
The maximum and mean speed up (over all hyperparameter conﬁgurations) are shown.,
Obs Opt Loss L1-reg L2-reg Selection BP.,
"Err mean Lift SGD MAE 0.0 7.8 × 10−7 mean-train 8.487 Drag ADAM MAE 7.8 × 10−7 0.0 train 20.25 Table 10: The hyperparameter conﬁgurations that correspond the best performing network (one with least mean prediction error) for the two observables of the ﬂow past airfoils, trained on random (Monte Carlo) training set.",
"26 5.2.1 Problem description The underlying computational domain is [−5, 5] and initial conditions are prescibed in terms of left state (ρL, uL, pL) and a right state (ρR, uR, pR) at the initial discontinuity x0.",
"These states are deﬁned as, ρL = 0.75 + 0.45G1(y) ρR = 0.4 + 0.3G2(y) wL = 0.5 + 0.5G3(y) wR = 0 pL = 2.5 + 1.6G4(y) pR = 0.375 + 0.325G5(y) x0 = 0.5G6(y).",
"(5.7) y ∈Y = [0, 1]6 is a parameter and Gk(y) = 2yk −1 for k = 1, ..., 6.",
We use the Lebesgue measure on Y as the underlying measure.,
"The observables for this problem are chosen as the average integral of the density over ﬁxed intervals Lj(y) = 1 |Ij| ∫ Ij ρ(x,Tf ; y)dx, j = 1, 2, 3, (5.8) where I1 = [−1.5, −0.5], I2 = [0.8, 1.8], I3 = [2, 3] are the underlying regions of interest.",
"A priori, this problem appears to be simpler as it is only one-dimensional when compared to the two-dimensional airfoil.",
"However, the diﬃculty stems from the large variance of the initial data.",
"As seen from (5.7), the ratio of the standard deviation to the mean, for the underlying variables, is very high and can be as high as 100%.",
This is almost an order of magnitude larger than the corresponding initial stochasticity of the ﬂow past airfoil problem and we expect this large initial variance to be propagated into the solution.,
"This is indeed borne out by the results shown in ﬁgure 16, where we plot the mean (and standard deviation) of the density, both initially and at time Tf = 1.5.",
"Moreover, the initial stochastic inputs span a wide range of possible scenarios.",
"Particular choices of the parameters i.e, (y1, y2, y3, y5, y6) = (7/9, 1/24, 0, 1/32, 1/13, 0) and (y1, y2, y3, y5, y6) = (0.1611, 2/3, 0.698, 0.8213, 0.8015, 0), correspond to the classical Sod shock tube and Lax shock tube problems [16].",
"Thus, we can think of this stochastic shock tube problem as more or less generic (scaled) Riemann problem for the Euler equations.",
"In a recent paper [32], the authors have analyzed the role of the underlying variance in the general- ization errors for deep neural networks trained to regress functions.",
"In particular, high variance can lead to large generalization errors.",
"In this sense, this shock tube problem might be harder to predict with neural networks than the ﬂow past airfoil.",
(a) T = 0 (b) T = 1.5 Figure 16: Mean (± standard deviation) for the density of the stochastic shock tube problem.,
5.2.2 Generation of training data.,
"In order to generate the training and test sets, we denote Qsob as the ﬁrst 16256 Sobol points on the domain [0, 1]6.",
The training set S consists of the ﬁrst 128 of these Sobol points whereas as the test 27 set is T = Qsob.,
"For each point yj ∈Qsob, the maps or (rather their numerical surrogate) L∆ 1,2,3(yj) are generated from high-resolution numerical approximations U∆(y) obtained using a second-order ﬁnite volume scheme.",
"The domain is discretized into a uniform mesh of K = 2048 disjoint intervals, with the HLL numerical ﬂux and with the left and right cell-interface values obtained using a second-order WENO reconstruction.",
Time marching is performed using the second-order Runge-Kutta method with a CFL number of 0.475.,
"Once the ﬁnite volume solution is computed, the corresponding observables (5.8) at each y are approximated as L∆ j (y) = Í i∈Λj ρi(y) #(Λj) , Λj = {i | xi ∈Ij}, j = 1, 2, 3, (5.9) where xi is the barycenter of the cell Ωi.",
5.2.3 Results.,
We follow the ensemble training procedure outlined in section 4.,
"The network architecture, speciﬁed in table 3 is used together with the 114 hyperparameter conﬁgurations of the previous section, with 5 retrainings for each hyperparameter conﬁguration.",
The best performing networks for each observable are identiﬁed as the ones with lowest prediction error (2.21) and these networks are presented in table 11.,
"From the table, we observe very slight diﬀerences for the best performing networks for the three observables.",
"However, these networks are indeed diﬀerent from the best performing networks for the ﬂow past airfoil (compare with table 4).",
"Moreover, the predictions errors are larger than the ﬂow past airfoil case, ranging between 2% for the observable L1 to approximately 6% for the other two observables.",
This larger error is expected as the variance of the underlying maps is signiﬁcantly higher.,
"Nevertheless, a satisfactory prediction accuracy is obtained for only 128 training samples.",
Results of the ensemble training Obs Opt Loss L1-reg L2-reg Selection BP.,
Err mean L1 ADAM MSE 7.8 × 10−6 0 wass-train 2.212 L2 ADAM MSE 7.8 × 10−6 0 wass-train 6.575 L3 ADAM MSE 7.8 × 10−5 0 mean-train 5.467 Table 11: The hyperparameter conﬁgurations that correspond to the best performing network (one with least mean prediction error) for the three observables of the stochastic shock tube problem.,
procedure are depicted in the form of histograms for the prediction errors in ﬁgure 17.,
"From this ﬁgure, we observe that although there are some outliers, most of the hyperparameter conﬁgurations resulted in errors comparable to the best performing networks (see table 11).",
"Furthermore, a large majority of the hyperparameters resulted in substantially (a factor of 3 −4) smaller prediction errors, when compared to the linear least squares regression (5.4).",
A sensitivity study (not presented here) indicates very similar robustness results as for the ﬂow past the airfoil.,
"In ﬁgure 18, we plot the prediction error with respect to the number of samples and observe that the mean error (over hyperparameter conﬁgurations) decays with an exponent of 0.5 for the observables L2,3.",
"On the other hand, the decay is a bit slower for L1.",
"However, the low value of the constants still indicates signiﬁcant compression allowing us to approximate this rather intricate problem with accurate neural networks.",
"Finally, we compute the underlying probability distributions for each observable with the DLQMC algorithm 3.2.",
"In ﬁgure 19, we plot the maximum and mean real speedup (5.6), over hyperparameter conﬁgurations, for each observable, with respect to the number of training samples.",
"From this ﬁgure, we observe maximum speedups of about 16 for the observables L1,2 and 5 for the observable L3, indicating that the DLQMC algorithm is signiﬁcantly superior to the baseline QMC algorithm, even for this problem.",
"28 (a) L1 (b) L2 (c) L3 Figure 17: Histograms depicting distribution of the percentage relative mean L2 prediction error (2.21) (X- axis) over number of hyperparameter conﬁgurations (samples, Y-axis) for the observables of the stochastic shock-tube problem.",
"(a) L1 (b) L2 (c) L3 Figure 18: Relative percentage mean square prediction error (2.21) (Y-axis) for the neural networks approximating the stochastic shock tube problem, with respect to number of training samples (X-axis).",
"For each number of training samples, we plot the mean, minimum and maximum (over hyperparameter conﬁgurations) of the prediction error.",
Only the selected (optimal) retraining is shown.,
(a) L1 (b) L2 (c) L3 Figure 19: Real speedups (5.6) for the DLQMC algorithm over the baseline QMC algorithm (Y-axis) with respect to number of training samples (X-axis) for the stochastic shock tube problem.,
The maximum and mean speed up (over all hyperparameter conﬁgurations) are shown.,
"29 6 Discussion Problems in CFD such as UQ, Bayesian Inversion, optimal design etc are of the many query type i.e, solving them requires evaluating a large number of realizations of the underlying input parameters to observable map (2.3).",
Each realization involves a call to an expensive CFD solver.,
The cumulative total cost of the many-query simulation can be prohibitively expensive.,
"In this paper, we have tried to harness the power of machine learning by proposing using deep fully connected neural networks, of the form (2.6), to learn and predict the underlying parameters to observable map.",
"However, a priori, this constitutes a formidable challenge.",
"In section 2.3, we argue by a combination of theoretical considerations and numerical experiments that the crux of this challenge it to ﬁnd neural networks to learn maps of low regularity in a data poor regime.",
"We are in this regime as the many parameters to observable maps (2.3) in ﬂuid dynamics, can be at best, Lipschitz continuous.",
"Moreover, given the cost, we can only aﬀord to compute a few training samples.",
"We overcome these diﬃculties with the following novel ideas, ﬁrst, we chose to focus on learning observables rather than the full solution of the parametrized PDE (2.1).",
"Observables are more regular than ﬁelds (see section 2.3) and might have less underlying variance and are thus, easier to learn, see [32] for a more recent theoretical justiﬁcation.",
"Second, we propose using low-discrepany sequences to generate the training set S. The equi-distribution property of these points might ensure lower generalization errors than those resulting from training data generated at random points, see the forthcoming paper [34] for further details.",
"Finally, we propose an ensemble training procedure (see section 4) to systematically scan the hyperparameter space in order to winnow down the best performing networks as well as to test sensitivity of the trained networks to hyperparameter choices.",
We presented two prototypical numerical experiments for the compressible Euler equations in order to demonstrate the eﬃcacy of our approach.,
"We observed from the numerical experiments that the trained networks were indeed able to provide low prediction errors, despite being trained on very few samples.",
"Moreover, a large number of hyperparameter conﬁgurations were close in performance to the best per- forming networks, indicating a signiﬁcant amount of robustness to most hyperparameters.",
"More crucially, the obtained generalization errors followed the theoretical predictions of section 2.3 and indicating a high degree of compression, explaining why the networks generalized well.",
It is essential to state that these low prediction errors are obtained even when the cost of evaluating the neural network is several orders of magnitude lower than the full CFD solve.,
"As a concrete application of our deep learning algorithm 2.1, we combined it with a Quasi-Monte Carlo (QMC) method to propose a deep learning QMC algorithm 3.2, for performing forward UQ, by approximating the underlying probability distribution (3.1) of the observable.",
"In theorem 3.3, we prove that the DLQMC algorithm is guaranteed to out-perform the baseline QMC algorithm as long as the underlying neural network generalizes well.",
"This is indeed borne out in the numerical experiments, where we observed about an order of magnitude speedup of the DLQMC algorithm over the corresponding QMC algorithm and two orders of magnitude speedup over a Monte Carlo algorithm.",
"Thus, we have demonstrated the viability of using neural networks to learn observables and to solve challenging problems of the many-query type.",
Our approach can be compared to a standard model order reduction algorithm [52].,
"Here, generating the training data and training the networks is the oﬄine step whereas evaluating the network constitutes the online step.",
"Our results compare favorably with attempts to use Model order reduction for hyperbolic problems [1, 9], particularly when it comes to the low costs of training and evaluation.",
"On the other hand, typical MOR algorithms provide a surrogate for the solution ﬁeld, where we only predict the observable of interest in the approach presented here.",
"Our results can be extended in many directions; we can adapt this setting to learn the full solution ﬁeld, instead of the observables.",
"However, this might be harder in practice as the ﬁeld is even less regular than the observable.",
"Our approach is entirely data driven, in the sense that we do not use any speciﬁc information about the underlying parameterized PDE (2.1).",
"Thus, the approach can be readily extended to other forms of (2.1), for instance the Navier-Stokes equations or other elliptic and parabolic PDEs.",
"Finally, we plan to apply the trained neural networks in the context of Bayesian inverse problems and shape optimization under uncertainty, in forthcoming papers.",
30 Acknowledgements The research of SM is partially support by ERC Consolidator grant (CoG) NN 770880 COMANFLO.,
A large proportion of computations for this paper were performed on the ETH compute cluster EULER.,
References [1] R. Abgrall and R. Crisovan.,
Model reduction using L1-norm minimization as an application to nonlinear hyperbolic problems.,
"Methods Fluids 87 (12), 2018, 628651.",
"[2] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang.",
Stronger generalization bounds for deep nets via a compression approach.,
"In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 254-263.",
"PMLR, Jul 2018.",
[3] A. R. Barron.,
Universal approximation bounds for superpositions of a sigmoidal function.,
IEEE Trans.,
"Theory, 39(3), 930-945, 1993.",
"[4] H. Bijl, D. Lucor, S. Mishra and Ch.",
"Uncertainty quantiﬁcation in computational ﬂuid dynamics., Lecture notes in computational science and engineering 92, Springer, 2014.",
"[5] H. B¨olcskei, P. Grohs, G. Kutyniok, and P. Petersen.",
Optimal approximation with sparsely connected deep neural networks.,
"arXiv preprint, available from arXiv:1705.01714, 2017.",
[6] A. Borzi and V. Schulz.,
Computational optimization of systems governed by partial diﬀerential equations.,
SIAM (2012).,
[7] H. J. Bungartz and M. Griebel.,
Sparse grids.,
"Acta Numer., 13, 2004, 147-269.",
[8] R. E. Caﬂisch.,
Monte Carlo and quasi-Monte Carlo methods.,
"Numer., 1, 1988, 1-49.",
"[9] Crisovan, R.; Torlo, D.; Abgrall, R.; Tokareva, S. Model order reduction for parametrized nonlinear hyperbolic problems as an application to uncertainty quantiﬁcation.",
"348 (2019), 466489 [10] G. Cybenko.",
Approximations by superpositions of sigmoidal functions.,
"Approximation theory and its applications., 9 (3), 1989, 17-28 [11] Constantine M. Dafermos.",
Hyperbolic Conservation Laws in Continuum Physics (2nd Ed.).,
Springer Verlag (2005).,
[12] W. E and B. Yu.,
The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems.,
"6 (1), 2018, 1-12.",
"[13] W. E, J. Han and A. Jentzen.",
Deep learning-based numerical methods for high-dimensional parabolic partial diﬀerential equations and backward stochastic diﬀerential equations.,
"5 (4), 2017, 349-380.",
"[14] W. E, C. Ma and L. Wu.",
A priori estimates for the generalization error for two-layer neural networks.,
"ArXIV preprint, available from arXiv:1810.06397, 2018.",
[15] R. Evans et.,
De novo structure prediction with deep learning based scoring.,
"Google DeepMind working paper, 2019.",
"[16] U. S. Fjordholm, S. Mishra and E. Tadmor, Arbitrarily high-order accurate entropy stable essentially nonoscillatory schemes for systems of conservation laws.",
SIAM J. Numer.,
"Anal., 50 (2012), no.",
"[17] U. S. Fjordholm, R. K¨appeli, S. Mishra and E. Tadmor, Construction of approximate entropy measure valued solutions for hyperbolic systems of conservation laws.",
"Math., 17 (3), 2017, 763-827.",
"31 [18] U. S. Fjordholm, K. O. Lye, S. Mishra and F. R. Weber, Numerical approximation of statistical solutions of hyperbolic systems of conservation laws.",
"In preparation, 2019.",
"[19] R. Ghanem, D. Higdon and H. Owhadi (eds).",
"Handbook of uncertainty quantiﬁcation, Springer, 2016.",
[20] I. Goodfellow.,
Y. Bengio and A. Courville.,
"Deep learning, MIT Press, (2016), available from http://www.deeplearningbook.org [21] Edwige Godlewski and Pierre A. Raviart.",
Hyperbolic Systems of Conservation Laws.,
"Mathematiques et Applications, Ellipses Publ., Paris (1991).",
"[22] I. G. Graham, F. Y. Kuo, J.",
"A. Nichols, R. Scheichl, Ch.",
Schwab and I. H. Sloan.,
Quasi-Monte Carlo ﬁnite element methods for elliptic PDEs with lognormal random coeﬃcients.,
"131 (2), 2015, 329368.",
[23] J. Han.,
A. Jentzen and W. E. Solving high-dimensional partial diﬀerential equations using deep learning.,
"PNAS, 115 (34), 2018, 8505-8510.",
[24] J S. Hesthaven.,
Numerical methods for conservation laws: From analysis to algorithms.,
"SIAM, 2018.",
"[25] C. Hirsch, D. Wunsch, J. Szumbarksi, L. Laniewski-Wollk and J. pons-Prats (editors).",
"Uncertainty management for robust industrial design in aeronautics Notes on numerical ﬂuid mechanics and multidisciplinary design (140), Springer, 2018.",
"[26] K. Hornik, M. Stinchcombe, and H. White.",
Multilayer feedforward networks are universal approxi- mators.,
"Neural networks, 2(5), 359-366, 1989.",
[27] Diederik P. Kingma and Jimmy Lei Ba.,
Adam: a Method for Stochastic Optimization.,
"International Conference on Learning Representations, 1-13, 2015.",
"[28] I. E. Lagaris, A. Likas and D. I. Fotiadis.",
Artiﬁcial neural networks for solving ordinary and partial diﬀerential equations.,
"IEEE Transactions on Neural Networks, 9 (5), 1998, 987-1000.",
[29] L. D. Landau and E. M. Lipschitz.,
"Fluid Mechanics, 2nd edition, Butterworth Heinemann, 1987, 532 pp.",
"[30] Y. LeCun, Y. Bengio and G. Hinton.",
Deep learning.,
"Nature, 521, 2015, 436-444.",
[31] R. J. LeVeque.,
"Finite diﬀerence methods for ordinary and partial diﬀerential equations, steady state and time dependent problems.",
SIAM (2007).,
"[32] K. O. Lye, S. Mishra and R. Molinaro.",
A Multi-level procedure for enhancing accuracy of machine learning algorithms.,
"Preprint, 2019, available from ArXiv:1909:09448.",
[33] S. Mishra.,
"A machine learning framework for data driven acceleration of computations of diﬀerential equations, Math.",
"in Engg., 1 (1), 2018, 118-146.",
[34] S. Mishra and T. K. Rusch.,
Enhacing accuracy of machine learning algorithms by training on low-discrepancy sequences.,
"In preparation, 2020.",
"[35] T. P. Miyanawala and R. K, Jaiman.",
An eﬃcient deep learning technique for the Navier-Stokes equations: application to unsteady wake ﬂow dynamics.,
"Preprint, 2017, available from arXiv :1710.09099v2.",
[36] H. N. Mhaskar.,
"Neural networks for optimal approximation of smooth and analytic functions, Neural Comput., 8 (1), 1996, 164-177.",
[37] J. Munkres.,
"Algorithms for the Assignment and Transportation Problems Journal of the Society for Industrial and Applied Mathematics, 5 (1), 1957, 32–38.",
"32 [38] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro.",
Towards understanding the role of over-parametrization in generalization of neural networks.,
"arXiv preprint arXiv:1805.12076, 2018.",
[39] P. Petersen and F. Voightlaender.,
Optimal approximation of piecewise smooth functions using deep ReLU neural networks.,
"ArXIV preprint, available from arXiv:1709.05289, 2018.",
[40] M. Raissi and G. E. Karniadakis.,
Hidden physics models: machine learning of nonlinear partial diﬀerential equations.,
"Phys., 357, 2018, 125-141.",
"[41] M. Raissi, A. Yazdani and G. E. Karniadakis.",
Hidden ﬂuid mechanics: A Navier-Stokes informed deep learning framework for assimilating ﬂow visualization data.,
"ArXIV preprint, available from arXiv:1808.04327, 2018.",
[42] D. Ray and J.,
"S, Hesthaven.",
An artiﬁcial neural network as a troubled cell indicator.,
"Phys., 367, 2018, 166-191 [43] D. Ray, P. Chandrasekhar, U. S. Fjordholm and S. Mishra.",
"Entropy stable scheme on two-dimensional unstructured grids for Euler equations, Commun.",
"Phys., 19 (5), 2016, 1111-1140.",
[44] D. Ray.,
"Entropy-stable ﬁnite diﬀerence and ﬁnite volume schemes for compressible ﬂows, Doctoral thesis, 2017, available from https://deepray.github.io/thesis.pdf [45] S. Ruder.",
An overview of gradient descent optimization algorithms.,
"Preprint, 2017, available from arXiv.1609.04747v2.",
[46] C. Schwab and J. Zech.,
Deep learning in high dimension.,
"Technical Report 2017-57, Seminar for Applied Mathematics, ETH Z¨urich, 2017.",
[47] Shai Shalev-Shwartz and Shai Ben-David.,
Understanding machine learning: From theory to algo- rithms.,
"Cambridge university press, 2014.",
[48] A. M. Stuart.,
Inverse problems: a Bayesian perspective.,
"Acta Numerica, 19, 2010, 451-559.",
"[49] J. Tompson, K. Schlachter, P. Sprechmann and K. Perlin.",
Accelarating Eulerian ﬂuid simulation with convolutional networks.,
"Preprint, 2017.",
Available from arXiv:1607.03597v6.,
[50] L. N. Trefethen.,
"Spectral methods in MATLAB, SIAM, (2000).",
[51] F. Troltzsch.,
Optimal control of partial diﬀerential equations.,
"AMS, (2010).",
"[52] A. Quateroni, A. Manzoni and F. Negri.",
"Reduced basis methods for partial diﬀerential equations: an introduction, Springer Verlag (2015).",
[53] C. Villani.,
Topics in Optimal Transportation.,
"American Mathematical Society, Graduate Studies in Mathematics, Vol.",
58 (2013) [54] D. Yarotsky.,
Error bounds for approximations with deep ReLU networks.,
"Neural Networks, 94, 2017, 103-114 [55] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.",
Understanding deep learning requires rethinking generalization.,
"In International Conference on Learning Represen- tations, 2017.",
"[56] Linfeng Zhang, De-Ye Lin, Han Wang, Roberto Car, and Weinan E. Active Learning of Uni- formly Accurate Inter-atomic Potentials for Materials Simulation.",
"ArXIV preprint, available from arXiv:1810.11890, 2018.",
"[57] Chollet, Fran¸cois and others Keras https://keras.io, 2015.",
"[58] MartinAbadi et al, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems https: //www.tensorflow.org/, 2015.",
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 1 Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation Deep Shankar Pandey 1, Hyomin Choi 2, and Qi Yu 1 Abstract—Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty- aware.",
The resulting evidential models can quantify fine-grained uncertainty using learned evidence.,
"However, the Subjective- Logic framework constrains evidence to be non-negative, re- quiring specific activation functions whose geometric proper- ties can induce activation-dependent learning-freeze behavior—a regime where gradients become extremely small for samples mapped into low-evidence regions.",
We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics.,
"Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes.",
"Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classi- fication problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.",
"Index Terms—Evidential Deep Learning, Fine-Grained Uncer- tainty Quantification, Subjective Logic, Zero Evidence Region I.",
"INTRODUCTION With recent growth in computational capabilities, availabil- ity of large-scale data, and algorithmic improvements, Deep Learning (DL) models have found great success in many real- world applications such as speech recognition [1], machine translation [2], and computer vision [3].",
"However, these highly expressive models can easily fit the noise in the training data, leading to overconfident predictions [4].",
"This challenge is compounded in specialized domains (e.g., medicine, public safety, and military operations) where labeled data is limited and costly to obtain.",
Accurate uncertainty quantification is essential for the successful application of DL models in these domains.,
"To this end, DL models have been augmented to become uncertainty-aware [5], [6], [7].",
"However, commonly used extensions require expensive sampling operations [5], [6], which significantly increase the computational costs [8].",
"The recently developed evidential deep learning (EDL) models bring together evidential theory [9], [10] and deep neural architectures that turn a deterministic neural network 1 Deep Shankar Pandey and Qi Yu are with Rochester Institute of Technology, Rochester, NY, USA.",
"Email: {dp7972, qyuvks}@rit.edu 2 Hyomin Choi is with AI Lab, InterDigital, CA, USA.",
Email: hyomin.choi@interdigital.com 2 This work has been submitted to the IEEE for possible publication.,
"Copyright may be transferred without notice, after which this version may no longer be accessible.",
uncertainty-aware.,
"By leveraging the learned evidence, eviden- tial models are capable of quantifying fine-grained uncertainty that helps to identify the sources of ‘unknowns’.",
"Furthermore, since only lightweight modifications are introduced to existing DL architectures, additional computational costs remain mini- mal.",
"Such evidential models have been successfully extended to classification [11], regression [12], meta-learning [13], and open-set recognition [14] settings.",
"1: Cifar-100 Result Despite the attractive uncertainty quantification capacity, evidential mod- els often achieve com- petitive predictive perfor- mance only on relatively simple learning problems.",
"Their performance can de- grade on more complex, large-scale datasets even in standard classification settings.",
"As shown in Figure 1, an evidential model using ReLU activation and an evidential MSE loss [11] achieves around 36% test accuracy on CIFAR-100, nearly 40% lower than a standard softmax model.",
"In addition, many eviden- tial variants are sensitive to architecture or hyperparameter changes, requiring careful tuning for stable performance.",
The experiment section provides more details on these cases.,
"To better understand this phenomenon, we perform a the- oretical analysis of evidential learning in the standard clas- sification setting.",
"Our results identify an activation-induced learning-freeze behavior, where the interaction between non- negative evidence parameterization and specific activation functions can map samples into “zero-evidence regions” (re- gions of vanishing evidence gradients).",
"Importantly, this behavior arises within the design choices of the EDL framework itself.",
EDL couples non-negative evidence parameterization with a KL-based prior that intentionally promotes high epistemic uncertainty at class boundaries and in regions far from the training distribution—an effect that helps prevent overconfident errors.,
Activation functions and regularizers determine how evidence accumulates under this framework.,
"Our analysis shows that commonly used non- negative activations can inadvertently create “zero-evidence” regions where gradients become extremely small, making evidence updates for nearby samples ineffective.",
"More specifically, EDL models acquire limited new ev- idence from samples mapped into these low-evidence re- gions because the corresponding evidence gradients approach zero.",
"Moreover, the learning signal decreases proportionally arXiv:2512.23753v1 [cs.LG] 27 Dec 2025 JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 2 Fig.",
2: Intuitive visualization of a zero-evidence region for evidential models in the evidence space for binary classifica- tion.,
"Samples mapped into such regions have extremely small gradients, leading to limited model update during training.",
"GRED encourages larger gradients for ‘zero-evidence’ samples, enabling consistent learning across samples.",
"as samples are mapped closer to the zero-evidence region, irrespective of supervised information.",
This activation-induced stagnation is illustrated in Figure 2 (with detailed discussion in Section IV-C).,
We analyze several existing evidential variants and observe this behav- ior consistently across models and settings.,
"Motivated by these insights, we introduce a novel Generalized Regularized Evidential model (GRED) that employs positive evidence regularization to encourage evidence accumulation even in low-evidence regimes.",
A preliminary version of this work has been published as a conference paper [15].,
"Improving on RED, we propose gen- eralized regularized evidential models that mitigate learning stagnation across a family of evidential activations (Section IV).",
We theoretically show the effectiveness of the correct- evidence regularization (Theorem 3) and provide expanded analysis of evidential losses (Section IX).,
"We further extend GRED to challenging few-shot classification and blind face restoration tasks, and carry out detailed uncertainty analysis, and demonstrate the broader utility of evidential uncertainty.",
"Our major contributions can be summarized as follows: • We identify an activation-induced learning-freeze behavior in evidential models, wherein data samples mapped to “zero- evidence” regions receive vanishing evidence gradients.",
"For these samples, the learning signal decreases proportionally as they are mapped closer to the zero-evidence region in the evidence space.",
• We theoretically show that evidential models with exp acti- vation produce stronger gradients near low-evidence regions compared to other activations.,
"• We introduce a generalized evidence regularization strategy that encourages evidence updates across activation regimes, enabling more consistent learning from all samples.",
"• We conduct extensive experiments across multiple settings including 4 benchmark classification tasks, few-shot classi- fication, and blind face restoration, validating the developed theory and demonstrating the effectiveness of our approach.",
RELATED WORKS a) Uncertainty Quantification in Deep Learning: Accu- rate quantification of predictive uncertainty is essential for de- velopment of trustworthy Deep Learning (DL) models.,
"To this end, DL models have been augmented to become uncertainty- aware [16] using a variety of approaches such as deep ensem- ble–based approaches [8], [7], Bayesian neural networks [17], [5], [6], second-order distribution–based approaches [18], [19], [20], and credal-set–based approaches [21], [22], [23], [24], [25], [26].",
"Deep ensemble techniques [8], [7] construct an ensemble of neural networks and the agreement/disagreement across the ensemble components is used to quantify different uncertainties.",
"Ensemble-based methods significantly increase the number of model parameters, and are computationally expensive at both training and test times.",
Bayesian neural networks [5][6][17] have been developed that consider a Bayesian formalism to quantify different un- certainties.,
"For instance, Blundell et al.",
"[6] use Bayes-by- backdrop to learn a distribution over neural network param- eters, whereas Gal et al.",
[5] enable dropout during inference phase to obtain predictive uncertainty.,
Bayesian methods resort to some form of approximation to address the intractability issue in marginalization of latent variables.,
"Moreover, these methods are also computationally expensive as they require sampling for uncertainty quantification.",
"Towards accurate UQ, Credal Bayesian deep learning (CBDL) models [21], [22], [27] have also been developed that use concepts from the imprecise probability theory [28] for comprehensive uncertainty quantification.",
"These models aim to approximate the credal set of posterior distributions during training [21], based on which, the models infer the credal set of predictive distributions during inference.",
"CBDL models are more robust to prior/likelihood distribution misspecification compared to BNN, have been effectively applied to continual learning settings [29], and provide more robust epistemic uncertainty quantification capabilities [22] especially in sit- uations of prior/likelihood misspecification.",
"However, these models, due to the use of credal sets that require reasoning over multiple distributions, are computationally expensive compared to BNN, limiting their usage.",
"In contrast, Evidential DL approaches only require a single forward pass through the deep learning models to quantify uncertainty, making them computationally lighter compared to BNN and CBDL models.",
"b) Evidential Deep Learning: Uncertainty in be- lief/evidence theory [30], [31] and its neural extensions [32], [33] have been studied under Dempster–Shafer Theory [34], fuzzy logic [35], [36], and Subjective Logic [10].",
"Eviden- tial deep learning is closely related to second-order distri- bution–based uncertainty quantification [18], [19], [20] and frequently employs Subjective Logic [10] for uncertainty rea- soning.",
Evidential models introduce a conjugate higher-order evidential prior for the likelihood distribution that enables the model to capture the fine-grained uncertainties.,
"For instance, JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 3 Dirichlet prior is introduced over the multinomial likelihood for evidential classification [14], [37], [38], and the normal- inverse-gamma prior is introduced over the Gaussian likeli- hood [12], [39] for the evidential regression models.",
The robustness [40] and calibration [41] of evidential mod- els have been extensively studied.,
"Usually, these models are trained with evidential losses and heuristically designed regu- larizers to guide uncertainty behavior [13], [42].",
"Some variants incorporate out-of-distribution (OOD) data into training [43], [44], but this assumption may not hold in real-world settings.",
A recent survey [45] provides a comprehensive overview.,
Recent works have examined the reliability of uncertainty measures in EDL.,
Wimmer et al.,
[46] and Bengs et al.,
[47] highlight cases where the decomposition of aleatoric and epis- temic uncertainty may be inconsistent.,
Jurgens et al.,
[48] study epistemic uncertainty behavior and report situations where evidential uncertainty can be misleading.,
Shen et al.,
"[49] extend EDL by explicitly modeling additional forms of model uncertainty, while second-order approaches [18], [20] propose theoretically grounded uncertainty measures addressing limi- tations in earlier formulations.",
Orthogonal to these existing evidential works—which pri- marily investigate the interpretation or reliability of evidential uncertainty—we study the training dynamics of evidential models.,
"Specifically, we characterize the activation-induced learning-freeze behavior: certain non-negative evidence ac- tivations can map samples into low-evidence regions where gradients become extremely small, limiting effective evidence updates.",
We introduce a theoretically justified regularization strategy that mitigates this stagnation and enables more con- sistent evidence accumulation across activation regimes.,
"In this work, we focus on evidential classification models and consider settings without access to the out-of-distribution data during training, improving applicability in real-world scenarios.",
LEARNING IN EVIDENTIAL MODELS We first describe learning in standard classification models.,
We then describe evidential deep learning model basics for classification.,
"Afterward, we analyze the gradient dynamics of evidential training to characterize the activation-induced learning-freeze behavior that arises when certain non-negative evidence activations map samples into low-evidence regions.",
Standard Classification Models Standard classification models use a softmax transformation on the output from the neural network FΘ for input x to obtain the class probabilities in a K-class classification prob- lem.,
Such models are trained with cross-entropy-based losses.,
These models have achieved state-of-the-art performance on many benchmark problems.,
a) Gradient Analysis: Consider a standard cross-entropy trained model for K−class classification.,
"Let the overall network be represented by fΘ(·), and let o = fΘ(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y.",
The output at node i after the softmax layer is given by smi = exp(oi) PK k=1 exp(ok) = exp(oi) Sce (1) where Sce = PK k=1 exp(ok).,
"For a given sample (x, y), the cross-entropy loss (Lce), and the gradient of this loss with respect to the pre-softmax values o are given by Lce = − K X k=1 yk log(smk) = log Sce − K X k=1 ykok (2) gradk = ∂Lce ∂ok =  1 Sce ∂Sce ∂ok −yk  = exp(ok) Sce −yk (3) = smk −yk (4) The gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as 0 ≤smk ≤1 and yk ∈{0, 1}.",
The model is updated using gradient descent-based optimization objectives.,
"For input x, the neural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y̸=gt = 0.",
"When yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value.",
"Only when smi = 0, the gradient is zero, and the model is not updated.",
"In all other cases when smi ̸= 0, there is a non-zero gradient dependent on smi, and the model is updated to minimize the smi as expected.",
"When yi = 1, the gradient signal is gradi = smi −1 and the model optimizes the parameters to minimize this value.",
"As smi ∈[0, 1], only when the model outputs a large logit on i (corresponding to the ground-truth class) and small logit for all other nodes, smi = 1, the gradient is zero, and the model is not updated.",
"For the cases when smi < 1, there is a non-zero gradient dependent on smi and the model is updated to maximize the smi=gt and minimize all other smi̸=gt as expected.",
The gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables learning from all the training samples.,
The above gradient analysis shows that standard classi- fication models trained with cross-entropy-based loss can effectively learn from all the training samples.,
"Nevertheless, these models lack a systematic mechanism to quantify different sources of uncertainty, a highly desired property in many real- world problems.",
B. Evidential Deep Learning Models for Classification Fig.,
3: Graphical model for Evidential Models Evidential deep learning models for classification formulate model training as an evidence acquisition process and consider a higher-order Dirichlet prior Dir(p|α) over the predictive Multinomial distribution Mult(y|p).,
"Different from a stan- dard Bayesian formulation which optimizes Type-II Maximum JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 4 Likelihood to learn the Dirichlet hyperparameter [50], eviden- tial models directly predict α using data features x and then generate the prediction y by marginalizing the Multinomial parameter p. Figure 3 describes this generative process.",
Such higher-order prior enables the model to systematically quantify different sources of uncertainty.,
"It is worth noting that the uncertainty behavior of vanilla EDL [11] in low-evidence or boundary regions is an intentional design choice arising from its Dirichlet prior and KL-based regularization, which promote conservative (high epistemic) uncertainty away from training data.",
"In evidential models, the Softmax in the standard neural networks for classification is replaced by a non-negative activation function A, where A(x) ≥0 ∀x ∈[−∞, ∞], such that for input x, the neural network model FΘ with parameters Θ can output evidence e for different classes.",
"Dirichlet parameter α is evaluated as α = e + 1 to ensure α ≥1, where e = A(FΘ(x)) = A(o), which quantify fine- grained uncertainties in addition to the prediction y for the input x.",
"Then, Dirichlet strength, S, for K−class classification problem is computed by S = K X k=1 (ek + 1) (5) The activation function A(·) assumes three common forms to transform the output into evidence: (1) ReLU(·) = max(0, ·), (2) SoftPlus(·) = log(1 + exp(·)), and (3) exp(·).",
Evidential models assign input sample to that class for which the output evidence is greatest.,
"Moreover, they quantify the confidence in the prediction for K−class classification problem through vacuity ν = K S (i.e., measure of lack of confidence in the prediction.)",
"For any training sample (x, y), the evidential models aim to maximize the evidence for the correct class, minimize the evidence for the incorrect classes, and output accurate confidence.",
"To this end, three variants of evidential loss functions have been proposed [11]: 1) Bayes risk with the sum of squares loss, 2) Bayes risk with cross- entropy loss, and 3) Type-II Maximum Likelihood loss.",
Please refer to Eq.,
"29, and Eq.",
30 in the Appendix for the specific forms of these losses.,
"Additionally, incorrect evidence regularization terms are introduced to guide the model to output low evidence for classes other than the ground truth class (See Appendix VIII for discussion on the regularization).",
"With evidential training, evidential deep learning models are expected to output high evidence for the correct class, low evidence for all other classes, and output high vacuity for unseen/OOD samples.",
"C. Theoretical Analysis of Evidential Classification Models To identify the underlying reason that causes the perfor- mance gap of evidential models as described earlier, we consider a K−class classification problem and a representative evidential model trained using Bayes risk with the sum of squares loss given in Eq.",
We first provide important definitions that are critical for our theoretical analysis.,
Definition 1 (Zero Evidence Sample and Zero Evidence Region).,
A zero evidence sample is a data sample for which the model outputs zero evidence for all classes.,
A zero evidence region is the area in the evidence space that contains all the zero evidence samples.,
"For a reasonable evidential model, novel data samples not yet seen during training, difficult data samples, and Out-Of- Distribution (OOD) samples should become zero evidence samples and should be mapped in the zero evidence region.",
"Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero.",
Consider an input x with one-hot ground truth label y.,
"Let the ground truth class index be gt, i.e., ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0.",
"Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively.",
"In this evidential model, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (6) Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule: ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok (7) Based on the actual form of A, we have three cases: • Case I: ReLU(·) to transform logits to evidence ek = ReLU(ok) =⇒∂ek ∂ok = ( 1 if ok > 0 0 otherwise (8) For a zero evidence sample, the logits ok satisfy the rela- tionship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 • Case II: SoftPlus(·) to transform logits to evidence ek = Softplus(ok) =⇒∂ek ∂ok = Sigmoid(ok) (9) For a zero evidence sample, the logits ok →−∞=⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"• Case III: exp(·) to transform logits to evidence ek = exp(ok) =⇒ ∂ek ∂ok = exp(ok) = αk −1 (10) For a zero evidence sample, αk →1 =⇒ ∂ek ∂ok →0.",
"So, for all three instances of the evidential activation, ∂ek ∂ok → 0 as ek →0 & ek = 0 =⇒ ∂ek ∂ok = 0.",
"Moreover, there is no term in the first part of the loss gradient in Eq.",
7 to counterbalance these zero-approaching gradients.,
"As the training sample is mapped to the region near the zero evidence region (i.e., ek →0), the evidence gradients ( ∂ek ∂ok ) approach to zero (i.e., ∂ek ∂ok →0), and the loss gradient (a.k.a., the learning JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 5 signal) also approaches zero (i.e., ∂LMSE(x,y) ∂ok →0) irrespective of the supervised learning signal.",
"For zero evidence training samples, for any node k, ∂LMSE(x, y) ∂ok = 0 (11) For zero evidence training samples, since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
This shows that the evidential models fail to learn from a zero evidence data sample.,
"For completeness, we present the detailed proof of the evidential models trained using Bayes risk with the sum of squares error along with other evidential losses in Appendix VII, and impact of incorrect evidence regularization in Ap- pendix VIII.",
"a) Remark: When an evidential model outputs zero evidence for all classes (i.e., a data sample that the model has never seen and for which the model accurately outputs “I don’t know”, i.e., ek = 0 ∀k ∈[1, K]), the gradients of standard evidential losses vanish, and the supervised in- formation in such samples cannot contribute to parameter updates.",
"Such samples may naturally appear during training (for example, novel, ambiguous, or OOD-like inputs), but the model receives no learning signal from them because they lie in the zero evidence region.",
"Similarly, samples mapped near the zero evidence region receive significantly diminished gradients: their learning signal becomes much weaker than that of samples with higher evidence, regardless of the strength of the supervised label.",
Corollary 1.,
Incorrect evidence regularization does not pro- vide a learning signal for zero evidence samples and therefore cannot induce parameter updates for such samples.,
"Intuitively, the incorrect evidence regularization encourages the model to reduce evidence for non–ground-truth classes, but it does not increase the evidence of the ground-truth class.",
"As a result, its gradients do not affect the zero-evidence condition.",
"Consequently, the regularization can move samples closer to the “zero evidence region” in the evidence space, but it cannot create a non-zero gradient for samples already mapped to this region.",
"Thus, incorrect evidence regularization does not supply the missing gradient needed for zero-evidence samples to contribute to learning.",
"For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential activation function leads to a larger gradient update on the model parameters than SoftPlus and ReLu.",
Consider an evidential loss L (formally defined in Eq.,
"29, and Eq.",
30) is used to train the evidential model.,
"Let o, e ∈RK denote the neural network output vector before applying the activation A, and the evidence vector, respectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, ∀k ∈[K], we have: 1.",
ReLU: ∂L ∂w  ReLU = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w = 0 (see Eq.,
SoftPlus: ∂L ∂w  SoftPlus = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w (see Eq.,
9) (13) = X k ∂L ∂ek ∂ok ∂w Sigmoid(ok) (14) 3.,
Exponential: ∂L ∂w  Exp = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w (see Eq.,
"10) (15) = X k ∂L ∂ek ∂ok ∂w exp(ok) (16) = X k ∂L ∂ek ∂ok ∂w {[1 + exp(ok)]Sigmoid(ok)} (17) Thus, we have  ∂L ∂w  Exp ≥  ∂L ∂w  SoftPlus ≥  ∂L ∂w  ReLU, which implies that A = exp leads to a larger update to the network than both SoftPlus and ReLU.",
This completes the proof.,
"b) Remark: The above proof implies that the training of evidential models is most effective with the exponential acti- vation function as it has larger gradient update (and effectively stronger learning signal) for points near the zero evidence region i.e., for points with ok ≤0 ∀k ∈[0, K].",
We now carry out additional analysis with a representative evidential model in K−class classification problem.,
"We con- sider an input x with one-hot label of y, PK k=1 yk = 1.",
"For this evidential framework, the Type-II Maximum Likelihood loss (LLog(x, y)) and its gradient with the logits o (Eq.",
"41) are given by LLog(x, y) = log S − K X k=1 yk log αk (18) gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk ∂ek ∂ok (19) Case I and II: ReLU(·) and SoftPlus(·) to transform logits to evidence.",
"• Zero evidence region: For ReLU(·) based evidential models, if the logits value for class k i.e., ok is negative, then the corresponding evidence for class k i.e., ek = 0, ∂ek ∂ok = 0 & gradk = ∂LLog(x,y) ∂ok = 0.",
"So, there is no update to the model through the nodes that output negative logits value.",
"In the case of SoftPlus(·) based evidential models, there is no update to the model when training samples lie in zero evidence regions.",
This is possible in the condition of ok →−∞.,
"In other cases, there will be some small finite small update in the accurate direction from the gradient.",
"• Range of gradients: The range of gradients for both ReLU(·) and SoftPlus(·) based evidential models are JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 6 identical.",
"Considering the gradient for the ground truth node i.e., yk = 1, the range of gradients is [ 1 K −1, 0].",
"For all other nodes other than the ground truth node i.e., yk = 0, the range of gradients is [0, 1 K ].",
"So, for classification problems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth class will be bounded in a small range and is likely to be very small.",
"• High incorrect evidence region: If the evidence for class k is very large i.e., ek →∞, then for ReLU(·), ∂ek ok = 1, and for SoftPlus(·), ∂ek ok = Sigmoid(ok) →1, 1 αk = 1 ek+1 →0, 1 S →0, & gradk = ∂LLog(x,y) ∂ok →0.",
"For large positive model evidence, there is no update to the corre- sponding node of the neural network.",
"The evidence can be further broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect evidence (corresponding to the evidence for any other class other than the ground truth class).",
"When the correct class evidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which is desired.",
"When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.",
"However, the evidential models with ReLU and SoftPlus fail to minimize incorrect evidence when the incorrect evidence value is large.",
These necessities the need for incorrect evidence regularization terms.,
"Case III: Exponential, exp(·), to transform logits to evi- dence.",
Considering Eq.,
"33, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (αk −1) (20) • Zero evidence region: In case of exp(·) based evidential models, except in the extreme cases of αk →∞, there will be some signal to guide the model.",
"In cases outside the zero evidence region (i.e., outside αk →∞), there will be some finite small update in the accurate direction from the gradient.",
"Moreover, for same evidence values, the gradient of exp based model is larger than the SoftPlus based evidential model by a factor of 1+exp(ok).",
"Compared to SoftPlus models, the larger gradient is expected to help the model learn faster in low- evidence regions.",
"• Range of gradients: For the ground truth node, i.e., , yk = 1, the range of gradients is [−1, 0].",
"For all nodes other than the ground truth node i.e.,, yk = 0, the range of gradients is [0, 1].",
"Thus, the gradients are expected to be more expressive and accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models.",
"• High evidence region: If the evidence for class k is very high i.e., ek →∞, then αk −1 ≈αk and gradk = smk −yk.",
"In other words, the model’s gradient updates become identical to the standard classification model (see Section III-A) without any learning issues.",
"Due to smaller zero evidence region, more expressive gra- dients, and no issue of learning in high incorrect evidence region, the exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based evidential models.",
"As can be seen, the ReLU based activation completely destroys all the information in the negative logits and has the largest region in evidence space in which training data have zero evidence.",
"SoftPlus activation improves over the ReLU, and compared to ReLU, has a smaller region in evidence space where training data have zero evidence.",
"However, SoftPlus based evidential models fail to correct the acquired knowledge when the model has strong wrong evidence.",
"Moreover, these models are likely to suffer from the vanishing gradients problem when the number of classes increases (i.e., classification problem becomes more challeng- ing).",
"Finally, exponential activation has the smallest zero evidence region in the evidence space without suffering from the issues of SoftPlus based evidential models.",
"Still, the learning signal for all evidential models reduces proportionally as the training data points become closer to zero evidence region, and the learning signal becomes zero for samples in zero evidence region of the evidence space irrespective of the supervised signal in the training data point.",
This problem exists for all the activation functions.,
"AVOIDING ZERO EVIDENCE REGIONS THROUGH CORRECT EVIDENCE REGULARIZATION We introduce a generalized correct evidence regularization for evidential classification models that provides a meaningful gradient for samples in low- or zero-evidence regions, while leaving standard evidential losses unchanged for high-evidence samples.",
"Correct Evidence Regularization As shown in Section III-A, cross-entropy–trained softmax models naturally provide a strong gradient signal for the ground-truth class when its logit is highly negative.",
"In evi- dential models, however, the gradients produced by standard evidential activations vanish as the evidence approaches zero.",
"To encourage a learning behavior closer to that of cross- entropy models in these regions, we propose introducing a regularization term Lcor(x, y) that satisfies ∂Lcor(x, y) ∂ogt = −1 when K X k=1 ek = 0, ∂Lcor(x, y) ∂ogt →−1 as K X k=1 ek →0.",
"Motivated by this analysis, we propose the following vacuity-guided regularization: Lcor(x, y) = −λcorogt, (21) where λcor = ν = K S denotes the vacuity produced by the evi- dential model.",
The vacuity behavior is characterized as λcor = 1 as PK k=1 ek = 0 & λcor →1 as PK k=1 ek →0.,
This choice ensures that the regularization magnitude ap- proaches 1 as the total evidence PK k=1 ek approaches zero.,
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 7 To allow the evidential losses to dominate learning for high- evidence samples, we introduce an evidence-dependent indi- cator function in the loss: Lcor(x, y) = ( −λcorogt if ogt < 0, 0 otherwise = −I λcorogt, (22) where I = 1(ogt < 0) disables the regularization once the model assigns sufficiently positive evidence to the ground-truth class.",
"The term is active primarily in low-evidence regions and diminishes as the sample moves away from the zero- evidence region ,with the greatest magnitude achieved in the zero evidence region.",
"Thus, it has the effect of pushing the samples away from the zero evidence region.",
This key property is summarized in the following theorem.,
Correct evidence regularization provides a non- vanishing gradient signal for training samples mapped to zero- evidence regions.,
The regularization depends only on the logit ogt of the ground-truth class.,
"Hence, ∂Lcor(x, y) ∂ok k̸=gt = 0, and non–ground-truth nodes receive no update.",
"Because the indicator I = 1(ogt < 0) activates the term in low-evidence regions, we focus on this case.",
"For ygt = 1, the regularization and its gradient become Lcor(x, y) = −λcorogt, (23) ∂Lcor(x, y) ∂ogt = −λcor.",
"(24) The vacuity λcor = K S lies in [0, 1] and achieves its maximum of 1 when the evidence is zero.",
"Thus, samples mapped to zero-evidence regions receive a gradient of −1, provide a meaningful update signal that promotes increased evidence for the ground-truth class.",
"As evidence grows, vacuity decreases, and the influence of the regularization diminishes, allowing the standard evidential losses to guide learning for high-evidence samples.",
"Hence, the proposed regularization ensures non-zero gradients for zero-evidence samples and restores gradient flow in regions where standard evidential losses alone produce vanishing updates.",
B. Generalized Regularized Evidential Models The correct evidence regularization term in Eq.,
21 is ex- pressed in terms of the logit.,
"When SoftPlus or Exponential activations are used, the regularization can also be written di- rectly in terms of the output evidence because these activations are invertible.",
"This is not the case for ReLU, whose non- invertibility prevents an evidence-based formulation.",
"Theo- rem 2 indicates that the Exponential activation provides strong gradients for samples near the zero-evidence region, but its output may grow rapidly for large positive logits, making optimization more difficult.",
"To balance these behaviors we introduce a novel evidential activation function, referred to (a) Evidence-Logit Trend (b) Gradient Plot Fig.",
4: Output Evidence and Gradient plot of different eviden- tial activations for different Logit values as Shifted Exponential Linear Unit (SELU) that generalizes existing activation functions with some appealing properties: ei = SELU(oi) = ( oi + 1 if oi > 0 exp(oi) otherwise (25) The activation behaves similarly to the Exponential activa- tion function for negative logits and has the largest gradient (compared to SoftPlus and ReLU) for samples close to the zero evidence region.,
"For positive logits, the activation behaves linearly, and the evidence value does not explode as the logit value increases.",
The output evidence and the corre- sponding gradient plots from different activation functions are visualized in Figure 4.,
We present evidence-based formulation of the correct evidence regularization for different activation functions in Table I.,
"TABLE I: Evidence-Based Regularization Activation Evidence Regularization (Lcor) ReLU ei = max(0, oi) N/A SoftPlus ei = log(exp(oi) + 1) −Iλcor log(exp(egt) −1) SELU see Eq.",
25 −Iλcor log(egt) Exponential ei = exp(oi) −Iλcor log(egt) C. Evidential Model Training We formulate the overall objective used to train the proposed Generalized Regularized evidential model (RED).,
"The model is trained to increase evidence for the ground-truth class, reduce evidence for incorrect classes, and ensure that samples in low- or zero-evidence regions receive a meaningful learning signal.",
"The combined loss is L(x, y) = Levid(x, y) + η1Linc(x, y) + Lcor(x, y) (26) where Levid(x, y) is the loss based on the evidential frame- work given by Eq.",
"29 (See Appendix VII), Linc(x, y) represents the incorrect evidence regularization (See Appendix Section VIII), Lcor(x, y) represents the pro- posed novel correct evidence regularization term in Eq.",
"22, and η1 = λ1 × min(1.0, epoch index/10) controls the impact of incorrect evidence regularization to the overall model training.",
"In this work, we consider the forward-KL-based incorrect evidence regularization given in Eq.",
49 based on [11].,
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 8 Figure 2 provides an intuitive view of learning in the evidence space.",
"Ideally, samples from Class 1 should lie in the blue region (high evidence for Class 1), samples from Class 2 in the green region, and unseen or OOD samples in the zero- evidence region.",
"Training with Levid and Linc encourages these behaviors; however, when a sample is mapped to the zero-evidence region, the gradients of standard evidential losses vanish.",
"Thus, although the true label is available, model does not update its knowledge when training such samples.",
"Samples with low correct evidence and high incorrect evidence may also be driven toward this region (blue and green arrows in Figure 2), after which their update becomes inactive under standard evidential losses.",
This activation-dependent behavior occurs across evidential models.,
The GRED behavior is illustrated by the red arrows in Figure 2.,
"The correct evidence regularization is weighted by vacuity and therefore contributes most strongly in the zero- evidence region, where Levid and Linc provide no gradients.",
"As evidence increases and vacuity decreases, the influence of Lcor fades, and the standard evidential losses dominate the learning signal.",
"In this way, GRED ensures that samples across all evidence levels contribute to parameter updates while preserving the intended behavior of evidential training for high-evidence regions.",
"V. EXPERIMENTS We evaluate our method across a broad range of benchmarks and architectures to validate the theoretical analysis, demon- strate the effectiveness of correct evidence regularization, and assess generalization and uncertainty quantification.",
"Our experiments span standard supervised classification, few-shot learning, and a real-world image restoration task.",
"We consider MNIST [51], CIFAR-10 and CIFAR-100 [52], and Tiny-ImageNet [53] for classification; 100-way 1-shot and 100-way 5-shot CIFAR-100 for few-shot learning; and blind face restoration [54] using FFHQ [55] and CelebA [56].",
"To evaluate robustness across architectures, we employ LeNet[57] for MNIST, ResNet18 [58] for CIFAR experi- ments, and Swin-Transformer [59], [60] for Tiny-ImageNet.",
"For few-shot learning, we use the transformer-based Visual Prompt Tuning (VPT) framework [61], which adapts large pretrained vision transformers using lightweight prompts—a setting that stresses uncertainty estimation due to extremely limited supervision.",
"For blind face restoration, we use the VQGAN/Transformer-based CodeFormer model [54].",
We first present experiments that empirically verify the gra- dient behavior characterized in Section III.,
"We then evaluate the proposed correct evidence regularization on all datasets and architectures, followed by ablation studies analyzing the contribution of each evidential loss component and the im- pact on calibration and uncertainty metrics.",
"We then extend the proposed evidential model to few-shot classification and blind face restoration, demonstrating consistent improvements across all settings.",
We then carry out out-of-distribution anal- ysis of the proposed GRED model with challenging few-shot classification setting.,
"Unless noted otherwise, table mean/std results are averaged over three seeds; training curves show GT : 3 GT : 5 GT : 2 GT : 6 Fig.",
5: Toy dataset with 4 data points 0 2 4 6 8 Iterations (× 10) 0.00 0.25 0.50 0.75 1.00 Accuracy Standard model Evidential model (a) Training Accuracy Trend 0 2 4 6 8 Iterations (× 10) 0 1 2 Loss Standard Model Evidential Model (b) Training Loss Trend Fig.,
6: Training of standard and evidential models one representative run.,
"Additional ablations, hyperparameter details, and clarifications are provided in the Appendix.",
Learning Dynamics and Failures in Evidential Models a) Sensitivity to the change of the architecture.,
: We first consider a toy illustrative experiment with two frameworks: (1) a standard Softmax model and (2) an evidential model.,
"Both use a LeNet [57] architecture similar to that considered in EDL [11], with a minor modification to the architecture: dropout is removed.",
"To construct the toy dataset, we randomly select 4 labeled data points from the MNIST training dataset, as shown in Figure 5.",
"For the evidential model, we use ReLU to transform the network outputs to evidence and train the model with the MSE-based evidential loss [11] given in Eq.",
"28, without incorrect evidence regularization.",
We train both models using only these 4 training data points.,
Figure 6 compares the training accuracy and loss trends of the evidential model with the standard softmax model (trained with cross-entropy).,
"Before training, both models have 0% accuracy and high loss, as expected.",
"For the evidential model, in the first few iterations the accuracy increases to 50%, indicating that some samples are being fitted.",
"Afterward, the accuracy plateaus: the evidential model maps two of the train- ing samples to the zero evidence region, where the gradients of standard evidential losses vanish.",
"In this toy setting, the model therefore does not fully fit all four training points, empirically reflecting the behavior characterized in Theorem 1.",
"It is also worth noting that the range of the evidential model’s loss is significantly smaller than that of the standard model, mainly due to the bounded nature of the evidential MSE loss (i.e., it lies in [0, 2]; see the Appendix for a detailed theoretical analysis).",
"By contrast, the standard model trained with cross- entropy easily fits the trivial dataset, reaching near-zero loss and 100% accuracy after a few iterations.",
"Additionally, we visualize the total evidence for each train- ing sample in this toy experiment.",
We plot the total evidence across the first 100 iterations in Figure 7.,
"The evidential JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 9 0 25 50 75 100 Iteration 0 2 4 Evidence 3 5 2 6 Fig.",
"7: Zero evidence trend during model training model’s predictions are correct for the samples with ground- truth labels 3 and 6, and incorrect for the remaining two.",
"After a few iterations, the latter two samples are mapped to the zero-evidence region, and their total evidence remains near zero.",
"In this regime, the model receives no gradient from these samples, and the overall training accuracy stabilizes at 50% even after 100 iterations.",
"In contrast, the standard model continues to update on all four samples and achieves 100% accuracy.",
"This toy setting highlights how vanishing gradients in the zero-evidence region can affect evidential learning dynamics, even in simple cases.",
b) Sensitivity to hyperparameter tuning: Evidential mod- els are trained using evidential losses given in Eq.,
30 with incorrect evidence regularization to guide the model for accurate uncertainty quantification.,
"We study the impact of the incorrect evidence regularization strength (i.e., hyperparameter λ1) on the evidential model’s performance using CIFAR-100 experiments.",
We consider the Type-II Maxi- mum Likelihood loss in Eq.,
30 with different λ1 to control KL regularization.,
"As shown in Figure 8, when some regulariza- tion is introduced, the evidential model’s test performance im- proves slightly.",
"However, when large regularization is used, the model focuses strongly on minimizing the incorrect evidence, pushing a large number of training data samples to region near the zero evidence region.",
"As can be seen, the generalization performance of evidential models is highly sensitive to λ1 values.",
A similar trend is seen across all the losses and settings (results on other loss functions and settings are presented in the Appendix).,
"Such incorrect evidence regularization can cause the model to push many training samples into or close to the zero-evidence regions, thereby reducing the effective learning signal from those samples.",
"At the same time, incorrect evidence regularization is essential to correct incorrect ac- quired evidence and improve uncertainty estimates.",
"Therefore, choosing a reasonable regularization strength is important for achieving accurate uncertainty quantification, especially on challenging datasets and settings, which we present next.",
c) Challenging datasets and settings.,
: We next consider a standard cross-entropy-trained classification model for the CIFAR-100 dataset and construct evidential extensions using the Type-II Maximum Likelihood loss in Eq.,
30 and the Bayes risk with cross-entropy loss in Eq.,
"29 without incorrect evidence regularization, using ReLU to transform logits to Fig.",
8: Impact of different incorrect evidence regularization strengths to the test set accuracy on CIFAR-100 (a) Evid.,
Log Loss in Eq.,
30 (b) Evid.,
CE Loss in Eq.,
9: Learning trends in CIFAR-100 for standard and evi- dential models with different evidential losses evidence.,
"As shown in Figure 9, compared to the standard classification model, the evidential models exhibit lower pre- dictive performance (around 10%–20% lower for Eq.",
"29, and more than 30% lower for the MSE-based loss in Figure 1).",
"This behavior coincides with many training samples being mapped into or near the zero evidence region, where the model expresses high vacuity and the gradients from standard evidential losses vanish.",
"When incorrect evidence regularization is added, more samples can be driven toward the zero-evidence region, which may further reduce predictive accuracy if the regularization is too strong.",
"In such cases, even though correct labels are available, the contribution of those samples to parameter updates becomes negligible once they reach near the zero-evidence region.",
d) Sub-Optimal Learning Caused by Incorrect Evidence Regularization: Existing evidential models are theoretically equipped to capture the fine-grained uncertainties through the higher-order conjugate prior distribution over the likelihood distribution.,
"For classification, the evidential models introduce the Dirichlet prior over the multinomial likelihood distribution and train with evidential losses, such as Eq.",
"Additionally, these evidential models leverage incorrect evidence regulariza- tion given in Eq.",
"49 to ensure accurate uncertainty quantifi- cation, especially in the most challenging settings.",
"To more clearly demonstrate the influence of the incorrect evidence regularization, we first consider the FGSM [62] adversarial attack applied to an evidential model with an Exponential activation function trained on CIFAR-100.",
"We employ the JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 10 (a) Accuracy Trend (b) Vacuity Trend Fig.",
"10: Adversarial attack trends for different incorrect evi- dence regularization strengths evidential log loss, given in Eq.",
30 and train the model for 200 epochs to study the accuracy-vacuity trends of the trained model for different strengths of the adversarial attack on the test set.,
"As shown in Fig 10, as the attack strength increases, the overall accuracy of the model decreases.",
"Since the evidential model is able to quantify the fine-grained uncertainty, we hope it can detect the attack through the predicted uncertainty.",
"However, without the incorrect evidence regularization, the predicted vacuity remains low as the attack strength increases, as shown in Fig 10(b).",
"With a larger incorrect evidence regu- larization, the model becomes aware of its lack of knowledge for adversarial samples and outputs higher vacuity.",
"However, when the incorrect evidence regularization is high, the model’s learning capability becomes compromised: Fig 10(a) shows that a larger λ1 leads to a lower accuracy.",
"Towards robust models, adversarial training methods have been developed and could be extended to evidential deep learning models [40].",
"However, we observe that adversarial training of evidential models is sensitive to incorrect evidence regularization values (section V-B, Figure 15), and adversarial training becomes ineffective.",
"To further illustrate the need for incorrect evidence regu- larization, we next present the accuracy-uncertainty results for the 1024−class classification of the CodeFormer model for the FFHQ dataset.",
We consider the accuracy of the evidential transformer in the codebook prediction (details of the model are presented in the Appendix X-B4) We present the accuracy- vacuity curves for the evidential models trained with and without incorrect evidence regularization term in Figure 11.,
"For a model with accurate uncertainty information, model’s accuracy should be higher on low vacuity predictions.",
"In other words, the model should be accurate on its confident predictions.",
"However, when no incorrect evidence regular- ization is used, the model is wrongly confident on all code predictions i.e., the uncertainty is not reliable.",
"Moreover, the vacuity is not expressive and is bound on a narrow range of 0.005 to 0.0015.",
"With a reasonable incorrect evidence regularization value, e.g., λ1 = 0.01 for the model training, the accuracy-vacuity curves become more reasonable.",
"With a larger incorrect evidence regularization strength, model’s accuracy in codebook prediction increases with lower vacuity threshold: model is accurate on the most confident predictions.",
"However, the incorrect evidence regularization tends to push training samples towards the zero evidence region, which hurts the model’s training data efficiency, and the generalization capability.",
0.000 0.005 0.010 0.015 Vacuity Threshold 15.0 17.5 20.0 Accuracy Codebook Accuracy KL: 0.0 (a) λ1 = 0 0.2 0.4 0.6 0.8 Vacuity Threshold 25 50 75 100 Accuracy Codebook Accuracy KL: 0.01 (b) λ1 = 0.01 Fig.,
"11: (a) Without Incorrect Evidence Regularization (b) With Incorrect Evidence Regularization of λ1=0.01 B. Generalized Regularized Evidential Models (GRED) We now evaluate the proposed generalized regularized ev- idential models, which enable evidential networks to learn from all samples, including those mapped to the zero evidence region.",
"We experiment with multiple activation functions and their regularized variants (i.e., trained with the correct evidence regularizer) using the Type-II evidential loss in Eq.",
"Across all datasets and architectures, introducing the cor- rect evidence regularization consistently improves generaliza- tion (Table II), validating its effectiveness.",
"Figure 12 further shows that GRED remains stable even under strong incorrect evidence regularization, whereas baseline evidential models degrade because they cannot update on zero-evidence samples.",
Complete results and hyperparameter details are provided in the Appendix.,
We next examine training dynamics on MNIST with two ev- idential losses (Fig.,
"The exp activation performs strongest due to its minimal zero-evidence region, and adding correct evidence regularization further improves learning by ensuring that all samples contribute gradients.",
We further consider the evidential baseline model trained with Type-II Maximum Likelihood-based loss with incorrect evidence regularization strength of λ1 = 1.0 and 10.0.,
We 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Regularization Strength 0 20 40 60 80 Accuracy SoftPlus GRED-SoftPlus SELU GRED-SELU exp GRED-exp ReLU Fig.,
12: Results for different regularization strengths for model trained with evidential log loss in Eq.,
"30 on CIFAR-100 JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 11 TABLE II: Evidential Classification Results.",
Mean and standard deviation are reported for each model after averaging across 3 runs.,
Bold values indicate the best performance for each dataset.,
Dataset ReLU SoftPlus GRED-SoftPlus SELU GRED-SELU exp GRED-exp MNIST 98.10±0.01 98.20±0.07 98.61±0.07 98.08±0.16 98.68±0.12 98.83±0.06 98.97±0.08 CIFAR-10 19.50±13.44 95.15±0.22 95.28±0.10 95.21±0.06 95.32±0.02 95.24±0.12 95.44±0.26 CIFAR-100 55.50±3.90 75.61±0.16 76.09±0.25 75.50±0.30 75.86±0.12 75.76±0.40 76.00±0.09 Tiny-ImageNet 33.08±0.94 90.25±0.02 90.63±0.06 90.15±0.04 90.58±0.03 90.01±0.06 90.63±0.05 (a) Evidential MSE loss (b) Evidential Log loss Fig.,
13: Test accuracy with correct evidence regularization introduce the proposed novel correct evidence regularization to the evidential model.,
"As can be seen in Figure 14, the model with correct-evidence regularization has superior gen- eralization performance compared to the baseline evidential models in all the cases.",
"This is mainly due to the fact that with proposed correct evidence regularization, the evidential model can also learn from the zero evidence training samples to acquire new knowledge instead of ignoring it.",
Our proposed model considers knowledge from all the training data and aims to acquire new knowledge to improve its generalization instead of ignoring the samples for which it has no knowledge.,
"Finally, as seen in Figure 14 (d), even though strong incorrect evidence regularization hurts the model’s generalization, the proposed model with correct evidence regularization is robust and generalizes better, empirically validating our Theorem 3.",
"Limited by space, we present additional results in Appendix X-B2 (additional results of CIFAR-100 are presented in the Appendix).",
We evaluate the learning capabilities of evidential models under adversarial training.,
"Specifically, we train an evidential model with an exp activation function using the eviden- tial Type-II Maximum Likelihood-based loss, and incorrect evidence regularization strength of λ1 = 0.1 and 1.0 on the CIFAR-100 dataset.",
"For generating adversarial samples, we apply the FGSM method with an attack strength of ϵ = 0.05(additional details are provided in the Appendix X-A).The performance of the evidential models trained with incorrect evidence regularization values of 0.1 and 1.0 on the adversarial test set is presented in Fig.",
"The evidential model struggles to learn from all training samples, resulting in poor performance on the adversarial test set.",
"Meanwhile, using the proposed evidence regularizer enables the model to learn from all samples, leading to decent performance on the test dataset (a) exp-based Model (b) SoftPlus-based Model (c) SELU-based Model (d) SoftPlus-based Model Fig.",
14: Impact of correct evidence regularization to test accuracy for different evidential models (a) λ = 0.1 (b) λ = 1.0 Fig.,
15: Impact of proposed regularization Lcor to Adversarial Training of evidential models C. Ablation Study a) Impact of loss function.,
": We first examine the effect of different evidential losses on MNIST using the exp activation and incorrect-evidence regularization strengths λ1 ∈{0, 1}.",
Training with the evidential MSE loss (Eq.,
28) consistently yields lower test performance than the other two losses (Eq.,
"This behavior is expected because the evidential MSE loss is bounded in [0, 2], which restricts gradient magnitude and slows learning.",
Additional activation- wise comparisons and theoretical discussion are provided in the Appendix.,
"Unless otherwise stated, subsequent experiments use the exp evidential activation with the Type-II Maximum Like- lihood loss (Eq.",
"30), which offers stable optimization and JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 12 a clearer probabilistic interpretation.",
A deeper comparison between Eq.,
30 is left for future work.,
(a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 Fig.,
16: Impact of evidential losses on test set accuracy b) Uncertainty vs Accuracy Trends: We next analyze the uncertainty behavior on CIFAR-100.,
Figure 17 shows accuracy–vacuity curves for different incorrect-evidence regu- larization strengths λ1.,
"Vacuity reflects the lack of confidence in the predictions, and the accuracy of an effective evidential model should increase with a lower vacuity threshold.",
"Without any incorrect evidence regularization (i.e., λ1 = 0), the evidential model is highly confident in its predictions and all test sample predictions are concentrated on the low vacuity region.",
"As the incorrect evidence regularization strength is increased, the model outputs more accurate confidence in the predictions.",
"Strong incorrect evidence regularization hurts the generalization over the test set as indicated by low accuracy when all test samples are considered (i.e., vacuity threshold of 1.0).",
Our correct-evidence regularized evidential model shows reasonable uncertainty behavior: the model’s test set accuracy increases as the vacuity threshold is decreased.,
17: Accuracy-Vacuity curve We further evaluate accuracy on the top-K% most confident test predictions (Table III).,
"For example, among the top 20% most confident samples, the proposed GRED model achieves 99.40% accuracy, outperforming all baseline models.",
"Across all values of K, GRED matches or exceeds the strongest base- line, demonstrating that the improved uncertainty estimates translate into better ranking of prediction confidence.",
c) Calibration Analysis.,
: We evaluate calibration using Expected Calibration Error (ECE) on the CIFAR-100 test set.,
Figure 18 reports ECE trends across KL regularization strengths and the reliability diagrams.,
"Across most settings, GRED achieves calibration on par with vanilla EDL, with TABLE III: Accuracy on Top-K% Confident Samples (%) Model 10% 20% 50% 70% 80% 100% ReLU 99.20 98.45 90.60 77.74 70.28 55.50 SELU 99.30 99.00 96.02 89.87 85.84 75.50 SoftPlus 98.70 98.65 95.94 89.91 85.80 75.61 Exp 99.40 99.20 96.54 90.46 86.19 75.76 GRED-Exp 99.60 99.40 96.40 90.46 86.26 76.00 marginal improvements in some hyperparameter values.",
"As KL regularization increases, both models become increasingly underconfident, reflected by rising ECE values as seen in Figure 18(b)-(d) reliability diagrams.",
"At large KL values (e.g., λ1 = 1.0), EDL [11] exhibits a deceptively low ECE (1.6%) due to model collapse: its accuracy drops to 34.3%, and the model outputs near-uniform predictions (“I don’t know”) for many test samples.",
"In contrast, GRED maintains functional predictive performance (70.‘% accuracy, 26.4% ECE).",
This highlights how ECE alone can be misleading when a model collapses to high-vacuity predic- tions.,
Addressing the broader underconfidence trend observed in evidential models is an important direction for future work.,
(a) ECE vs KL strength (b) KL = 1.0 (c) KL = 0.5 (d) KL = 0.001 Fig.,
18: Calibration analysis with log loss.,
(a) ECE trends (b)-(d) Reliability diagrams comparing GRED with EDL for different KL regularization values D. Extension to Blind Face Restoration Codeformer models [54] have been developed that intro- duce VQ-GAN-based networks with a transformer architecture leading to state-of-the-art blind face restoration performance.,
"However, the blind face restoration problem is ill-posed by nature, and many of the restored faces are unlikely to be faithful to the true face images.",
"Moreover, the blind face restoration problem, by design, introduces uncertainty in the downstream restoration task.",
"In this section, we extend the JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 13 codeformer model using the ideas from our generalized reg- ularized evidential deep learning model to develop eviden- tial codeformers with fine-grained uncertainty-quantification capabilities.",
"We use the fine-grained uncertainty information to improve the codebook lookup of the codeformer, which leads to significant improvement on the blind face restoration benchmark, demonstrating the potential of the uncertainty- aware model.",
"The encoder-decoder based codeformer models [54] are trained in 3 stages: 1) Stage I with high-quality images to learn the codebook and train the decoder parameters, 2) Stage II with low-quality and high-quality image pairs to train the transformer classifier along with the encoder structure, and 3) an optional Stage III training of controllable feature trans- formation module to find good balance between the quality and fidelity of face restoration.",
We modify the transformer introduced in Stage II to an evidential transformer structure by replacing the softmax layer in the transformer with the evidential activation functions.,
We train the evidential trans- former on the FFHQ dataset based on Type II likelihood-based evidential loss in Eq.,
30 with incorrect evidence regularization strength of λ1 = 1.0.,
The evidential transformer outputs the K-dimensional evidence vector to identify D-dimensional code item from the K × D shaped codebook for each of the M positions of the decoder input.,
"For each position of the decoder input, based on the K-dimensional evidence vector e = (e1, e2, ..., eK)⊤, the vacuity ν, and K-dimensional belief vector b = (b1, b2, ..., bK)⊤can be computed.",
"In all codeformer-based experiments, K = 1024, D = 256, and the decoder input is a 16 × 16 × 256 shaped tensor with M = 16×16 = 256.",
Additional model details are presented in the Appendix X-B4.,
"The belief vector can be used to measure the model’s confusion (via dissonance [39]), and belief for each class can be used to make the code item prediction (the class prediction being the class for which the model outputs maximum belief).",
The vacuity represents the model’s lack of confidence in the prediction and can be used to identify the model’s confident predictions.,
"If the evidential transformer outputs a highly confident prediction (indicated by low vacuity), the model is expected to be accurate, and such predictions can be trusted.",
"In contrast, if the evidential classifier is not confident in the codebook pre- diction (e.g., due to the low quality of the input or insufficient knowledge of the model, then the code item selected at the decoder input is expected to be incorrect.",
"In this case, instead of relying on the transformer’s top predicted code item, we could consider the evidential model’s beliefs to obtain more accurate code for the decoder.",
"Based on this insight, we intro- duce a novel uncertainty-guided Top−t belief-based codebook selection scheme for inference.",
"For the decoder input positions that the evidential transformer is confident (indicated by a low vacuity ν ≤νthr), we trust the evidential model and select the code item from the codebook for which the model outputs the maximum belief.",
"In contrast, for the decoder input positions that the evidential model is not confident (indicated by a high vacuity ν > νthr), we consider the top t code items of the codebook for which the model’s belief is the largest.",
We then consider a belief-weighted combination of the predicted codes to obtain the final code item for the decoder input.,
"Mathematically, given the codebook C = {c1, c2, ..., cK}, the decoder input dm for each position in the M−dimensional decoder input is obtained as: dm = ( cmax i if ν ≤νthr Pt j=1 bmax j cj max Pt l=1 bmax l Otherwise (27) where ν represents the vacuity predicted at mth decoder input position, cmax i represents the ith code item in the codebook C such that the evidential model’s belief is maximum for class i, bmax 1 , ..., bmax t represent the t greatest belief values among the K beliefs, and cmax 1 , ...cmax t are the corresponding code items of codebook with the t greatest belief values.",
"When t = 1 or the vacuity threshold vthr = 1, the above inference scheme simplifies to the standard evidential model.",
"When t = K, the model considers all the codebook items and weights them by the predicted belief to obtain the decoder input.",
"With t > 1, and reasonable vacuity threshold values, the inference scheme considers multiple code items for blind face restoration and uses its belief to weight the code items.",
Evidential Model PSNR↑ MSE↓ L1 Loss↓ CodeFormer [54] 21.90 446.82 13.72 Evid.,
CodeFormer-ReLU 6.62 15436.89 102.38 Evid.,
CodeFormer-SELU 21.31 514.82 14.97 GRED-SELU 21.84 451.93 13.69 Evid.,
CodeFormer-Softplus 21.17 528.47 15.41 GRED-Softplus 21.81 454.03 13.86 Evid.,
"CodeFormer-Exp 21.46 491.01 14.63 GRED-Exp 21.79 456.25 13.76 GRED-Exp(t = 5) 22.27 409.64 12.93 GRED-Exp(t = 10) 22.33 403.69 12.86 TABLE IV: CelebA Blind Face Restoration Results We now carry out blind face restoration experiments using the CelebA dataset and present the overall results in Table IV, where we consider t values of 5 and 10.",
"We consider a set of metrics, including Peak Signal to Noise Ratio (PSNR), Mean Squared Error (MSE), and the L1 loss between the generated image and the ground truth image for evaluation.",
"We observe that the ReLU based evidential model fails to achieve reasonable performance due to its sub-optimal learning (as indicated by low PSNR, and high MSE and L1 loss values in Table IV).",
"The evidential codeformer models with proposed correct evidence regularization (i.e., the GRED variants for SELU-based , softplus-based, and exponential-based evidential models) consistently improve compared to the corresponding evidential codeformer models as the GRED regularization enables the evidential models to learn from all the training samples.",
"Moreover, using the novel uncertainty-guided Top-k strategy leads to significant improvements in terms of PSNR (with a boost of around 0.43 db), L1 loss (decrease of around 0.42 units), and MSE (decrease by around 43 units).",
"We carry out additional ablations to show the superiority of using belief weighting compared to uniform weighting, the impact of the t value, and the vacuity threshold in the Appendix.",
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 14 E. Extension to Few-Shot Classification Few-shot learning operates in a regime of extremely lim- ited supervision, making uncertainty awareness particularly important for trustworthiness and robustness [63], [64].",
"Prior works have explored evidential methods for uncertainty-aware few-shot learning [39], [65], but they suffer from the same zero-evidence learning limitations identified in Section III.",
We improve upon these approaches by incorporating our proposed correct-evidence regularization into a modern few-shot frame- work.,
We build an evidential Visual Prompt Tuning (VPT) model by modifying the transformer classifier in VPT [61] to output evidential activations instead of softmax probabilities.,
The model is trained using the full evidential objective in Eq.,
"26 with incorrect evidence regularization strength λ1 = 1.0, and we evaluate both with and without the proposed correct-evidence regularization across multiple activation func- tions.",
"Additional implementation details, along with results for other λ1 values, are provided in Appendix X-B3.",
Table V summarizes performance on the challenging 100- way 1-shot and 100-way 5-shot CIFAR-100 benchmarks.,
Stan- dard evidential models perform poorly in these extreme low- data settings.,
"In contrast, the proposed GRED variants con- sistently outperform their non-regularized counterparts across all activation functions, demonstrating improved generaliza- tion from limited supervision.",
"Importantly, this improvement comes with no additional computational overhead and natu- rally yields high-quality uncertainty estimates.",
The benefit of uncertainty is further illustrated in the ac- curacy–vacuity curves in Fig.,
"As the vacuity threshold increases, we retain only the model’s most confident predic- tions, and accuracy rises sharply.",
"For example, at a vacuity threshold of 0.6, the accuracy in the 100-way 1-shot setting improves from roughly 50% to above 90%.",
This highlights the practical advantage of evidential uncertainty for filtering reliable predictions in few-shot scenarios.,
Evidential Model 100-Way 1-Shot 100-way 5-Shot ReLU 1.00±0.00 1.00±0.00 SELU 36.81±5.69 49.39±3.51 GRED-SELU 45.88±1.31 76.09±0.78 Softplus 38.85±3.15 47.87±4.78 GRED-Softplus 46.63±1.59 74.08±1.61 Exp 37.49±3.18 49.05±5.51 GRED-Exp 46.54±1.45 77.00±1.01 TABLE V: Few-Shot CIFAR-100 Classification Results (a) 100-way 1-shot trend (b) 100-way 5-shot trend Fig.,
19: Few-Shot CIFAR-100 Accuracy-Vacuity curves VI.,
OUT-OF-DISTRIBUTION DETECTION We evaluate the ability of evidential models to assign high epistemic uncertainty to out-of-distribution (OOD) inputs.,
"Following standard practice, CIFAR-100 test samples serve as in-distribution (ID) and SVHN as OOD.",
"We fine-tune the few- shot models on 1-shot and 5-shot CIFAR-100 with three KL regularization strengths (0.0, 1.0, 100.0), using the uncertainty score 1 −max p(y) for ID–OOD discrimination.",
AUROC is used as the evaluation metric.,
Figure 20 shows AUROC results across all settings.,
"GRED consistently improves OOD separability, with the largest gains under moderate KL regularization.",
"In the 5-shot scenario with KL= 1.0, GRED boosts AUROC from 0.633 to 0.882, demonstrating substantially improved epistemic uncertainty modeling.",
"Large KL values (e.g., 100.0) yield diminishing returns, but GRED remains superior to the baseline.",
"Overall, correct evidence regularization is crucial for robust OOD behavior in few-shot regimes.",
(a) 1-shot learning (b) 5-shot learning Fig.,
20: AUROC-OOD detection trends.,
GRED consistently improves OOD separability across KL strengths.,
"CONCLUSION In this work, we presented a theoretical analysis revealing a fundamental limitation of evidential deep learning models: their gradients vanish in the zero-evidence region, preventing the model from learning from precisely the samples where supervision is most needed.",
We showed that exponential activations provide stronger learning signals in this regime and introduced a correct-evidence regularization term that restores meaningful gradients for low- and zero-evidence samples.,
"This yields GRED, a generalized regularized evidential model that learns from all training examples.",
"Extensive experiments across classification, few-shot learning, adversarial evalua- tion, OOD detection, and blind face restoration demonstrate consistent gains in generalization and uncertainty reliabil- ity.",
"GRED mitigates activation-induced learning freeze and advances the development of trustworthy, uncertainty-aware neural networks.",
"REFERENCES [1] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech recognition, vol.",
"Springer, 2019.",
"[2] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain, “Machine translation using deep learning: An overview,” in 2017 in- ternational conference on computer, communications and electronics (comptelix), pp.",
"162–167, IEEE, 2017.",
"[3] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep learning for computer vision: A brief review,” Computational intelligence and neuroscience, vol.",
"2018, 2018.",
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 15 [4] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp.",
"427–436, 2015.",
"[5] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing model uncertainty in deep learning,” in international conference on machine learning, pp.",
"1050–1059, PMLR, 2016.",
"[6] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight uncertainty in neural network,” in International conference on machine learning, pp.",
"1613–1622, PMLR, 2015.",
"[7] T. Pearce, F. Leibfried, and A. Brintrup, “Uncertainty in neural networks: Approximately bayesian ensembling,” in International conference on artificial intelligence and statistics, pp.",
"234–244, PMLR, 2020.",
"[8] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive uncertainty estimation using deep ensembles,” Advances in neural information processing systems, vol.",
"[9] G. Shafer, A mathematical theory of evidence, vol.",
"Princeton university press, 1976.",
"[10] A. Jøsang, Subjective logic, vol.",
"Springer, 2016.",
"[11] M. Sensoy, L. Kaplan, and M. Kandemir, “Evidential deep learning to quantify classification uncertainty,” Advances in neural information processing systems, vol.",
"[12] A. Amini, W. Schwarting, A. Soleimany, and D. Rus, “Deep eviden- tial regression,” Advances in Neural Information Processing Systems, vol.",
"14927–14937, 2020.",
"[13] D. S. Pandey and Q. Yu, “Multidimensional belief quantification for label-efficient meta-learning,” in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pp.",
"14391– 14400, June 2022.",
"[14] W. Bao, Q. Yu, and Y. Kong, “Evidential deep learning for open set action recognition,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
"13349–13358, 2021.",
"[15] D. S. Pandey and Q. Yu, “Learn to accumulate evidence from all training samples: Theory and practice,” in International Conference on Machine Learning, pp.",
"26963–26989, PMLR, 2023.",
"[16] E. H¨ullermeier and W. Waegeman, “Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods,” Machine learning, vol.",
"457–506, 2021.",
"[17] A. Mobiny, P. Yuan, S. K. Moulik, N. Garg, C. C. Wu, and H. Van Nguyen, “Dropconnect is effective in modeling uncertainty of bayesian deep networks,” Scientific reports, vol.",
"1–14, 2021.",
"Sale, V. Bengs, M. Caprio, and E. H¨ullermeier, “Second-order uncertainty quantification: A distance-based approach,” 2024.",
"[19] Y. Ren, A. Bahamou, and D. Goldfarb, “Kronecker-factored quasi- newton methods for deep learning,” arXiv preprint arXiv:2102.06737, 2021.",
"Sale, P. Hofman, L. Wimmer, E. H¨ullermeier, and T. Nagler, “Second-order uncertainty quantification: Variance-based measures,” arXiv preprint arXiv:2401.00276, 2023.",
"[21] M. Caprio, S. Dutta, K. J. Jang, V. Lin, R. Ivanov, O. Sokolsky, and I. Lee, “Credal bayesian deep learning,” arXiv e-prints, pp.",
"arXiv–2302, 2023.",
"[22] M. Caprio, M. Sultana, E. Elia, and F. Cuzzolin, “Credal learning theory,” arXiv preprint arXiv:2402.00957, 2024.",
"[23] F. G. Cozman, “Credal networks,” Artificial intelligence, vol.",
"199–233, 2000.",
"[24] M. Zaffalon, “The naive credal classifier,” Journal of statistical planning and inference, vol.",
"5–21, 2002.",
"[25] M. Caprio, Y.",
"Sale, E. H¨ullermeier, and I. Lee, “A novel bayes’ theorem for upper probabilities,” in International Workshop on Epistemic Uncertainty in Artificial Intelligence, pp.",
"1–12, Springer, 2023.",
"Sale, M. Caprio, and E. H¨ollermeier, “Is the volume of a credal set a good measure for epistemic uncertainty?,” in Uncertainty in Artificial Intelligence, pp.",
"1795–1804, PMLR, 2023.",
"[27] B. Ristic, C. Gilliam, M. Byrne, and A. Benavoli, “A tutorial on uncer- tainty modeling for machine reasoning,” Information Fusion, vol.",
"30–44, 2020.",
"[28] T. Augustin, F. P. Coolen, G. De Cooman, and M. C. Troffaes, Intro- duction to imprecise probabilities, vol.",
"John Wiley & Sons, 2014.",
"[29] P. Lu, M. Caprio, E. Eaton, and I. Lee, “Ibcl: Zero-shot model generation for task trade-offs in continual learning,” arXiv preprint arXiv:2305.14782, 2023.",
"[30] T. Denœux, D. Dubois, and H. Prade, “Representations of uncertainty in artificial intelligence: Probability and possibility,” A Guided Tour of Artificial Intelligence Research: Volume I: Knowledge Representation, Reasoning and Learning, pp.",
"69–117, 2020.",
"[31] T. Denœux and L. M. Zouhal, “Handling possibilistic labels in pat- tern classification using evidential reasoning,” Fuzzy sets and systems, vol.",
"409–424, 2001.",
"[32] T. Denoeux, “A neural network classifier based on dempster-shafer theory,” IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, vol.",
"131–150, 2000.",
"[33] T. Denœux, “An evidential neural network model for regression based on random fuzzy numbers,” in International Conference on Belief Functions, pp.",
"57–66, Springer, 2022.",
"[34] G. Shafer, “Dempster-shafer theory,” Encyclopedia of artificial intelli- gence, vol.",
"330–331, 1992.",
"[35] T. Denœux, “Quantifying prediction uncertainty in regression using random fuzzy sets: the ennreg model,” IEEE Transactions on Fuzzy Systems, vol.",
"3690–3699, 2023.",
"[36] C. W. De Silva, Intelligent control: fuzzy logic applications.",
"CRC press, 2018.",
"[37] X. Zhao, F. Chen, S. Hu, and J.-H. Cho, “Uncertainty aware semi- supervised learning on graph data,” Advances in Neural Information Processing Systems, vol.",
"12827–12836, 2020.",
"[38] B. Charpentier, D. Z¨ugner, and S. G¨unnemann, “Posterior network: Uncertainty estimation without ood samples via density-based pseudo- counts,” Advances in Neural Information Processing Systems, vol.",
"1356–1367, 2020.",
"[39] D. S. Pandey and Q. Yu, “Evidential conditional neural processes,” arXiv preprint arXiv:2212.00131, 2022.",
"[40] A.-K. Kopetzki, B. Charpentier, D. Z¨ugner, S. Giri, and S. G¨unnemann, “Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based models reliable?,” in International Conference on Ma- chine Learning, pp.",
"5707–5718, PMLR, 2021.",
"[41] C. Tomani and F. Buettner, “Towards trustworthy predictions from deep neural networks with fast adversarial calibration,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol.",
"9886–9896, 2021.",
"[42] W. Shi, X. Zhao, F. Chen, and Q. Yu, “Multifaceted uncertainty estima- tion for label-efficient deep learning,” Advances in neural information processing systems, vol.",
"[43] A. Malinin and M. Gales, “Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness,” Advances in Neural Information Processing Systems, vol.",
"[44] A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,” Advances in neural information processing systems, vol.",
"[45] D. Ulmer, “A survey on evidential deep learning for single-pass uncer- tainty estimation,” arXiv preprint arXiv:2110.03051, 2021.",
"[46] L. Wimmer, Y.",
"Sale, P. Hofman, B. Bischl, and E. H¨ullermeier, “Quan- tifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures?,” in Uncertainty in Artificial Intelligence, pp.",
"2282–2292, PMLR, 2023.",
"[47] V. Bengs, E. H¨ullermeier, and W. Waegeman, “Pitfalls of epistemic un- certainty quantification through loss minimisation,” Advances in Neural Information Processing Systems, vol.",
"29205–29216, 2022.",
"[48] M. J¨urgens, N. Meinert, V. Bengs, E. H¨ullermeier, and W. Waegeman, “Is epistemic uncertainty faithfully represented by evidential deep learning methods?,” arXiv preprint arXiv:2402.09056, 2024.",
"[49] M. Shen, J. J. Ryu, S. Ghosh, Y. Bu, P. Sattigeri, S. Das, and G. W. Wornell, “Are uncertainty quantification capabilities of evidential deep learning a mirage?,” arXiv e-prints, pp.",
"arXiv–2402, 2024.",
"[50] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning.",
"Springer, 2006.",
"[51] Y. LeCun, “The mnist database of handwritten digits,” http://yann.",
"com/exdb/mnist/, 1998.",
"[52] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” -, 2009.",
"Le and X. Yang, “Tiny imagenet visual recognition challenge,” CS 231N, vol.",
"7, p. 3, 2015.",
"[54] S. Zhou, K. Chan, C. Li, and C. C. Loy, “Towards robust blind face restoration with codebook lookup transformer,” Advances in Neural Information Processing Systems, vol.",
"30599–30611, 2022.",
"[55] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.",
"4401–4410, 2019.",
"[56] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability, and variation,” arXiv preprint arXiv:1710.10196, 2017.",
"JOURNAL OF LATEX CLASS FILES, VOL.",
"9, SEPTEMBER 2020 16 [57] Y. LeCun, P. Haffner, L. Bottou, and Y. Bengio, “Object recognition with gradient-based learning,” in Shape, contour and grouping in computer vision, pp.",
"319–345, Springer, 1999.",
"[58] K. He, X. Zhang, S. Ren, and J.",
"Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp.",
"770–778, 2016.",
"[59] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in Proceedings of the IEEE/CVF international conference on computer vision, pp.",
"10012–10022, 2021.",
"[60] E. Huynh, “Vision transformers in 2022: An update on tiny imagenet,” arXiv preprint arXiv:2205.10660, 2022.",
"[61] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, “Visual prompt tuning,” in European Conference on Computer Vision, pp.",
"709–727, Springer, 2022.",
"[62] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.",
"[63] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., “Matching net- works for one shot learning,” Advances in neural information processing systems, vol.",
"[64] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.",
"1126– 1135, JMLR.",
"[65] D. S. Pandey and Q. Yu, “Multidimensional belief quantification for label-efficient meta-learning,” in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp.",
"14391–14400, 2022.",
"[66] K. Knopp, “Weierstrass’s factor-theorem,” in Theory of Functions: Part II, pp.",
"1–7, Dover, 1996.",
"[67] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.",
"Deep Shankar Pandey received his Bachelor’s de- gree in Electronics and Communication Engineering from Institute of Engineering, Pulchowk Campus, Tribhuvan University, Nepal in 2017.",
He completed his PhD degree in Computing and Information Sci- ences at Rochester Institute of Technology with Dr. Qi Yu as his PhD advisor in 2025 where he worked on Uncertainty Aware Meta Learning for Learning from Limited Data.,
"He is currently working as Applied Scientist at Amazon, and this work was done prior to joining Amazon.",
"He has authored several publications in top-tier venues, including NeurIPS, CVPR, ICML, and AAAI.",
His research focuses on developing trustworthy uncertainty-aware machine learning models with an emphasis on real-world applications.,
"Hyomin Choi received the Ph.D. degree in engineer- ing science from Simon Fraser University, Burnaby, BC, Canada, in 2022.",
"He is a senior staff engineer at the AI Lab, InterDigital, in Los Altos, CA, USA.",
"He was a research engineer at the System IC Research Center, LG Electronics, Seoul, Korea between 2012 and 2016.",
"He received the 2017 Vanier Canada Graduate Scholarship, the 2023 Governor General’s Gold Medal from Simon Fraser University, and the 2023 IEEE Transactions on Circuits and Systems for Video Technology Best Paper Award.",
He is currently a Member of the IEEE-CAS Multimedia Systems and Applications Technical Committee.,
"His research interests encompass end-to-end learning- based image/video coding, video coding for machines, and machine learning with applications in multimedia processing.",
Qi Yu is a Professor in the School of Informa- tion (iSchool) at Rochester Institute of Technology (RIT).,
He earned his PhD in Computer Science from Virginia Polytechnic Institute and State University (Virginia Tech).,
Dr. Yu’s primary research interests are in artificial intelligence (AI) and machine learn- ing (ML).,
"He has authored over 150 publications, with many appearing in top-tier venues, including NeurIPS, ICML, ICLR, AAAI, IJCAI, AISTATS, CVPR, ICCV, and ECCV.",
"Dr. Yu actively contributes to the research community as an area chair or senior program committee member for major conferences in AI, ML, and computer vision.",
"Additionally, he serves as an Associate Editor for the IEEE Transactions on Services Computing and the IEEE Transactions on Cognitive and Developmental Systems.",
"Activation Analysis of a Byte-Based Deep Neural Network for Malware Classiﬁcation Scott E. Coull FireEye, Inc. scott.coull@ﬁreeye.com Christopher Gardner FireEye, Inc. christopher.gardner@ﬁreeye.com Abstract—Feature engineering is one of the most costly aspects of developing effective machine learning models, and that cost is even greater in specialized problem domains, like malware classiﬁcation, where expert skills are necessary to identify useful features.",
"Recent work, however, has shown that deep learning models can be used to automatically learn feature representations directly from the raw, unstructured bytes of the binaries them- selves.",
"In this paper, we explore what these models are learning about malware.",
"To do so, we examine the learned features at multiple levels of resolution, from individual byte embeddings to end-to-end analysis of the model.",
"At each step, we connect these byte-oriented activations to their original semantics through parsing and disassembly of the binary to arrive at human- understandable features.",
"Through our results, we identify several interesting features learned by the model and their connection to manually-derived features typically used by traditional machine learning models.",
"Additionally, we explore the impact of training data volume and regularization on the quality of the learned features and the efﬁcacy of the classiﬁers, revealing the somewhat paradoxical insight that better generalization does not necessarily result in better performance for byte-based malware classiﬁers.",
"INTRODUCTION To effectively protect users from the latest malware threats, detection mechanisms must be capable of adapting as quickly as the threats themselves.",
"Traditional machine learning-based antivirus (i.e., next-gen AV) solutions provide this capability by generalizing from previous examples of malware, but often require laborious development of hand-engineered features by domain experts to gain a true advantage.",
"Moreover, these features are often speciﬁc to each type of executable ﬁle (e.g., Portable Executable, Mach-O, ELF, etc.",
"), further compounding the amount of overhead required.",
"Recently, however, a series of deep neural network models [1]–[3] have been proposed that operate directly on the raw bytes of executable ﬁles to de- tect malware - effectively learning the feature representations directly from the data with no information about its syntax or semantics.",
"Surprisingly, these byte-based classiﬁers have reported accuracy exceeding 0.96 and area under the ROC curve (AUC) of greater than 0.98.",
"Given the success of these approaches, an obvious ques- tion arises: what exactly are these neural networks learning?",
"Answering this question is important in developing a rigor- ous understanding of the generalization capabilities of these models, as well as their robustness to evasion.",
"Furthermore, this analysis will help to shed light on the connection between manual feature engineering and feature representation learning in the malware classiﬁcation domain.",
"In this paper, we seek to answer this question by providing a deep and broad analysis of activations in a byte-based deep neural network classiﬁer that is representative of the architectures proposed in previous work.",
"Unlike previous work, however, we expand our analysis beyond simply looking at the location of the activations to understanding the speciﬁc features that are learned and their connection to the semantics of the executable as a malware analyst would understand them.",
We perform this analysis un- der a variety of training regimes to gain a better understanding of the bias-variance tradeoff that exists for byte-based models in the unique problem area of malware classiﬁcation.,
"Speciﬁcally, we examine the question at three levels: (1) the embedding layer to uncover learned similarities among independent byte values, (2) the ﬁrst convolutional layer to identify low-level features over short byte sequences, and (3) end-to-end analysis for complex features combined over several layers of aggregation in the model.",
"At each of these layers, we compare three models trained under increasing data volumes and levels of regularization to understand the relationship between these training variables, the features learned by the models, and the efﬁcacy of those models in correctly classifying malware.",
"Where possible, we bridge the gap between raw-byte activations and the semantics of the executable through automated parsing and disassembly of the activation locations in an effort to obtain human- understandable explanations for the model’s predictions.",
"Overall, the results of our experiments surface a number of interesting ﬁndings about the use of byte-based deep neural networks for malware classiﬁcation.",
"For one, our results high- light the importance of depth in learning rich, semantically- meaningful features.",
"Simple code-related features only appear as important features at the lowest levels of the models, while end-to-end features tended to mirror those features typically derived from manual feature engineering efforts (e.g., invalid checksum, presence of certiﬁcates in signed binaries, etc.).",
"Meanwhile, import-related features (i.e., Windows APIs) are present throughout all levels of our analysis, from the embed- ding layer to end-to-end features.",
"Perhaps the most important of these ﬁndings is a paradoxical one in which increased training data volume and regularization results in more generic features applicable to both classes, but decreased classiﬁcation efﬁcacy – perhaps indicating a degree of bias toward malware- oriented features is beneﬁcial in malware classiﬁcation.",
"arXiv:1903.04717v2 [cs.LG] 20 Mar 2019 Raw Bytes 102,400 bytes Embedding Layer (10 Dimensions) Convolutional + Max Pooling Layers x5 Fully-Connected Layer Sigmoid P(Malware) Byte Relationships High-level End-to-End Features Low-Level Features Fig.",
1: High-level CNN architecture.,
The remainder of the paper is organized as follows.,
"We begin in Section II by providing background about the repre- sentative architecture we examine, the data used to train and evaluate the models, and the tools used in our experiments.",
"Next, in Sections III through V, we compare our ﬁndings for the embedding, low-level features, and end-to-end features from three models trained with increasing levels of data volume and regularization.",
"Finally, in Section VI, we provide discussion of the results and their implications for future development on byte-based deep neural network models for malware classiﬁcation.",
"BACKGROUND In this section, we begin by ﬁrst discussing the primary components of our analysis and the overall methodology we will use to extract human-understandable features from the model.",
Model Architecture Previous work on byte-based malware classiﬁers has been heavily inﬂuenced by convolutional neural network architec- tures (CNNs) for character-level text classiﬁcation proposed by Zhang et al.,
"In general, previous models use a CNN architecture over a ﬁxed-length byte sequence with an embed- ding layer and varying numbers of convolutional and pooling layers.",
The MalConv architecture proposed by Raff et al.,
"[2], for instance, uses a single, gated convolutional layer and global pooling to accommodate large input sequences, while Krˇc´al et al.",
[3] uses four convolutional layers and Johns [1] uses ﬁve.,
All architectures except for Krˇc´al et al.,
use a learnable embedding layer.,
"In addition, all architectures treat the input byte sequence equally without taking into account contextual information about structure, syntax, or semantics.",
"The architecture that we use in this paper, as shown in Figure 1, is essentially the same as the one proposed by Johns [1].",
"It contains a learnable, 10-dimensional embedding layer and ﬁve alternating convolutional and pooling layers that hierarchically combine feature outputs from previous layers.",
A single fully-connected layer and sigmoid function are used to perform classiﬁcation.,
"The input length of our classiﬁer is restricted to a 100KB sequence, where longer sequences are truncated and shorter sequences are padded with a distinguished padding symbol (i.e., the input alphabet contains 257 symbols).",
"We note that examining the relation- ship between feature representation and model architecture remains an interesting avenue of future work, though due to space constraints could not be experimentally pursued here.",
Interested readers can ﬁnd a comparison of our ﬁndings with those of Demetrio et al.,
on the MalConv architecture [5] in Section VI.,
"Data In our experiments, we use separate datasets for training, efﬁcacy testing, and activation analysis.",
"Our baseline dataset consists of 15.62M distinct Windows Portable Executable (PE) ﬁles [6] collected between July 2015 and July 2017 from a combination of VirusTotal [7], ReversingLabs [8], and other proprietary sources.",
"The dataset was downsampled using a stratiﬁed sampling strategy from a collection of more than 100M binaries to ensure uniform representation across malware families, and the ﬁnal dataset contained 20% malware and 80% goodware.",
"Additionally, we created a small dataset that is more in line with previous academic work consisting of 7.27M PE ﬁles from the same sources as the baseline dataset, but collected over a much shorter time frame of July 2016 to November 2016.",
No sampling was applied to the small dataset and its proportion of malware was 50%.,
"For testing classiﬁcation performance, we use the complete feed of PE ﬁles provided by VirusTotal and ReversingLabs from June 1, 2018 to August 31, 2018, which contains a total of 16.55M unique PEs with a 50/50 split of malware and goodware.",
We use two small-scale datasets to perform our feature analysis.,
"The ﬁrst is a random sample of 4,000 PE ﬁles taken from our baseline dataset with an equal split between malware and goodware, which we use to understand broad activation trends.",
"The second is a set of six artifacts from the NotPetya, WannaCry, and BadRabbit ransomware families, including loaders, payloads, and encryptors.",
"We use this ransomware dataset to extract speciﬁc, concrete features for examination.",
Our datasets are designed to follow best practices for malware classiﬁcation research presented by Pendlebury et al.,
"[9], including time-based train/test splits, realistic class imbalances, and stratiﬁed sampling of malware family to reduce biases.",
Our validation and testing sets contain samples that are strictly newer than the training set used to train our models.,
"For our baseline dataset, we implement an 80/20 goodware to malware ratio to capture the realistic assumption that the majority of ﬁles encountered by malware classiﬁers on user machines are, in fact, benign [10].",
Our baseline dataset also used stratiﬁed sampling to ensure that no single malware family is over-represented in our data.,
"As a ﬁnal note, no special effort was made to ﬁlter our data sources, and as a result the dataset contains packed and otherwise obfuscated PE ﬁles, in addition to standard binaries.",
2: Visualization of embedding layers for byte-based malware classiﬁers using MDS.,
Colored dots represent outliers discovered using HDBSCAN.,
Outliers not shown for Baseline+Dropout due to space restrictions.,
TABLE I: Training details and results for three byte-based malware classiﬁcation models on our test dataset.,
"Train Data Test Results Model Size Mal:Good F1 AUC Small 7.27M 50:50 0.943 0.98 Baseline 15.62M 20:80 0.919 0.96 Baseline+Dropout 15.62M 20:80 0.869 0.87 C. Methodology We train three CNN models using the above architecture and data: (1) a baseline model using the baseline training dataset, (2) a small model using the small dataset, and (3) a baseline+dropout model using the baseline data with the ad- dition of dropout layers added according to recommendations by Srivastava et al.",
"[11] (with dropout rates of 0.1, 0.25, 0.25, 0.25, 0.5, and 0.5).",
"All models were initialized with Xavier initialization, and trained for ten epochs using the Momentum optimizer with decay.",
Training and validation losses for all models converged before the end of the tenth epoch.,
The results of our initial evaluation of each model on our test dataset are shown in Table I.,
"In general, the performance of the small and baseline models falls in line with the results reported in previous work, while the model using dropout performs signiﬁcantly worse.",
"Notice that, paradoxically, the small model performs the best despite having signiﬁcantly older and fewer data points to train on.",
"This result is at odds with common wisdom that more data and improved regularization will result in better generalization and, consequently, better classiﬁcation performance.",
We will explore the reasons for this in the coming sections by comparing the learned features from each model.,
"EMBEDDING LAYER Here, we start our exploration by comparing the embedding layers of our three models.",
"Since the embedding matrix is treated as a set of learnable parameters during training, the locations of each of the bytes (plus padding symbol) can be manipulated within the 10-dimensional embedding space.",
"Intuitively, if one or more bytes can be used interchangeably within the learned features (particularly those of low-level convolutional ﬁlters), then they will be densely clustered within the embedding space.",
"Conversely, outliers within this space indicate byte values that are distinguished in some way and incur signiﬁcant cost to swap.",
"To identify these clusters and their outliers, we apply hierarchical density-based clustering (HDBSCAN) [12] on the points in the embedding space.",
"As the name suggests, this algorithm extracts clusters based on their density, but unlike traditional DBSCAN does not require near-uniform density among clusters since the appropriate distance (i.e., ϵ) is inferred for each neighborhood from the data, in a hierarchical manner.",
"The two parameters that we control are (1) the minimum number of points necessary to deﬁne a cluster, which we set to two, and (2) the minimum number of samples to deﬁne a neighborhood used to calculate cluster density, which we set to one.",
"Both parameters settings allow for a fairly liberal and relaxed density calculation and should maximize the number of clusters (resp., minimize outliers).",
"We visualize the embedding space, as shown in Figure 2, using metric multi-dimensional scaling (MDS) [13], which we found provides a good approximation for the underlying clustering behavior.",
A few broad trends are immediately evident when com- paring the embedding space visualizations and discovered outliers – the number of outliers and overall sparsity of the space increases as we move from our small model to the regularized baseline+dropout model.,
"The small model has 13 outliers, which increases to 31 for baseline, and up to 158 for baseline+dropout.",
"Both the small and baseline model had a small number of clusters, with one of those clusters containing the vast majority of bytes, indicating that many of the bytes are Fig.",
3: Distribution of top-100 activations across 96 ﬁrst-level ﬁlters.,
4: Distribution of top-100 activation locations across ﬁle offsets.,
interchangeable within higher-level features.,
"At the same time, the increase in sparsity among the models and large numbers of outliers for the baseline+dropout model could be indicative of the models moving toward more restrictive byte sequence features in the low-level convolutional ﬁlters.",
"Additionally, the outliers shared by the small and baseline models are quite informative of what features might be learned at the upper layers of the model.",
"These outliers include typical x86 register values (eax/0x0, ecx/0x1, edx/0x2, ebx/0x3), several ASCII characters (‘e’/0x65, ‘o’/0x6f, ‘t’/0x74), and the ’call’ instruction (0xe8).",
"Notably, the baseline model also includes several more ASCII characters (‘W’/0x57, ‘s’/0x73, ‘\n’/0xa) and instructions (loop/0xe0, loopne/0xe2), which again points to increasing speciﬁcity in the learned features.",
"More generally, however, we see a clear predilection across all models toward features associated with ASCII strings and control ﬂow instructions.",
"As we will see in the next section, these outlier bytes will play a key role in deﬁning the low-level features, such as import names and instruction sequences.",
"LOW-LEVEL FEATURES With the results of our embedding analysis in mind, we now move up one level to the ﬁrst convolutional layer.",
"In our architecture, this layer has 96 ﬁlters with a kernel width of eleven bytes and a stride of one, resulting in 102,390 convolutions over our embedded 100KB input sequence.",
We examine these convolutions in two ways.,
"First, we look for broad trends in the locations and ﬁlter usage for the three models, along with how those trends differ between malware and goodware classes.",
"Second, we look at the speciﬁc features identiﬁed by the models in our ransomware dataset by parsing the binaries using PEFile [14] and disassembling them with BinaryNinja [15].",
"To begin, let us draw some insight into the differences in ﬁlter usage and activation locations across our models by extracting the top-100 activations from each of our 4,000 PE ﬁle samples.",
Figure 3 shows the distribution of these acti- vations across our 96 ﬁrst-level ﬁlters.,
One interesting trend that arises is that utilization of ﬁlters increases dramatically as we go from our small model to the baseline+dropout model.,
"At the same time, the vast majority of activations remain on a single ﬁlter while the remainder become more diffusely spread among the other ﬁlters.",
"While the ﬁlters used in the malware and goodware classes remains mostly the same, the number of malware activations is signiﬁcantly higher than goodware for the small and baseline models, but equalizes for the baseline+dropout model.",
"More concretely, the ﬁlters in the small and baseline models have ﬁve times more malware activations than the baseline+dropout model, which itself shows only a nominal (55% vs. 45%) bias toward malware activations.",
"This is an interesting observation because it hints at the idea that these low-level features are, in some way, more biased toward malware even though the underlying dataset may have an imbalance toward goodware – the small model has 50% goodware, while baseline has 80% goodware.",
"By adding regularization in the form of dropout, the model appears to be learning more generically-applicable features across the two classes, but at the cost of worse overall classiﬁcation performance.",
"We also examine the location of the top activations by ﬁle offset, as illustrated in Figure 4.",
"Again, we observe some dis- tinctive trends as we increase dataset size and regularization, but only in our malware class.",
"In particular, the location of activations moves from the start of the ﬁles (i.e., PE headers) in the small model and gets distributed throughout the ﬁle as we move toward the baseline+dropout model.",
"Most notably, the baseline+dropout model no longer focuses the majority of activations on the PE headers, but instead spreads those acti- vations around the same high-activation areas found in other models.",
"That is, the areas of interest identiﬁed by the models remains relatively stable, but their overall impact on the low- level activations is markedly different.",
"For goodware, on the other hand, the overall distribution looks quite similar across all models, albeit with less concentration toward the start of the ﬁles in the baseline+dropout model.",
"Taken together with the ﬁlter analysis above, we see the impact of regularization as a trend of moving away from heavily-weighted, malware- oriented features and toward more generic features with more uniform activation values.",
TABLE II: Example features from disassembled ransomware.,
"Features Model Strings Instructions Small Filter 71: ‘C’, ‘r’, ‘@’ Filter 16: Push sequences (0x40f0c8L): tGenKey.",
"(0x40f0d0L): CryptDec (0x40f0d8L): rypt.... (0x40f0e0L): CryptEnc (0x40f0e8L): rypt.... (0x10007edbL): je,0x10007ff1 (0x10007ee1L): push,0xff (0x10007ee6L): push,edi (0x10007ee7L): push,0x10007ca5 (0x10007eecL): push,0x4 Baseline Filter 83: ‘r’, ‘s’ Filter 57: Function calls (0x40d850L): ....GetP (0x40d858L): rocAddre (0x40d860L): ss..R.Lo (0x40d868L): adLibrar (0x40d870L): yA....Gl (0x4046b4L): push,0x0 (0x4046b6L): push,0x0 (0x4046b8L): push,0x1 (0x4046baL): push,0x0 (0x4046bcL): call,dword,15042 Baseline+Dropout Filter 11: ‘Directory’ Filter 61: mov sequences (0x40d9e0L): ctoryW.. (0x40d9e8L): N.Create (0x40d9f0L): ,Director (0x40d9f8L): yW....Ge (0x40da00L): tTempPat (0x408d65L): je, 0x408d6a (0x408d67L): mov, dword , edx (0x408d6aL): mov, esi, dword (0x408d6dL): mov, dword, esi (0x408d70L): mov, ecx, dword To further substantiate our intuitions about low-level fea- tures, we now compare the disassembly of the activation locations for several of the most prevalent ﬁlters applied to our ransomware dataset.",
"Doing so provides both a notion of the types of features identiﬁed, as well as how those features vary across our models.",
"Generally, the ﬁlters appear to have two primary types of features: common instruction sequences and ASCII strings (e.g., import names).",
Some example features for each type are shown in Table II.,
"While all ﬁlter activations have a certain level of variability due to incidental detection of the relatively short eleven-byte sequences, there is a clear trend toward more precise, limited features in our baseline and base- line+dropout models versus the small model.",
"That is, ﬁlters for the baseline+dropout model are precisely constrained to very speciﬁc strings and instruction sequences, whereas the small model has relatively loose ﬁlters resulting in more spurious activations.",
"For example, Table II shows import activations for the small and baseline models where the commonality is based on the number of speciﬁc ASCII characters in the convolution, thereby allowing activations for a large class of similar imports, while the baseline+dropout model has entire ﬁlters looking for only a single string, such as “Directory.” Similarly, the most prevalent ﬁlter in the baseline+dropout model, ﬁlter 55, activates solely on areas of padding between sections containing multiple padding bytes (0xff).",
"We observe similar behaviors among the instruction sequences, where increasingly speciﬁc context restrictions are imposed, from an activation containing any push instruction in the small model to a speciﬁc sequence of mov instructions in the case of the baseline+dropout model.",
"Overall, these results highlight some very important prop- erties of our models.",
"For one, we see the relationship between the outlier bytes identiﬁed in our embedding analysis and their role as keys or anchors used to deﬁne many of the most important ﬁlter features in the ﬁrst convolutional layer.",
"There is also certainly a heavy reliance on import name features, with that reliance increasing in the baseline+dropout model.",
"Additionally, the instruction sequences coincidentally capture some useful behavioral features, such as the use of push-call sequences to ﬁngerprint certain types of function calls (e.g., Windows APIs with multiple parameters).",
"Together, the import name and instruction sequence features represent two sides of the same underlying import usage behavior that is often included in manually-derived features, though the connection between the import and where (or if) it is actually used is clearly lost due to the inability of the CNN architecture to understand the offsets and indirection used.",
"Finally, we begin to see clues that the poor performance of the baseline+dropout model may be due to the reduction in importance for malware- oriented features, which again challenges our natural intuition that increased generalization should result in improved classi- ﬁcation performance.",
"V. END-TO-END FEATURES For our ﬁnal set of experiments, we take a holistic view on the learned features by connecting segments of the input sequence with their contribution to the ﬁnal score produced by the model.",
"We use the SHapley Additive exPlanations (SHAP) framework proposed by Lundberg and Lee [16] to assign contributions to each input feature – in our case individual bytes – using concepts derived from cooperative game theory (i.e., Shapley value).",
"Speciﬁcally, we use the GradientSHAP [17] method, which combines the Integrated Gradients [18] and SmoothGrad [19] techniques with the SHAP framework.",
"Intuitively, the method calculates the expectation of the gradi- ents between the given input and a large number of randomly sampled feature vectors generated from a background dataset.",
The end result is a precise attribution of the input features that contributed to a given classiﬁcation.,
"Our experiments used the 4,000 sample analysis dataset as the background set, 1,000 random samples, and no local smoothing when computing SHAP values.",
"We also note that GradientSHAP requires a fully differentiable model, which is incompatible with our embedding layer.",
"Instead we perform the explanation after Checksum Set to 0 Well-formed TimeDateStamp Contains Resource Directory Malicious Benign Offset .text Section Name, .rdata Section Name ImageBase and SectionAlignment Contains Rich Header Checksum Set to 0 .data Section Information (Virtual Size more than x10 larger than Physical Size) Missing standard directories (no certificate, no exceptions) Export and Import Table for Embedded PE File Malicious Benign Offset Contains Resource Directory Imports: InternetCloseHandle, InternetOpenURLA, CreateServiceA Malicious Benign Offset .data Section Information (Virtual Size more than x10 larger than Physical Size) Contains IAT, Contains Rich Header Fig.",
5: Distribution of SHAP values for the WannaCry worm.,
SHAP values greater than zero indicate maliciously-oriented features.,
The largest malicious and benign SHAP values are annotated with their underlying semantics in the disassembly.,
embedding and recover each byte’s SHAP value by taking their sum over the embedding dimensions.,
"We apply GradientSHAP to each ransomware artifact in our analysis dataset and, like our low-level ﬁlter analysis, reference the disassembly of the ﬁles to connect the byte- oriented contributions to human-understandable features.",
"To make the analysis of features easier, we combined contiguous bytes with the same contribution directionality (i.e., toward malware or goodware) into segments and deﬁne the segment SHAP value as the sum of the constituent byte SHAP values.",
Figure 5 shows these segment SHAP values for the Wan- naCry worm (SHA1: db349b97c37d22f5ea1d1841e3c89eb4).,
"Positive SHAP values indicate that the bytes in the segment are pushing the model toward a classiﬁcation of malware, while negative values indicate a contribution toward a goodware verdict.",
"Starting again with broad observations across the three models, we see that the increasing speciﬁcity of features in the baseline and baseline+dropout models is evident even in our end-to-end analysis.",
"With the baseline+dropout model in particular, the evidence for malware classiﬁcation is concen- trated on a small number of segments related to only a few underlying structures in the PE ﬁle.",
"By contrast, the small model has evidence spread throughout the byte sequence, including segments that overlap with the other two models.",
"At the same time, the small model also includes a large number of spurious benign features that signiﬁcantly reduce the malware probability for the ﬁle.",
"Diving deeper, we see a number of important (i.e., high SHAP value) features annotated in the Figure 5 that clearly align with features typically found via manual feature en- gineering.",
One such feature is the detection of a checksum set to zero in the small and baseline models.,
"While not an inherently malicious property of the ﬁle, a large fraction of malware contain incorrect checksums, which makes it a useful distinguishing feature.",
"Interestingly, in the baseline+dropout model, the checksum-related feature actually ﬂips and be- comes a goodware-oriented feature that checks for a non-zero value – reinforcing our ﬁndings that regularization forces the model away from malware-centric features.",
"Other features of note include detecting the absence of the standard security directory that exists with signed code, identifying import and export tables within an embedded PE ﬁle, and several import names.",
"We hypothesize that these complex, high-level features are learned based on the fact that structures found later in the PE ﬁle are shifted, which creates unusual locality among lower-level features that the model is able to take advantage of due to its hierarchical pooling architecture.",
The results of our analysis on the ransomware artifacts also contained one high-level feature whose role in pushing the model towards benign classiﬁcations was not obvious to us at ﬁrst glance.,
"Speciﬁcally, we identiﬁed several instances where the most important benign end-to-end features referenced the so-called Rich header [20], as annotated in Figure 5.",
"The Rich header is added only by Microsoft’s linker and contains an XOR encrypted set of linker metadata about the binary, such as the number of objects and linker version used.",
"However, since this information is effectively randomized we should not expect the model to learn useful information from this area of the binary.",
"What we believe is happening is that the model is, like other high-level structures, learning whether the Rich header is present based on the location of a string that directly follows the Rich header.",
"If the Rich header is detected, then the model knows that a Microsoft linker is used and, as it turns out, a non-trivial fraction of malware is built using non-Microsoft toolchains (e.g., Delphi) [21], [22].",
"The linker and its version are actually common manually-derived features, and the model appears to have effectively inferred that same feature via the complex interplay of several low-level features, demonstrating the potential of byte-based models to extract unusual but highly-effective features without human intervention.",
"CONCLUSION Byte-based deep learning classiﬁers have proven to be a viable alternative to traditional machine learning for malware classiﬁcation, without the cost of manual feature engineering.",
"Through our experiments, we have explored what the deep neural networks are learning about Windows PE ﬁles in order to separate malware from goodware.",
"Our ﬁndings show that import-related features play a very important role in the operation of the models – appearing at all levels of our analysis, from embedding to end-to-end features.",
"We also discovered that the embedding layer and low-level convolutional ﬁlters learned features related to ASCII strings and instruction sequences, with some ﬁlters capturing useful behavioral indicators like the use of system calls through push- call sequences.",
"However, these instruction-oriented features do not appear in our end-to-end analysis.",
"Instead, the depth and complexity of the model’s architecture allowed it to learn intuitive and meaningful features, including incorrect checksums and the presence of certiﬁcates.",
"The models were also able to learn some novel features, such as the presence of the Rich header as a proxy for linker type information.",
"Many of these features are used in machine learning classiﬁers with manually-derived features (e.g., Saxe and Berlin [23]).",
Perhaps the most interesting result was the paradoxical impact that increased data volume and regularization had on classiﬁcation performance.,
"Although the highly-regularized dropout model learned more general features across both mal- ware and goodware classes, its performance was substantially worse than a model with an older and smaller training set.",
"One explanation for this behavior is that, malware-oriented features (or the lack thereof) are strong indicators of class membership for both malware and goodware – similar to the way traditional antivirus signatures operate.",
"Another potential explanation is that the baseline+dropout model was not trained for long enough to fully converge, which Srivastava et al.",
warn could take 2-3 times longer than the original architecture [11].,
"However, we note that validation loss had plateaued before the ﬁnal training epoch, which makes that explanation somewhat less likely.",
"To underscore the importance of these ﬁndings in identify- ing promising avenues of future work, we can compare our results to those of Demetrio et al., who performed a similar analysis on the MalConv byte-based malware classiﬁer [5].",
"In their analysis, Demetrio et al.",
"found that nearly all of the most inﬂuential activations for the MalConv model occurred in the headers of the PE ﬁle due to its use of a single, gated convolutional layer and global pooling.",
"Overall, the MalConv features were found to be mostly devoid of semantic meaning.",
"These results stand in stark contrast to our own, which show that with sufﬁcient model depth and training data volume, deep neural networks can learn feature representations that closely mimic those created by subject-matter experts.",
"Moreover, our results indicate that inﬂuential features are learned throughout the entire length of the input byte sequence, thereby providing evidence that the type of hierarchical architecture examined here may learn more diverse and robust features.",
"Taken as a whole, our results provide interesting insight into the role that architecture, regularization, and data characteristics play in developing byte-based malware classiﬁers, and hopefully improve our overall understanding of the malware classiﬁca- tion problem.",
"REFERENCES [1] J. Johns, “Representation Learning for Malware Classiﬁcation,” Conference on Applied Machine Learning for Information Security, 2017.",
"Available: https://www.ﬁreeye.com/content/ dam/ﬁreeye-www/blog/pdfs/malware-classiﬁcation-slides.pdf [2] E. Raff, J. Barker, J. Sylvester, R. Brandon, B. Catanzaro, and C. Nicholas, “Malware Detection by Eating a Whole Exe,” arXiv preprint arXiv:1710.09435, 2017.",
"[3] M. Krˇc´al, O.",
"ˇSvec, M. B´alek, and O. Jaˇsek, “Deep Convolutional Malware Classiﬁers Can Learn from Raw Executables and Labels Only,” International Conference on Learning Representations (Workshop), 2018.",
"Available: https://openreview.net/forum?id=HkHrmM1PM [4] X. Zhang, J. Zhao, and Y. LeCun, “Character-level Convolutional Net- works for Text Classiﬁcation,” in Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, ser.",
"NIPS’15, 2015, pp.",
"[5] L. Demetrio, B. Biggio, G. Lagorio, F. Roli, and A. Armando, “Explain- ing Vulnerabilities of Deep Learning to Adversarial Malware Binaries,” in ITASEC19, 2019.",
"[6] “Windows Portable Executable,” https://docs.microsoft.com/en-us/ windows/desktop/debug/pe-format.",
"[7] “VirusTotal,” https://www.virustotal.com.",
"[8] “ReversingLabs,” https://www.reversinglabs.com.",
"[9] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro, “TESSERACT: Eliminating Experimental Bias in Malware Classiﬁ- cation across Space and Time,” in Proceedings of the 28th USENIX Security Symposium, 2019.",
"Yen, V. Heorhiadi, A. Oprea, M. K. Reiter, and A. Juels, “An Epidemiological Study of Malware Encounters in a Large Enterprise,” in Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security.",
"ACM, 2014, pp.",
"[11] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut- dinov, “Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting,” J. Mach.",
"1929–1958, Jan. 2014.",
"[12] R. J. Campello, D. Moulavi, and J. Sander, “Density-based Clustering Based on Hierarchical Density Estimates,” in Paciﬁc-Asia conference on knowledge discovery and data mining.",
"Springer, 2013, pp.",
"[13] I. Borg and P. Groenen, Modern Multidimensional Scaling.",
Theory and Applications.,
"Springer, 1997.",
"[14] “PEFile,” https://github.com/erocarrera/peﬁle.",
"[15] “BinaryNinja,” https://binary.ninja.",
[16] S. M. Lundberg and S.-I.,
"Lee, “A Uniﬁed Approach to Interpreting Model Predictions,” in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.",
"Curran Associates, Inc., 2017, pp.",
"[17] “GradientSHAP,” https://github.com/slundberg/shap.",
"[18] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic Attribution for Deep Networks,” arXiv preprint arXiv:1703.01365, 2017.",
"[19] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg, “SmoothGrad: Removing Noise by Adding Noise,” arXiv preprint arXiv:1706.03825, 2017.",
"[20] “The Undocumented Microsoft “Rich” Header,” https://bytepointer.com/ articles/the microsoft rich header.htm.",
"[21] P. Rascagneres and W. Mercer, “Delphi Used to Score Against Pales- tine,” https://blog.talosintelligence.com/2017/06/palestine-delphi.html, 2017.",
"[22] I. Muhammad, S. Ahmed, and A. Vaish, “Increased Use of a Delphi Packer to Evade Malware Classiﬁca- tion,” https://www.ﬁreeye.com/blog/threat-research/2018/09/ increased-use-of-delphi-packer-to-evade-malware-classiﬁcation.html, 2018.",
"[23] J. Saxe and K. Berlin, “Deep Neural Network Based Malware Detection Using Two Dimensional Binary Program Features,” in Proceedings of the 10th International Conference on Malicious and Unwanted Software (MALWARE).",
"IEEE, 2015, pp.",
"Medical Imaging with Deep Learning 2022 Short Paper – MIDL 2022 Predicting Thrombectomy Recanalization from CT Imaging Using Deep Learning Models Haoyue Zhang∗1,2 HaoyueZhang@mednet.ucla.edu Jennifer S Polson∗1,2 jpolson@g.ucla.edu Eric J Yang1 EricJYang@ucla.edu Kambiz Nael3 kanael@mednet.ucla.edu William Speier1,3 Speier@ucla.edu Corey W. Arnold1,2,3,4 cwarnold@ucla.edu 1 Computational Diagnostics Lab, University of California, Los Angeles, CA 90024 USA 2 Department of Bioengineering, University of California, Los Angeles, CA 90024 USA 3 Department of Radiology, University of California, Los Angeles, CA 90024, USA 4 Department of Pathology, University of California, Los Angeles, CA 90024, USA Editors: Accepted for publication at MIDL 2022 Abstract For acute ischemic stroke (AIS) patients with large vessel occlusions, clinicians must de- cide if the benefit of mechanical thrombectomy (MTB) outweighs the risks and potential complications following an invasive procedure.",
Pre-treatment computed tomography (CT) and angiography (CTA) are widely used to characterize occlusions in the brain vasculature.,
"If a patient is deemed eligible, a modified treatment in cerebral ischemia (mTICI) score will be used to grade how well blood flow is reestablished throughout and following the MTB procedure.",
An estimation of the likelihood of successful recanalization can support treatment decision-making.,
"In this study, we proposed a fully automated prediction of a patient’s recanalization score using pre-treatment CT and CTA imaging.",
We designed a spatial cross attention network (SCANet) that utilizes vision transformers to localize to pertinent slices and brain regions.,
Our top model achieved an average cross-validated ROC-AUC of 77.33 ± 3.9%.,
This is a promising result that supports future applications of deep learning on CT and CTA for the identification of eligible AIS patients for MTB.,
"Keywords: Acute Ischemic Stroke, Computed Tomography, Deep Learning, Vision Trans- formers 1.",
"Introduction Stroke is the fifth leading cause of death and the leading cause of long-term disability; of the 795,000 new and recurrent strokes each year, acute ischemic stroke (AIS) accounts for 87% of cases (Tsao et al., 2022).",
Mechanical thrombectomy (MTB) is the leading treatment for patients with clots in large blood vessels.,
"In this procedure, a blood clot is surgically removed from an artery to achieve recanalization, i.e., restored blood flow.",
"As a standard measurement for recanalization achieved, a modified treatment in cerebral ischemia (mTICI) score (Tomsick, 2007) is assigned to patients post-treatment.",
"This post-treatment score is clinically significant, as it has been shown that favorable scores, i.e., mTICI 2c or greater, are associated with better clinical outcomes in the long term (´Angel Chamorro et al., 2017).",
∗Haoyue Zhang and Jennifer S Polson contributed equally.,
"© 2022 H. Zhang, J.S.",
"Polson, E.J.",
"Yang, K. Nael, W. Speier & C.W.",
arXiv:2302.04143v2 [eess.IV] 17 Apr 2024 Zhang Polson Yang Nael Speier Arnold Figure 1: Overview of the proposed deep learning architecture.,
"The figure details the entire architecture, neighborhood branch modules, spatial attention transformer module (SAT), and cross attention transformer module (CAT).",
Unfavorable scores (mTICI less than 2c) indicate that the treatment did not effectively clear the blood vessel.,
Imaging has been identified as one modality to illustrate patient physiology that could influence the likelihood of a successful MTB procedure.,
Predicting final mTICI score prior to a procedure can provide doctors and with more more information when considering treatment options.,
"Deep learning has been shown to leverage the amount of detail in images to improve prediction accuracy (LeCun et al., 2015).",
"Current literature presents models that perform semi-automated prediction of mTICI score based on pre- treatment CT imaging, with inconsistent performance (Hilbert et al., 2019; Siddiqui et al., 2021).",
"We propose a fully automated model that uses both CT and CTA images to predict mTICI score post-treatment, incorporating attention modules into a deep learning network to effectively localize to informative stroke regions without requiring manual segmentation.",
Data and Methods The cohort used for this study comprises patients treated from 2012-2019.,
"A patient was included in the cohort if they had CT and CTA imaging, underwent thrombectomy for stroke, and were assigned an mTICI score post-MTB.",
"Of the 254 eligible patients, 69 patients were excluded due to missing either CT or CTA series, and 8 were excluded to due unclear stroke location, leaving 177 patients total.",
"The dataset matched demographic distributions seen in other stroke studies, and the target labels were approximately balanced.",
"Patient images were processed using a previously published pipeline adapted for CT, which included brain extraction and registration to a CT template in MNI space (Zhang et al., 2021b).",
"Utilizing a ResNet backbone, CT and CTA images were used as a slice-wise input (input size 26x224x224) for a global 2D convolutional block, chosen because of the large slice thickness found in stroke protocols (He et al., 2016).",
The outputs from neighboring slices were then fed into ResNet34-based branches that shared weights across slice neighborhoods.,
The model leveraged two versions of transformer attention modules.,
"A spatial attention transformer (SAT) utilized multi-head attention on each slice to focus on salient regions (Dosovitskiy et al., 2021).",
"Within each neighborhood branch, a cross attention transformer (CAT) identified important slices.",
"Finally, the branch outputs were subjected to a weighted softmax layer to ultimately generate binary predictions.",
The architecture and modules are 2 Predicting Thrombectomy Recanalization from Admission CT Imaging Using Deep Learning Models summarized in Figure 1.,
"The model was trained for 200 epochs with early stopping, using the Adam optimizer with weight decay, a batch size of 12, and a learning rate of 0.0001.",
Model ROC-AUC Accuracy Precision Sensitivity Specificity Siddiqui et al.,
0.717 – – – – Hilber et al.,
"0.65 ± 0.10 – – – – Radiomics 0.6859 ± 0.043 0.6877 ± 0.039 0.6417 ± 0.068 0.7425 ± 0.123 0.6421 ± 0.126 ResNet34 0.5840 ± 0.036 0.5656 ± 0.046 0.5410 ± 0.067 0.8500 ± 0.300 0.3253 ± 0.296 SCANet 0.7732 ± 0.039 0.7523 ± 0.042 0.7424 ± 0.145 0.8250 ± 0.174 0.6905 ± 0.215 Table 1: Performance of our current model benchmarked against results from literature as well as previously published models applied to this cohort (Hilbert et al., 2019; Siddiqui et al., 2021).",
Results and Discussion The results of our experiments are summarized in Table 1.,
The average ROC-AUC achieved by SCANet was 0.7732 ± 0.039.,
"This is a significant improvement over the previously published fully automatic deep learning model(Siddiqui et al., 2021).",
"Our method also demonstrates higher and more robust performance metrics than the state-of-the-art model requiring manual clot segmentation (Hilbert et al., 2019).",
"In addition to the literature benchmarks, SCANet performs better than a radiomics-based model and standard deep learning architecture when trained on the same cohort (Zhang et al., 2021a).",
"Clinicians decide to perform MTB based on likelihood of successful recanalization, but it is unknown what factors underlie MTB responses.",
"Clinical images such as CT and CTA contain valuable information to predict procedure outcome, and deep learning models have the capability to learn representations from highly dimensional imaging data.",
"This study sought to predict final MTB recanalization in a fully automatic manner, leveraging re- cent advances in vision transformers to localize to the stroke region.",
We showed that our proposed model outperforms prior fully- and semi-automated machine and deep learning models.,
"The primary limitation of our study is the small sample size, which precludes more robust validation.",
"A few future directions include experimenting on a larger dataset across several institutions, optimizing the preprocessing pipeline to more effectively preserve high resolution CTA, and correlation of the immediate treatment response with long-term out- comes.",
"These steps can produce a model that more accurately predicts MTB recanalization, in turn helping doctors and patients in the treatment decision process.",
References Alexey Dosovitskiy et al.,
An image is worth 16x16 words: Transformers for image recognition at scale.,
"ICLR, 2021.",
Kaiming He et al.,
Deep residual learning for image recognition.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.",
Adam Hilbert et al.,
Data-efficient deep learning of radiological image data for outcome prediction after endovascular treatment of patients with acute ischemic stroke.,
"Computers in Biology and Medicine, 115:103516, 2019.",
"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.",
Deep learning.,
"Nature, 521(7553):436–444, 2015.",
Fazeel M Siddiqui et al.,
Quantitative assessment of hyperdense sign measured by hounsfield units is associated with unsuccessful mechanical thrombectomy.,
"Clinical Neuroradiology, 31(4):1111–1119, 2021.",
Thomas A. Tomsick.,
"Timi, tibi, tici: I came, i saw, i got confused.",
"American Journal Of Neuroradiology, 28(2):382–384, 2007.",
Connie W. Tsao et al.,
Heart disease and stroke statistics-2022 update: A report from the american heart association.,
"Circulation, 145 (8):e391–e426, 2022.",
Haoyue Zhang et al.,
A machine learning approach to predict acute ischemic stroke thrombectomy reperfusion using discriminative mr image features.,
"In 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI), pages 1–4, 2021a.",
Haoyue Zhang et al.,
Intra-domain task-adaptive transfer learning to determine acute ischemic stroke onset time.,
"Computerized Medical Imaging and Graphics, 90:101926, 2021b.",
´Angel Chamorro et al.,
Complete reperfusion is required for maximal benefits of mechanical thrombectomy in stroke patients.,
"Scientific Reports, 7(1):11636, 2017.",
"DeepCFL: Deep Contextual Features Learning from a Single Image Indra Deep Mastan and Shanmuganathan Raman Indian Institute of Technology Gandhinagar Gandhinagar, Gujarat, India {indra.mastan, shanmuga}@iitgn.ac.in Abstract Recently, there is a vast interest in developing image fea- ture learning methods that are independent of the training data, such as deep image prior [35], InGAN [28, 29], Sin- GAN [27], and DCIL [8].",
"These methods perform vari- ous tasks, such as image restoration, image editing, and image synthesis.",
"In this work, we proposed a new train- ing data-independent framework, called Deep Contextual Features Learning (DeepCFL), to perform image synthe- sis and image restoration based on the semantics of the input image.",
The contextual features are simply the high dimensional vectors representing the semantics of the given image.,
DeepCFL is a single image GAN framework that learns the distribution of the context vectors from the input image.,
"We show the performance of contextual learning in various challenging scenarios: outpainting, inpainting, and restoration of randomly removed pixels.",
DeepCFL is appli- cable when the input source image and the generated target image are not aligned.,
We illustrate image synthesis using DeepCFL for the task of image resizing.,
"Introduction Recently, there has been a remarkable success for im- age restoration and image synthesis methods that do not use training data [7,8,27,28,30,31,35] .",
One of the major chal- lenges for the deep feature learning methods above is the limited contextual understanding in the absence of feature learning from training samples [7].,
"Contextual learning is mostly studied for image inpainting [25] and image trans- formation tasks [23], where many pairs of source and target images are used to learn the image context.",
"Restoration of missing pixels in an image is a classical inverse problem [4–6, 10, 18, 32, 33, 44, 46].",
"It addresses various applications such as image editing, restoration of damaged paintings, image completion, and image outpaint- ing.",
"The image transformation model allows formulation for a variety of tasks such as style transfer, single image animation, and domain transfer [23].",
(a) Original (b) Corrupted (c) DIP [35] (d) DeepCFL (e) Image synthesis using DeepCFL (input image in red color frame).,
Figure 1: The ﬁgure show image restoration (ﬁrst row) and im- age synthesis (second row).,
"Here, DIP [35] is a pixel-loss based setup.",
DeepCFL is a single image GAN framework for contextual learning.,
"DeepCFL could ﬁll the masked regions well for image restoration and also perform new object synthesis, which could not be performed using the pixel-based comparison of DIP [35].",
"Traditionally, image restoration is formulated as opti- mization problems, where the objective function includes a loss term and an image prior term, e.g., sparse [1, 9] and low-rank [13] priors.",
The desired image is reconstructed by ﬁnding the solution for the optimization problem.,
"Deep learning models have shown an ability to capture image pri- ors implicitly by minimizing the loss over the training sam- ples [3, 17, 19, 25, 36–38, 40, 43].",
"However, training data- based methods have their limitations, such as generalizabil- ity to new images [7,35].",
"Recently, there is a growing interest in developing meth- ods that are independent of training data to perform image restoration and image synthesis tasks [7,8,27,28,30,31,35].",
Ulyanov et al.,
"proposed deep image prior (DIP) [35], which shows that the handcrafted structure of the convolution neu- ral network (CNN) provides an implicit image prior [35].",
"However, image prior learning using pixel-to-pixel loss in [35] is limited to the tasks which have a spatial correspon- dence between the pixels of the source image and the target image [23].",
One approach would be to learn the internal arXiv:2011.03712v1 [cs.CV] 7 Nov 2020 Figure 2: Contextual Features Learning (DeepCFL).,
The ﬁgure shows the framework for the outpainting.,
"The corrupted image x is fed into the generator G. Here, G is an encoder-decoder network which outputs an image G(x) = y.",
"Next, VGG19 (φ) computes the contextual features φ(x) of x and φ(y) of y.",
Then we compute contextual features loss (CFL) and reconstruction loss (RL) and minimize total loss (TL).,
The main idea of DeepCFL is to synthesize new features by com- paring image statistics at contextual features space.,
Note that CFL compares the context vectors φ(x) and φ(y) using CVL and CAL (see Fig.,
9 for above example).,
Figure 3: The ﬁgure shows the con- text vectors comparison in the adversarial framework.,
φ(x) and φ(y) are computed from the input image x and the restored image G(x) = y.,
The masked regions of x are restored in y.,
"For simplicity, we address restored regions in y as masked regions of y.",
"Here, Dreal and Dfake are the two instances of D which share the network parameters.",
patch distribution from the input image when the source and the target images are not aligned.,
The single image GAN frameworks show applications where the spatial mapping between the source and the tar- get images is not well-deﬁned [27–30].,
Shocher et al.,
"pro- posed an internal learning (IL) framework to synthesize re- alistic image patches using image-conditional GAN, called InGAN [28, 29].",
Shaham et al.,
"showed an unconditional generative model for image synthesis, named SinGAN [27].",
Mastan et al.,
have shown the single image GAN framework for denoising-super resolution and image resizing [8].,
The pixel-to-pixel loss framework in [35] and the inter- nal patch distribution learning frameworks in [27–29] do not perform image reconstruction by considering the con- text of the objects.,
An image could be considered as a collection of high dimensional context vectors [23].,
"These high dimensional vectors are the image statistics captured at the intermediate layers of the features extractor such as VGG19 network [22, 23].",
"An interesting question would be that, given an incomplete image summary, can we syn- thesize new context vectors and use them to reconstruct the image.",
The context of an image is critical to perform im- age restoration and image synthesis tasks (Fig.,
"8) [7, 28, 30, 35].",
We present a single image GAN framework (DeepCFL) which studies the contextual features in the im- age.,
"The problem is novel as it aims to learn the distribution of the contextual features (contextual learning) in the image instead of internal patch distribution, as in the case of In- GAN [28,29] and SinGAN [27].",
We have shown a pictorial representation of DeepCFL in Fig.,
"The aim is to utilize the image features of the origi- nal image I, which are present in the corrupted image x.",
We generate a restored image y which utilizes image features from x.,
We use an encoder-decoder network G to generate y.,
"Then, we iteratively minimize the total loss (TL) between the corrupted image and the restored image.",
TL is a combi- nation of contextual features loss (CFL) and reconstruction loss (RL).,
2 shows that CFL allows feature learning us- ing two different tools: contextual adversarial loss (CAL) and context vectors loss (CVL).,
The detailed description of each component of the framework and the formal deﬁni- tions of the loss functions are described Sec.,
CAL performs distribution matching in the adversarial framework to synthesize new context vectors for the cor- rupted image x. CVL computes the direct difference be- tween the context vectors extracted from the corrupted im- age x and the restored image y.,
"Therefore, in CFL, CAL generates new context vectors and CVL improvises them.",
"RL is a pixel-to-pixel loss (i.e., mean squared error), which ensures the preservation of image features in the restored images.",
"Intuitively, the main idea is to generate new con- text vectors using CFL and map them to the image features implicitly through pixel-based comparison using RL.",
"We have studied the performance of DeepCFL for the following tasks: image outpainting, inpainting of arbitrary holes, and restoration of r% pixels missing in the corrupted image.",
We also show the applications in the presence of non-aligned image data using image resizing.,
The key con- tributions of this work are summarized below.,
• We propose a single image GAN framework for contex- tual features learning (DeepCFL).,
The framework per- forms well on image outpainting tasks (Fig.,
4 and Ta- ble 1).,
We also illustrate that DeepCFL synthesizes new objects when resizing the image (Fig.,
• DeepCFL investigates image reconstruction considering the contextual features.,
The contextual features learning is useful for the applications that use only a single image as input.,
We show the generalizability of DeepCFL by performing multiple applications (Sec.,
"• We provide a detailed analysis of contextual features learning by illustrating reconstruction in various chal- lenging setups such as arbitrary hole inpainting, restora- tion of a high degree of corruption, restoration of im- ages with a word cloud, ablation studies, and limitations (Sec.",
"Related work Deep feature learning captures good image features by using the strong internal data repetitions (self-similarity prior) [11, 15, 30, 42, 45], hand-crafted structure [7, 35], and explicit regularizer [21].",
"DeepCFL is a single image GAN setup, which is different from features learning frame- works proposed earlier [19, 23, 24, 26, 34, 37, 39, 41].",
"Sin- gle image GAN frameworks performs variety of tasks such as image editing [27], retargeting [29], denoising super- resolution [8], and video inpainting [16, 42].",
"Our contex- tual learning framework is somewhat related to [7,8,28,35].",
"InGAN [28, 29] and SinGAN [27] are single image GAN frameworks for learning the internal patch distribution.",
DCIL leverage internal learning with the contextual loss [8].,
"DeepCFL is related to [7, 8, 28, 35] and does not employ a masked patch discriminator for CAL [37].",
It does not use a features expansion network and relies on the features recon- struction capabilities of the encoder-decoder network [37].,
Our Framework DeepCFL is a single image GAN framework to synthe- size new context vectors that are consistent with the seman- tics of the input source image.,
The task is to extract features from the source image and synthesize a new target image.,
The source image could be a clean or a corrupted image.,
The target image could be of the same size as the source image or a different size.,
"For example, in the case of image restoration, we use a corrupted source image with missing pixel values.",
The contextual features are used to ﬁll the missing regions of the corrupted image.,
"For image synthe- sis, a clean image is used to synthesize new images of differ- ent sizes.",
"Below, we discuss image restoration and context vectors before we describe the DeepCFL framework.",
"Let I denote the set of original images, X denote the set of corrupted images, and Y denote the set of restored images.",
"Let x denotes a corrupted image, i.e., x ∈X.",
"x is computed by removing pixels from an original image I using a binary mask m as follows: x = I ⊙m, where ⊙ is the Hadamard product and I ∈I.",
The mask m deﬁnes the underlying image restoration application.,
"For example, in image outpainting of 20% pixels, the mask removes the 10% pixels each along the right side and the left side of the image.",
"For the restoration of r% pixels, the mask contains r% zeros at random locations.",
"For image inpainting, the mask contains arbitrary shapes.",
"The objective is to restore the image details in x, which were removed by m. Image restoration procedure.",
"The task is to generate a new image G(x) = y, which contains the restored pixels.",
"Here, G is the generator network which maps the corrupted image to a restored image y, i.e., y ∈Y .",
The corrupted im- age x could be considered as a source image as it contains the features from the original image I.,
The main intuition is to estimate the context for masked regions of y based on the image features present at the unmasked regions of x (Fig.,
The image restoration process iteratively mini- mizes the loss computed between x and y.,
What are context vectors?,
The context vectors of an im- age I are the image statistics present at intermediate lay- ers of a feature extractor φ(I).,
VGG19 has been widely used to extract image statistics.,
"Formally, given an image I, let φ(I) = {φl(I)}N l=1 denote the set of context vectors extracted from I.",
"Here, φ : I →CV is the pre-trained VGG19 network [12] which maps image I ∈I to its con- text vectors φ(I) ∈CV .",
φl(·) denotes the feature extracted from the layer l of φ(·) and N is the number of layers in φ.,
Why context vectors are important?,
8 show that the contextual learning framework would allow image restoration and image synthesis based on the seman- tics of the input (refer Fig.,
6 for more examples).,
"For example, in the case of restoration of missing pixels, the key observation is to improve the masked regions in the re- store image y using the unmasked regions in the corrupted image x.",
It is done by matching the distribution of the con- textual features of the corrupted image φ(x) and the contex- tual features of the restored image φ(y) (Sec.,
We now discuss the DeepCFL framework shown in Fig.,
"It consists of a generator G, a discrimi- nator D, and a features extractor φ.",
The corrupted image x is fed into G. The generator outputs an image y = G(x).,
"Next, we feed x and y into φ(·) to compute φ(x) and φ(y).",
Then we minimize the total loss (TL) computed between x and y (Eq.,
The two primary components of TL are the contextual features loss (CFL) and the reconstruction loss (RL).,
"CFL synthesizes new context vectors for the masked regions in φ(y), where the features learning procedure is assisted by contextual features in φ(x).",
D is used for com- puting CFL.,
RL is computed between the unmasked regions of x and y to provide image feature consistency in y.,
Network Design Generator.,
The generator G : X →Y maps the source image x ∈X to the target image y ∈Y .,
G is a depth- 5 encoder-decoder network without skip connections (ED).,
"The ED architecture works as the implicit regularizer to sta- bilize the image feature learning [7, 35].",
It exploits the in- herent self-similarity present in the source image.,
We use context normalization [37] to maximize features learning.,
"Intuitively, DeepCFL is unsupervised in the sense that no training data are used to train the generator network for any of the tasks.",
It is a single image GAN framework which uses pre-trained VGG19 as the features extractor.,
VGG19 is widely used in style transfer works for deﬁning loss at VGG features space.,
The feature extractor distills strong prior in the framework [8].,
Discriminator.,
"The discriminator D : CV →M maps the context vectors to a discriminator map µ ∈M, where each entry in µ denotes the probability of the context vec- tor coming from the distribution of the contextual feature of the original image.",
3 illustrates the discriminator task to distinguish context vectors φ(x) and φ(y).,
"The genera- tor G learns the context vectors through its interaction with D. We use a multi-scale discriminator (MSD), where each output is a weighted average of the output from several dis- criminators (we have illustrated D using a single CNN for simplicity in Fig.",
Note that the discriminators in MSD would resize the context vectors.,
Loss Function The goal of the loss function is to maximize the feature learning from source x by comparing it with generated im- age G(x) = y.,
The total loss (TL) is deﬁned in Eq.,
"Ltl(x, y, G, D, φ) = λG Lcfl(x, y, G, D, φ) +λR Lrl(x, y, G) (1) Here, Lcfl denotes CFL and Lrl denotes RL.",
The terms λG and λR are the coefﬁcients of CFL and RL.,
We have picto- rially shown CFL and RL in Fig.,
The total loss described in Eq.,
1 compares the image features in two ways: CFL and RL.,
"CFL provides new image features to y, which are consistent with the object context of x. RL maximizes the likelihood of randomly initialized network weights.",
3.2.1 Contextual Features Loss (CFL) The purpose of CFL is to learn the distribution of context vectors to synthesize image features in y based on the se- mantics of the input x.,
We extract context vectors φ(x) and φ(y) and then minimize the loss described in Eq.,
"Lcfl(x, y, G, D, φ) = λcalLcal(G, D; φ) + λcvlLcvl(φ(x), φ(y)) (2) Here, Lcfl denotes CFL, Lcal denotes CAL, and Lcvl de- notes CVL.",
λcal and λcvl are the coefﬁcients of CAL and CVL.,
2 shows that CFL compares the context vectors in two ways.,
(1) Context vector comparison in the adversarial framework using CAL.,
(2) Contextual features comparison by computing cosine distance in CVL.,
CAL is an adversar- ial loss computed using the generator G and the discrimi- nator D. It is aimed to synthesize new contextual features that are indistinguishable from the features of the source image.,
The CVL computes the difference between contex- tually similar vectors to make the synthesized features of y similar to the features of x.,
Context Adversarial Loss (CAL).,
We have used the LS- GAN [20] variant of the adversarial learning framework.,
"G∗= min G max D Lcal(G, D; φ) (3) Here, G∗is the generator with optimal parameters.",
The loss Lcal is deﬁned in Eq.,
"Lcal(G, D; φ) =Ex∼pdata(x)[(D(φ(x)) −1)2] + Ex∼pdata(x)[D(φ(G(x)))2] (4) Eq.",
4 shows the distribution matching of context vectors of the restored image φ(y) = φ(G(x)) and context vectors of the corrupted image φ(x).,
The discriminator D tries to determine whether the context vectors are from x or y (see Fig.,
"Intuitively, this would help us to ﬁll the context of the masked regions of y = G(x) by learning the context of the objects in unmasked areas in x.",
"We have described G, D, and φ in Sec.",
Context Vector Loss (CVL).,
The main purpose of CVL is to improve the quality of contextual features in φ(y) learned by CAL.,
"Lcvl(φl(x), φl(y)) is the sum of the contextual loss [23] computed at each layer l in φ.",
We have deﬁned CVL for layer l in Eq.,
"Lcvl(φl(x), φl(y), l) = −log(CX(φl(x), φl(y))) (5) Here, CX is the contextual similarity deﬁned using the co- sine distance between the features contained in φl(x) and φl(y).",
"Note that CX is computed by ﬁnding for each fea- ture φl(y)j, a feature φl(x)i that is most similar to it and then summed for all φl(y)j.",
2 illustrate the matched context vectors of φl(x)i and φl(y)j by an arrow.,
"Intu- itively, the feature matching performed between the con- text vectors of masked regions of y and the context vectors of unmasked regions of x enables feature reﬁnements for the new context vectors created by CAL.",
We used conv4 2 layer of φ to compute context vectors as the higher lay- ers capture the high-level content in terms of objects struc- ture [12].,
"It is interesting to note that CVL is different from perceptual loss ∥φl(x) −φl(y)∥, which computes features difference without using contextual similarity criterion.",
(a) Original (b) Corrupted (c) DIP [35] (d) MEDS [7] (e) InGAN [28] (f) DeepCFL Figure 4: Image outpainting.,
The ﬁgure shows the restoration of 20% pixels in the image.,
DIP [35] and MEDS [7] ﬁll the missing regions but do not preserve the structure of the objects.,
"Internal learning of InGAN [28] performed better, but the generated new image features are not very clear.",
DeepCFL incorporates the contextual understanding and is observed to perform better (Table 1).,
3.2.2 Reconstruction Loss (RL).,
RL is aimed to preserve image features and it is computed between corrupted image x and restored image G(x) = y (Fig.,
Let Lrl denotes RL.,
We deﬁne Lrl in Eq.,
"Lrl(G, x, y) = ∥G(x) ⊙m −x∥ (6) Eq.",
6 shows the comparison between unmasked regions of x with the unmasked regions of y.,
"The unmasked regions in x contains image features from I and masked regions in x are corrupted due to mask, i.e., x = I ⊙m.",
RL is a pixel-wise loss and it imposes a strong self-similarity prior [35].,
"Applications Here, we discuss the following applications of Deep- CFL.",
(1) Image outpainting: extension of an image along the sides.,
(2) Image inpainting of irregular holes in the im- age.,
(3) Content-aware image resizing: synthesis of new objects when we resize an image.,
(4) Restoration in the presence of high degree of corruption: 50% pixels1.,
"1We have used original implementations of DIP [35], MEDS [7], and DCIL [8].",
We implemented image restoration using the internal learning of InGAN [28].,
We have provided the implementation details in the sup- plementary material.,
Image Outpainting.,
"Image outpainting relates to image extension, which cre- ates new features while maintaining the semantics of the scene.",
"Image extension uses training data to learn image context and then generates the complete scene given partial information [25, 34, 37, 39].",
Our outpainting task does not use any training samples and synthesize features using only the corrupted image.,
We address outpainting as an image extension for convenience.,
A good image outpainting approach would ﬁll the im- age features based on the semantics of the object present at the boundaries.,
The ability of the generator to synthesize new contextual features over a large spatial extent along the sides depends upon the contextual learning.,
"Unlike pixel- to-pixel loss, the context vectors based loss functions CFL (Eq.",
"2) aims to ﬁll new features in the masked regions of the restored image, which are semantically similar to the unmasked regions of the corrupted image (refer Sec.",
"4, we show outpainting of 20% missing pix- els, where the corrupted image is generated by removing 10% pixels along the right side and the left side.",
"DIP [35], MEDS [7], and InGAN [28] are contextual features learning independent methods.",
"Image outpainting is better achieved (a) Original image (b) Corrupted (c) DIP, 0.90 (d) MEDS, 0.89 (e) InGAN, 0.90 (f) DeepCFL, 0.91 Figure 5: Inpainting.",
"The ﬁgure shows the inpainting of arbitrary holes for DIP [35], MEDS [7], InGAN [28], and DeepCFL (ours).",
DeepCFL minimize the features spillover of trees and attains better perceptual quality.,
using the semantics of the objects in the contextual learning- based DeepCFL framework.,
"Table 1 shows the quantitative comparison on the standard datasets from [14], Set5 and Set14 datasets [7].",
It could be observed that DeepCFL out- performs the other methods for outpainting.,
We have pro- vided more details in the supplementary material.,
"DIP [35] MEDS [7] InGAN [28] DeepCFL SD 0.91 23.73 0.91 21.70 0.92 22.89 0.92 24.13 Set14 0.89 22.12 0.89 20.24 0.90 21.19 0.90 22.52 Set5 0.88 19.03 0.88 19.35 0.89 19.29 0.90 21.50 Table 1: Quantitative comparision using SSIM values (top) and PSNR values (bottom) for image outpainting of 20% pixels on standard dataset (SD), Set5 and Set14 datasets.",
Image Inpainting.,
The input image has non-uniform corrupted regions spread across the entire image in the inpainting task.,
"It is a natural way by which an image could get corrupted [19,26].",
"The critical property to perform inpainting without using training data is to utilize the internal self-similarity prop- erty of the natural images [35,42].",
The computation of the MSE between the generator output and the corrupted image tends to capture strong self-similarity prior [35].,
DeepCFL leverages this learning by incorporating the context vectors comparison.,
The features learning procedure for inpainting is similar to outpainting described in Sec 4.1.,
5 shows the visual results for arbitrary hole inpaint- ing.,
It could be observed that the contextual learning of DeepCFL minimizes the features spillover between differ- ent objects and ﬁll the arbitrary holes considering the se- mantics of the image.,
"The quantitative comparison (SSIM) for inpainting is as follows: DIP [35]: 0.90, MEDS [7]: 0.88, InGAN [29] 0.90, and DeepCFL (ours): 0.91.",
We have provided more comparisons of generated images in the supplementary material.,
DeepCFL performs comparably to other frameworks.,
"The estimation of the parameters from a single image is highly sensitive to the hyper-parameters (e.g., learning rate) [7, 35].",
We believe that the restoration quality of our method and other methods could be improved further using the hyper-parameter search.,
"Image Resize We have discussed image outpainting, which is different from content-aware image resize, where the task is to resize the image while preserving the salient objects of the im- age [29].",
DeepCFL is able to synthesize new objects when resizing the input image (Fig.,
The source image is scaled 2× along the height and the width.,
"Therefore, the pixel cor- respondence between the source and the generated target images is not well deﬁned.",
The image resize is done by us- ing the generator to scale the input and then computing the adversarial loss in a cycle consistent way.,
8 show the challenging scenario of object synthe- sis for various single image GAN frameworks.,
"Inspired by InGAN [29], our framework DeepCFL studies deep con- textual features.",
DeepCFL is different from DCIL [8] as it uses the adversarial framework on VGG features space for image outpainting.,
"In contrast, DCIL uses the adversarial framework on the image space for Denoising-super reso- lution.",
We believe that the results of various single image GAN framework in Fig.,
8 could be improvised further.,
Restoration of 50% pixels.,
"To investigate contextual features leaning in the pres- ence of a high degree of corruption, we perform restoration of 50% missing pixels spread across the entire image uni- formly at random.",
"It is a different setup than outpainting and inpainting, where one has to ﬁll a missing region (i.e., a contiguous array of pixels).",
We further increase the task difﬁculty by using the corrupted image containing a word cloud.,
We denote the above setup as RestoreWC 50% (WC denotes word-cloud).,
It is a challenging setup because the small font present in the corrupted image would require to ﬁll ﬁne image features details.,
We show image restoration in RestoreWC 50% setup in Fig.,
The quantitative comparison (SSIM) for RestoreWC 50% is as follows.,
"DIP [35]: 0.92, MEDS [7]: 0.93, In- GAN [29]: 0.92, and DeepCFL (ours): 0.92.",
It could be ob- served that DeepCFL performs comparably to other frame- works.,
It might be because the image features computed from the highly corrupted image might not be sufﬁcient for (a) Input (b) Seam Carving [2] (c) InGAN [28] (d) DCIL [8] (e) DeepCFL (ours) Figure 6: Image Resize.,
The ﬁgure shows the synthesis of small objects (fruits) and large objects (building).,
Seam Carving (SC) [2] does not preserve the structure well when resizing.,
"For example, the shape of the fruits in small object synthesis is deformed in SC output.",
InGAN [28] preserve the structure for small objects but does not preserve for large object (building).,
DCIL [8] synthesizes new objects when resizing.,
"For example, object structure is preserved well when scaling 2× along the width of the building.",
DeepCFL also preserves object structure when synthesizing new objects.,
It could be observed that DeepCFL does not duplicate the objects along the expended dimension.,
"For example, DeepCFL synthesizes the fruits when resizing (the images are best viewed after zooming).",
"(a) Original (b) Corrupted (c) DIP, 0.87 (d) MEDS, 0.92 (e) InGAN, 0.93 (f) DeepCFL, 0.93 Figure 7: RestoreWC 50%.",
The ﬁgure shows restoration in the presence of a word cloud.,
"DeepCFL restore image features details comparable to DIP [35], MEDS [7], and InGAN [28].",
"(a) Input (b) InGAN [29] (c) DCIL [8] (d) DeepCFL Figure 8: The challenge is to synthesize a new object [8, 29].",
"DeepCFL observed that object synthesis is achievable at a different scale, similar to DCIL [8].",
"DeepCFL output image with better features near the elbow, but the background is not clear.",
restoration in the single image GAN framework.,
"Therefore, contextual learning is a bit less effective.",
We believe that the pixel-based loss would not have the object synthesis abili- ties of the single image GAN frameworks (Fig.,
Ablation Studies and Limitations We show the usefulness of contextual learning in the ad- versarial framework in Fig.,
The restored image features are highlighted in the cropped images.,
It could be observed that the single image GAN framework (DeepCFL) synthe- sizes image features for image restoration.,
"10, we show an ablation study to disentangle the reconstruction using context vector loss (CVL), context ad- versarial loss (CAL), and contextual features loss (CFL) as deﬁned in Sec.",
The CFL setup performs better as it uses adversarial learning and context vector learning together.,
11 shows the restoration in the presence of two dis- (a) Original image (b) Corrupted image (c) DIP [35] (d) DIP [35] + CL (e) DeepCFL Figure 9: Ablation study (1).,
The ﬁgure shows the outpainting of 20% pixels.,
DIP is a pixel-loss based setup.,
We integrated con- textual loss (CL) with DIP [35] in “DIP [35] + CL” to show image restoration using CL and without GAN framework.,
DeepCFL is a GAN framework and it is observed to restore image features well.,
criminator architectures setup: single scale discriminator (SSD) and multiscale discriminator (MSD).,
InGAN [29] shows that MSD improves the performance signiﬁcantly for image synthesis.,
"We observed that higher model capac- ity did not signiﬁcantly improve image restoration, similar to [7] as the masked SSIM for SSD setup is (0.971) is close to MSD setup (0.976).",
"The visual performance enhance- ment would be because MSD setup enforces image statistics consistency at multiple levels, which is harder than solving at a single scale SSD setup.",
Our intuition is that solving a hard problem would help to learn better image features [7].,
"Moreover, quantitative enhancement is close.",
Our interpre- tation of it is as follows.,
MSD in DeepCFL is operating on the context vectors.,
"The scaling of the context vectors in MSD of DeepCFL and scaling the image in [8, 27, 29] are completely different operations.",
The performance enhance- ment for image restoration using the scaling of context vec- tor might not be very effective.,
12 shows the reconstruction when the information in the corrupted image is not sufﬁcient to ﬁll the missing regions.,
The limitation is due to the lack of feature learning from the training samples in the single image GAN frame- work.,
A similar limitation has also been reported for image manipulation tasks [27].,
Restoration of an object which is partially present in the image would also be exciting.,
"How- ever, it is not within the scope of this work.",
13 shows the the restoration of 90% pixels (r = 90) using image features learning from 10% pixels.,
It could be observed that it is difﬁcult to understand the semantics of the scene from 10% pixels.,
The experiment conﬁrms our observation that the adversarial learning of image context is less effective for the high degree of corruption.,
We show more results in the supplementary material.,
"(a) Original image (b) Masked image (c) CAL, 0.88 (d) CVL, 0.91 (e) CFL, 0.92 Figure 10: Ablation study (2).",
"The input is the masked image, which contains 50% corrupted pixels.",
"Here, the contextual feature loss CFL = CAL + CVL.",
It could be observed that CAL and CVL together enhance the restoration quality in CFL.,
(a) Original (b) Masked (c) SSD (d) MSD Figure 11: Ablation study (3).,
The ﬁgure shows text removal in the presence of single-scale discriminator (SSD) and multi-scale discriminators (MSD) setups.,
"SSD setup makes thin marks in the restored output, which are a bit less detectable in the MSD setup.",
Discussion DeepCFL is a single image GAN framework.,
The data- driven supervised feature learning setups use paired exam- ples of ground truth (GT) and corrupted images.,
The cor- rupted images are fed into the network and generated out- puts are matched with the GT image.,
DeepCFL is not (a) Original (b) Masked (c) DIP [35] (d) DeepCFL Figure 12: Limitation (1).,
"The aim is to restore a partially present object in the corrupted image (i.e., head).",
The features in the masked image is not enough to restore head in the mirror.,
"Therefore, we could observe that the reconstruction using DIP [35] and DeepCFL is not performed well.",
(a) Real image (b) Masked image (c) DIP 0.92 (d) MEDS 0.91 (e) InGAN 0.91 (f) DeepCFL 0.91 Figure 13: Limitation (2).,
The ﬁgure shows the restoration of 90% pixels.,
"DeepCFL preserves the image features comparable to DIP [35], MEDS [7], and InGAN [28].",
trained by showing training samples of GT and corrupted images.,
DeepCFL can be fairly compared only with train- ing data-independent methods as they also do not use train- ing samples.,
"Training based methods could synthesize im- age feature details that are not present in the input image, which is not possible in the training data-independent se- tups (Fig.",
12 and Fig.,
"The feature extractor VGG-19 contains layers at different scales, where each layer con- tains varying levels of abstractions.",
We believe that com- bining features from various VGG-19 layers would be help- ful.,
"Moreover, it would increase the model complexity.",
The scope of DeepCFL is limited to the contextual features present in conv4 2 layer.,
We propose as future work to per- form studies on how to increase VGG19 layers for feature comparison while minimizing the computational overhead.,
Conclusion We investigate deep contextual features learning (CFL) in the single image GAN framework for image restoration and image synthesis.,
The main challenge to accomplish the above tasks is when the information contained in the input image is not sufﬁcient for synthesizing the necessary image features.,
"DeepCFL synthesizes image features based on the semantics to perform outpainting, inpainting, restoration of r% pixels, and image resizing.",
"It would be interesting to study the performance of the single image GAN framework in the setting of videos similar to [16,42].",
Acknowledgments.,
Indra Deep Mastan was supported by Visves- varaya Ph.D. fellowship.,
Shanmuganathan Raman was supported by SERB Core Research Grant and SERB MATRICS.,
"References [1] Michal Aharon, Michael Elad, Alfred Bruckstein, et al.",
K- svd: An algorithm for designing overcomplete dictionaries for sparse representation.,
"IEEE Transactions on signal pro- cessing, 54(11):4311, 2006.",
[2] Shai Avidan and Ariel Shamir.,
Seam carving for content- aware image resizing.,
"In ACM Transactions on graphics (TOG), volume 26, page 10.",
[3] Siavash Arjomand Bigdeli and Matthias Zwicker.,
Im- age restoration using autoencoding priors.,
"arXiv preprint arXiv:1703.09964, 2017.",
"[4] Antoni Buades, Bartomeu Coll, and J-M Morel.",
A non-local algorithm for image denoising.,
"In Computer Vision and Pat- tern Recognition, 2005.",
"IEEE Computer Society Conference on, volume 2, pages 60–65.",
"IEEE, 2005.",
"[5] Harold C Burger, Christian J Schuler, and Stefan Harmeling.",
Image denoising: Can plain neural networks compete with bm3d?,
"In 2012 IEEE conference on computer vision and pattern recognition, pages 2392–2399.",
"IEEE, 2012.",
"[6] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian.",
Image denoising by sparse 3-d transform- domain collaborative ﬁltering.,
"IEEE Transactions on image processing, 16(8):2080–2095, 2007.",
[7] Indra Deep Mastan and Shanmuganathan Raman.,
Multi- level encoder-decoder architectures for image restoration.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 0–0, 2019.",
[8] Indra Deep Mastan and Shanmuganathan Raman.,
Dcil: Deep contextual internal learning for image restoration and image retargeting.,
"WACV, 2020.",
"[9] Weisheng Dong, Lei Zhang, Guangming Shi, and Xin Li.",
Nonlocally centralized sparse representation for im- age restoration.,
"IEEE transactions on Image Processing, 22(4):1620–1630, 2012.",
[10] Alexei A Efros and Thomas K Leung.,
Texture synthesis by non-parametric sampling.,
"In Proceedings of the sev- enth IEEE international conference on computer vision, vol- ume 2, pages 1033–1038.",
"IEEE, 1999.",
[11] Michael Elad and Michal Aharon.,
Image denoising via sparse and redundant representations over learned dictionar- ies.,
"IEEE Transactions on Image processing, 15(12):3736– 3745, 2006.",
"[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.",
Im- age style transfer using convolutional neural networks.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016.",
"[13] Shuhang Gu, Qi Xie, Deyu Meng, Wangmeng Zuo, Xi- angchu Feng, and Lei Zhang.",
Weighted nuclear norm min- imization and its applications to low level vision.,
"Interna- tional journal of computer vision, 121(2):183–208, 2017.",
"[14] Felix Heide, Wolfgang Heidrich, and Gordon Wetzstein.",
Fast and ﬂexible convolutional sparse coding.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5135–5143, 2015.",
[15] Daniel Glasner Shai Bagon Michal Irani.,
Super-resolution from a single image.,
"In Proceedings of the IEEE Interna- tional Conference on Computer Vision, Kyoto, Japan, pages 349–356, 2009.",
"[16] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon.",
Deep video inpainting.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5792–5801, 2019.",
"[17] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi.",
Photo-realistic single image super-resolution using a generative adversarial network.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
[18] Anat Levin.,
Blind motion deblurring using image statis- tics.,
"In Advances in Neural Information Processing Systems, pages 841–848, 2007.",
"[19] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro.",
Image inpainting for irregular holes using partial convolutions.,
"In Proceedings of the European Conference on Computer Vision (ECCV), pages 85–100, 2018.",
"[20] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.",
Least squares generative adversarial networks.,
"In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 2794–2802, 2017.",
"[21] Gary Mataev, Peyman Milanfar, and Michael Elad.",
Deepred: Deep image prior powered by red.,
"In Proceedings of the IEEE International Conference on Computer Vision Work- shops, pages 0–0, 2019.",
"[22] Roey Mechrez, Itamar Talmi, Firas Shama, and Lihi Zelnik- Manor.",
Learning to maintain natural image statistics.,
"arXiv preprint arXiv:1803.04626, 2018.",
"[23] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor.",
The contextual loss for image transformation with non-aligned data.,
"European Conference on Computer Vision (ECCV), 2018.",
"[24] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi, and Mehran Ebrahimi.",
Edgeconnect: Generative image inpainting with adversarial edge learning.,
"arXiv preprint arXiv:1901.00212, 2019.",
"[25] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros.",
Context encoders: Feature learning by inpainting.,
"In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 2536–2544, 2016.",
"[26] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li, Shan Liu, and Ge Li.",
Structureﬂow: Image inpainting via structure-aware appearance ﬂow.,
"In Proceedings of the IEEE International Conference on Computer Vision, pages 181– 190, 2019.",
"[27] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli.",
Sin- gan: Learning a generative model from a single natural im- age.,
"The IEEE International Conference on Computer Vision (ICCV), 2019.",
"[28] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani.",
Internal distribution matching for natural image retargeting.,
"arXiv preprint arXiv:1812.00231, 2018.",
"[29] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani.",
Ingan: Capturing and remapping the “dna” of a natural image.,
"In International Conference on Computer Vision (ICCV), 2019.",
"[30] Assaf Shocher, Nadav Cohen, and Michal Irani.",
“zero-shot” super-resolution using deep internal learning.,
"In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3118–3126, 2018.",
[31] Oleksii Sidorov and Jon Yngve Hardeberg.,
"Deep hyper- spectral prior: Single-image denoising, inpainting, super- resolution.",
"In Proceedings of the IEEE International Con- ference on Computer Vision Workshops, pages 0–0, 2019.",
"[32] Denis Simakov, Yaron Caspi, Eli Shechtman, and Michal Irani.",
Summarizing visual data using bidirectional similar- ity.,
"In 2008 IEEE Conference on Computer Vision and Pat- tern Recognition, pages 1–8.",
"IEEE, 2008.",
"[33] Jian Sun, Zongben Xu, and Heung-Yeung Shum.",
Image super-resolution using gradient proﬁle prior.,
"In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8.",
"IEEE, 2008.",
"[34] Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, and William T Free- man.",
Boundless: Generative adversarial networks for image extension.,
"arXiv preprint arXiv:1908.07007, 2019.",
"[35] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.",
Deep image prior.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
"[36] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.",
High-resolution image synthesis and semantic manipulation with conditional gans.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
"[37] Yi Wang, Xin Tao, Xiaoyong Shen, and Jiaya Jia.",
Wide- context semantic image extrapolation.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1399–1408, 2019.",
"[38] Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li.",
High-resolution image inpainting using multi- scale neural patch synthesis.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), vol- ume 1, page 3, 2017.",
"[39] Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, and Shuicheng Yan.",
Very long natural scenery image prediction by outpaint- ing.,
"In Proceedings of the IEEE International Conference on Computer Vision, pages 10561–10570, 2019.",
"[40] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang.",
Generative image inpainting with con- textual attention.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5505– 5514, 2018.",
"[41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang.",
Free-form image inpainting with gated convolution.,
"In Proceedings of the IEEE International Con- ference on Computer Vision, pages 4471–4480, 2019.",
"[42] Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John Collomosse, and Hailin Jin.",
An internal learning approach to video inpainting.,
"In Proceedings of the IEEE International Conference on Computer Vision, pages 2720–2729, 2019.",
"[43] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.",
Learning deep cnn denoiser prior for image restoration.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.",
[44] Lei Zhang and Wangmeng Zuo.,
Image restoration: From sparse and low-rank priors to deep priors [lecture notes].,
"IEEE Signal Processing Magazine, 34(5):172–179, 2017.",
[45] Maria Zontak and Michal Irani.,
Internal statistics of a single natural image.,
"In CVPR 2011, pages 977–984.",
"IEEE, 2011.",
[46] Daniel Zoran and Yair Weiss.,
From learning models of nat- ural image patches to whole image restoration.,
"In 2011 In- ternational Conference on Computer Vision, pages 479–486.",
"IEEE, 2011.",
"THE DEEP ARBITRARY POLYNOMIAL CHAOS NEURAL NETWORK OR HOW DEEP ARTIFICIAL NEURAL NETWORKS COULD BENEFIT FROM DATA-DRIVEN HOMOGENEOUS CHAOS THEORY A PREPRINT Sergey Oladyshkin Department of Stochastic Simulation and Safety Research for Hydrosystems, Institute for Modelling Hydraulic and Environmental Systems, Stuttgart Center for Simulation Science, University of Stuttgart, Pfaffenwaldring 5a, 70569 Stuttgart, Germany Sergey.Oladyshkin@iws.uni-stuttgart.de Timothy Praditia Department of Stochastic Simulation and Safety Research for Hydrosystems, Institute for Modelling Hydraulic and Environmental Systems, Stuttgart Center for Simulation Science, University of Stuttgart, Pfaffenwaldring 5a, 70569 Stuttgart, Germany Ilja Kr¨oker Department of Stochastic Simulation and Safety Research for Hydrosystems, Institute for Modelling Hydraulic and Environmental Systems, Stuttgart Center for Simulation Science, University of Stuttgart, Pfaffenwaldring 5a, 70569 Stuttgart, Germany Farid Mohammadi Department of Hydromechanics and Modelling of Hydrosystems, Institute for Modelling Hydraulic and Environmental Systems, University of Stuttgart, Pfaffenwaldring 61, 70569 Stuttgart, Germany Wolfgang Nowak Department of Stochastic Simulation and Safety Research for Hydrosystems, Institute for Modelling Hydraulic and Environmental Systems, Stuttgart Center for Simulation Science, University of Stuttgart, Pfaffenwaldring 5a, 70569 Stuttgart, Germany Sebastian Otte Neuro-Cognitive Modeling, Computer Science Department, University of T¨ubingen, Sand 14, 72076 T¨ubingen, Germany June 27, 2023 ABSTRACT Artificial Intelligence and Machine learning have been widely used in various fields of mathemat- ical computing, physical modeling, computational science, communication science, and stochastic analysis.",
Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days.,
"Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function.",
"However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the arXiv:2306.14753v1 [cs.NE] 26 Jun 2023 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions.",
"In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE).",
"From the PCE perspective, the (linear) response on each node of a DANN could be seen as a 1st degree multi-variate polynomial of single neurons from the previous layer, i.e.",
linear weighted sum of monomials.,
"From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals.",
"Ad- ditionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications.",
"Therefore, the prevailing han- dling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals.",
"To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN.",
"Doing so, we generalize the conventional structure of DANNs to Deep arbitrary polynomial chaos neural networks (DaPC NN).",
They decompose the neural signals that travel through the multi-layer structure by an adaptive construction of data-driven multi-variate orthonormal bases for each layer.,
"Moreover, the introduced DaPC NN provides an opportunity to go beyond the linear weighted su- perposition of single neurons on each node.",
"Inheriting fundamentals of PCE theory, the DaPC NN offers an additional possibility to account for high-order neural effects reflecting simultaneous in- teraction in multi-layer networks.",
"Introducing the high-order weighted superposition on each node of the network mitigates the necessity to introduce non-linearity via activation functions and, hence, reduces the room for potential subjectivity in the modeling procedure.",
Although the current DaPC NN framework has no theoretical restrictions on the use of activation functions.,
The current paper also summarizes relevant properties of DaPC NNs inherited from aPC as analytical expressions for statistical quantities and sensitivity indexes on each node.,
We also offer an analytical form of partial derivatives that could be used in various training algorithms.,
"Technically, DaPC NNs require similar training procedures as conventional DANNs, and all trained weights determine automatically the corresponding multi-variate data-driven orthonormal bases for all layers of DaPC NN.",
"The paper makes use of three test cases to illustrate the performance of DaPC NN, comparing it with the per- formance of the conventional DANN and also with plain aPC expansion.",
Evidence of convergence over the training data size against validation data sets demonstrates that the DaPC NN outperforms the conventional DANN systematically.,
"Overall, the suggested re-formulation of the kernel network structure in terms of homogeneous chaos theory is not limited to any particular architecture or any particular definition of the loss function.",
The DaPC NN Matlab Toolbox is available online and users are invited to adopt it for own needs.,
"Keywords Artificial Intelligence · Machine Learning · Deep Artificial Neural Network · Polynomial Chaos Expansion · Arbitrary Polynomial Chaos · Orthogonal decomposition · High-order neural interactions · Deep Arbitrary Polynomial Chaos 1 Introduction During the last decades, Artificial Intelligence (AI) and Machine learning (ML) have been widely used in various fields of mathematical computing, physical modeling, computer science, geosciences, communication science, and stochastic analysis.",
The terminology AI has been suggested by John McCarthy in 1956 as a neutral title of a Dartmouth workshop [63] to distinguish the research field from cybernetics and also to escape the influence of its originator Norbert Wiener [114].,
"The closely related term ML has been introduced later in 1959 by Arthur Samuel, where the author explored the logical rules of the game of checkers [90].",
"Originally, AI and ML have been focused on learning strategies employing logical rules, which were often formalized using an apparatus of discrete mathematics and graph theory.",
"However, with increasing computational power [67] and data availability [69], the fields of AI and ML today employ a much broader spectrum of approaches that originate from stochastic analysis, cybernetics, geosciences, information theory and other disciplines.",
"In particular, approaches based on Deep Artificial Neural Networks (DANN) introduced in cybernetics by Alexey Ivakhnenko and Valentin Lapa in 1967 [43] are currently very popular in AI and ML (see e.g.",
[91] for a detailed historical recapitulation).,
"DANNs generalize the concept of Artificial Neural Networks (ANN) suggested by Warren McCulloch and Walter Pitts in 1943 [64] to multi-layer structures , i.e.",
"This form of deep learning also gained a strong visibility in society, providing solutions for a broad variety of tasks including recognition of images [105], videos [24], voice [8] and text [95].",
"2 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Depending on the modelling task, the exact form of the so-called loss function [35] is usually specified to determine the final DANN representation.",
"Various loss functions can be found in literature, suitable for classification tasks [13, 56], Bayesian interpolation [60] or physical regularization [47, 84].",
"In addition, multiple DANN layers and corresponding neural connections can be customized in various ways, turning DANN into convolutional [19, 85], recurrent [38, 117] or other desired architectures [48].",
"Due to the magnificent number of works based on the DANN representation, the corresponding literature can hardly be covered in a research paper, and the authors refer to literature [37, 36] and reviews [18, 92] for further information.",
"The keystone of DANNs can be seen as a certain type of non-linear function that maps from input in Rn to output in R. To construct such a function, DANNs use a multi-layered approach, where each node of a layer is a linear combination of non-linear univariate functions, known as activation functions.",
"Overall, this yields a chain-rule interaction of neu- rons in multi-layered architecture [7].",
"However, there is an alternative branch of approaches that also maps Rn to R using linear combinations of non-linear kernel functions or vectors and, hence, could be seen as one-layer approaches.",
"This alternative ML branch consists of polynomial chaos expansions (PCE) introduced in 1938 [113], Kriging intro- duced in 1951 [55] that is also known as Gaussian process emulator/regression (GPE: [115]) or Wiener–Kolmogorov prediction [27], Support vector regression (SVR) introduced in 1974 [108] and Relevance vector machines (RVM) introduced in 2000 [107].",
"The common fundamental between PCE, GPE, SVR and RVM can be found in [22].",
"Regardless of the different mathematical definitions and structures, both one- and multi-layered ML approaches con- struct the final non-linear representation by determining all their unknown constants.",
"These constants are known as coefficients (terminology for PCE, GPE, SVR and RVN) or weights (terminology for ANN and DANN).",
"For one-layer approaches, the response is established by solving a linear system of equations that encodes the linear combination of non-linear basis functions (polynomials, kernels, vectors, etc.).",
"For multi-layer approaches, the response is constructed by solving a non-linear system of equations that reflects the so-called neural signal processing.",
The current paper does not aim at discussing the pros and contras behind various approaches to map Rn to R. We will rather pay attention to how one-layer findings could be helpful for the multi-layered structure of DANNs.,
"Indeed, let us have a close look into the kernel structure of conventional DANNs.",
It considers the processing of the neural signal in one node as displayed in Figure 1a.,
The response of each node (i.e.,
arrows leaving the central circle) contains linear weighted superposition of single neuron responses (i.e.,
the arrows entering the central circle) coming from the previous layer.,
"In the figure, the weighting is represented by w and the superposition by Σ.",
"To obtain the final neuronal response, the superposition is passed through an activation function A that is usually non-linear.",
The corresponding weights w of the linear superposition for the input to the featured neurons are to be found by training.,
"However, such a linear weighted superposition of incoming neuron outputs on each node could lead to a redundant representation, as it is not necessarily satisfying orthogonality in signal processing.",
This aspect has been addressed in the literature on Support Vector Networks in 1995 [26] employing the SVR concept [108].,
Also imposing orthogonality within the DANNs has been explored in the context of Recurrent Neural Networks [65] assuring the efficiency of the training procedure.,
"The paper [111] highlights the benefit of using orthogonal weight matrices in Recurrent Neural Networks, as they preserve gradient norm during backpropagation making them highly desirable for optimization purposes.",
"However, the imposition of hard constraints on orthogonality within Recurrent Neural Networks may negatively affect convergence speed [111] as applying Gaussian prior regularization may not be appropriate for many applications.",
"Additionally, in order to overcome the difficulty of training Recurrent Neural Networks caused by vanishing and exploding gradients, a new approach have been addressed that learns a unitary weight matrix with eigenvalues [116, 9], enabling optimization in the complex domain.",
"Nevertheless, accounting for orthogonality seems to be very promising, as it could mitigate redundancy in DANN representation [112] and potentially could provide a better ability for generalization [46].",
"Additionally, the actual non-linearity of DANNs is triggered by the non-linearity of the choosen activation functions.",
"However, the choice of the activation functions is an extremely non-trivial task itself [66] and can be very subjective [94], posing additional challenges for DANN users.",
"Very recently, the work [23] suggested to replace non-linear activation functions via non-linear polynomial representations, introducing co-called Π-Nets.",
"The introduced Π-Nets consider high-order polynomial terms, which consistently improves the performance in discriminative and generative tasks for images and audio processing [23].",
"Nevertheless, similar to conventional DANNs, Π-Nets do not yet consider orthogonality in processing the neural signal and, hence, could also lead to redundancy in representation.",
"We argue that employing orthogonal (or even better orthonormal) decompositions for processing neural signals could be extremely relevant, especially for data-poor applications.",
"In the current work, we will take the reader on a journey to the early stages of ML.",
"We will pay special attention to the PCE, introduced via the homogeneous chaos theory by Norbert Wiener in 1938 [113] as already mentioned above.",
"The so-called non-intrusive version of the PCE [33, 59] and its advanced extensions towards sparse quadrature [50], sparse regression [4, 16] or multi-element approaches [6, 57] gained popularity for surrogate building in computation- ally demanding modelling tasks [31, 73].",
They all employ the idea of multi-variate polynomial representation.,
"The 3 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT P d=0,1 A .",
(a) Ψ P ∀d A .,
(b) Figure 1: Sketches of the neural processing in (a) DANN and (b) DaPC NN fundamental concept of PCE theory lies in projection of functions onto a space spanned by orthonormal polynomials that capture the non-linear dependencies.,
"Using the DANN vocabulary, the PCE could be seen as a one-layer ML approach, where the response is approximated as a linear combination of non-linear multivariate orthonormal polynomial basis functions.",
"At the same time, employ- ing the PCE vocabulary, the response of one node in a DANN could be seen as a 1st degree multi-variate polynomial of single neurons from the previous layer, i.e.",
"linear weighted superposition, then passed through an activation function.",
"To be more specific, the 1st degree multi-variate polynomial representation in one node of a DANN consists of 1st degree monomials that reflect the incoming neural signals and the 0th degree term that is known as bias.",
"Obviously, this representation via linear weighted superposition of monomials does not necessarily fulfill the orthogonality or orthonormality condition that is targeted by PCE approach for the majority of applications.",
Such a non-orthogonal linear weighted superposition of incoming neuron outputs is a possibly redundant represen- tation of the overall signal flow: any neural signal could contain some partial information from other neural signals.,
"Similarly, the non-linear polynomial representation in Π-Nets [23] could also profit from orthonormal decomposition of neural signals.",
"Therefore, employing multi-variate orthonormal polynomial bases to process the neural signal on each node, as in the PCE approach, could be beneficial for both conventional DANN structures and for advanced Π-Nets.",
Another approach that combines variational autoencoder and PCE methods is expected to appear soon in the literature forming the PCE-Net model [97].,
"Unfortunately, due to the data-driven nature of all these networks, a direct transfer of classical PCE theory or its generalized extension [120] for constructing orthonormal representations on each node is not possible.",
"The reason is that PCE requires knowledge of probability density functions for all inputs, and this knowledge is not available in the context of DANNs, where the probability density function for all inputs to all nodes in all layers would have to be known.",
"Therefore, the current paper suggests to account for the data-driven nature of neural signals in DANNs and thus uses the data-driven generalization of PCE known as arbitrary polynomial chaos (aPC: [75]) to construct corresponding multi- variate orthonormal representations on each node of DANNs.",
"Doing so, we generalize the conventional structure of DANNs to Deep arbitrary polynomial chaos neural networks (DaPC NN)1.",
They decompose the neural signals traveling through the multi-layer structure through an adaptive construction of data-driven multi-variate orthonormal bases for each layer.,
"Doing so, we provide an opportunity to go beyond the linear weighted superposition of single (univariate) neurons on each node that is traditionally employed in various DANN architectures.",
"Inheriting fundamentals of the PCE, the DaPC NN offers an additional possibility to account for high-order neural effects that reflect simultaneous interaction of neurons in the multi-layer network.",
"Thus, the modeler is prompted to specify the DaPCE NN architecture through not only the number of layers, the number of nodes per layer and the activation function as in conventional DANNs, but also through the desired polynomial degree of non-linearity per layer.",
Figure 1b schematically illustrates the neural processing in the DaPC NN through a multi-variate orthonormal basis Ψ and a corresponding high-order weighted superposition P ∀d .,
"Similar to the conventional DANN, unknown weights of the DaPC NN should be determined by training.",
1The authors of the current paper came across the interesting work under revision [123] denoted as Deep aPCE that seems to be developed in parallel with the current DaPC NN work.,
"In order to avoid any confusion for the reader, we would like to clarify that both works employ the data-driven fundamentals of aPC [75].",
"However, regardless the similarity in the abbreviations, the Deep aPCE makes use of DANNs to construct the aPC expansion coefficients, but the DaPC NN employs aPC representations to construct a re-formulated DANN.",
We refer the reader to the original paper [123] for more details.,
"4 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT In this sense, the paper does not aim to contribute to the DANN architecture, to any particular definition of loss func- tions, or to optimal training schemes.",
"Instead, we offer a mathematical reformulation of the kernel DANN structure in terms of homogeneous chaos theory.",
"Moreover, introducing the high-order weighted superposition on each node may remove the necessity to introduce non-linearity via activation functions if desired.",
"Hence, one can reduce the room for potential subjectivity in the modeling procedure.",
We expect that joining the multi-layered structure of neural networks together with the theory of polynomial chaos expansion could be beneficial for ML tasks and also creates a foundation for further investigations.,
The rest of the paper is structured as follows: Section 2 summarizes the necessary background on the data-driven aPC in Section 2.1 and the conventional DANN structure in Section 2.2.,
Section 2.2 also points out the implicit Gaussian assumptions of neural signal processing in conventional DANN structures from the view point of PCE theory.,
Section 2.3 offers our novel DaPC NN formulation.,
"It introduces the adaptive, deep, multi-variate orthonormal polynomial representation via a multi-layered structure.",
Section 2.3 also provides relevant properties that the DaPC NN inherits from aPC.,
Section 2.4 briefly outlines an example of the conventional training procedure to compute the weights by providing the required partial derivatives.,
Section 3 illustrates the performance of DaPC NN together with a conventional DANN and the aPC.,
"In this comparison, we use exactly the same training data and loss function for DANN, DaPC NN and aPC in a total of three different test cases.",
"Additionally, Section 3 shows evidence of convergence against reference validation data sets, comparing how the size of the training data sets affects the performance of the considered ML approaches.",
2 Machine learning with non-redundant decomposition 2.1 Arbitrary Polynomial Chaos Expansion Theory of polynomial chaos expansion (PCE) was originally introduced by Norbert Wiener [113] in 1938.,
"In PCE, the dependence of the model response (output) on all inputs is expressed using projection onto an orthogonal or orthonormal multi-variate polynomial basis [33, 104].",
"For the current paper, we will employ a purely data-driven generalization of the PCE introduced in 2012 by Oladyshkin and Nowak [75] that will open the pathway for a data- driven deep ML structure.",
"2.1.1 Homogeneus chaos theory Let us consider a model response R(ω) that depends on some multi-dimensional input ω = {ω1, .",
", ωn} from the input space Ω, where n is the number of inputs.",
"Let L2(Ω) denote the L2-space on Ω, weighted by the assumed probability distribution.",
"According to PCE theory [21, 113], the model response R(ω) ∈L2(Ω) can be expanded in ω in the following manner to map Rn to R: R(ω) = ∞ X i=0 wiΨi(ω) ≈ M X i=0 wiΨi(ω), (1) where Ψi(ω) are basis functions from a multi-variate orthonormal (w.r.t.",
"the assumed probability distribution) polyno- mial basis  Ψ0(ω), .",
", ΨM(ω) defined on the input space Ω, and wi are corresponding coefficients that determine the form of the expansion in equation (1).",
The total number of expansion terms M depends on the number of inputs N and on the desired degree d of the polynomial representation as M = (n + d)!/(n!d!).,
"This formulation of M is based on a total-degree truncation, and other alternatives exist [104].",
"For a comprehensive discussion of the require- ments for existence and completeness of an orthonormal polynomial basis  Ψ0(ω), .",
", ΨM(ω) of L2(Ω), we refer to [30, 32, 104].",
"The multi-variate polynomial basis  Ψ0(ω), .",
", ΨM(ω) is comprised of the tensor product of univariate orthonor- mal polynomials  ϕ(0) j , .",
"., ϕ(d) j of degree d for the inputs ωj, assuming that the inputs are statistically independent: Ψα(ω) = N Y j=1 ϕ(αj) j (ωj), N X j=1 αj ≤d.",
"(2) Here α = (α1, .",
", αN) ∈NN 0 is a multivariate index describing polynomial degree that contains the combina- toric information how to enumerate all possible products of individual univariate basis functions and contains the corresponding polynomial degree for input ωj within the univariate polynomials ϕ(αj) j (ωj).",
"The set of polynomials 5 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT  ϕ(αj) j | αj = 0, .",
", d forms an orthonormal basis of polynomial degree at most d for each input ωj: D ϕ(αj) j (ωj), ϕ (α′ j) j (ωj) E L2(Ω) = δαj,α′ j, (3) where δαj,α′ j represents the Kronecker delta ∀αj, α′ j = 0, .",
", d. 2.1.2 Data-driven orthonormal representation The original theory of homogeneous chaos [113] is based on orthonormal Hermite polynomials ϕ(αj) j (ωj) satisfying eq.",
"(3), which are optimal [75, 104] for Gaussian distributed inputs ω.",
"Further extensions to a number of parametric statistical distributions (Gamma, Beta, Uniform, etc.)",
"have been suggested in [119] and [118], based on the Askey scheme [10] of orthonormal polynomials.",
"Such approaches assume an exact knowledge of the involved probability density functions [104], which is not available in various applications or often requires additional assumptions [86].",
"To assure a purely data-driven representation for ML, in the current paper we will consider the data-driven generalization of the polynomial chaos expansion known as the arbitrary polynomial chaos (aPC) introduced in [75].",
The necessity to adapt to arbitrary distributions in practical tasks is discussed in more detail in [72].,
The aPC technique adapts to arbitrary probability distribution shapes of the inputs and can be inferred from limited input data through a few statistical moments [75].,
"Formally, univariate orthonormal polynomials bases ϕ(αj) j (ωj) of polynomial degree αj can be written as a sum of the following monomials: ϕ(αj) j (ωj) = 1 √καj αj X i=0 m(αj) i ωi j, αj = 0, d, (4) where καj = m(αj) αj is a constant representing the norm of the univariate polynomial.",
"The corresponding monomial coefficients m(αj) i can be defined according to the (empirical or theoretical) raw moments of the inputs ωj as follows [75, 79]:   µ0(ωj) µ1(ωj) .",
µαj(ωj) ... ... ... ... µαj−1(ωj) µαj(ωj) .,
µ2αj−1(ωj) µαj(ωj) µαj+1(ωj) .,
"µ2αj(ωj)     m(αj) 0... m(αj) αj−1 m(αj) αj   =   0 ... 0 1  , (5) where µk(ωj) denotes the k-th raw stochastic moment of the input ωj.",
"Therefore, constructing the multi-variate orthonormal polynomial basis  Ψ0(ω), .",
", ΨM(ω) of degree d for an aPC representation R(ω) in equation (1) is based on the data-driven formulation in equations (2), (4) and (5).",
"In particular, the aPC approach is based on 2d raw stochastic moments only.",
These moments can be evaluated directly from an available data set of limited size.,
Matrix on the left-hand side of equation (5) is known as the Hankel matrix of moments [49] and its properties have been analysed in [98].,
"The resulting polynomials are real if, and only if, the Hankel matrix of moments is positive definite, see also the related Hamburger moment problem, e.g.",
"[5, 30, 96, 104].",
"From the practical point of view, the solution of the linear system of equations in (5) can be obtained directly numerically, via lower-order moments representation [75], via recursive relations [1], via Gram- Schmidt orthogonalization [45] or via the Stieltjes procedure [102].",
"Overall, the polynomial representation in equation (1) is one of the oldest ML approaches to map Rn to R .",
"It quantifies the response R to the inputs ω through an orthonormal basis  Ψ0(ω), .",
", ΨM(ω) and computes the expansion coefficients wi 2.",
"Each coefficient wi for i > 0 indicates how much variance one or another term brings into the overall composition (see also relation to global sensitivity analysis in [74, 122]).",
"The unknown expansion coefficients wi can be determined using Galerkin projection [54, 75], numerical integration, regression or collocation approaches [58, 73, 109].",
"2Traditionally, the response R has often been used as an approximation of some full-complexity physical model M in order to learn about the non-linear dependence of model output on modelling inputs ω, i.e.",
R(ω) ≈M(ω).,
"In that sense the PCE projection in equation (1) is often used a surrogate (response surface or reduced model), that is considered to be a special case of supervised machine learning.",
6 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT 2.2 Deep Artificial Neural Networks Let us shortly summarize the key aspects of conventional DANN structures.,
They all rely on the multi-layer concept introduced by Alexey Ivakhnenko and Valentin Lapa in 1967 [43].,
"Similar to the method introduced in Section 2.1, DANNs maps Rn to R by providing an approximation of the response (output).",
"In DANNs, information flows from input nodes, through co-called hidden layers, and then to output [35].",
The term hidden refers to the fact that the computations that occur within the hidden layer are not visible from the outside of the network.,
"Each hidden layer in a neural network contains several hidden nodes, also known as hidden neurons.",
"These nodes are responsible for performing computations on the input data and transmitting the results to other nodes in the network, ultimately leading to the output.",
It has gained significant popularity recently because it is a universal approximator [39] and due to technological advances in computing power.,
"2.2.1 Conventional DANN structure We will consider a conventional Deep Artificial Neural Network (DANN) with L hidden layers and corresponding numbers N (L) of hidden neurons for each layer L (L = 1, .",
"Similar to the ML approach in Section 2.1, DANNs provide the response R(ω) as a non-linear dependence [7] on the same multidimensional input ω = {ω1, .",
", ωn} from the input space Ω, where n is the number of inputs.",
"We will denote the response on each hidden layer L as a vector R(L) = n R(L,1), .",
", R(L,N L)o .",
"This vector represents the responses (outputs) from the corresponding hidden node 1, .",
", N L on the current layer L. Then, the response of the DANN representation R(ω) could be seen [35] as recursive encapsulation of responses from hidden layers that contains N (L) hidden neurons for each layer L. Formally, representation R(ω) could be written in the following form: R(ω) = R(L)  R(L−1)  .",
" R(1) (ω)  , (6) where the input for each hidden layer R(L) is the response from the previous layer R(L−1) (L = 2, .",
"The input for the first layer R(1) is the overall input ω = {ω1, .",
", ωn} Each response R(L,N) of the node N in the hidden layer L is defined according to the ANN representation [35] as follows: R(L,N)  R(L−1) =w(L,N) 0 + M (L) X i=1 w(L,N) i A(L)(R(L−1,i)).",
"(7) Here, R(L−1) = n R(L−1,1), .",
", R(L−1,N L−1)o is the response from the previous layer L −1 (where R(1) = ω), M (L) is the number of weights for node N (L−1), A(L) is the activation function3 for the layer L , w(L,N) 0 is the bias in the node N of the layer L and w(L,N) i (i = 1, .",
", M (L)) are the weights in the node N of layer L. The notation R(L,N) in equation (7) represents the non-activated response in the current paper and, hence, A(L)(R(L,N)) corresponds to the activated response.",
"We would like to clarify to the reader, that there are two common ways [7] to write the equation for the response on a hidden node of a DANN using post and pre-activation function.",
"The post- activation function applies directly to the inputs from the previous layer before computing the weighted sum (as in equation (7)), while the pre-activation function applies to the weighted sum of inputs from the previous layer.",
"However, once the pre-activation formulation is utilized, the output of DANN is generated without use of an activation function.",
"Despite these two different ways of writing the equation, they ultimately lead to the same formal representation of the response as a function of the inputs.",
This is because the two formulations are mathematically equivalent and can be transformed into each other using simple algebraic manipulations.,
There is a variety of non-linear (and also linear) functions that are commonly applied as activation functions A(L) for an arbitrary input I.,
"The most popular activation functions choice are the sigmoid in equation (8), the hyperbolic tangent in equation (9) and the rectified linear unit in equation (10): A(L) sig (I) = 1 1 + e−I .",
"(8) 3The activation function could be also specified for each individual node N in the layer L as A(L,N ), but in order to keep transparency for the reader we keep the formulation where the activation function A(L) is the same for all nodes of the layer 7 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT A(L) tanh(I) = eI −e−I eI + e−I .",
"(9) A(L) ReLU(I) = max(I, 0).",
(10) The decision to choose a specific function depends heavily on the prediction task and the data type [94].,
"Furthermore, care has to be taken when choosing the activation functions, as it can lead to the so-called vanishing or exploding gradient problems during DANN training [3].",
"The weights can be seen as a vector w =  w(L,N) i , i = 1, .",
", Nw with the total number of weights Nw that depends on the number of layers L, the number of hidden neurons N (L) per layer L (L = 1, .",
", L) and the number of weights M (L) per hidden neurons N (L) of the layer L. The weights w(L,N) i of the DANN define the final form of the representation R(ω) in equation (6), and they are determined via a training procedure.",
More details of the training procedure are discussed in Section 2.4.,
"2.2.2 Connection between DANN and aPC According to equation (7), the conventional structure of DANNs propagates the signal from inputs to the response through deep layers, where each node of the layer employs a linear combination of zeroth and first-order monomials.",
The monomials represent the activated output from the nodes of the previous layer.,
"Considering the definitions in Section 2.1, the linear representation via monomials is a particular case of the PCE theory, where the polynomial basis of 0th degree ϕ(0) j and 1st degree ϕ(1) j is defined explicitly as: ϕ(0) j (ω∗) = m(0) 0 , ϕ(1) j (ω∗) = m(1) 0 + m(1) 1 ωi, (11) where ωi is some input of a hidden node N in a layer L. As it have been stated in Section 2.1.1, the PCE theory requires that the set of polynomials (as well in equation 11)) forms an orthonormal basis for each input ωj in the input space Ω, i.e.",
satisfying equation (3).,
"For example, the original theory of homogeneous chaos [113] is based on Hermite polynomials that are orthonormal for Gaussian dis- tributed inputs.",
"The use of any other basses for Gaussian distributed inputs will result in an erroneous non-orthogonal decomposition and considered to be not optimal [75, 104].",
The aPC theory [75] generalizes the original PCE theory by allowing for arbitrary input distributions and constructs the orthonormal polynomial basis from the available data- driven input distribution that is encoded in raw moments.,
"However, the conventional DANN imposes the basis via a particular form of 0th and 1st order polynomial in equation (11), but we do not know for which underlying distri- bution that polynomial basis would satisfy the orthonormality conditions, i.e.",
whether it is optimal.,
"To find out the underlying orthonormal distribution for the conventional DANN that satisfies equation (3), we will explore equations (18) and (22) from the paper [79], that are written as following:   m(0) 0 0 · · · 0 m(1) 0 m(1) 1 · · · 0 ... ... ... ... m(αj) 0 m(αj) 1 · · · m(αj) αj     µ0 µ1 ... µαj  =   1 0 ... 0   and   m(0) 0 0 · · · 0 m(1) 0 m(1) 1 · · · 0 ... ... ... ... m(αj) 0 m(αj) 1 · · · m(αj) αj     µ1 µ2 ... µαj+1  =   m(0) 0 µ1 1 ... 0  .",
"Solving the system of linear equations presented above, we can reconstruct the first two raw moments behind the polynomial basis in equation (11) that are used in the conventional DANN.",
"This process entails: µ1 = 0, µ2 = 1.",
"(12) Therefore, we can conclude that conventional DANN structures implicitly assume a Gaussian distribution with zero mean and unit variance (i.e.",
standard Gaussian distribution) for propagating the signal through hidden layers.,
"In other 8 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT words, the conventional DANN representation optimally preserves orthonormality, if and only if, all neural inputs of all hidden layers were standard Gaussian.",
"However, if the inputs of all hidden layers do not follow a standard Gaussian distribution, then the conventional DANN structure commonly used in machine learning may not be the most optimal way to propagate the signals through the hidden layers.",
"Applied ML tasks could often be used in situations where the propagation of neural signals from layer to layer is not necessarily Gaussian, let alone standardized to unit variance.",
"Therefore, based on results presented in [75] and [79], we conclude that the linear representation via non-orthonormal monomials is not optimal.",
Not even batch normalization [41] of each node’s input could mitigate the mentioned effect due to its linear nature.,
"Theoretically, some general non-linear transformation could map the distributions of all neural inputs onto the Gaussian, but this is not feasible due to the data-driven nature of such neural inputs.",
2.3 Deep Arbitrary Polynomial Chaos Neural Network Let us generalize the structure of conventional DANNs in Section 2.2 to overcome their redundancy caused by the non-orthonormal representation of neural signals.,
"To do so, we will employ the orthogonal representation via the data-driven theory of polynomial chaos expansion introduced in Section 2.1.",
This also introduces the possibility to consider additional high-order interactions between neurons through the non-linear multivariate terms from the PCE representation in the conventional DANN structure in equation (7).,
2.3.1 Deep orthonormal polynomial representation We will consider the number of deep layers L and the corresponding number of neurons N (L) for each layer L as in Section 2.2.,
"Similar to Section 2.1 and Section 2.2, we will map the multi-dimensional input ω = {ω1, .",
", ωn} to a response R(ω).",
"Combining the deep ML representation in equation (6) with the orthonormal expansion in equation (1), we will construct a generalized representation denoted as Deep Arbitrary Polynomial Chaos Neural Network (DaPC NN).",
"Here, the response R(L,N) of the node N in the hidden layer L forms a vector of layer responses R(L) = n R(L,1), .",
", R(L,N L)o as follows: R(L,N)  R(L−1) = M (L) X i=0 w(L,N) i Ψ(L) i h A(L)(R(L−1,1)), ..., A(L)(R(L−1,N L−1)) i , (13) where R(L−1) = n R(L−1,1), .",
", R(L−1,N L−1)o is the neural response from the previous layer L −1 (again with R(1) = ω), M (L) is the total number of terms for each node on the layer L, A(L) is an activation function for the layer L, Ψ(L) i is a multivariate orthonormal (w.r.t.",
"the probability distribution given by response of the previous layer) polynomial from the basis  Ψ(L) 0 , .",
", Ψ(L) M (L) of degree d(L) for the layer L and w(L,N) i are the weights of the node N in layer L, where the term w(L,N) 0 represents bias as in the conventional DANN definition in equation (7).",
The DaPC NN representation in equation (13) reflects the non-linear interaction between the neurons using high-order multivariate terms and orthonormal representation in contrast to DANNs in equation (7).,
"According to Section 2.1, the total number of weights M (L) on each node N of the layer L depends on the number of layer inputs N (L−1) from the previous layer and on the desired degree d(L) of the polynomial representation for the layer L as: M (L) = (N (L−1) + d(L))!",
N (L−1)!d(L)!,
"(14) To ensure the optimal transfer of neural signals from layer to layer and to mitigate redundancy within each node by the DaPC NN representation, we will construct the multivariate orthonormal polynomial basis  Ψ(L) 0 , .",
", Ψ(L) M (L) of degree d(L) for each layer L depending on the layer input, i.e.",
depending on the activated response from the previous layer A(L)(R(L−1)).,
As the input layer (L = 1) directly corresponds to the inputs ω (i.e.,
"R(1) = ω) and the inputs ω follow some arbitrary (but given by each specific application) data-driven distribution, the response R(1) follows the exactly same distribution.",
"For example, the input training data set could be employed in a purely data-driven way to serve as empirical distribution.",
"After that, the responses R(L) (L = 2, .., L) on the all nodes of the multi-layered structure will follow distributions that result from all previous weights, biases, polynomials and activation functions.",
"That 9 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT means, the procedure of orthonormalization proceeds sequentially (forward) through the layers.",
"To maintain the data-driven approach of the orthonormalization procedure, we will utilize the aPC representation for computing the data-driven multi-variate orthonormal basis.",
"This basis is introduced in equations (2), (4) and (5), which will be used to construct the corresponding orthonormal bases  Ψ(L) 0 , .",
", Ψ(L) M (L) in each layer.",
"Automatically, by ensuring the orthogonal decomposition, the weights w(L,N) i of a particular node N and a layer L gain a meaning according to global sensitivity analysis [74]: the weights reflects the partial contribution of each single neuron (linear univariate terms) or simultaneous combination of neurons (non-linear multivariate terms) to the total variance of the response R(L,N) for the node N and the layer L. The training procedure itself will be discussed in Section 2.4, where the weights w = n w(L,N) i , i = 1, .",
", Nw o in equation (13) will be obtained via a similar training procedure as for DANNs.",
"However, independent from any particular training procedure, the weights w determine uniquely the corresponding data-driven orthonormal bases  Ψ(L) 0 , .",
", Ψ(L) M (L) for each layer L (L = 1, .",
", L) of DaPC NNs.",
"Indeed, the orthonormal basis on a particular layers L depends on the neural response from the previous layer R(L−1) only.",
"Remarking that R(1) = ω, the corre- sponding data-driven orthonormal bases  Ψ(2) 0 , .",
", Ψ(2) M (2) for the second layer (i.e.",
"L = 2) can be constructed via equations (2), (4) and (5).",
The neural response R(2) on the second layer itself is again fully determined by the cor- responding weights through equation (13).,
"Taking into consideration the recursive encapsulation of neural responses in equation (6), it is easy to see that orthonormal bases on all layers are dictated by the weights only.",
"Therefore, all trained weights w determine uniquely the corresponding data-driven orthonormal bases  Ψ(L) 0 , .",
", Ψ(L) M (L) for each layer L (L = 1, .",
", L) of DaPC NN without any additionally actions.",
Figure 2 schematically illustrates the structure of the introduced DaPC NN.,
"Similar to DANNs, the structure of DaPC NNs is specified via hidden layers L (L = 1, .",
", L), hidden nodes N (N = 1, .",
", N (L)) and activation functions AL.",
"Each layer L is equipped with an orthonormal basis  Ψ(L) 0 , .",
", Ψ(L) M (L) .Basically, neural signals traveling from layer to layer should pass through a sort of data-driven filter that constructs an optimal orthonormal representation for each layer as illustrated in Figure 2.",
"Such an orthonormal basis is the same for all nodes N (1, .",
", N (L)) of a given layer L and employs the neural signal coming as response R(L−1,N) from the previous layer L −1.",
"Moreover, the suggested DaPC NN structure offers flexibility to specify the degree of non-linearity d(L) for each particular layer L (L = 1, .",
", L) in order to go beyond a linear representation of univariate neurons, if desired.",
2.3.2 Properties of the DaPC NN Let us introduce relevant properties of the DaPC NN that could be useful for practical applications.,
"Property 1: Due to the orthonormal DaPC NN representation, the expected value µ  R(L,N) and total variance σ2  R(L,N) of the response R(L,N) on each particular node N of any layer L (including the last layer forming the total response R) can be quantified analytically [75] using the following explicit form: µ  R(L,N) = w(L,N) 0 , (15) σ2  R(L,N) = PM (L) i=1  w(L,N) i 2 , (16) where the explicit analytical relations in equation (15) are written with respect to the response R(L−1) from the previous layer L and not with respect to the inputs ω.",
"Property 2: Due to the orthonormal DaPC NN representation, Sobol [101] sensitivity indices S(L,N) I of a particular node N and layer L can be explicitly computed as follows according to global sensitivity analysis [74, 103]: S(L,N) I =  w(L,N) I 2 PM (L) i=1  w(L,N) i 2 , (17) where the Sobol index S(L,N) I reflects the relative partial contribution of each single neuron (linear univariate terms) or simultaneous combination of neurons (non-linear multivariate terms) to the total variance of the response R(L,N) for that particular node N in a layer L. 10 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT ω ω1 ω2 ... ωn-1 ωn .",
"L−1 R(L-1,1) R(L-1,2) ... R(L-1,NL-1-1) R(L-1,NL-1) Ψ(L) Ψ(L) 0 Ψ(L) 1 .",
"Ψ(L) M(L)-1 Ψ(L) M(L) L R(L,1) R(L,2) ... R(L,NL-1) R(L,NL) .",
R(L) d(L) Figure 2: An illustration of the DaPC NN structure.,
"The input of layer L is formed by the output of layer L −1 and the orthonormal basis  Ψ(L) 0 , .",
", Ψ(L) M (L) of the polynomial degree d(L).",
Property 3: An analytical form for the partial derivatives of the DaPC NN representation with respect to each particular weight can be obtained by applying the chain rule of differentiation [35].,
"Considering the recursive encap- sulation in equation (6) and the orthonormal representation in equation (13), we obtain the following analytical form for the partial derivative 4 of the response R(L,N) in the node N of the layer L with respect to a weight w(L∗,N∗) k (∀k ∈[1, M (L∗)]) in the node N∗of the layer L∗when L > L∗: ∂R(L,N) ∂w(L∗,N∗) k = M (L) X i=0 w(L,N) i N(L−1) X j=0 ∂Ψ(L) i ∂A(L) · · ∂A(L) ∂R(L−1,j) ∂R(L−1,j) ∂w(L∗,N∗) k !",
"(18) and the following simplified analytical form when L = L∗: ∂R(L∗,N∗) ∂w(L∗,N∗) k = Ψ(L∗) k h A(L∗)(R(L∗−1,1)), ..., A(L∗)(R(L∗−1,N L∗−1)) i , (19) where, similar to above, the same analytical relations for partial derivatives hold for the DaPC NN response R as the response of the last layer R(L).",
"Remark 1: The data-driven aPC representation RaPC(ω) in Section 2.1 is a particular case of a DaPC NN repre- sentation RDaPCNN(ω) when the number of hidden layers is equal to one: RaPC(ω) = RDaPCNN(ω), for L = 1.",
"(20) 4Partial derivatives are functions of the inputs ω 11 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Remark 2: The conventional DANN representation RDANN(ω) in Section 2.2 is a particular case of the DaPC NN representation RDaPCNN(ω) when the degree of expansion in each layer is equal to one and the basis is defined according to equation (11) for a standard Gaussian distribution of neuronal signals in each hidden node: RDANN(ω) = RDaPCNN(ω), for dL = 1, ∀L = 1, .",
", L. (21) Remark 3: The DaPC NN offers a possibility to reflect non-linearities of neural responses both by introducing the high-order expansion d(L) for the responses R(L,N) of the hidden nodes and by introducing non-linearity in the ac- tivation functions A(L).",
"Theoretically, there are no restrictions on the use of activation functions within the current DaPC NN framework according to equation (7).",
The modeler has the freedom to determine an activation function if it would be advantageous for a specific modeling procedure since equations (18) and (19) provide the analytical form of the partial derivatives that include the activation function.,
"However, simultaneously introducing non-linearities both in expansion degrees and in activation functions could lead to difficulties in network training caused by strong numerical round-off errors.",
We would rather suggest to employ linear activation functions while shifting the representation of non-linear dependence to the orthonormal decomposition.,
"Especially, any linear normalization (similar to batch nor- malization [41]) will mitigate potential numerical challenges during the training procedure.",
"Therefore, in the current paper we suggest to employ the analytical relations introduced in Property 1 to have a handle on total (explained) variances for the overall network.",
This can be achieved via the following normalized activation function: A(L)(I) = I −µ (I) σ (I) .,
"(22) 2.4 Computation of weights 2.4.1 Training data The exact forms of aPC in Section 2.1, DANN in Section 2.2 and DaPC NN in Section 2.3 are determined via corre- sponding weights that are typically computed using training data in a training procedure.",
"Let us denote the training inputs by ωT = {ωT1, .",
", ωTN } and the corresponding training responses by RTN = {RT1, .",
", RTN }, where TN is a value greater than zero representing the size of training data sets DTN = {(ωT i, RT i), i = 1, .",
"Such data could be obtained from runs of an original physical model for surrogate modelling [75, 15], or directly from a given database for other learning tasks [28, 56, 40].",
"In order to assure transparency in the current paper, we will consider a fixed training set of size TN, avoiding any data mining procedure such as active learning [78, 93].",
"Thus, the data set DTN is the complete training set for the discussed ML approaches.",
"2.4.2 Training procedure Deep structures like conventional DANNs in Section 2.2 or the newly introduced DaPC NN in Section 2.3 require solving a non-linear system of equations to obtain the unknown weights w(L,N) i .",
There are two possible ways to solve the corresponding system of non-linear equations.,
"The first way is based on deterministic approaches, such as gradient-based search [88] or the Levenberg-Marquardt algorithm [61] that have been widely used in deep learning.",
"In the current paper, we will follow such a popular way for training of DaPC NN to ensure that the training process is comparable to the traditional practices for conventional DANN structures.",
"However, for the sake of completeness we would like to mention, that the underlying problem is ill-posed, so that a unique deterministic solution to the training problem may not exist [2, 121].",
"Therefore, the second way helps to solve the challenges related to ill- posedness of the problem.",
"It is based on stochastic inference [51] by seeing the weights as random variables, and then conditioning the weights w(L,N) i on the available training data [83].",
"However, straightforward stochastic approaches such as Monte Carlo [99] or even Markov chain Monte Carlo [34] are computationally very expensive, and they suffer in cases with high non-linearity and high-dimensionality [81].",
"Recently, there are some works trying to overcome the computational problem of stochastic inference assuming Gaussianity and mutual independence of weights [17].",
"Alternatively, combinations of deterministic approaches with a regularization helping to deal with overfitting [35] have been very popular in the last decades for DANNs.",
"Therefore, in the current paper, we will adopt the widely used approach, and extend the optimization procedure by Tikhonov or ridge regularization [106], to ensure that the DaPC NN training is consistent with the current practice for conventional DANN structures.",
o ensure that the training of the DaPC NN is consistent with the current methodology used for conventional DANN structures.,
"Thus, we consider the following optimization problem regarding the unknown 12 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT weights forming the vector w =  w(L,N) i , i = 1, .",
", Nw for all neurons distributed within the all layers : w = arg minw  1 TN TN X i=1 (R(ωTi) −RTi)2 + 1 Nw Nw X i=1  w(L,N) i 2 , (23) where the weights should be optimized simultaneously to minimize the so-called loss function [35] in equation (23).",
The first term in equation (23) reflects the deviation of the response R from the training data via the mean squared error (MSE).,
The second term corresponds to a regularization known as mean square weights (MSW) 5.,
"Other loss functions are also known in the literature, depending on the overall objective such as a Bayesian interpolation [60], physical regularization [47, 84] or classification tasks [13, 56] .",
"Moreover, the scalability to larger datasets relies on the chosen architecture and training methods.",
The effective model complexity proposed in [70] identifies specific regimes where increasing the number of training samples may negatively impact performance.,
"In order to directly compare the DaPC NN structure with the conventional DANN under identical conditions, we choose the Levenberg-Marquardt algorithm [61] to solve 6 equation (23) for all structures due to its robustness.",
We also use the exact same training procedure for all structures using the exactly same loss-function in equation (23) and the exactly same training data sets DTN .,
"To provide gradients to the Levenberg-Marquardt algorithm, we use the analytical derivatives for DaPC NN introduced in Property 3 of Section 2.3.2, whereas we use typical backpropagation to provide gradients.",
"Thus, the orthogonal bases in DaPC NNs are updated simultaneously with the weights during the training procedure (see also Figure 2).",
"Moreover, as pointed out in Section 2.3.1, the weights determine uniquely the corresponding data-driven orthonormal bases in equations (2) and (2) for each layer of DaPC NNs by solving the linear system of equations (5) introduced in Section 2.1.",
"Consequently, the DaPC NN can be seen as a black box, where only the unknown weights need to be determined using one or another training procedure.",
"This makes it easy for users to operate the DaPC NN in a similar way to the conventional DANN, without requiring them to have any special knowledge of its internal workings.",
The training procedure of ML approaches with single-layer ML representations [29] like the PCE usually determines the unknown weights by solving a linear system of equations.,
"Therefore, the unknown weights wi of the aPC repre- sentation in Section 2.1 are directly obtained by solving a linear system of equations (5) (see details in [73, 12, 11]).",
"For numerical robustness, we use Moore-Penrose inversion [68, 82, 14] (also known as pseudoinverse) to solve the regularized [106] least-squares problem of equation (23) as in the original aPC work [75] that is available in Matlab file exchange [76].",
"In the current paper, we focus on fundamental issues of deep neuronal network structures exploring the concept of orthonormal decomposition and the possibility to introduce high-order neural interaction, rather than delving into specifications of the loss function or training algorithm.",
"Therefore, the reader is invited to adopt the introduced DaPC NN structure for own needs, introducing any own specific loss function or training procedure.",
The DaPC NN is available online for the reader through Matlab file exchange [77].,
"2.4.3 Technical remark on DaPC NN bases Technically, the training procedure specified in equation (23) remains the same for the DaPC NN as well as for the DANN and the aPC, and the authors of the current paper do not aim at any novelty here.",
"However, we would like to underline that the DaPC NN constructs its data-driven orthonormal bases implicitly during the training procedure, based on the input distribution and upon the responses of hidden nodes that react to adjustments of the weights during the training procedure.",
"This means, the orthonormal bases on all layers are adaptively recomputed during the iterative training procedure while searching for the optimal weights.",
It could be seen as re-adjusting of orthonormal bases simultaneously within the iteration procedure over the weights.,
"3 Illustration of Performance and Comparison In the previous Section 2, we have introduced the DaPC NN using the data-driven orthonormal decomposition from aPC theory.",
"The current section will illustrate the performance of the suggested DaPC NN, comparing it with the 5See also Bayesian regularized neural networks in [71]) 6We are aiming to not impose any preferences on the training algorithms in the current paper and we encourage the developers of training algorithms to incorporate their approaches to the DaPC NN framework.",
13 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT performance of a conventional DANN structure and also with an aPC expansion.,
"To do so, we will employ exactly the same training data set DTN =  (ωT i, RT i), i = 1, .",
", TN of the size TN for all three ML approaches.",
The quality of the training itself (e.g.,
MSE) will not be strongly discussed as the training procedure provides only a small discrepancy between the ML model response R(ωT ) and the training data RTN .,
"7 Such a discrepancy partially indicates whether the chosen architecture, loss function and training algorithm are appropriate for analysed problem.",
"Instead, we rather focus on the prediction ability for a separate data set that has not been used during the training procedure.",
"Therefore, we will employ a validation data set [44] DVN =  (ωV i, RV i), i = 1, .",
", VN of the size VN with the validation inputs ωVN = {ωV1, .",
", ωVN } and the corresponding validation responses RVN = {RV1, .",
"Because the quality of the prediction is strongly dependent on the size TN of the available training data set [75, 39], we will assess the prediction quality for validation data sets DVN under various sizes of the training data set TN.",
"To measure the prediction ability, we will assess the mean square error, which is usually used in ML, as well as a weighted error that will be defined below in eq.",
"(24),(25),(26), which is used in the uncertainty quantification community.",
"The mean square error could be also normalized by the variance, converting it to the coefficient of determination if desired.",
"The corresponding estimates of the mean square error MSETN [R(ωVN )], the relative error of mean Eµ TN [R(ωVN )] and the relative error of standard deviation Eσ TN [R(ωVN )] are defined as: MSETN [R(ωVN )] = 1 VN TN X i=1 (R(ωVi) −RVi)2 , (24) Eµ TN [R(ωVN )] = µ [R(ωVN )] −µ [RVN ] µ [RVN ] , (25) Eσ TN [R(ωVN )] = σ [R(ωVN )] −σ [RVN ] σ [RVN ] , (26) where µ [·] is the expected value and σ [·] is the standard deviation over the validation data set.",
The expected value µ[·] and standard deviation σ[·] are obtained via the empirical mean and standard deviation over the validation set DVN .,
"Because we are interested in the overall reliability of the ML response and to guarantee fairness in comparison, we omit the powerful property of aPC in computing the mean and variance without additional evaluations of the response.",
"Instead, we will estimate them numerically using the final aPC response constructed on the validation data set, similarly to DANN and DaPC NN.",
"As test cases, we use examples of functional approximation as in surrogate modeling.",
"To test different aspects, we consider an Ishigami problem with three inputs [42] in Section 3.2, an ON-10 problem with ten inputs [80] in Section 3.3 and also a carbon dioxide benchmark problem with non-linear shock propagation [52] in Section 3.4.",
"3.1 Architecture and technical specification of ML models Architectures of the considered ML models used in the current and upcoming test cases are specified via the total num- ber of layers L, corresponding numbers of nodes N (L) for each layer L as [N (1), .",
", N (L)], degrees of non-linearity d(L) for on each layer L as [d(1), .",
", d(L)], activation functions A(L) (same for all layers L) and loss function (LF).",
"For the sake of transparency, we will also provide the total number of unknown weights or coefficients as Nw in up- coming Sections.",
"This is particularly useful for aPC and DaPC NN, as the number of unknown weights may not be as intuitive as for conventional DANN.",
We will adopt the architectures of the analyzed ML models taking into considera- tion the size TN of the training data sets.,
"Nevertheless, we would like to emphasize that the setup of the corresponding architectures are the responsibility of the ML modeler.",
"For our study, we selected the architectures with equal care and effort for all three compared approaches, trying to achieve the most reliable results.",
"Nevertheless, the degrees of freedom in choosing the number of layers, the number of nodes per layer, the degree of non-linearity (if allowed) and the activation function pose strong challenges onto the modeling procedure, requiring a deep understanding of each underlying ML approach.",
"For instance, in addition to the standard settings for the conventional DANN architecture, the DaPC NN architecture incorporates additional degrees of freedom by choosing the non-linearity degree on specific layers, which introduces unknown weights associated with the number of input neurons from the previous layer, as discussed in Section 2.2.",
Exploring all possible configurations of deep architectures using a trial-and-error approach is 7 14 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT not feasible due to the vast number of combinations.,
"Consequently, developing an adaptive strategy for setting up the architecture could significantly improve the performance of deep multi-layer representations.",
This approach would require further research to minimize the potential for subjectivity in the modeling procedure.,
"The authors of the cur- rent paper recognize the potential of the Bayesian framework for optimizing hyper-parameters such as the number of layers, number of nodes, and degree of non-linearity.",
"However, direct Bayesian analysis seems to be computationally intensive, and it may be necessary to rely on approximate indicators that involve certain assumptions [80].",
"However, this topic goes beyond the scope of the current paper but could be the subject of future research.",
We encourage the reader to test the suggested DaPC NN approach and also known aPC and DANN approaches for own needs.,
The conventional DANN approach used here can be found under Matlab software [62] using the fitnet functionality.,
"The aPC and DaPC NN approaches are available online for the reader through Matlab file exchange [76] and [77], correspondingly.",
"3.2 Ishigami test case Table 1: Architectures of aPC, DANN and DaPC NN for Ishigami test case.",
"ML model TN L N (L) d(L) A(L) Nw LF 10 1 1 2 none 10 MSE+MSW aPC 100 1 1 4 none 35 MSE+MSW 1000 1 1 6 none 84 MSE+MSW 10 3 [6,6,1] [1,1,1] eq.",
"(9) 78 MSE+MSW DANN 100 4 [9,6,3,1] [1,1,1,1] eq.",
"(9) 127 MSE+MSW 1000 4 [10,8,6,1] [1,1,1,1] eq.",
"(9) 189 MSE+MSW 10 2 [3,1] [2,2] eq.",
"(22) 40 MSE+MSW DaPC NN 100 3 [3,3,1] [2,2,2] eq.",
"(22) 70 MSE+MSW 1000 3 [3,3,1] [3,3,2] eq.",
"(22) 130 MSE+MSW 3.2.1 Problem set up As the first test case, we will employ the widely used Ishigami function [42], as it shows strong non-linearity accom- panied with non-monotonicity: M(ω) = sin (ω1) + a sin2 (ω2) + bω4 3 sin (ω1) , (27) where we will use the particular case with a = 7 and b = 0.1.",
"The distribution of the three input random variables ω is given by mutually independent uniform distributions with ωi ∼U(−π, π).",
"0 2 4 6 8 Training Response 0 2 4 6 8 Prediction Response aPC DANN DaPC NN 0 2 4 6 8 Validation Reference 0 2 4 6 8 Validation Response aPC DANN DaPC NN Figure 3: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: Ishigami test case with three inputs and the size of training data set equal to 10.",
"To obtain a training data set, we use Sobol sequences [100] for the underlying distribution of inputs.",
"To analyze how the quality of the prediction depends on the size of the training data set, we use the training data sets DTN of size TN equal to 10, 100 and 1000.",
"To assess the quality of prediction, we generate a validation data set DVN of the size VN = 103, via Monte Carlo sampling [99].",
"15 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT -5 0 5 10 Training Response -5 0 5 10 Prediction Response aPC DANN DaPC NN -5 0 5 10 Validation Reference -5 0 5 10 Validation Response aPC DANN DaPC NN Figure 4: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: Ishigami test case with three inputs and the size of training data set equal to 100.",
"-10 -5 0 5 10 15 Training Response -10 -5 0 5 10 15 Prediction Response aPC DANN DaPC NN -10 -5 0 5 10 15 Validation Reference -10 -5 0 5 10 15 Validation Response aPC DANN DaPC NN Figure 5: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: Ishigami test case with three inputs and the size of training data set equal to 1000.",
"3.2.2 Training and Validation Figures 3-5 demonstrate how predictions made by aPC, conventional DANN and DaPC NN match the training and validation data sets for the size TN of training data equals to 10, 100 and 1000.",
"In principle, all considered ML approaches show the necessary flexibility to approximate the training data of different sizes, but the DaPC NN shows a superior performance with the smallest scatter in the left plots of Figures 3-5.",
It is remarkable here that the DaPC NN is equipped with fewer degrees of freedom (number of weights) in comparison to the DANN .,
Table 1 show the technical specifications for the Ishigami test case.,
"Additionally, Table 1 indicated the total number of unknown weights/coefficients by Nw as for aPC and DaPC NN it could be less intuitive as for conventional DANN.",
"Overall, the selection of the ML model architectures should take into account the size of the training data sets TN to provide certain flexibility during the training procedure.",
"However, flexibility alone is not sufficient to make reliable prediction outside of the training data sample and only shows quality of the training approach .",
"Therefore, we pay stronger attention to the right plots in Figures 3-5, reflecting the validation performance.",
All three approaches are powerless in case of the very small data set used for training (right plot in Figure 3).,
"As expected, increasing the size of training data set to 100 (right plot in Figure 4) or even 1000 (right plot in Figure 5) helps to overcome these issues during the validation phase.",
"In particular, the DaPC NN demonstrates the best performance in the validation phase as well, even with a moderate sample size of training data.",
Figures 3- 5 indicate that the flexibility of DANN tends to have over-fitting issues regarding the regularisation as specified in equation (23).,
"Opposite to theDANN, the DaPC NN shows less issues with over-fitting, inhering the orthonormality from the aPC representation and also the required flexibility from DANN.",
"Due to its definition, the aPC itself seems to have not enough of flexibility to capture the validation data set as well as the DaPC NN.",
"We also have observed in the current and upcoming test cases, that omitting regularization in loss function only slightly degrades the results of DaPC NN and aPC, whereas the conventional DANN shows extremely poor prediction.",
"Hence, pure comparison focusing on least-square solution without regularization seems to be not attractive.",
"16 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT 101 102 103 Number of model runs / training points 10-3 10-2 10-1 100 101 102 Mean Squared Error aPC DANN DaPC NN 101 102 103 Number of model runs / training points 10-4 10-3 10-2 10-1 100 Relative error of Mean Value aPC DANN DaPC NN 101 102 103 Number of model runs / training points 10-4 10-3 10-2 10-1 100 Relative error of Standard Deviatin aPC DANN DaPC NN Figure 6: Performance of aPC, DANN and DaPC NN for Ishigami test case with three inputs: Convergence of Mean Square Error, Relative Error of Mean Value and Relative Error of Standard Deviation relatively the reference validation data set.",
"3.2.3 Evidence of Convergence It Figures 3-5, it appears that DANNs have a stronger spreading of the point clouds and also higher density of spreading in comparison to DaPC NN and aPC models.",
"However, determining the density of spreading based on visual inspection alone can be challenging.",
"Therefore, we will pay closer attention the metrics introduced in the beginning of Section, which provides more clarity on this matter.",
"Figure 6 shows the convergence in terms of mean square error, relative error of the mean value and relative error of the standard deviation as a function of the training data size TN, applied to the validation data set DVN for aPC expansion, conventional DANN and DaPC NN.",
"The figure reveals that a faster convergence has been reached for DaPC NN in terms of all investigated convergence metrics, summarising the observations made in the previous section.",
The relative errors of mean and the standard deviation are smaller with the aPC than with the conventional DANN.,
"3.3 ON-10 test case Table 2: Architectures of aPC, DANN and DaPC NN for ON-10 test case.",
"ML model TN L N (L) d(L) A(L) Nw LF 100 1 1 2 none 66 MSE+MSW aPC 500 1 1 3 none 286 MSE+MSW 1000 1 1 4 none 1001 MSE+MSW 100 4 [9,6,3,1] [1,1,1,1] eq.",
"(9) 175 MSE+MSW DANN 500 4 [15,10,5,1] [1,1,1,1] eq.",
"(9) 386 MSE+MSW 1000 4 [15,10,5,1] [1,1,1,1] eq.",
"(9) 386 MSE+MSW 100 2 [5,1] [2,3] eq.",
"(22) 386 MSE+MSW DaPC NN 500 2 [10,1] [2,3] eq.",
"(22) 946 MSE+MSW 1000 2 [10,1] [2,3] eq.",
"(22) 946 MSE+MSW 3.3.1 Problem set up As second test case, we will consider a non-linear analytical function M(ω) of ten (n = 10) uncertain inputs ω = {ω1, .",
", ωn} from the paper [80]: M(ω) =(ω2 1 + ω2 −1)2 + ω2 1 + 0.1ω1 exp(ω2) + 1 + n X i=3 ω3 i i , (28) where the inputs ω in equation (28) are considered to be independent uniformly distributed with ωi ∼U(−5, 5) for i = 1 .",
"For the current test case, we will consider training data sets DTN of the sizes TN equal to 100, 500 and 1000.",
"Similar to the previous test case, we will generate our training data according to the Sobol sequence [100] based on the 17 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT underlying distributions of inputs ω.",
"As before, we employ a validation data set {(ωV i, RV i), i = 1, .",
", VN} of the size VN = 103 generated by Monte Carlo sampling [99] from the distribution of inputs ω.",
"3.3.2 Prediction and Validation Figures 7-9 show the predictions made by the aPC, DANN, and DaPC NN compared with the corresponding data during the training and validation phases.",
Architectures of the considered ML models are presented in Table 2 of Appendix A.,
"With a low number of training data (Figure 7), the DANN and DaPC NN a show better ability to capture the training data compared to the aPC expansion.",
"Here, the aPC demonstrates a lack of flexibility during training.",
"During validation, however, all methods fail to produce consistent results when the small set is used as shown in Figure 7.",
"When trained with more data (Figure 8 and Figure 9), DaPC NN produces predictions with relatively high accuracy compared to aPC and DANN even during validation; where aPC and DANN do not benefit significantly from the increase of training data.",
"Moreover, the orthonormal structure of the DaPC NN representation handles the overfitting issues extremely well.",
"Table 2 presents the technical specifications for the ON-10 test case, including the total number of unknown weights/coefficients denoted by Nw for all ML models.",
"Again, when selecting an appropriate ML model architecture, it is important to consider the size of the training data set TN that can influence the flexibility of the training process and the predictive power of the model.",
"For example, the number of unknown weights in the DaPC NN can be two times higher than the size of the training data.",
It seems that the high non-linearity of the considered test case strongly limits the aPC to obtain the necessary polyno- mial representation.,
A high degree of freedom is very helpful in this case.,
"Opposite to that, the DANN seems to suffer significantly from over-fitting, regardless of the regularization in that scenario.",
"Overall, the DaPC NN predictions are more consistent, with lower scatter compared to the other methods for the considered test case.",
"0 50 100 Training Response -20 0 20 40 60 80 100 120 Prediction Response aPC DANN DaPC NN 0 50 100 Validation Reference -20 0 20 40 60 80 100 120 Validation Response aPC DANN DaPC NN Figure 7: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: ON-10 test case with ten inputs and the size of training data set equal to 100.",
"0 50 100 150 Training Response 0 50 100 150 Prediction Response aPC DANN DaPC NN 0 50 100 150 Validation Reference 0 50 100 150 Validation Response aPC DANN DaPC NN Figure 8: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: ON-10 test case with ten inputs and the size of training data set equal to 500.",
"18 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT 0 50 100 150 Training Response 0 50 100 150 Prediction Response aPC DANN DaPC NN 0 50 100 150 Validation Reference 0 50 100 150 Validation Response aPC DANN DaPC NN Figure 9: Prediction of aPC, DANN and DaPC NN for Training (left) and Validation (right) data: ON-10 test case with ten inputs and the size of training data set equal to 1000 3.3.3 Evidence of convergence Figure 10 shows convergence in terms of mean square error, relative error of mean value and relative error of standard deviation over the training data size TN.",
"Both the aPC and DANN converge to a similar point, meaning that the performance of both methods is comparable.",
"Figure 10 also indicates that increasing the number of training points does not improve the aPC and DANN performance significantly, again confirming our observation in the previous Section 3.3.2.",
"The DaPCNN, on the other hand, distinctly outperforms the aPC and DANN, with approximately two orders of magnitudes lower prediction errors in validation.",
"The effect of increasing the number of training points can be seen clearly in the DaPCNN performance, showing better learning capacity compared to aPC and DANN.",
"102 103 Number of model runs / training points 102 103 104 105 Mean Squared Error aPC DANN DaPC NN 102 103 Number of model runs / training points 10-3 10-2 10-1 100 Relative error of Mean Value aPC DANN DaPC NN 102 103 Number of model runs / training points 10-3 10-2 10-1 100 Relative error of Standard Deviatin aPC DANN DaPC NN Figure 10: Performance of aPC, DANN and DaPC NN for ON-10 test case with ten inputs: Convergence of Mean Square Error, Relative Error of Mean Value and Relative Error of Standard Deviation relatively the reference validation data set.",
"3.4 CO2 Benchmark problem 0.6 0.8 1 1.2 1.4 Injection rate #10 -4 0 500 1000 1500 2000 Frequency 2 2.5 3 3.5 4 Relative permeability degree 0 200 400 600 800 1000 Frequency 0 0.2 0.4 0.6 0.8 Reservoir porosity 0 500 1000 1500 2000 2500 Frequency Figure 11: Distributions of injection rate (m3/s), relative permeability degree (-), and reservoir porosity (-) 19 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Table 3: Architectures of aPC, DANN and DaPC NN for CO2 benchmark problem employing Sobol sequences.",
"ML model TN L N (L) d(L) A(L) Nw LF 100 1 1 3 none 20 MSE+MSW aPC 500 1 1 5 none 56 MSE+MSW 1000 1 1 10 none 286 MSE+MSW 100 3 [8,6,1] [1,1,1,1] eq.",
"(9) 93 MSE+MSW DANN 500 3 [8,6,1] [1,1,1,1] eq.",
"(9) 93 MSE+MSW 1000 4 [10,8,6,1] [1,1,1,1] eq.",
"(9) 189 MSE+MSW 100 2 [3,1] [2,2] eq.",
"(22) 40 MSE+MSW DaPC NN 500 2 [3,3] [2,2] eq.",
"(22) 80 MSE+MSW 1000 2 [3,1] [3,3] eq.",
"(22) 80 MSE+MSW 3.4.1 Benchmark set up As third test case, we will consider a carbon dioxide (CO2) benchmark model that has already been used to com- pare different ML approaches in the earlier paper [52].",
"The particularity of that problem consists in a strong shock propagation, where various ML approaches relying on smooth functions could suffer a lot.",
"The problem refers to a multi-phase flow in porous media, where CO2 is injected into a deep aquifer and then spreads in a geological forma- tion.",
This yields a pressure build-up and a plume evolution.,
CO2 injection into the subsurface could be a possible practice to mitigate the CO2 emission into the atmosphere.,
The CO2 benchmark model proposed by K¨oppel et al.,
[54] is a reduced version of the model in a benchmark problem defined in the paper [25].,
"This reduction consists of a radial flow in the vicinity of the injection well, and is made primarily due to the high computational demand of the original CO2 model.",
"It is assumed that the fluid properties such as the density and the viscosity are constant, and all processes are isothermal.",
"The CO2 and the brine build two separate and immiscible phases, and mutual dissolution is neglected.",
"Additionally, the formation is isotropically rigid and chemically inert, and capillary pressure is negligible.",
We consider the CO2 saturation to be the quantity of interest as a model response that is a function of the coordinates for space x and time t as introduced in [54].,
"Overall, the considered CO2 benchmark problem is strongly non-linear because the CO2 saturation spreads as a strongly non-linear front that could be challenging to capture via surrogates.",
"For detailed information on the governing equations, the modeling assumptions and numerical approaches, the reader is referred to the original publication [54].",
"Following the comparison study [54], we consider the combined effects of three sources of uncertainty.",
"We take into account the uncertainty of boundary conditions via the injection rate, the uncertainty of constitutive relation introduced via in the relative permeability definitions and the uncertainty of material properties represented by the porosity of the geological formation.",
Figure 11 shows the distribution of these three inputs taken from the original data set [53].,
"Similar to the previous test cases, we will train the aPC, DANN and DaPC NN employing the exactly same training data sets DTN while varying the size TN.",
"However, the final trained ML representation could depend not only on the size TN of the training data, but also on how the training data have been constructed.",
"Indeed, at least for the polynomial representation, the training points must be selected with dedicated strategies in order to avoid additional oscillations known as the Runge phenomenon [89].",
"In particular, wrong training points could lead to a very strong oscillations near the discontinuities (Gibbs effect) of the considered CO2 benchmark.",
"Therefore, it will be also interesting to see how the Runge phenomenon may affect the quality of DANN and DaPC NN training.",
"In order to address that question, we will consider two training strategies.",
"In the first training strategy, we will use the Sobol sequence [100] to construct the training sets of sizes TN equals to 100, 500 and 1000 similarly to the previous test cases.",
"In the second training strategy, we will employ Gaussian integration points [110, 32] to generate the training data sets of sizes TN equal to 27, 216 and 1331 according to the available distribution [75] of inputs displayed in Figure 11.",
"In short, the optimal choice [110] of training points (i.e.",
Gaussian quadrature rule) corresponds to the roots of the polynomial of one degree higher than the order used in the polynomial representation.,
The Gaussian integration points form a full tensor (FT) grid in the space of inputs.,
"This is what leads to sizes of the training data sets equal to 27, 216 and 1331 related to the polynomial representation of 3rd, 5th and 10th degrees correspondingly.",
"Strictly, this training rule can be fully satisfied for aPC representation following the Gaussian quadrature rule [110], but there is no proof for multi-layered structures such as DANN or DaPC NN indicating that the corresponding Gaussian training strategy could mitigate the Runge phenomenon.",
"To assess the quality of prediction, we will employ the validation data set DVN of the size VN = 104 [53] generated according to the variability of the inputs (Monte Carlo approach) shown in Figure 11.",
"20 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Table 4: Architectures of aPC, DANN and DaPC NN for CO2 benchmark problem employing Gaussian integration points.",
"ML model TN L N (L) d(L) A(L) Nw LF 27 1 1 2 none 10 MSE+MSW aPC 216 1 1 5 none 56 MSE+MSW 1331 1 1 10 none 286 MSE+MSW 27 3 [8,6,1] [1,1,1,1] eq.",
"(9) 93 MSE+MSW DANN 216 3 [8,6,1] [1,1,1,1] eq.",
"(9) 93 MSE+MSW 1331 4 [10,8,6,1] [1,1,1,1] eq.",
"(9) 189 MSE+MSW 27 2 [3,1] [2,2] eq.",
"(22) 40 MSE+MSW DaPC NN 216 2 [3,1] [2,2] eq.",
"(22) 40 MSE+MSW 1331 2 [3,1] [3,3] eq.",
(22) 80 MSE+MSW 3.4.2 Prediction and Validation We will reproduce the CO2 saturation along the radial distance from the injection well for the fixed time instance of 100 days in accordance with the CO2 benchmark scenario [54].,
"To do so, we will train aPC expansion, conventional DANN and DaPC NN for each numerical discretion cell (i.e.",
250 times).,
"In this sense, we will construct 250 net- works/expansions that seek to capture features of CO2 displacement in the subsurface.",
The related ML architectures are presented in Table 3 .,
"Figure 12 demonstrates the performance of the considered ML models during validation, using the Sobol sequence training data with TN = 1000.",
All three approaches are far away in capturing the reference values of mean and standard deviation for CO2 saturation even considering a sufficiently large size of the training data.,
All three ML models suffer strongly from the Gibbs effect (i.e oscillations) caused by the strong non-linearity of shock propagation (see examples in [54]).,
The regularization in equation (23) seems to show the most significant effect for the aPC.,
"Opposite to that, using Gaussian integration points as training data helps to mitigate the Runge phenomenon and to reduce the Gibbs effect substantially not only for the aPC as expected, but also for the DaPC NN.",
Corresponding numbers of training points and technical specifications of the considered ML models are presented in Table 4 .,
"Figures 13-15 demonstrate the predictions made by aPC expansion, conventional DANN and DaPC NN.",
"It seems that the DaPC NN inherits partially the aPC properties that help overcome the Runge phenomenon, and the optimal training strategy relying on the distribution of the inputs helps to construct acceptable ML model at low com- putational costs (e.g.",
Figures 13 or Figures 14).,
"Unfortunately, the training strategy based on Gaussian integration points does not improve the performance of the conventional DANN.",
"Indeed, as we have clarified in Section 2.2.2, the conventional DANN structure relies on Gaussian distribution of inputs with zero mean and unit variance, which is not fulfilled in the considered CO2 benchmark case.",
"0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Mean Value of Saturation aPC DANN DaPC NN Reference 0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Standard Deviation of Saturation aPC DANN DaPC NN Reference Figure 12: Prediction of mean (left) and standard deviation (right) using aPC, DANN and DaPC NN for Validation data: CO2 benchmark problem using 1000 Sobol sequences as training data set.",
"3.4.3 Evidence of convergence We will assess the convergence in terms of mean square error, mean value and standard deviation of CO2 saturation over the training data size TN for both previously mentioned training strategies.",
"In order to make the quantities of interest in correspondence with the original benchmark study [52] and its follow-up [87], we will consider the 21 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT 0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Mean Value of Saturation aPC DANN DaPC NN Reference 0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Standard Deviation of Saturation aPC DANN DaPC NN Reference Figure 13: Prediction of mean (left) and standard deviation (right) using aPC, DANN and DaPC NN for Validation data: CO2 benchmark problem using 27 Gaussian integration points as training data set.",
"0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Mean Value of Saturation aPC DANN DaPC NN Reference 0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Standard Deviation of Saturation aPC DANN DaPC NN Reference Figure 14: Prediction of mean (left) and standard deviation (right) using aPC, DANN and DaPC NN for Validation data: CO2 benchmark problem using 216 Gaussian integration points as training data set.",
"0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Mean Value of Saturation aPC DANN DaPC NN Reference 0 50 100 150 200 250 Distance 0 0.2 0.4 0.6 0.8 1 Standard Deviation of Saturation aPC DANN DaPC NN Reference Figure 15: Prediction of mean (left) and standard deviation (right) using aPC, DANN and DaPC NN for Validation data: CO2 benchmark problem using 1331 Gaussian integration points as training data set.",
"following space-averaged quantities: MSETN = 1 250 · 1 VN VN X i=1 ∥R(ωVi) −RVi∥2 L2 , (29) Eµ TN = 1 250 ∥µ[R(ωVN )] −µ[RVN ]∥L2 , (30) Eσ TN = 1 250 ∥σ[R(ωVN )] −σ[RVN ]∥L2 .",
"(31) 22 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Figure 16 shows convergence in terms of the absolute averaged mean square error in equation (29), absolute averaged error of mean value in equation (30) and absolute averaged error of standard deviation in equation (31) over the training data size TN for aPC, conventional DANN and DaPC NN.",
Figure 16 displays the convergence of the analyzed ML approaches trained on the Sobol sequences and as well on the Gaussian integration points (marked by superscript FT).,
We observe that the performance of DANN does not profit from an increasing training data size.,
"Also, it does not benefit from the considered choices of training points and seems to be affected by oscillations caused by the strong non-linearity of the underlying problem.",
"Additionally, increasing the training data size does not help the aPC expansion if the training points are not optimally distributed (see more details in [75]), but it is clearly visible how the use of Gaussian integration points help overcome the Runge phenomenon.",
"Moreover, the Gaussian integration points strongly help in DaPC NN training and also reduce the Gibbs effect, rendering the results more reliable.",
"We also would like to remark, that turning the DaPC NN architecture into a one-layer structure reproduces the aPC results under the same technical specification.",
"However, we have selected the multi-layered DaPC NN architecture as specified in Table 4 for illustration of performance in order to provide the broader picture.",
"Overall, we conclude that all considered ML approaches suffer a lot in capturing the strong shock propagation in the CO2 benchmark model, as all three rely on smooth functions.",
"102 103 Number of model runs / training points 10-4 10-3 10-2 10-1 100 Absolute Mean Squared Error DANN DaPCANN aPCFT DANNFT DaPCANNFT 102 103 Number of model runs / training points 10-4 10-3 10-2 10-1 100 Absolute error of Mean Value 102 103 Number of model runs / training points 10-3 10-2 10-1 100 101 Absolute error of Standard Deviation aPC DANN DaPC NN aPCFT DANNFT DaPC NNFT Figure 16: Performance of aPC, DANN and DaPC NN trained on Sobol sequences and Gaussian integration points for CO2 benchmark problem: Convergence of Absolute Averaged Mean Square Error, Absolute Averaged Error of Mean Value and Absolute Averaged Error of Standard Deviation relatively the reference validation data set.",
"3.5 Final Remarks In the current paper, we offer a view on neural signal processing in deep artificial neural networks from the PCE per- spective, introducing in that way the DaPC NN.",
"By employing PCE fundamentals, we demonstrate that orthonormality conditions in each node of the conventional DANN structure are fulfilled if the neural signal propagating through the multi-layer architecture was normally distributed with zero mean and unit variance.",
"This situation is not necessarily satisfied for the majority of data-driven applications and, hence, could lead to redundant representation, where one neural signal could contain partial information that is also coming from other neurons.",
"Moreover, introducing the PCE into the DANN structure provides an opportunity to go beyond the linear weighted superposition of single univariate neurons on each node.",
"From the modelling perspective, the user is prompted to specify the DaPC NN architecture through the number of layers, number of nodes per layer, activation function and loss function similar as in a conventional DANNs.",
"Addi- tionally, the novel DaPC NN requires specification of the desired degree of non-linearity for each hidden layer.",
"The latter aspect leads to high-order weighted superposition on each node of the network, so that non-linear activation functions become optional.",
This reduces the potential for subjectivity in the modeling procedure.,
"However, it also introduces a new responsibility of choosing the degree of non-linearity, which in turn raises new research questions.",
"Technically, the DaPC NN requires a similar training procedure as any conventional DANN, and all trained weights determine automatically the corresponding multi-variate data-driven orthonormal bases for all layers of DaPC NN.",
The DaPC NN Matlab Toolbox is available online and users are invited to adopt it for own needs [77].,
"To illustrate the performance of the DaPC NN, we have investigated three test cases, comparing the DaPC NN with the conventional DANN and also with aPC expansion.",
"To do so, we have employed identical training procedures for DANN, DaPC NN and aPC, minimizing the mean squared error, regularized via ridge regression.",
"In the training procedure, we employ exactly the same training and validation data sets for all three ML approaches.",
"Additionally, we 23 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT vary the size of the training data set and assess the convergence of the different ML approaches on a validation data set by evaluating the mean square error, error of mean and error of standard deviation.",
"In the first and second test cases, we observe that the DaPC NN reaches superior results in comparison to the aPC and DANN, even considering that we use only a moderate training data size.",
"In the third CO2 benchmark test case, we observe that all considered ML approaches suffer a lot in capturing the strong CO2 shock propagation, as they all rely on smooth functions.",
"However, using Gaussian integration points as training dataset helps to mitigate the Runge phenomenon and reduce the Gibbs effect substantially not only for the aPC but also for the DaPC NN.",
This emphasizes that the DaPC NN inherits partially the aPC properties that help to overcome the Runge phenomenon.,
"Therefore, an optimal training strategy relying on the distribution of the inputs helps to construct an acceptable ML model at low computational costs.",
"Unfortunately, the performance of the DANN does not profit from the considered choices of training points.",
Increasing the training data size does not seem to be helpful against oscillations caused by the strong non-linearity of the underlying problem.,
"The latter point can be explained by the derivations in the current paper, where the conventional DANN structure is shown to be optimal for Gaussian distribution of inputs (and also Gaussian signal processing on each node) that is hardly fulfilled in the considered CO2 benchmark.",
Summary and Conclusions The current paper analyzes the neural signal processing in deep artificial neural networks from the point of view of polynomial chaos (PCE) theory.,
"The response on each node of a deep network has been seen from the PCE perspective, making use of orthonormal polynomial basis functions.",
"The proposed generalization of the conventional structure of DANNs towards the DaPC NN decomposes the neural signals, employing an adaptive construction of data-driven multi-variate orthonormal bases on each node in the multi-layer structure.",
"Moreover, by introducing PCE theory to represent the response of each node, it offers an additional opportunity to go beyond the linear weighted superposition of single univariate neurons as in conventional DANN structures.",
"In that sense, the introduced DaPC NN structure can be seen as a generalization of the conventional DANN with incorporation of aPC theory.",
The newly introduced DaPC NN assures orthonormal decomposition on each node and also offers an additional possibility to account for high-order neural effects.,
"Doing so, the weights gain a clear meaning according to global sensitivity analysis and they reflect the partial contribution of each single neuron (linear univariate terms) or simultaneous combination of neurons (non-linear multivariate terms) to the total variance of the response on each node.",
"Concluding the analysis of the current paper, we anticipate that avoiding the redundant representation and accounting for high-order neural effects could increase the performance of the neural network.",
"Following the observation in the current paper, we stress that the high non-linearity of the underlying problems could limit the ability of the plain aPC with only one-layer polynomial representation.",
"As opposed to that, the encoded flexibility of DANNs seems to suffer significantly from over-fitting, regardless of the regularization of weights.",
"Overall, the DaPC NN shows the ability to predict quantities of interest more consistently with lower variance in comparison to DANNs.",
"Furthermore, we also observe that omitting the regularization in the loss function only slightly degrades the results of DaPC NN and aPC, whereas the conventional DANN is significantly more affected by it.",
"Remarking that aspect, we accentuate that joining the fundamentals of homogeneous chaos theory with the deep representation of neural networks requires additional research, where various architectures accompanied by specific loss functions and training algorithms could be investigated.",
"In particular, concepts employed in recurrent neural networks, neural ordinary differential equations, physical regularization or Bayesian regularization could be adopted directly to the DaPC NN structure.",
"Moreover, the introduced DaPC NN structure opens a pathway to use analytical properties quantifying the importance of neural signal on each node .",
"In particular, implementing an explicit analytical form of data-driven orthonormal polynomial bases in equations (17)-(23) from [75] could significantly accelerate the training process of DaPC NN.",
"Additionally, since the weights of DaPC NN reflect the meaning according to global sensitivity analysis, they could be partially omitted [20] to offer a sparse DaPC NN representation.",
The latter is highly beneficial in mitigating issues related to overfitting.,
"Additionally, state-of-the-art findings in the PCE community can further be included, such as various sparse learning techniques, multi-element decomposition, Bayesian learning, etc.",
"From a technical perspective, we would like to clarify that the primary goal of the current paper is not to achieve high computational efficiency in the training procedure.",
"Instead, we offer a unique perspective on the DANN structure.",
"Indeed, the computational time of the current version of DaPC NN is significantly higher than that of DANN, since it requires adaptive computation of the orthonormal basis during the training process.",
The procedure could be accelerated using explicit analytic relations for a moderate degree of expansion from [75].,
"Moreover, further speed up could be achieved when employing parallel and GPU-based computing.",
24 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT Acknowledgments The authors would like to thank the German Research Foundation (DFG) for the support of the project within the Cluster of Excellence ”Data-Integrated Simulation Science” (EXC 2075) and Project Number 327154368 (SFB 1313) at the University of Stuttgart.,
"References [1] M. Abramowitz and A. Stegun, I. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables.",
"Dover Publications, Inc., New York, 1965.",
[2] Jonas Adler and Ozan ¨Oktem.,
Solving ill-posed inverse problems using iterative deep neural networks.,
"Inverse Problems, 33(12):124007, Nov 2017.",
Neural Networks and Deep Learning: A Textbook.,
"Springer, 1 edition, 9 2018.",
"[4] R Ahlfeld, B Belkouchi, and Francesco Montomoli.",
SAMBA: sparse approximation of moment-based arbitrary polynomial chaos.,
"Phys., 320:1–16, 2016.",
[5] NI Akhiezer.,
The classical moment problem.,
Hafner Publ.,
"Co., New York, 2, 1965.",
[6] Osama Alkhateeb and Nathan Ida.,
Data-Driven Multi-Element Arbitrary Polynomial Chaos for Uncertainty Quantification in Sensors.,
"IEEE Transactions on Magnetics, 54(3), 2017.",
[7] M. Anthony and P.L.,
Neural Network Learning: Theoretical Foundations.,
"Cambridge University Press, 1999.",
"[8] Sercan ¨O Arık, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Ng, Jonathan Raiman, et al.",
Deep voice: Real-time neural text-to-speech.,
"In International Conference on Machine Learning, pages 195–204.",
"PMLR, 2017.",
"[9] Martin Arjovsky, Amar Shah, and Yoshua Bengio.",
Unitary evolution recurrent neural networks.,
"In International conference on machine learning, pages 1120–1128.",
"PMLR, 2016.",
[10] R. Askey and J.A.,
Some Basic Hypergeometric Orthogonal Polynomials that Generalize Jacobi Polynomials.,
319 in American Mathematical Society: Memoirs of the American Mathematical Society.,
"American Mathematical Society, 1985.",
[11] Anthony Curtis Atkinson and Alexander N Donev.,
"Optimum experimental designs, volume 5.",
"Clarendon Press, 1992.",
"[12] F. Augustin, A. Gilg, M. Paffrath, P. Rentrop, and U. Wever.",
Polynomial chaos for the approximation of uncertainties: Chances and limits.,
"European Journal of Applied Mathematics, 19(2):149–190, 2008.",
[13] Dana Harry Ballard and Christopher M. Brown.,
Computer Vision.,
"Prentice Hall Professional Technical Refer- ence, 1st edition, 1982.",
[14] Jo˜ao Carlos Alves Barata and Mahir Saleh Hussein.,
The Moore–Penrose pseudoinverse: A tutorial review of the theory.,
"Brazilian Journal of Physics, 42(1):146–165, 2012.",
"[15] Felix Beckers, Andr´es Heredia, Markus Noack, Wolfgang Nowak, Silke Wieprecht, and Sergey Oladyshkin.",
Bayesian calibration and validation of a large-scale and time-demanding sediment transport model.,
"Water Resources Research, 56(7):e2019WR026966, 2020.",
[16] G´eraud Blatman and Bruno Sudret.,
Sparse polynomial chaos expansions and adaptive stochastic finite elements using a regression approach.,
"C. R. M´ecanique, 336(6):518–523, 2008.",
"[17] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.",
Weight uncertainty in neural network.,
"In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1613–1622, Lille, France, 07–09 Jul 2015.",
"[18] Thierry Bouwmans, Sajid Javed, Maryam Sultana, and Soon Ki Jung.",
Deep neural network concepts for back- ground subtraction: A systematic review and comparative evaluation.,
"Neural Networks, 117:8–66, 2019.",
"[19] Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski.",
A systematic study of the class imbalance problem in convolutional neural networks.,
"Neural Networks, 106:249–259, 2018.",
"[20] Paul-Christian B¨urkner, Ilja Kr¨oker, Sergey Oladyshkin, and Wolfgang Nowak.",
The sparse polynomial chaos expansion: a fully bayesian approach with joint priors on the coefficients and global selection of terms.,
"Journal of Computational Physics, 488:112210, 2023.",
25 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT [21] R. H. Cameron and W. T. Martin.,
The orthogonal development of non-linear functionals in series of Fourier- Hermite functionals.,
"(2), 48:385–392, 1947.",
"[22] K. Cheng, Lu Z, Xia S., Oladyshkin S., and Nowak W. Surrogate models for uncertainty quantification: A unified bayesian framework.",
"Computer Methods in Applied Mechanics and Engineering, page Under Review, 2021.",
"[23] Grigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Ste- fanos Zafeiriou.",
P-nets: Deep polynomial neural networks.,
"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7325–7335, 2020.",
"[24] Gioele Ciaparrone, Francisco Luque S´anchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera.",
Deep learning in video multi-object tracking: A survey.,
"Neurocomputing, 381:61–88, 2020.",
"[25] Holger Class, Anozie Ebigbo, Rainer Helmig, Helge K Dahle, Jan M Nordbotten, Michael A Celia, Pascal Audigane, Melanie Darcis, Jonathan Ennis-King, Yaqing Fan, et al.",
A benchmark study on problems related to CO2 storage in geologic formations.,
"Computational Geosciences, 13(4):409, 2009.",
[26] Corinna Cortes and Vladimir Vapnik.,
Support-vector networks.,
"Machine learning, 20(3):273–297, 1995.",
[27] Noel AC Cressie.,
Spatial Prediction and Kriging.,
"Statistics for Spatial Data (Cressie NAC, ed).",
"New York: John Wiley & Sons, pages 105–209, 1993.",
"[28] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
Imagenet: A large-scale hierarchical image database.,
"In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009.",
[29] Li Deng.,
An overview of deep-structured learning for information processing.,
Asian-Pacific Signal & Information Proc.,
"Annual Summit & Conference (APSIPA-ASC), pages 1–14, October 2011.",
"[30] Oliver G. Ernst, Antje Mugler, Hans-J¨org Starkloff, and Elisabeth Ullmann.",
On the convergence of generalized polynomial chaos expansions.,
ESAIM Math.,
"Anal., 46(2):317–339, 2012.",
[31] J. Foo and G.E.,
Karniadakis.,
Multi-element probabilistic collocation method in high dimensions.,
"Phys., 229(5):1536–1557, 2010.",
[32] Walter Gautschi.,
Orthogonal polynomials: computation and approximation.,
Numerical Mathematics and Sci- entific Computation.,
"Oxford University Press, New York, 2004.",
Oxford Science Publications.,
[33] R. G. Ghanem and P. D. Spanos.,
Stochastic finite elements: A spectral approach.,
"Springer-Verlag, New York, 1991.",
"[34] W. Gilks, S. Richardson, and D. Spiegelhalter.",
Markov chain Monte Carlo in practice.,
"Chapmann & Hall, Boca Raton, 1996.",
"[35] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.",
Deep Learning.,
"MIT Press, 2016. http://www.",
deeplearningbook.org.,
[36] Daniel Graupe.,
"Principles of artificial neural networks, volume 7 of Advanced series in circuits and systems.",
"World Scientific Publishing Company, Singapore, 2013.",
Fundamentals of Artificial Neural Networks.,
A Bradford book.,
"MIT Press, Cambridge, 1995.",
[38] S. Hochreiter and J. Schmidhuber.,
Long short-term memory.,
"Neural computation, 9(8):1735–1780, 1997.",
"[39] Kurt Hornik, Maxwell Stinchcombe, and Halbert White.",
Multilayer Feedforward Networks are Universal Ap- proximators.,
"Neural Networks, 2(5):359–366, 1989.",
"[40] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.",
"Densely connected convolutional networks, 2018.",
[41] Sergey Ioffe and Christian Szegedy.,
"Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015.",
[42] T. Ishigami and T. Homma.,
An importance quantification technique in uncertainty analysis for computer mod- els.,
In [1990] Proceedings.,
"First International Symposium on Uncertainty Modeling and Analysis, pages 398– 403, 1990.",
[43] A.G. Ivakhnenko and V.G.,
Cybernetics and forecasting techniques.,
"[44] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.",
"An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorpo- rated, New York, 2014.",
"26 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT [45] A.S. Witteveen Jeroen, Sunetra Sarkar, and Bijl Hester.",
Modeling physical uncertainties in dynamic stall in- duced fluid–structure interaction of turbine blades using arbitrary polynomial chaos.,
"Computers and Structures, 85(11-14):866–878, 2007.",
"[46] Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao.",
Orthogonal deep neural networks.,
"IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 2019.",
"[47] Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan S. Read, Jacob A. Zwart, Michael Steinbach, and Vipin Kumar.",
Physics-guided machine learning for scientific discovery: An application in simulating lake temperature profiles.,
ACM/IMS Trans.,
"Data Sci., 2(3), May 2021.",
"[48] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford.",
Multivariate LSTM-FCNs for time series classification.,
"Neural Networks, 116:237–245, 2019.",
[49] S. Karlin.,
"Total Positivity, volume I. Stanford University Press, Stanford, 1968.",
[50] A. Keese and H. G. Matthies.,
Sparse quadrature as an alternative to Monte Carlo for stochastic finite element techniques.,
"Mech., 3:493–494, 2003.",
[51] Andrei Nikolaevich Kolmogorov and Albert T Bharucha-Reid.,
Foundations of the theory of probability: Second English Edition.,
"Dover Publications, Inc., New York, 2018.",
"[52] M. K¨oppel, I. Kr¨oker, and C. Rohde.",
Intrusive uncertainty quantification for hyperbolic-elliptic systems gov- erning two-phase flow in heterogeneous porous media.,
"Geosci., 21(4):807–832, 2017.",
"[53] Markus K¨oppel, Fabian Franzelin, Ilja Kr¨oker, Sergey Oladyshkin, Gabriele Santin, Dominik Wittwar, Andrea Barth, Bernard Haasdonk, Wolfgang Nowak, Dirk Pfl¨uger, and Christian Rohde.",
Datasets and executables of data-driven uncertainty quantification benchmark in carbon dioxide storage.,
"[54] Markus K¨oppel, Fabian Franzelin, Ilja Kr¨oker, Sergey Oladyshkin, Gabriele Santin, Dominik Wittwar, Andrea Barth, Bernard Haasdonk, Wolfgang Nowak, Dirk Pfl¨uger, and Christian Rohde.",
Comparison of data-driven un- certainty quantification methods for a carbon dioxide storage benchmark scenario.,
"Computational Geosciences, Nov 2019.",
[55] Daniel G Krige.,
A statistical approach to some basic mine valuation problems on the witwatersrand.,
"Journal of the Southern African Institute of Mining and Metallurgy, 52(6):119–139, 1951.",
"[56] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.",
ImageNet Classification with Deep Convolutional Neural Networks.,
"ACM, 60(6):84–90, May 2017.",
"[57] Ilja Kr¨oker, Wolfgang Nowak, and Christian Rohde.",
A stochastically and spatially adaptive parallel scheme for uncertain and nonlinear two-phase flow problems.,
"Computational Geosciences, 19(2):269–284, 2015.",
[58] Heng Li and Dongxiao Zhang.,
Probabilistic collocation method for flow in porous media: Comparisons with other stochastic methods.,
"Water Resources Research, 43(9):1–13, 2007.",
[59] G. Lin and A.M. Tartakovsky.,
"An efficient, high-order probabilistic collocation method on sparse grids for three- dimensional flow and solute transport in randomly heterogeneous porous media.",
"Water Res., 32(5):712– 722, 2009.",
[60] David JC MacKay.,
Bayesian interpolation.,
"Neural computation, 4(3):415–447, 1992.",
[61] Donald W. Marquardt.,
An algorithm for least-squares estimation of nonlinear parameters.,
"Journal of the Society for Industrial and Applied Mathematics, 11(2):431–441, 1963.",
[62] MATLAB.,
version 9.7.0.1216025 (r2019b).,
"https://www.mathworks.com/help/stats/fitrgp.html, 2019.",
[63] John McCarthy.,
Review of the question of artificial intelligence.,
"Annals of the History of Computing, 10(3):224–229, 1988.",
[64] Warren S McCulloch and Walter Pitts.,
A logical calculus of the ideas immanent in nervous activity.,
"The bulletin of mathematical biophysics, 5(4):115–133, 1943.",
"[65] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey.",
Efficient orthogonal parametri- sation of recurrent neural networks using householder reflections.,
"In International Conference on Machine Learning, pages 2401–2409.",
"PMLR, 2017.",
[66] Hrushikesh Narhar Mhaskar and Charles A Micchelli.,
How to choose an activation function.,
"In Advances in Neural Information Processing Systems, pages 319–326, Denver, 1994.",
"[67] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hor- moz Shahrzad, Arshak Navruzyan, and Nigel Duffy.",
Evolving deep neural networks.,
"In Artificial intelligence in the age of neural networks and brain computing, pages 293–312.",
"Elsevier, 2019.",
27 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT [68] Eliakim H Moore.,
On the reciprocal of the general algebraic matrix.,
"Soc., 26:394–395, 1920.",
"[69] Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald, and Edin Muharemagic.",
Deep learning applications and challenges in big data analytics.,
"Journal of big data, 2(1):1–21, 2015.",
"[70] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.",
Deep double descent: Where bigger models and more data hurt.,
"Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.",
[71] Hayrettin Okut.,
Bayesian regularized neural networks for small n big p data.,
"Artificial neural networks-models and applications, 2016.",
"[72] S. Oladyshkin, H. Class, R. Helmig, and W. Nowak.",
A concept for data-driven uncertainty quantification and its application to carbon dioxide storage in geological formations.,
"Water Res., 34:1508–1518, 2011.",
"[73] S. Oladyshkin, H. Class, R. Helmig, and W. Nowak.",
An integrative approach to robust design and probabilistic risk assessment for CO2 storage in geological formations.,
"Geosci., 15(3):565–577, 2011.",
"[74] S Oladyshkin, FPJ De Barros, and W Nowak.",
Global sensitivity analysis: a flexible and efficient framework with an example from stochastic hydrogeology.,
"Advances in Water Resources, 37:10–22, 2012.",
[75] S. Oladyshkin and W. Nowak.,
Data-driven uncertainty quantification using the arbitrary polynomial chaos expansion.,
"Safe., 106:179–190, 2012.",
[76] Sergey Oladyshkin.,
aPC matlab toolbox: Data-driven arbitrary polyno- mial chaos.,
"https://www.mathworks.com/matlabcentral/fileexchange/ 72014-apc-matlab-toolbox-data-driven-arbitrary-polynomial-chaos, 2022.",
[77] Sergey Oladyshkin.,
DaPC NN: Deep arbitrary polynomial chaos neu- ral network.,
"https://www.mathworks.com/matlabcentral/fileexchange/ 112110-dapc-nn-deep-arbitrary-polynomial-chaos-neural-network, 2022.",
"[78] Sergey Oladyshkin, Farid Mohammadi, Ilja Kroeker, and Wolfgang Nowak.",
Bayesian3 Active Learning for the Gaussian Process Emulator Using Information Theory.,
"Entropy, 22(8):890, 2020.",
[79] Sergey Oladyshkin and Wolfgang Nowak.,
Incomplete statistical information limits the utility of high-order polynomial chaos expansions.,
"Reliability Engineering & System Safety, 169:137–148, 2018.",
[80] Sergey Oladyshkin and Wolfgang Nowak.,
"The Connection between Bayesian Inference and Information Theory for Model Selection, Information Gain and Experimental Design.",
"Entropy, 21(11):1081, 2019.",
"[81] Theodore Papamarkou, Jacob Hinkle, M. Todd Young, and David Womble.",
"Challenges in Markov chain Monte Carlo for Bayesian neural networks, 2021.",
[82] Roger Penrose.,
On best approximate solutions of linear matrix equations.,
"In Mathematical Proceedings of the Cambridge Philosophical Society, volume 52, pages 17–19.",
"Cambridge University Press, 1956.",
"[83] Timothy Praditia, Matthias Karlbauer, Sebastian Otte, Sergey Oladyshkin, Martin V Butz, and Wolfgang Nowak.",
Learning groundwater contaminant diffusion-sorption processes with a finite volume neural network.,
"Water Resources Research, page e2022WR033149, 2022.",
"[84] Timothy Praditia, Thilo Walser, Sergey Oladyshkin, and Wolfgang Nowak.",
Improving thermochemical energy storage dynamics forecast with physics-inspired neural network architecture.,
"Energies, 13(15):3873, 2020.",
[85] Waseem Rawat and Zenghui Wang.,
Deep convolutional neural networks for image classification: A compre- hensive review.,
"Neural computation, 29(9):2352–2449, 2017.",
[86] JR Red-Horse and AS Benjamin.,
A probabilistic approach to uncertainty quantification with limited informa- tion.,
"Reliability Engineering & System Safety, 85(1):183–190, 2004.",
"[87] Michael F Rehme, Fabian Franzelin, and Dirk Pfl¨uger.",
B-splines on sparse grids for surrogates in uncertainty quantification.,
"Reliability Engineering & System Safety, 209:107430, 2021.",
[88] Sebastian Ruder.,
"An overview of gradient descent optimization algorithms, 2017.",
[89] Carl Runge.,
¨Uber empirische Funktionen und die Interpolation zwischen ¨aquidistanten Ordinaten.,
"Zeitschrift f¨ur Mathematik und Physik, 46(224-243):20, 1901.",
[90] Arthur L Samuel.,
Some studies in machine learning using the game of checkers.,
"IBM Journal of research and development, 3(3):210–229, 1959.",
[91] Juergen Schmidhuber.,
"Annotated history of modern ai and deep learning, 2022.",
Technical Report IDSIA-22-22.,
[92] J¨urgen Schmidhuber.,
Deep learning in neural networks: An overview.,
"Neural networks, 61:85–117, 2015.",
28 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT [93] Burr Settles.,
Active learning literature survey.,
"Computer Sciences Technical Report 1648, University of Wisconsin–Madison, 2009.",
[94] Sagar Sharma and Simone Sharma.,
Activation functions in neural networks.,
"Towards Data Science, 6(12):310– 316, 2017.",
"[95] Baoguang Shi, Xiang Bai, and Cong Yao.",
An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition.,
"IEEE transactions on pattern analysis and machine intelligence, 39(11):2298–2304, 2016.",
[96] JA Shohat and JD Tamarkin.,
"The problem of moments, mathematical surveys no.",
"American Mathematical Society, New York, 1950, 1943.",
"[97] Paz Fink Shustin, Shashanka Ubaru, Vasileios Kalantzis, Lior Horesh, and Haim Avron.",
"Pcenet: High dimen- sional surrogate modeling for learning uncertainty, 2022.",
[98] W. McC.,
On the determinants of moment matrices.,
"Statist., 17(2):711–721, 1989.",
[99] Adrian FM Smith and Alan E Gelfand.,
Bayesian statistics without tears: a sampling–resampling perspective.,
"The American Statistician, 46(2):84–88, 1992.",
"[100] Ilya M Sobol’, Danil Asotsky, Alexander Kreinin, and Sergei Kucherenko.",
Construction and comparison of high-dimensional Sobol’ generators.,
"Wilmott, 2011(56):64–79, 2011.",
[101] Il’ya Meerovich Sobol’.,
On sensitivity estimation for nonlinear mathematical models.,
"Matematicheskoe modelirovanie, 2(1):112–118, 1990.",
"[102] J Stieltjes, T. Quelques recherches sur la th´eorie des quadratures dites m´echaniques.",
"Oeuvres I, pages 377–396, 1884.",
[103] Bruno Sudret.,
Global sensitivity analysis using polynomial chaos expansions.,
"Reliability Engineering & System Safety, 93(7):964–979, 2008.",
Bayesian Networks in Dependability.,
[104] T.J. Sullivan.,
Introduction to Uncertainty Quantification.,
Texts in Applied Mathematics.,
"Springer International Publishing, Cham, 2015.",
"[105] Chunwei Tian, Yong Xu, and Wangmeng Zuo.",
Image denoising using deep CNN with batch renormalization.,
"Neural Networks, 121:461–473, 2020.",
"[106] Andrey Nikolaevich Tikhonov, Vasiliy Iakovlevich Arsenin, VY Arsenin, et al.",
Solutions of ill-posed problems.,
"Vh Winston, 1977.",
[107] Michael E Tipping.,
The relevance vector machine.,
"In Advances in neural information processing systems, pages 652–658, 2000.",
[108] Vladimir Vapnik and Alexey Chervonenkis.,
Theory of pattern recognition.,
"Nauka, Moscow, 1974.",
[109] John Villadsen and Michael L Michelsen.,
Solution of differential equation models by polynomial approximation.,
"Prentice-Hall, New Jersey, 1978.",
[110] John Villadsen and Michael L Michelsen.,
Solution of differential equation models by polynomial approxima- tion(book).,
"Englewood Cliffs, N. J., Prentice-Hall, Inc., 1978.",
"460 p, 1978.",
"[111] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal.",
On orthogonality and learning recurrent networks with long term dependencies.,
"In International Conference on Machine Learning, pages 3570–3578.",
"PMLR, 2017.",
"[112] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu.",
Orthogonal convolutional neural networks.,
"In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11505–11515, 2020.",
[113] Norbert Wiener.,
The homogeneous chaos.,
"American Journal of Mathematics, 60(4):897–936, 1938.",
[114] Norbert Wiener.,
"Cybernetics, or Control and Communication in the Animal and the Machine.",
"Actualit´es Scientifiques et Industrielles [Current Scientific and Industrial Topics], No.",
"Hermann et Cie., Paris; The Technology Press, Cambridge, Mass.",
"; John Wiley & Sons, Inc., New York, 1948.",
[115] Christopher K Williams and Carl Edward Rasmussen.,
"Gaussian processes for machine learning, volume 2.",
"MIT press Cambridge, MA, 2006.",
"[116] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas.",
Full-capacity unitary recurrent neural networks.,
"Advances in neural information processing systems, 29, 2016.",
"[117] Lin Xiao, Bolin Liao, Shuai Li, and Ke Chen.",
Nonlinear recurrent neural networks for finite-time solution of general time-varying linear matrix equations.,
"Neural Networks, 98:102–113, 2018.",
29 Deep Arbitrary Polynomial Chaos Artificial Neural Network A PREPRINT [118] D. Xiu and G. E. Karniadakis.,
Modeling uncertainty in flow simulations via generalized polynomial chaos.,
"Journal of Computational Physics, 187:137–167, 2003.",
[119] D. Xiu and G.E.,
Karniadakis.,
The Wiener-Askey Polynomial Chaos for stochastic differential equations.,
"SIAM Journal of Scientific Computing, 24(2):619–644, 2002.",
[120] Dongbin Xiu and George Em Karniadakis.,
The Wiener–Askey polynomial chaos for stochastic differential equations.,
"SIAM Journal on Scientific Computing, 24(2):619–644, 2002.",
[121] P. Yee and S. Haykin.,
"Pattern classification as an ill-posed, inverse problem: a regularization approach.",
"In 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 597–600 vol.1, 1993.",
"[122] Yingqi Zhang, Yaning Liu, George Pau, Sergey Oladyshkin, and Stefan Finsterle.",
Evaluation of multiple reduced-order models to enhance confidence in global sensitivity analyses.,
"Gas Control, 49:217– 226, 2016.",
"[123] Xiaohu Zheng, Jun Zhang, Ning Wang, Guijian Tang, and Wen Yao.",
"Mini-data-driven Deep Arbitrary Polyno- mial Chaos Expansion for Uncertainty Quantification, 2021.",
Noname manuscript No.,
"(will be inserted by the editor) Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker the date of receipt and acceptance should be inserted later Abstract Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives.",
"To address this challenge, we propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix.",
We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed.,
"In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction.",
"At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the prediction task enhancing the model interpretability.",
"We conducted numerical experiments for regression, classification, and feature selection tasks, comparing our model against popular machine learning models, the state-of-the-art deep learning-based embedding feature selection techniques, and a transformer model for tabular data.",
Our results demonstrate that the proposed model does not only yield an attractive prediction performance compared to the competitors but also provides meaningful and interpretable results that potentially could assist the decision-making process in real-world applications.,
A PyTorch implementation of the model is available on GitHub at the following link.1 1 Introduction The radial basis function (RBF) is a family of models used for function interpolation and approximation that are defined as a linear combination of radially symmetric basis functions [14].,
"The RBF approach has many properties that make it attractive as a mathematical tool for interpolation [54, 67].",
"Once the basis function and its hyperparameters are determined, the weights that multiply the basis functions can be found by solving a convex optimization problem or directly through matrix inversion.",
"The RBF model has been generalized in the context of approximation by using basis functions centered on a subset of the data, that can be interpreted as one hidden layer neural network (RBFNN) with RBF’s activation function as shown in [14].",
"In [60, 61] the authors showed that under some conditions on the basis function, RBFNNs are universal approximators as neural networks (NNs) [39].",
"National University of Singapore Department of Industrial Systems Engineering and Management Singapore E-mail: dannydag@nus.edu.sg 1 https://github.com/dannyzx/Gaussian-RBFNN arXiv:2307.05639v2 [cs.LG] 11 May 2024 2 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker RBFs have been used for function interpolation or approximation for many decades in different applications.",
"In the work presented in [66, 31] the authors showed that from the regularization principles and through a solution of a variational problem, the RBF model is a subclass of regularization networks.",
"Within this family, the Gaussian radial basis function neural network (GRBFNN) is a particular case of the RBF approach, defined by employing Gaussian kernels as activation functions.",
"The Gaussian kernel exhibits flexibility by assuming various forms based on the covariance matrix, offering diverse strategies for capturing relationships within data.",
"The standard Gaussian kernel, frequently employed, assumes an isotropic covariance, meaning the covariance matrix is a diagonal matrix, and all elements on the diagonal are equal.",
Governed by its shape (i.e.,
"width) parameter, this kernel treats each feature equally.",
"Given that only one parameter requires determination, the shape parameter is commonly treated as a hyperparameter, allowing users to define it in advance [12, 55].",
"However, the shape parameter need not be a fixed constant but can be learnable which means, that during the training process, the shape parameter is adjusted and optimized alongside the other model parameters, solving in this case a nonconvex optimization problem [71, 85].",
"A diagonal covariance matrix, accommodates scenarios where distinct variances along each feature axis are necessary.",
Real-world situations often introduce challenges where features display different variances or correlations.,
"To address this, employing a Gaussian kernel with a full covariance matrix becomes essential.",
"Hence, the Mahalanobis distance Gaussian kernel, with its utilization of a full covariance matrix, adeptly captures both feature scaling and inter-feature correlations.",
"In general, various methods have been proposed to estimate the model parameters in the context of approximation.",
Some of them are inspired by the work presented in [66] where the location of the centers and a weighted norm (instead of the classical Euclidean norm) are considered part of the learning problem together with the weights.,
The possibility of using a superposition of kernels with a distinct set of hyperparameters has been also considered [66].,
In [56] they propose to compute the width factors by the nearest neighbors heuristic and a clustering procedure for the centers.,
"A different approach has been used in [76], where the centers’ locations are considered as additional parameters of the optimization problem as well as the weights.",
"In the same work, they also considered learning the width of the Gaussian kernel around each center.",
A similar approach has been presented in [70] where a diagonal precision matrix is also considered for each RBF center.,
In [15] they developed a Gauss-Seidel decomposition type algorithm [8] where the optimization of the weights and the centers are alternated along the iterations.,
"In [83], a two-stage fuzzy clustering approach is introduced to partition the input space into multiple overlapping regions, subsequently utilized for constructing a local GRBFNN.",
"Another noteworthy technique, referred to as variable projection [84], is employed to reduce the parameter count in the optimization problem associated with GRBFNN.",
"Furthermore, in the study conducted in [81], an enhancement to the computational speed of the model is achieved by leveraging the separability property inherent in the Gaussian basis function.",
"In the work presented in [47], they suggest a modification to the activation function, incorporating a raised cosine radial basis function modulated by an exponential function.",
"Recently, an accelerated gradient-based method has been presented in [36] to improve the learning performance of the RBFNN model.",
Important research to improve the generalization power of RBFNNs is in [10].,
This has been achieved by adding a regularization term that penalizes the second derivative of the output of each neuron.,
The technique is also known in the case of NNs in [11].,
"As in the case of NNs, RBFNNs are considered black-box models, and consequently, the underlying process of how the input features are used to make predictions is unclear to humans, including those who developed the models.",
Explainable AI (XAI) is a rapidly growing field of research that aims to make AI models more transparent and interpretable to humans.,
"XAI techniques provide insight into the decision-making processes of AI models, allowing users to understand how models arrive at their outputs and to identify potential biases or errors.",
"For this reason, sometimes simpler models given just by a linear combination of the input variables are preferred since the coefficients can assess the importance of each feature in the prediction task.",
"On the other hand, simpler models tend to be less accurate than complex ones.",
"As a result, it is crucial to propose models with powerful predictive 3 capabilities that can also provide simple explanations to support decision-making in complex real-world applications.",
"Thus, recognizing the importance of each input feature in a prediction task from a machine learning model has significant implications in various fields, including genomics [9], environmental science [23], emergency medicine [80], cancer research [38], and finance [58].",
"In these domains, the model interpretability is crucial as the predictive performance as described in [34, 5, 1].",
Feature importance ranking and feature selection are two key techniques used in XAI.,
Feature importance ranking is a fundamental aspect of machine learning that addresses the question of how individual features contribute to the predictive performance of a model.,
"For example in linear regression, each of the coefficients represents the rate of change of the conditional mean of the target/response variable to the corresponding feature when all the others are kept fixed.",
"In tree-based models, variable importance is assessed by considering the reduction in impurity or information gain attributed to each feature.",
Features with higher impurity reduction or information gain are deemed more crucial in the decision-making process of the model.,
"Considering the complexity of modern machine learning models, understanding the significance of each input variable is crucial for model interpretability, transparency, and effective decision-making.",
Obtaining the feature importance ranking can accommodate performing the feature selection process.,
Feature selection involves choosing a subset of the data features to improve model performance due to the curse of dimensionality [6].,
"According to the taxonomy in [35] there are three kinds of feature selection methods: filter, wrapper, and embedded.",
"Filter methods for feature selection are techniques that use statistical measures to rank the importance of each feature independently of the model, such as Chi-squared feature selection and variance thresholding.",
"These methods are called ""filters"" because they filter out irrelevant or redundant features from the dataset before the learning algorithm is applied.",
"Wrapper methods work by selecting a subset of features, training the learning algorithm on that subset, evaluating its performance using cross-validation, and then repeating the process with different feature subsets.",
This iterative approach can be time-consuming and inefficient.,
A popular wrapper method is the forward/backward feature elimination [37].,
Embedded methods refer to learning algorithms that have feature selection incorporated.,
Embedded methods are optimal and time-efficient because they use the target learning algorithm to select the features.,
"Some of the embedded methods are linear models such as the lasso (least absolute shrinkage and selection operator) regression [75], and tree-based methods such as the random forest (RF) [13] and gradient boosting (GB) [29].",
The aforementioned models are inherently easier to interpret and have become prevalent tools across practitioners.,
"Recently, deep feature selection (DFS) [46] and the approach proposed in [78] highlight the important features of NN architectures.",
An emerging new kind of feature selection methods are the post-hoc explanators such as SHAP (Shapley additive explanations) [52] and LIME (local interpretable model-agnostic explanations) [68].,
They are applied after the model has made its predictions and provide insights into why the model has made a certain decision.,
"In conjunction with XAI, models must proficiently extract and recognize essential features from the data, serving as effective feature extractors to ensure accurate predictions.",
"One approach to achieving this is through the discovery of underlying factors of variation that generated the data, which may live in a subspace of lower dimensionality than the input space.",
"As an example, the success of deep learning models has also been imputed to their capability to learn and exploit latent representations through a cascade of multiple non-linear transformations of the dataset [7].",
"From an unsupervised learning perspective, the popular principal component analysis (PCA) [63, 40] can be used to prune out irrelevant directions in the data, constructing a latent space given as a linear combination of uncorrelated factors.",
"In supervised learning, the Fisher linear discriminant [27] learns a new reduced representation searching for those vectors in the latent space that best discriminates among classes.",
"In statistical regression, extensive literature exists on approaches aimed at identifying a low-dimensional subspace within the input feature space that adequately captures the statistical relationship between input features and responses such as sufficient dimension reduction (SDR) [2, 20, 19] and effective dimension reduction (EDR) [45].",
These methodologies are closely related to the concept of active subspace.,
"4 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker The active subspace method (ASM) [18], can be used to discover directions of maximum variability of a particular function by applying a PCA to a dataset composed of its gradients.",
By eliminating directions defined by the eigenvectors associated with zero eigenvalues one can provide a reduced representation (i.e.,
the active subspace) where most of the original function variability is preserved.,
"The ASM has shown to be relevant in many areas of science and engineering such as in hydrology [41], shape optimization [51], and disease modeling [50].",
"It enables efficient exploration of the model input space and can help reduce computational costs associated with sensitivity analysis, uncertainty quantification, and optimization.",
2 Main contribution Our main contribution is to enhance the interpretability of the GRBFNN model while maintaining its attractive predictive performance.,
More in particular: • Supervised Dimensionality Reduction in Active Subspaces: We enhance the GRBFNN by incorporating a learnable precision matrix.,
"After completing the model training, valuable latent information about the prediction task is extracted by analyzing the precision matrix spectrum.",
The eigenvalues of the precision matrix offer insights regarding the curvature of the Gaussian basis function at the GRBFNN centers along the corresponding eigenvectors.,
Dominant eigenvalues correspond to eigenvectors explaining a substantial portion of the variability within the GRBFNN model.,
"Consequently, analyzing eigenvalues and eigenvectors respectively helps us understand the degrees of freedom and directions where the model exhibits the most variability.",
"As a result, our proposed model can be employed for supervised dimensionality reduction, such as projecting the learned model into a 2-dimensional active subspace for visualization and analysis but also for optimization purposes [51, 44].",
"• Feature Importance Ranking Estimation: Simultaneously, for enhanced transparency and in- terpretability, we estimate the feature importance ranking of the learning task, enabling the use of our model for feature selection purposes.",
This is facilitated by recognizing that the eigenvectors also serve as the Jacobian of the linear transformation in a new coordinate system defined within the active/latent space thus enabling the assessment of the importance of the input feature to the GRBFNN model output.,
"• Impact of Regularization: To improve the smoothness and the generalization capability of our model, we introduce two regularization parameters: one for the weights and the other one for the elements of the precision matrix of the Gaussian basis function.",
"To better analyze the behavior of our model, we investigate the synergy between them.",
"Interestingly, numerical results suggest that a stronger role is played by the regularizer of the precision matrix rather than the one that controls the magnitude of the weights.",
"In conclusion, we perform numerical experiments to compare the GRBFNN model against other well-established machine learning models, including support vector machines (SVMs) [21], Random Forest (RF) [13], multilayer perception (MLP) [69], extreme gradient boosting (XGB) [17], as well as state-of-the-art deep learning feature selection embedding methods presented in [46, 78].",
"Additionally, we assess our model against a recent transformer architecture specifically designed for tabular data, known as the FT-Transformer (FT-T) [32].",
The outcomes demonstrate that our model not only attains competitive prediction performances but also furnishes meaningful feature importance ranking and interpretable insights that potentially can assist decision-making in real-world applications.,
"5 3 Model Description Radial basis functions have been introduced for solving interpolation problems, which consist of building the following interpolant f(x) = M X m=1 wmφ(||x −xm||) (1) where we have M weights wm ∈R, a continuous function φ : R+ →R which represents the basis function, and the centers xm.",
"One can solve the interpolation problem by solving the following linear system by imposing the interpolation condition Φw = y (2) where the N × M (in this case with M = N) symmetric matrix Φ has elements Φnm = φ(||xn −xm||), with w = (w1, .",
", wM), the response or target variable vector y = (y1, .",
", yN) and the nth data point xn belonging to the N × D data matrix X.",
It has been proven [54] that for some RBF (eg.,
the Gaussian) the matrix Φ is not singular if all the data points are distinct with N > 2.,
Our first modification to the model in Eq.,
1 concerns the kernel.,
We are primarily interested in learning and exploiting hidden correlation structures in the dataset so that we can equip our RBF model with a Gaussian basis function with a symmetric positive definite matrix as follows φ(||x −xj||) = exp  −1 2(x −xj)T P(x −xj)  = exp  −1 2(x −xj)T UT U(x −xj)  (3) the matrix P is a D × D symmetric and positive definite precision matrix that can be expressed as upper triangular matrix multiplication using U.,
"The function approximation problem, in this case, can be solved by minimizing the following nonconvex optimization problem and defining the vector u = vech(U), where the operator vech is the half vectorization of matrices which means that the upper triangular entries of the matrix U are collected inside the vector u min w,u E(w, u) (4) and the error function in the regression case takes the following form E(w, u) = 1 2 N X n=1 (yn −f(xn))2 = 1 2 N X n=1 yn − M X m=1 wm exp  −1 2(xn −xm)T UT U(xn −xm) !2 (5) The number of parameters to optimize in this case is P = M + D + D×(D−1) 2 .",
"From numerical experiments, the model f defined in Eq.",
1 can produce a very sharply peaked function at the end of the minimization of the error function defined in Eq.,
"In such cases, we encountered large values in the entries of the precision matrix P. Then, it is natural to force the smoothness of f through regularization.",
"The measure of the bumpiness of the function f is controlled by the second derivative of the function f that depends on both the weights w and the precision matrix P. Consequently, the regularizers have the responsibility to force the Gaussian kernel to be as flat as possible, penalizing large values of the entries of the matrix P along with the weights w and promoting the smoothness of f. After the considerations above, the regularized error function becomes R(w, u) = E(w, u) + G(w, u) (6) 6 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker where the penalty function is given by G(w, u) = 1 2λu||u||2 + 1 2λw||w||2 (7) The regularization parameter λu influences the precision matrix P, promoting the flatness of the Gaussian kernel in Eq.",
"Meanwhile, λw penalizes large weights.",
"Then we solve the following nonconvex optimization problem min w,u R(w, u) (8) with the partial gradients respect to w and u given as follows ∇R(w) = ΦT (y −Φw) + λww = ΦT r + λww (9) ∇R(u) = vech N X n=1 rn M X m=1 wmGnmΦnm !",
+ λuu (10) where the D × D matrix Gnm defined as Gnm = (xn −xm)(xn −xm)T U and rn the nth component of the vector r = y −Φw.,
Until now we assumed the centers are exactly given by our training dataset.,
This might be unfeasible and computationally very expensive for very large N. This issue can be easily solved by selecting the number of the M centers collected in the M ×D matrix C to be less than the number of data points N as shown in [14].,
Depending on how the centers are selected we can distinguish two different strategies: 1.,
"Unsupervised selection of the centers: in this case, one can choose an M centers cm at random among the data points or by running a clustering algorithm (e.g.",
k-means [48]).,
"Given the centers, the objective function is the same as in Eq.",
"6 except that now M < N min w,u R(w, u) (11) with R(w, u) = 1 2 N X n=1 yn − M X m=1 wm exp  −1 2(xn −cm)T UT U(xn −cm) !2 + G(w, u) (12) The partial gradients are the same as in Eq.",
9 and in Eq.,
10 together with the partial gradient respect to u in Eq.,
"10 unchanged, together with the total number of parameters P. 2.",
"Supervised selection of the centers: in this case the centers are considered learnable, adding D × M parameters in the optimization problem.",
"With this variation, the model has the following form with recasting the matrix containing the centers as a vector c = vec(C) min w,u,c R(w, u, c) (13) with R(w, u, c) = 1 2 N X n=1 yn − M X m=1 wm exp  −1 2(xn −cm)T UT U(xn −cm) !2 + G(w, u, c) (14) The partial gradient with respect to the mth center is the following ∇R(cm) = N X n=1 rnUT U(xn −cm)Φnm + λccm (15) together with the partial gradients in Eq.",
10 and rn the nth component of the vector r = y −Φw.,
The number of parameters is in this case P = M × D + M + D + D×(D−1) 2 .,
"Where in the penalty function we introduced the possibility to regularize the position of the centers, controlled by λc as follows G(w, u, c) = 1 2λu||u||2 + 1 2λw||w||2 + 1 2λc||c||2.",
"7 3.1 Extracting Insights from the GRBFNN: Feature Importance and Active Subspace After obtaining the parameters of the GRBFNN, we can extract valuable information from the spectrum of the matrix P. Specifically, we aim to determine whether the variability of the fitted model f is restricted to a lower-dimensional space compared to the original space, as well as to identify the directions in which the function f is most sensitive.",
This allows us to establish the active subspace.,
It is easy to observe that the exponent of Eq.,
3 is the following quadratic form also known as the squared Mahalanobis distance d2 M(x) = (x −xj)T P(x −xj) (16) which expresses the functional dependence of the Gaussian kernel on the input variable x.,
More insights can be revealed by expanding Eq.,
16 in terms of eigenvectors and eigenvalues d2 M(x) = (x −xj)T P(x −xj) = (x −xj)T VΓ VT (x −xj) = (z −zj)T Γ (z −zj) (17) shows that the second derivatives of Eq.,
"17 are represented by the eigenvalues in the diagonal matrix Γ , after a rotation in the latent space z ∈Z ⊂RK (with K = D) under the new basis defined by the eigenvectors in the (D × K) matrix V. The spectrum of the precision matrix P highlights the principal curvatures [33] of Eq.",
"The presence of zero eigenvalues indicates that the factors of variation in f are manifested in a lower- dimensional subspace than the input dimensionality D. Additionally, the eigenvector vk corresponding to the largest eigenvalue γk identifies the direction of maximum curvature of the quadratic function in Eq.",
"16, thereby pinpointing the direction in which f is most globally sensitive.",
"Furthermore, the Gaussian kernel in Eq.",
3 in the latent space Z is given by a product of D independent contributions φ(||x −xj||) = exp  −1 2(x −xj)T P(x −xj)  = exp ( −1 2 D X d=1 γd(zd −zjd) ) = D Y D=1 exp  −1 2γd(zd −zjd)  (18) enhancing the fact that the variability of the model f is axis aligned within the latent space.,
"To identify which input variables xd are more critical in the prediction task of our model, we can observe that the matrix V, which contains the eigenvectors of the matrix P, represents the Jacobian of the linear transformation that maps the input space to the latent space, as demonstrated in Eq.",
Considering the original input vector x as generated from a linear combination of latent variables z and the eigenvectors V x = Vz (19) they represent simply the following derivative ∂x ∂z = V showing that the kth eigenvector vk can be interpret as the contribution of the kth latent variable zk in the variation of x.,
Each element of the matrix V has to be transformed in its absolute value to obtain meaningful results.,
So we can define the matrix ¯V where each component is given by ¯vdk = |vdk|.,
"To obtain the feature importance ranking vector, we need to scale the eigenvectors vk by their corresponding eigenvalues γk as the eigenvectors are returned typically normalized to the unitary norm from numerical procedures.",
This scaling ensures that more importance is given to the directions with the most significant variation.,
The resulting D-dimensional feature importance ranking vector can be defined as follows Feature Importance = K X k=1 γk¯vk (20) A final normalization step is performed so the feature importance vector ranges between zero and one.,
"8 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 3.2 Numerical Examples In this section, we want to provide some simple examples to highlight graphically the behavior of the proposed model.",
We first start with two simple classification problems with N = 100 and D = 2.,
"In the first problem there two classes c1 and c2 that are normally distributed with mean µT c1 = [1, 1]T and µT c2 = [2.8, 2.8]T , respectively, and same covariance matrix Σ =  0.81 0.72 0.72 0.66  .",
The scatter plot of the two classes is shown in Fig.,
"1a where the yellow and purple dotted points represent class c1 and c2, respectively.",
"1b, we display the fitted GRBFNN model with M = 2 centers (highlighted by the red points) obtained with the unsupervised selection strategy by k-means clustering.",
"Furthermore, we plot the eigenvector v1 corresponding to the dominant eigenvalue γ1 of the matrix P as white arrows with the origin at the two centers.",
"This shows that the fitted model f obtains most of its variability along the direction of v1, which is orthogonal in this case to the contour levels of f. In Fig.",
"1d, we show the fitted model in the latent space obtained by projecting the dataset X to the new basis defined by the eigenvectors of P, defining the projected dataset Z = XV.",
"We observe that all the variation of f is aligned to the first latent variable z1, which indicates that the fraction γ1 PK k γk is approximately equal to 1.",
1c shows the feature importance estimated by our model using Eq.,
"20, which validates that the input feature x2 plays a more significant role in the discrimination power between the classes than x1.",
0 2 4 x1 −1 0 1 2 3 4 x2 a c1 c2 −2 0 2 x1 −2 −1 0 1 2 x2 b x1 x2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Feature Importance c 0 1 z1 0 1 z2 d 0.0 0.2 0.4 0.6 0.8 1.0 f(x) 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Fig.,
1: The GRBFNN behavior is graphically represented in four subfigures: (a) shows the classification problem with purple and yellow dots representing the two classes.,
"The subfigure (b) shows the fitted GRBFNN in the input space, while the (c) figure shows the fitted GRBFNN model in the active subspace.",
Contour levels show estimated class probabilities.,
The red dotted points represent the GRBFNN centers.,
The white arrow highlights the direction of the dominant eigenvector v1.,
"Finally, in (c) the subfigure shows the feature importance estimated from the GRBFNN.",
9 Another example of a classification problem is shown in Fig.,
"In this case, we have two noisy interleaving half circles with N = 100 and D = 2, as seen in Fig.",
"To achieve a stronger discriminative power from the model, we choose M = 16 centers.",
"In contrast to the previous example, not all of the model f variability is concentrated along the direction identified by the eigenvector v1 related to the dominant eigenvalue γ1.",
"In this case, the fraction γ1 PK k γk is approximately 0.8, meaning that the resulting feature importance in Eq.",
20 includes the contribution of the eigenvector v2.,
"This is highlighted in the barplot in Fig 2d, where we decomposed the feature importance showing the contribution of each term in Eq.",
"Since v2 is orthogonal to v1, it gives more importance to the feature x2 because v1 is quasi-parallel to the input x1.",
−1 0 1 2 x1 −1.0 −0.5 0.0 0.5 1.0 x2 a c1 c2 −1 0 1 x1 −2 −1 0 1 2 x2 b x1 x2 0.0 0.2 0.4 0.6 Feature Importance c γ1¯v1 γ2¯v2 -2 -1 0 1 z1 0 1 z2 d 0.0 0.2 0.4 0.6 0.8 1.0 f(x) 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Fig.,
2: The GRBFNN behavior is graphically represented in four subfigures: (a) shows the classification problem with purple and yellow dots representing the two classes.,
"The subfigure (b) shows the fitted GRBFNN in the input space, while the (c) figure shows the fitted GRBFNN model in the active subspace.",
Contour levels show estimated class probabilities.,
The red dotted points represent the GRBFNN centers.,
The white arrow highlights the direction of the dominant eigenvector v1.,
"Finally, in (c) the subfigure shows the feature importance estimated from the GRBFNN and highlights its composition.",
"We present an example of regression, where the function to be approximated is t(x) = sin(ax1+bx2), with a and b being real scalars.",
"3 shows the case where a and b are equal to 0.5, and the true function is depicted in Fig.",
"We then use our proposed model to obtain an approximation, as shown in Fig.",
"3b along the direction given by the eigenvector v1, with the centers represented by dotted red points.",
"Furthermore, we perform a supervised dimensionality reduction from the original two-dimensional space to the one-dimensional subspace defined by the first eigenvector v1.",
"This subspace captures the ’active’ part of the function where most of the variation is realized, as illustrated in Fig.",
"Finally, we 10 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker estimate the feature importance using Eq.",
"20 and find that x1 and x2 contribute equally, as expected.",
This result is depicted in Fig.,
"In this final example, we altered the values of the scalars a and b to -10 -5 0 5 10 x1 -10 -5 0 5 10 x2 a −10 −5 0 5 10 x1 -10 -5 0 5 10 x2 b x1 x2 0.0 0.1 0.2 0.3 0.4 0.5 Feature Importance c −2 −1 0 1 2 z1 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 f(x) d 0.0 0.2 0.4 0.6 0.8 1.0 t(x) 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Fig.",
"3: The GRBFNN behavior is depicted in four subfigures: (a) shows the regression problem with target function t(x) = sin(0.5x1 + 0.5x2), while (b) displays the fitted GRBFNN in the input space.",
"The dominant eigenvector v1 is indicated by a white arrow, and the GRBFNN centers are shown as red dotted points.",
The subfigure (d) shows the fitted GRBFNN model projected in the one-dimensional active subspace.,
"The function values at the input data and at the centers are represented by black and red dotted points, respectively.",
"Finally, in (c) the subfigure displays the feature importance estimated from the GRBFNN.",
Function values are normalized.,
"0.1 and 0.9, respectively.",
"This modification resulted in a change in the feature importance estimated by our model, as depicted in Fig.",
"In summary, the GRBFNN model beyond solving a classical regression/classification model provides the user with valuable information about the model behavior such as allowing visualization of the fitted model f of the active subspace thereby recognizing the underlying factors of variation of the data, and in parallel allowing to discover which are the most important input features related to the learned model about the prediction task.",
4 Numerical Experiments This section aims to provide a comprehensive evaluation of the proposed model by assessing its predictive performance along with the feature selection and feature importance ranking quality.,
"We consider two variants of the same GRBFNN model, one with unsupervised center selection (GRBFNNk) as given in Eq.",
11 and the other one with supervised center selection (GRBFNNc) as given in Eq.,
13 for comparison purposes.,
11 -10 -5 0 5 10 x1 -10 -5 0 5 10 x2 a −10 −5 0 5 10 x1 -10 -5 0 5 10 x2 b x1 x2 0.0 0.2 0.4 0.6 0.8 Feature Importance c −2 −1 0 1 z1 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 f(x) d 0.0 0.2 0.4 0.6 0.8 1.0 t(x) 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Fig.,
"4: The GRBFNN behavior is depicted in four subfigures: (a) shows the regression problem with target function t(x) = sin(0.1x1 + 0.9x2), while (b) displays the fitted GRBFNN in the input space.",
"The dominant eigenvector v1 is indicated by a white arrow, and the GRBFNN centers are shown as red dotted points.",
The subfigure (d) shows the fitted GRBFNN model projected in the one-dimensional active subspace.,
"The function values at the input data and at the centers are represented by black and red dotted points, respectively.",
"Finally, in (c) the subfigure displays the feature importance estimated from the GRBFNN.",
Function values are normalized.,
We compare the performance of these models with other popular models such as multi-layer perceptron (MLP) [69] and support vector machines (SVMs) [21].,
"As the GRBFNN model incorporates feature selection, it can be classified as an embedding method.",
"To provide a comprehensive benchmark, we also include other widely used embedding methods such as random forest (RF) [28] and extreme gradient boosting (XGB) [17], which have shown strong performance for tabular data.",
"We include state-of-the-art embedding deep learning methods such as deep feature selection (DFS) [46] and the method proposed in [78], referred to as FIDL (feature importance for deep learning) in this comparison for simplicity.",
"We also consider in this benchmark a recent transformer model designed for tabular data, known as the FT-Transformer (FT-T) [32].",
We perform a 5-fold cross-validation to identify the best set of hyperparameters for the models.,
"Once the best set of hyperparameters is determined, we conduct another 5-fold cross-validation using 20 different seeds.",
We performed a Wilcoxon signed-rank test [77] at α = 0.05 significance level to assess the statistical significance of the numerical results.,
"For the FIDL model, we were able to run the cross-validation procedure to find its best set of hyperparameters varying only one random seed due to the severe time and memory complexity of the model.",
"For the regression problems we use a root mean squared error (RMSE) while for the classification problems, we use accuracy as a metric to evaluate the models.",
"12 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 4.1 Datasets To test the predictive performance of our model we consider 20 different real-world problems as summarized in Tab.",
"We have a total of 6 binary classifications, 4 multiclass, 1 time series, and 9 regression problems.",
A detailed description of the datasets can be found in Section 7.1 of supplementary material.,
Table 1: Datasets considered in the benchmark.,
"Name N D Task Reference Digits 357 64 Binary classification [4] Iris 150 4 Multiclass classification [27] Breast Cancer 569 30 Binary classification [74] Wine 173 13 Multiclass classification [3] Australian 600 15 Binary classification [24] Credit-g 1000 20 Binary classification [24] Glass 214 9 Multiclass classification [26] Blood 748 4 Binary classification [82] Heart Disease 270 13 Binary classification [24] Vowel 990 12 Multiclass classification [22] Delhi Weather 1461 7 Time series [42] Boston Housing 506 14 Regression [59] Diabetes 214 9 Regression [72] Prostatic Cancer 97 4 Regression [73] Liver 345 5 Regression [53] Plasma 315 16 Regression [57] Cloud 108 5 Regression [24] DTMB-54151 42 21 Regression [25] DTMB-54152 42 21 Regression [25] Body Fat 252 14 Regression [65] Furthermore, we use synthetic datasets to provide a deeper comparison of the feature importance and feature selection results obtained by the methods since the ground truth of the feature importance related to the learning task is known.",
"This allows for the evaluation of the quality of the feature selection and ranking provided by the methods, as the true feature importance can be compared to the estimates obtained by the models.",
"The synthetic datasets considered are the following: – Binary classification [37] (P1): given y = −1, the ten input features are generated with (x1, .",
", x10) ∼N(0, I).",
"Given y = 1, x1 through x4 are standard normal conditioned on 9 ≤ P4 j=1 x2 j ≤16, and (x5, .",
", x10) ∼N(0, I).",
The first four features are relevant for P1.,
"– 3-dimensional XOR as 4-way classification [16] (P2): Consider the 8 corners of the 3- dimensional hypercube (v1, v2, v3) ∈−1, 13, and group them by the tuples (v1v3, v2v3), leaving 4 sets of vectors paired with their negations v(i), −v(i).",
"Given a class i, a point is generated from the mixture distribution (1/2)N(v(i), 0.5I) + (1/2)N(−v(i), 0.5I).",
Each example additionally has 7 standard normal noise features for a total of D = 10 dimensions.,
The first three features are relevant for P2.,
"– Nonlinear regression [28] (P3): The 10-dimensional inputs x are independent features uniformly distributed on the interval [0, 1].",
"The output y is created according to the formula y = 10 sin(πx1x2)+ 13 20(x3 −0.5)2 + 10x4 + 5x5 + ϵ with ϵ ∼N(0, 1).",
The first five features are relevant for P3.,
"In all those three cases we varied the number of data points with N ∈{100, 500, 1000}.",
4.2 Evaluation of the Predictive Performance Tab.,
"2 presents a summary of our numerical results, showing the mean from the cross-validation procedure for each model.",
"Significantly, the GRBFNN exhibits robust competitiveness when compared to other models.",
"Specifically, the GRBFNNc emerged as the top-performing method, demonstrating statistically significant superiority in 4 out of 20 datasets.",
"Notably, these datasets exclusively involve regression tasks, highlighting the particular aptitude of GRBFNNc in addressing such problems.",
"Conversely, the GRBFNNk proved to be more effective in classification tasks, emerging as the top-performing method in two datasets.",
This observation suggests that the unsupervised selection of centers might be particularly suitable for such tasks.,
It’s important to notice that the selection of the center strategy is merely a hyperparameter in the GRBFNN model.,
"Numerical results suggest that there is no clear superiority between the strategies for selecting centers, and both approaches are equally competitive.",
"Users might consider experimenting with both strategies, as aggregating the results between the GRBFNNc and the GRBFNNk reveals that the GRBFNN model outperforms other models with statistically significant results in 7 out of 20 datasets.",
"Numerical results, confirm that standard methods such as the SVM, RF, and MLP are still very good baselines.",
"In some datasets, the optimizer used for the training process of FIDL did not seem to converge and the resulting performance in those datasets is not reported.",
"In conclusion, the no free lunch theorem [79] reminds us that there is no singularly superior model applicable to all machine learning problems.",
The choice of method depends on the specific dataset and task requirements.,
"Table 2: Numerical results summary: The first ten datasets are for binary and multiclass classification, showing average accuracy achieved via cross-validation.",
"The last ten are for regression and time series, showing average RMSE via cross-validation.",
"The bold numbers signify the method with the best performance on the test data, demonstrating statistical significance based on the Wilcoxon test (α = 0.05).",
"An asterisk indicates that, although the method is the overall best, statistical significance has not been established.",
"GRBFNNk GRBFNNc SVM RF XGB MLP DFS FIDL FT-T Training Test Training Test Training Test Training Test Training Test Training Test Training Test Training Test Training Test Digits 1.000 0.994 0.995 0.989 1.000 0.992 1.000 0.983 1.000 0.983 1.000 0.990 1.000 0.990 1.000 0.975 1.000 0.975 Iris 0.982 0.971 0.943 0.941 0.974 0.958 0.989 0.951 0.980 0.955 0.989 0.949 0.985 0.959 0.962 0.967 0.998 0.958 Breast Cancer 0.987 0.978∗ 0.988 0.976 0.988 0.977 0.990 0.956 1.000 0.965 1.000 0.971 0.985 0.976 0.992 0.974 0.999 0.970 Wine 0.996 0.985∗ 0.991 0.979 0.992 0.983 1.000 0.980 1.000 0.965 1.000 0.971 1.000 0.984 1.000 0.961 1.000 0.981 Australian 0.911 0.861 0.853 0.847 0.884 0.858 0.998 0.865 1.000 0.848 0.924 0.868 0.818 0.812 0.883 0.851 0.988 0.842 Credit-g 0.787 0.760 0.812 0.758 0.841 0.765∗ 1.000 0.760 0.951 0.764 0.874 0.744 0.778 0.757 0.793 0.747 0.985 0.715 Glass 0.912 0.657 0.969 0.686 0.857 0.685 1.000 0.780 1.000 0.761 0.978 0.714 0.947 0.710 0.699 0.630 0.992 0.756 Blood 0.791 0.781 0.794 0.782 0.829 0.784 0.818 0.790 0.829 0.777 0.801 0.792∗ 0.789 0.777 0.762 0.762 0.836 0.752 Heart Disease 0.785 0.764 0.846 0.814 0.943 0.802 0.916 0.834 0.951 0.816 0.872 0.842 0.891 0.826 0.878 0.844∗ 0.996 0.788 Vowel 0.995 0.959 0.998 0.969 0.999 0.992 1.000 0.967 1.000 0.920 1.000 0.968 0.995 0.953 - - 0.998 0.984 Delhi Weather 0.115 0.111 0.098 0.107 0.109 0.108 0.071 0.190 0.063 0.144 0.106 0.110 0.541 0.371 - - 0.081 0.120 Boston Housing 0.224 0.344 0.217 0.361 0.181 0.361 0.135 0.364 0.018 0.338 0.211 0.359 0.280 0.388 - - 0.163 0.336∗ Diabetes 0.703 0.718 0.669 0.706∗ 0.680 0.711 0.662 0.745 0.617 0.739 0.655 0.706 0.661 0.711 - - 0.371 0.861 Prostatic Cancer 0.581 0.646 0.581 0.646 0.570 0.625 0.610 0.715 0.377 0.713 0.595 0.662 0.460 0.697 0.408 0.644 0.278 0.864 Liver 0.834 0.895 0.838 0.902 0.825 0.913 0.770 0.923 0.743 0.928 0.857 0.911 0.846 0.934 - - 0.491 1.113 Plasma 0.958 0.991 0.941 0.982 0.937 0.987 0.890 0.989 0.865 0.999 1.000 1.002 0.917 1.070 - - 0.094 1.143 Cloud 0.340 0.451 0.323 0.385 0.344 0.373 0.418 0.521 0.216 0.438 0.378 0.417 0.615 0.613 0.138 0.206∗ 0.190 0.490 DTMB-5415(1) 0.513 0.752 0.103 0.783 0.096 0.886 0.413 1.014 0.057 0.929 0.009 0.816 0.777 0.898 0.899 0.932 0.222 0.745∗ DTMB-5415(2) 0.031 0.112 0.020 0.071 0.088 0.226 0.342 0.902 0.001 0.794 0.052 0.238 0.048 0.300 0.073 0.077 0.293 0.575 Body Fat 0.090 0.132 0.086 0.127 0.153 0.148 0.070 0.170 0.112 0.185 0.074 0.131 0.113 0.137 1.523 2.650 0.076 0.142 14 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 4.3 The Impact of Regularization on the GRBFNN We aim to provide further insights into the behavior of the GRBFNN model by examining the relationship between its two regularizers, λw and λu.",
We show the results of the hyperparameter search procedure on the test data for Breast Cancer and Prostatic Cancer in Fig.,
"For the regression problems, dark colors indicate lower error while for classification problems lighter color indicates higher accuracy.",
The red frame indicates the best set of regularizers.,
"Interestingly, we observe that in Fig.",
5a and in Fig.,
5b the regularizer of the precision matrix λu impacts the performance of the GRBFNN more than the regularizer of the weights λw.,
"This phenomenon occurs in many datasets such as Digits, Breast Cancer, Credit-g, Glass, Diabetes, Prostatic Cancer, Cloud, DTMB-5415(2) and the Body Fat dataset (see Fig.",
13 in supplementary Section 7).,
"In these cases, we obtain the best combination of regularizers on the test data when λw is set to 0.",
This suggests that the regularization term λw has minimal influence on the learning task on those datasets.,
None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy (a) Breast Cancer dataset test accuracies.,
None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE (b) Prostatic cancer dataset test RMSE.,
5: Graphical interpretation of the sensitivity analysis concerning the two regularizers λw and λu on the Breast Cancer dataset (binary classification) in (a) and on the Prostatic Cancer dataset (regression) in (b).,
The red frame highlights the best combination of hyperparameters.,
A lighter color indicates higher accuracy and higher RMSE.,
This can indicate that promoting the ’flatness’ of the Gaussian basis function through penalizing the entries of the precision matrix P through λu may have a more pronounced regularization and generalization impact on the model than merely penalizing the amplitudes via λw.,
"Therefore, in situations where conducting a large hyperparameter search is not feasible due to computational constraints, it might be beneficial to prioritize the hyperparameter search solely on λu.",
"4.4 Supervised Dimensionality Reduction in the Active Subspace In Section 3.1, we mentioned that valuable insights into the behavior of the GRBFNN can be obtained by examining the eigenvalues and projecting the model onto the active subspace spanned by the eigenvectors of P. This enables us to visualize the function the GRBFNN is attempting to model (for example, in 2D), offering users an additional approach to comprehending the representation learned by our model.",
"In particular, we project the dataset X, along the first two eigenvectors relative to the largest eigenvalues of the matrix P, for visualization in 2D.",
The function values the then computed by projecting back the latent variables in the input space.,
6 we illustrate the active subspace for the Wine dataset.,
The eigenvalues’ decay (Fig.,
"6b) given by γ1 PK k γk , indicates that the GRBFNN model’s variability is primarily captured in two 15 dimensions, as the first two eigenvalues are the only ones significantly different from zero.",
"It is worth recalling that the eigenvalue γk of the matrix P represents the second derivative of the argument of the Gaussian basis function after rotating it in the latent space, along the corresponding principal axis vk.",
"Therefore, very low magnitude eigenvalues mean a lack of variability of our model in those principal directions, indicating that the true underlying factors of variation of the original problem may develop in a lower dimensional space.",
This assertion is validated by the corresponding active subspace displayed in Fig.,
"6a, revealing variations along both latent variables z1 and z2.",
"Additionally, it is noteworthy that the learned decision boundaries appear almost linear, given the near-linear separability of classes in this problem.",
-2 0 2 z1 -2 0 2 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) (a) Wine dataset active subspace.,
1 2 3 4 5 6 7 8 9 10 11 12 13 k 0.0 0.2 0.4 γk PK k γk (b) Wine dataset eigenvalues decay.,
6: Graphical interpretation of the active subspace in two dimensions (a) and corresponding eigenvalues decay (b) for the Wine dataset (multiclass classification).,
Function values are normalized between zero and one.,
At the end of Section 7 of the supplementary material we show in Fig.,
14 the active subspace and the related eigenvalue decay for all the datasets.,
"For the Digits, Iris, and Breast Cancer datasets, the embedding shows that the function f provides a remarkable discriminative power where most of the variability is obtained in just one dimension, along with the latent variable z1.",
"This is confirmed by the relative eigenvalues decays, which show that the first eigenvalue γ1 is the only one significantly different from zero.",
"In general, for almost linearly separable classification problems, the GRBFNN likely detects a one-dimensional active subspace as is the case for the Digits dataset.",
"In the regression cases, we can, for example, observe that the GRBFNN model identifies an active subspace of dimension K = 1 in many datasets such as the Prostate Cancer, Plasma, Cloud, DTMB-5415(1) and the DTMB-5415(2) dataset.",
"In other regression problems, the active subspace might reveal more complicated patterns as is the case of the Liver and Boston Housing datasets.",
"4.5 Evaluation of the Feature Importance Ranking In addition to analyzing the predictive performance and performing a supervised dimensionality reduction in the active subspace for visualization, we can also obtain information about the importance of input features x.",
This provides additional insights into the model behavior and enables the user to perform feature selection.,
"To evaluate the significance of the feature importance ranking, we will only consider the embedding methods.",
"This means that SVM, MLP, and FT-T will not be taken into account in this part of the benchmark since they do not directly provide information on the importance of each input feature.",
"16 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 4.5.1 Feature Importance Ranking for the Digits Dataset Some of the real-world datasets used to evaluate the predictive performance of the models can also be used to evaluate the quality of the feature importance ranking obtained once the model training has terminated such as the Digits and DTMB-5415(2) datasets.",
We train all the models using their best set of hyperparameters from the cross-validation procedure performed precedently to the whole dataset.,
(a) Mean of the class ’3’.,
(b) Mean of the class ’8’.,
Low High Feature Importance (c) GRBFNN.,
Low High Feature Importance (d) RF.,
Low High Feature Importance (e) XGB.,
Low High Feature Importance (f) DFS.,
Low High Feature Importance (g) FIDL.,
7: Graphical interpretation of the feature importance for the Digits dataset for all the models considered in this experiment.,
The feature importance should highlight the pixels where (a) and (b) differ.,
The GRBFNN employs an unsupervised center selection method (i.e.,
"GRBFNNk) for this dataset, as evidenced by the superior performance in Tab.",
2 with respect to GRBFNNc.,
"We can easily show the feature importance obtained from the Digits dataset, which is composed only of the digit ’8’ and ’3’.",
The mean of those two classes is visible in Fig.,
7a and Fig.,
"7b, where each feature corresponds to a particular pixel intensity of greyscale value.",
This suggests that is easy to interpret the feature importance identified from the models because the important features should highlight where the digits ’8’ and ’3’ differ the most.,
7 we can verify the feature importance detected from the models.,
"The GRBFNN employs an unsupervised center selection method for this dataset, as evidenced by the superior performance in Tab.",
2 compared to the model with supervised center selection.,
"Meaningful feature importance is observable for the GRBFNN, RF, and XGB models.",
The GRBFNN (Fig.,
"7c), similar to the RF (Fig.",
7d) the feature importance enhances pixels where those two classes differ.,
"In contrast, the XGB (Fig.",
7e) provides a sparser representation since almost all the importance is concentrated in only a few pixels.,
7f and Fig.,
7g seems that the two methods for feature importance learning for deep learning fail to provide explainable feature importances.,
4.5.2 Feature Importance Ranking for the DTMB-5415 Dataset We provide a similar validation analysis for the DTMB-5415(2) dataset.,
Based on the results presented in Tab.,
"2, the GRBFNN model achieved better performance on this dataset using the supervised selection of the centers compared to the unsupervised selection of the centers.",
"For this particular problem, we can estimate an approximate ground truth for the feature importance of the true function, by computing the gradients at specific input variables x.",
"To do this, we employ a finite difference process, evaluating gradients at 84 different points.",
The estimated ground truth is obtained by averaging the absolute values of the gradient vectors at these 84 points.,
"8, we 17 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 0.00 0.05 0.10 0.15 Feature Importance Approx.",
ground truth GRBFNN (a) GRBFNN.,
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 0.00 0.05 0.10 0.15 0.20 Feature Importance Approx.,
ground truth RF (b) RF.,
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 0.00 0.05 0.10 0.15 Feature Importance Approx.,
ground truth XGB (c) XGB.,
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 0.00 0.05 0.10 Feature Importance Approx.,
ground truth DFS (d) DFS.,
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 0.00 0.05 0.10 Feature Importance Approx.,
ground truth FIDL (e) FIDL.,
8: Graphical interpretation of the feature importance for the DTMB-5415(2) dataset for all the models considered in this experiment.,
"Blue bars represent the approximated ground truth feature importance, in green is the one estimated by the models.",
The GRBFNN employs a supervised center selection method (i.e.,
"GRBFNNc) for this dataset, as evidenced by the superior performance in Tab.",
2 with respect to GRBFNNk.,
show the approximated ground truth in blue in each bar plot.,
It should be noted that the true feature importance of the last five features is zero as these are not related to the true function.,
"Consequently, the feature importance obtained from the models shown in green should be able to detect this.",
Only the GRBFNN (Fig.,
8a) and the XGB (Fig.,
"8c) can recognize that the last five features are not related to the problem, while the RF (Fig.",
"8b), DFS (Fig.",
8d) and FIDL (Fig.,
8e) fail to identify that.,
"To summarize the results, in Fig.",
"9 we show the bar plot regarding the mean absolute error between the approximated ground truth feature importance and the one estimated from all the methods, showing that in this case, the GRBFNN obtains the lowest error in detecting the underlying important variables of the regression task.",
4.5.3 Feature Importance Ranking for the Synthetic Datasets We test the same models on other synthetic datasets presented at the end of Section 4.1 designed specifically to evaluate feature selection and feature importance ranking models.,
"3 we resume 18 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker GRBFNN RF XGB DFS FIDL 0.000 0.005 0.010 0.015 0.020 0.025 0.030 MAE Fig.",
9: Bars represent the mean absolute error (MAE) (y-axis) between the approximated ground truth feature importance and the one estimated from the models (x-axis).,
the numerical results obtained with the same cross-validation procedure as in the previous numerical experiments.,
Table 3: Numerical results summary for the three synthetic problems.,
Problem 1 (P1) and Problem 2 (P2) are binary and multiclass classification tasks respectively and numbers represent the accuracy values.,
Problem 3 (P3) is a regression task and the numbers represent RMSE values.,
Bold numbers indicate the best-performing method on the test data that is statistically significant according to the Wilcoxon test (α = 0.05).,
The asterisk means that the method is the best on average but not demonstrated to be statistically significant.,
"GRBFNNk GRBFNNc RF XGB DFS FIDL N Training Test Training Test Training Test Training Test Training Test Training Test P1 100 0.980 0.609 1.000 0.652 0.992 0.672 1.000 0.736 1.000 0.762 0.992 0.820∗ 500 0.958 0.898 0.981 0.892 1.000 0.882 1.000 0.803 1.000 0.905 0.942 0.914∗ 1000 0.962 0.931 0.973 0.925 1.000 0.901 1.000 0.910 0.991 0.897 0.934 0.918 P2 100 0.235 0.310 0.250 0.249 1.000 0.290 0.993 0.289 0.973 0.338 0.762 0.340 500 0.710 0.428 0.706 0.508∗ 1.000 0.486 1.000 0.483 0.898 0.497 0.568 0.496 1000 0.643 0.538 0.645 0.551∗ 0.871 0.546 1.000 0.507 0.633 0.537 0.497 0.455 P3 100 0.499 0.570 0.497 0.570 0.236 0.615 0.001 0.506 0.157 0.447 - - 500 0.232 0.283 0.184 0.255 0.163 0.440 0.062 0.316 0.190 0.289 - - 1000 0.258 0.284 0.198 0.221 0.142 0.387 0.101 0.264 0.196 0.240 - - Problem P1 is a binary classification problem and the FIDL shows the best performance for N = 100 and N = 500 (not statistically significant), while for N = 1000 the GRBFNNk (statistically significant) obtained the higher accuracy.",
"4, we have the feature importance related to problem P1.",
We discuss GRBFNNc for N = 100 while for N = 500 and N = 1000 we show GRBFNNk and refer as GRBFNN.,
"For N = 100, the GRBFNN provides meaningless feature importance across the methods due also to the lack of predictive performance obtained in this case, while the XGB and FIDL are the only models to recognize that the first four features are relevant.",
"For N = 500 and N = 1000, the feature importance from the GRBFNN improves substantially together with its predictive performance.",
The DFS even if it provides competitive accuracy compared with other methods has some difficulty in highlighting the importance of the first four features from the remaining ones especially for N = 500.,
"As further support and analysis, we evaluate the eigenvalues decay that can help us to highlight the numbers of factors of variation of the learned model f. In Fig.",
"10, we have the eigenvalues decay for problem P1 for the values of N considered.",
It is possible to notice that for N = 1000 and N = 500 the GRBFNN varies mainly along four components which are also the number of the important features for P1.,
"For N = 100, there is not a clear identification of those factors within the latent space/active subspace due to the low predictive performance of the model.",
19 Table 4: Summary of the models feature importance obtained on problem P1.,
Note that the first four features are relevant in P1.,
The GRBFNNc or GRBFNNk are renamed as GRBFNN depending on which one achieved superior performance according to Table 3.,
Model GRBFNN RF XGB DFS FIDL P1 (N = 100) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance P1 (N = 500) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance P1 (N = 1000) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance 1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 γk PK k γk (a) N = 100.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.1 0.2 0.3 γk PK k γk (b) N = 500.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.1 0.2 0.3 γk PK k γk (c) N = 1000.,
10: Eigenvalues decay for problem P1.,
Problem P2 is a difficult multiclass classification problem and DFS shows the best performance for N = 100 (statistically significant) while for N = 500 and N = 1000 the GRBFNNc (not statistically significant) obtains the highest accuracy.,
"5, we have the feature importance related to problem P2.",
We discuss GRBFNNk for N = 100 while for N = 500 and N = 1000 we show GRBFNNc and both referred as GRBFNN.,
"Similarly as in the previous case for N = 100, the GRBFNN and RF have some difficulty in detecting that the first three features are the most important while the FIDL provides the best feature ranking.",
"Similar to problem P1, the feature importance from the GRBFNN improves significantly for N = 500 and N = 1000 along with its predictive performance.",
"The GRBFNN and FIDL provide the most meaningful feature importance ranking in these cases, while the other methods fail to provide a clear separation between important and non-important variables.",
"11, we have the eigenvalues decay for problem P2 for the values of N considered.",
"For N = 1000, the GRBFNN varies mainly along three components which are also the number of the important features for P2, this behavior is less visible but still present for N = 100 and N = 500.",
Problem P3 is a nonlinear regression problem with the DFS showing the best performance for N = 100 (statistically significant) while for N = 500 and N = 1000 the GRBFNNc (statistically significant) obtains the lowest RMSE.,
"6, we can analyze the feature importance related to problem P3.",
"We discuss GRBFNNc for N = 100, N = 500 and N = 1000 and referred as GRBFNN.",
"For N = 100, seems that all the models recognize that only the first five features are important in problem P3.",
"Also for N = 500 and N = 1000 the GRBFNN, RF, XGB, and FIDL correctly ignore the 20 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker Table 5: Summary of the models feature importance obtained on problem P2.",
Note that the first three features are relevant in P2.,
The GRBFNNc or GRBFNNk are renamed as GRBFNN depending on which one achieved superior performance according to Table 3.,
Model GRBFNN RF XGB DFS FIDL P2 (N = 100) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.20 0.40 0.60 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance P2 (N = 500) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance P2 (N = 1000) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance 1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.5 0.8 γk PK k γk (a) N = 100.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 γk PK k γk (b) N = 500.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.1 0.2 0.3 γk PK k γk (c) N = 1000.,
11: Eigenvalues decay for problem P2.,
"contribution of the last five features, differently from DFS.",
"Interestingly, for N = 500 and N = 1000, the feature importance from GRBFNN differs from all the other models where they recognize the feature x4 as the most important only for N = 100.",
"12, we have the eigenvalues decay for problem P3 for the values of N considered.",
"For N = 100 the model varies only along one latent variable while N = 500 and N = 1000, we have approximately the first five eigenvalues that are different from zero.",
1 2 3 4 5 6 7 8 9 10 k 0.0 0.5 1.0 γk PK k γk (a) N = 100.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 γk PK k γk (b) N = 500.,
1 2 3 4 5 6 7 8 9 10 k 0.0 0.2 0.4 γk PK k γk (c) N = 1000.,
12: Eigenvalues decay for problem P3.,
21 Table 6: Summary of the models feature importance obtained on problem P3.,
Note that the first five features are relevant in P3.,
The GRBFNNc or GRBFNNk are renamed as GRBFNN depending on which one achieved superior performance according to Table 3.,
"Model GRBFNN RF XGB DFS FIDL P3 (N = 100) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.20 0.40 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.20 0.40 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance P3 (N = 500) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance P3 (N = 1000) x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 0.40 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.10 0.20 0.30 0.40 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.05 0.10 0.15 0.20 Feature Importance x1 x2 x3 x4 x5 x6 x7 x8 x9x10 0.00 0.20 0.40 Feature Importance 5 Conclusion and Future Work In this paper, we proposed modifications to the classical RBFNN model, to enhance its interpretability by highlighting the important features and estimating the active subspace of the model.",
This is achieved by incorporating a learnable precision matrix into the Gaussian kernel.,
The latent information about the learned model can be extracted from the eigenvalues and eigenvectors of the estimated precision matrix.,
"Our extensive numerical experiments covered regression, classification, and feature selection tasks, where we compared our proposed model with widely used methods such as SVM, MLP, RF, XGB, FT-T (a transformer architecture for tabular data), and state-of-the-art deep learning-based embedding methods such as DFS and FIDL.",
The results demonstrated that our GRBFNN model achieved attractive prediction performance while providing meaningful feature importance rankings.,
"One of the key observations from our experiments was the impact of the regularizer λu on the performance of the GRBFNN, which often prevails the effect of the weight regularizer λw.",
This finding suggests that prioritizing the regularization of the precision matrix yields more significant improvements in the model generalization performance.,
"Moreover, the model enables supervised dimensionality reduction in the active subspace, facilitating visualization and comprehension of complex phenomena.",
"In parallel, it offers insights into the impact of individual input features through a feature importance ranking.",
This capability not only facilitates a clearer understanding of the model but also allows for effective feature selection tasks.,
"In summary, by combining predictive power with interpretability, the GRBFNN offers a valuable tool for understanding complex nonlinear relationships in the data.",
"Overall, our work contributes to bridging the gap between black-box neural network models and interpretable machine learning, enabling users to not only make accurate predictions but also gain meaningful insights from the model behavior and improve decision-making processes in real-world applications.",
"Looking ahead, we plan to apply our model to tackle the so-called curse of dimensionality in expensive engineering optimization problems.",
"By leveraging the active subspace estimation, we aim to reduce the dimensionality of optimization problems without relying on direct gradient computations as in the classical active subspace method, which is not desirable in noisy simulation scenarios.",
"Additionally, we plan to broaden the application of our model in domains where accurate predictions and interpretable insights are crucial such as in healthcare.",
"22 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 6 Acknowledgement The research is supported by Prof. Shoemaker’s Distinguished Professor Chair fund from National University of Singapore (NUS) and her startup funds from NUS.",
References 1.,
Amina Adadi and Mohammed Berrada.,
Peeking inside the black-box: a survey on explainable artificial intelligence (xai).,
"IEEE access, 6:52138–52160, 2018.",
Kofi P Adragni and R Dennis Cook.,
Sufficient dimension reduction and prediction in regression.,
"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 367(1906):4385–4405, 2009.",
"Stefan Aeberhard, Danny Coomans, and Olivier De Vel.",
Comparative analysis of statistical pattern recognition methods in high dimensional settings.,
"Pattern Recognition, 27(8):1065–1077, 1994.",
Fevzi Alimoglu and Ethem Alpaydin.,
Methods of combining multiple classifiers based on different representations for pen-based handwriting recognition.,
"In Proceedings of the fifth Turkish artificial intelligence and artificial neural networks symposium (TAINN 96), 1996.",
"Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al.",
"Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai.",
"Information fusion, 58:82–115, 2020.",
Richard Bellman.,
Dynamic programming.,
"Science, 153(3731):34–37, 1966.",
"Yoshua Bengio, Aaron Courville, and Pascal Vincent.",
Representation learning: A review and new perspectives.,
"IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.",
Dimitri P Bertsekas.,
Nonlinear programming.,
"Journal of the Operational Research Society, 48(3):334–334, 1997.",
"Yue Bi, Dongxu Xiang, Zongyuan Ge, Fuyi Li, Cangzhi Jia, and Jiangning Song.",
An interpretable prediction model for identifying n7-methylguanosine sites based on xgboost and shap.,
"Molecular Therapy-Nucleic Acids, 22:362–372, 2020.",
Chris Bishop.,
Improving the generalization properties of radial basis function neural networks.,
"Neural computation, 3(4):579–588, 1991.",
Christopher M Bishop.,
Curvature-driven smoothing in backpropagation neural networks.,
"In Theory and Applications of Neural Networks, pages 139–148.",
"Springer, 1992.",
Christopher M Bishop et al.,
Neural networks for pattern recognition.,
"Oxford university press, 1995.",
Leo Breiman.,
Random forests.,
"Machine learning, 45(1):5–32, 2001.",
David S Broomhead and David Lowe.,
"Radial basis functions, multi-variable functional interpolation and adaptive networks.",
"Technical report, Royal Signals and Radar Establishment Malvern (United Kingdom), 1988.",
"Celina Buzzi, Luigi Grippo, and Marco Sciandrone.",
Convergent decomposition techniques for training rbf neural networks.,
"Neural Computation, 13(8):1891–1920, 2001.",
"Jianbo Chen, Mitchell Stern, Martin J Wainwright, and Michael I Jordan.",
Kernel feature selection via conditional covariance minimization.,
"Advances in Neural Information Processing Systems, 30, 2017.",
Tianqi Chen and Carlos Guestrin.,
Xgboost: A scalable tree boosting system.,
"In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16.",
"ACM, August 2016.",
"Paul G Constantine, Eric Dow, and Qiqi Wang.",
Active subspace methods in theory and practice: applications to kriging surfaces.,
"SIAM Journal on Scientific Computing, 36(4):A1500–A1524, 2014.",
R Dennis Cook.,
On the interpretation of regression plots.,
"Journal of the American Statistical Association, 89(425):177–189, 1994.",
R Dennis Cook.,
Regression graphics: Ideas for studying regressions through graphics.,
"John Wiley & Sons, 2009.",
Corinna Cortes and Vladimir Vapnik.,
Support-vector networks.,
"Machine learning, 20:273–297, 1995.",
David Deterding.,
Speaker normalization for automatic speech recognition.,
"University of Cambridge, Ph.",
"D. Thesis, 1989.",
Abhirup Dikshit and Biswajeet Pradhan.,
Interpretable and explainable ai (xai) model for spatial drought prediction.,
"Science of the Total Environment, 801:149797, 2021.",
Dheeru Dua and Casey Graff.,
"UCI machine learning repository, 2017.",
Danny D’Agostino.,
Generative models for anomaly detection and design-space dimensionality reduction in shape optimization.,
"Engineering Applications of Artificial Intelligence, 129:107566, 2024.",
Ian W. Evett and E. J. Spiehler.,
Rule induction in forensic science.,
"In KBS in Government, pages 107–118.",
"Online Publications, 1987.",
Ronald A Fisher.,
The use of multiple measurements in taxonomic problems.,
"Annals of eugenics, 7(2):179–188, 1936.",
Jerome H Friedman.,
Multivariate adaptive regression splines.,
"The annals of statistics, 19(1):1–67, 1991.",
Jerome H Friedman.,
Greedy function approximation: a gradient boosting machine.,
"Annals of statistics, pages 1189–1232, 2001.",
Kunihiko Fukushima.,
Cognitron: A self-organizing multilayered neural network.,
"Biological cybernetics, 20(3-4):121–136, 1975.",
"Federico Girosi, Michael Jones, and Tomaso Poggio.",
Regularization theory and neural networks architectures.,
"Neural computation, 7(2):219–269, 1995.",
"Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.",
Revisiting deep learning models for tabular data.,
"Advances in Neural Information Processing Systems, 34:18932–18943, 2021.",
Heinrich W Guggenheimer.,
Differential geometry.,
"Courier Corporation, 2012.",
"Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.",
A survey of methods for explaining black box models.,
"ACM computing surveys (CSUR), 51(5):1–42, 2018.",
Isabelle Guyon and André Elisseeff.,
An introduction to variable and feature selection.,
"Journal of machine learning research, 3(Mar):1157–1182, 2003.",
"Hong-Gui Han, Miao-Li Ma, and Jun-Fei Qiao.",
Accelerated gradient algorithm for rbf neural network.,
"Neurocomputing, 441:237–247, 2021.",
"Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman.",
"The elements of statistical learning: data mining, inference, and prediction, volume 2.",
"Springer, 2009.",
"Katja Hauser, Alexander Kurz, Sarah Haggenmüller, Roman C Maron, Christof von Kalle, Jochen S Utikal, Friedegund Meier, Sarah Hobelsberger, Frank F Gellrich, Mildred Sergon, et al.",
Explainable artificial intelligence in skin cancer recognition: A systematic review.,
"European Journal of Cancer, 167:54–69, 2022.",
"Kurt Hornik, Maxwell Stinchcombe, and Halbert White.",
Multilayer feedforward networks are universal approximators.,
"Neural networks, 2(5):359–366, 1989.",
Harold Hotelling.,
Analysis of a complex of statistical variables into principal components.,
"Journal of educational psychology, 24(6):417, 1933.",
"Jennifer L Jefferson, James M Gilbert, Paul G Constantine, and Reed M Maxwell.",
Active subspaces for sensitivity analysis and dimension reduction of an integrated hydrologic model.,
"Computers & geosciences, 83:127–138, 2015.",
42. https://www.kaggle.com/datasets/mahirkukreja/delhi-weather-data.,
"24 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 43.",
Diederik P Kingma and Jimmy Ba.,
Adam: A method for stochastic optimization.,
"arXiv preprint arXiv:1412.6980, 2014.",
"Jichao Li, Jinsheng Cai, and Kun Qu.",
Surrogate-based aerodynamic shape optimization with the active subspace method.,
"Structural and Multidisciplinary Optimization, 59(2):403–419, 2019.",
Ker-Chau Li.,
Sliced inverse regression for dimension reduction.,
"Journal of the American Statistical Association, 86(414):316–327, 1991.",
"Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman.",
Deep feature selection: theory and application to identify enhancers and promoters.,
"Journal of Computational Biology, 23(5):322–336, 2016.",
"Haijun Lin, Houde Dai, Yihan Mao, and Lucai Wang.",
An optimized radial basis function neural network with modulation-window activation function.,
"Soft Computing, pages 1–18, 2023.",
Stuart Lloyd.,
Least squares quantization in PCM.,
"IEEE transactions on information theory, 28(2):129–137, 1982.",
Ilya Loshchilov and Frank Hutter.,
"Decoupled weight decay regularization, 2019.",
Tyson Loudon and Stephen Pankavich.,
Mathematical analysis and dynamic active subspaces for a long term model of hiv.,
"arXiv preprint arXiv:1604.04588, 2016.",
"Trent W Lukaczyk, Paul Constantine, Francisco Palacios, and Juan J Alonso.",
Active subspaces for shape optimization.,
"In 10th AIAA multidisciplinary design optimization conference, page 1171, 2014.",
Scott M Lundberg and Su-In Lee.,
A unified approach to interpreting model predictions.,
"Advances in neural information processing systems, 30, 2017.",
James McDermott and Richard S. Forsyth.,
Diagnosing a disorder in a classification benchmark.,
"Pattern Recognition Letters, 73:41–43, 2016.",
Charles A Micchelli.,
Interpolation of scattered data: distance matrices and conditionally positive definite functions.,
"In Approximation theory and spline functions, pages 143–145.",
"Springer, 1984.",
Michael Mongillo et al.,
Choosing basis functions and shape parameters for radial basis function methods.,
"SIAM undergraduate research online, 4(190-209):2–6, 2011.",
John Moody and Christian J Darken.,
Fast learning in networks of locally-tuned processing units.,
"Neural computation, 1(2):281–294, 1989.",
"David W Nierenberg, Therese A Stukel, John A Baron, Bradley J Dain, E Robert Greenberg, and Skin Cancer Prevention Study Group.",
Determinants of plasma levels of beta-carotene and retinol.,
"American Journal of Epidemiology, 130(3):511–521, 1989.",
"Jean Jacques Ohana, Steve Ohana, Eric Benhamou, David Saltiel, and Beatrice Guez.",
Explainable ai (xai) models applied to the multi-agent environment of financial markets.,
"In International Workshop on Explainable, Transparent Autonomous Agents and Multi-Agent Systems, pages 189–207.",
"Springer, 2021.",
R Kelley Pace and Ronald Barry.,
Sparse spatial autoregressions.,
"Statistics & Probability Letters, 33(3):291–297, 1997.",
Jooyoung Park and Irwin W Sandberg.,
Universal approximation using radial-basis-function networks.,
"Neural computation, 3(2):246–257, 1991.",
Jooyoung Park and Irwin W Sandberg.,
Approximation and radial-basis-function networks.,
"Neural computation, 5(2):305–316, 1993.",
"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.",
"Pytorch: An imperative style, high-performance deep learning library.",
"In Advances in Neural Information Processing Systems 32, pages 8024–8035.",
"Curran Associates, Inc., 2019.",
Karl Pearson.,
On lines and planes of closest fit to systems of points in space.,
"The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559–572, 1901.",
"F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and 25 E. Duchesnay.",
Scikit-learn: Machine learning in Python.,
"Journal of Machine Learning Research, 12:2825–2830, 2011.",
"Keith W Penrose, AG Nelson, and AG Fisher.",
Generalized body composition prediction equation for men using simple measurement techniques.,
"Medicine & Science in Sports & Exercise, 17(2):189, 1985.",
Tomaso Poggio and Federico Girosi.,
Networks for approximation and learning.,
"Proceedings of the IEEE, 78(9):1481–1497, 1990.",
MJD Powell.,
Radial basis function methods for interpolation to functions of many variables.,
"In HERCMA, pages 2–24.",
"Citeseer, 2001.",
"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. """,
"why should i trust you?""",
explaining the predictions of any classifier.,
"In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135–1144, 2016.",
Frank Rosenblatt.,
The perceptron: a probabilistic model for information storage and organization in the brain.,
"Psychological review, 65(6):386, 1958.",
"Friedhelm Schwenker, Hans A Kestler, and Günther Palm.",
Three learning phases for radial-basis- function networks.,
"Neural networks, 14(4-5):439–458, 2001.",
Ya D Sergeyev.,
On the search of the shape parameter in radial basis functions using univariate global optimization methods.,
"Journal of Global Optimization, 79:305–327, 2021.",
"Jack W Smith, James E Everhart, WC Dickson, William C Knowler, and Robert Scott Johannes.",
Using the adap learning algorithm to forecast the onset of diabetes mellitus.,
"In Proceedings of the annual symposium on computer application in medical care, page 261.",
"American Medical Informatics Association, 1988.",
"Thomas A Stamey, John N Kabalin, John E McNeal, Iain M Johnstone, Fuad Freiha, Elise A Redwine, and Norman Yang.",
Prostate specific antigen in the diagnosis and treatment of ade- nocarcinoma of the prostate.,
radical prostatectomy treated patients.,
"The Journal of urology, 141(5):1076–1083, 1989.",
"W Nick Street, William H Wolberg, and Olvi L Mangasarian.",
Nuclear feature extraction for breast tumor diagnosis.,
"In Biomedical image processing and biomedical visualization, volume 1905, pages 861–870.",
"SPIE, 1993.",
Robert Tibshirani.,
Regression shrinkage and selection via the lasso: a retrospective.,
"Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(3):273–282, 2011.",
Dietrich Wettschereck and Thomas Dietterich.,
Improving the performance of radial basis function networks by learning center locations.,
"Advances in neural information processing systems, 4, 1991.",
Frank Wilcoxon.,
Individual comparisons by ranking methods.,
"In Breakthroughs in Statistics: Methodology and Distribution, pages 196–202.",
"Springer, 1992.",
Maksymilian Wojtas and Ke Chen.,
Feature importance ranking for deep learning.,
"Advances in Neural Information Processing Systems, 33:5105–5114, 2020.",
David H Wolpert.,
The lack of a priori distinctions between learning algorithms.,
"Neural computation, 8(7):1341–1390, 1996.",
"Xiang Yi Wong, Yu Kai Ang, Keqi Li, Yip Han Chin, Sean Shao Wei Lam, Kenneth Boon Kiat Tan, Matthew Chin Heng Chua, Marcus Eng Hock Ong, Nan Liu, Ahmad Reza Pourghaderi, et al.",
Development and validation of the sarica score to predict survival after return of spontaneous circulation in out of hospital cardiac arrest using an interpretable machine learning framework.,
"Resuscitation, 170:126–133, 2022.",
Siyuan Xing and Jian-Qiao Sun.,
"Separable gaussian neural networks: Structure, analysis, and function approximations.",
"Algorithms, 16(10):453, 2023.",
"I-Cheng Yeh, King-Jang Yang, and Tao-Ming Ting.",
Knowledge discovery on rfm model using bernoulli sequence.,
"Expert Systems with Applications, 36(3, Part 2):5866–5871, 2009.",
"Yunwei Zhang, Chunlin Gong, Hai Fang, Hua Su, Chunna Li, and Andrea Da Ronch.",
An efficient space division–based width optimization method for rbf network using fuzzy clustering algorithms.,
"Structural and Multidisciplinary Optimization, 60:461–480, 2019.",
"26 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker 84.",
Sanpeng Zheng and Renzhong Feng.,
A variable projection method for the general radial basis function neural network.,
"Applied Mathematics and Computation, 451:128009, 2023.",
"Sanpeng Zheng, Renzhong Feng, and Aitong Huang.",
The optimal shape parameter for the least squares approximation based on the radial basis function.,
"Mathematics, 8(11):1923, 2020.",
27 7 Supplementary Material 7.1 Datasets Description The following is a detailed description of the real-world datasets used in this study: – Digits [4]: They created a digit database by collecting 250 samples from 44 writers.,
"The samples written by 30 writers are used for training, cross-validation, and writer-dependent testing, and the digits written by the other 14 are used for writer-independent testing.",
"For the current experiment, we use the digits 3 and 8 for feature selection purposes so that N = 357 and D = 64.",
"– Iris [27]: One of the most famous datasets in the pattern recognition literature, contains 3 classes of 50 instances each (N = 150, D = 4), where each class refers to a type of iris plant.",
One class is linearly separable from the other 2; the latter are not linearly separable from each other.,
– Breast Cancer [74]: Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.,
They describe the characteristics of the cell nuclei present in the image.,
"In this dataset, N = 569, D = 30, and two classes.",
– Wine [3]: The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.,
There are thirteen different measurements taken for different constituents found in the three types of wine.,
"In this dataset, N = 173, D = 13, and three classes.",
"– Australian [24]: This is the famous Australian Credit Approval dataset, originating from the StatLog project.",
It concerns credit card applications.,
All attribute names and values have been changed to meaningless symbols to protect the confidentiality of the data.,
"In this dataset, N = 600, D = 15, and two classes.",
"– Credit-g [24]: This dataset classifies people described by a set of attributes as good or bad credit risks, there are D = 20 features, N = 1000 data points, and two classes in this dataset.",
– Glass [26] : The Glass identification database.,
The study of the classification of types of glass was motivated by criminological investigation.,
"There are D = 9 features, N = 214 data points, and two classes in this dataset.",
– Blood [82]: Data taken from the Blood Transfusion Service Center in Hsin-Chu City in Taiwan.,
The center passes its blood transfusion service bus to one university in Hsin-Chu City to gather blood donated about every three months.,
The target attribute is a binary variable representing whether he/she donated blood in March 2007.,
"This dataset has D = 4 features, N = 748 data points, and two classes.",
"– Heart Disease [24]: This database contains 76 attributes, but all published experiments refer to using a subset of D = 14 of them and N = 270.",
The goal is to predict the presence of heart disease in the patient.,
– Vowel [22]: Speaker-independent recognition of the eleven steady state vowels of British English using a specified training set of lpc derived log area ratios.,
"This dataset has D = 12 features, N = 990 data points, and eleven classes.",
"– Dehli Weather [42]: The Delhi weather dataset was transformed from a time series problem into a supervised learning problem by using past time steps as input variables and the subsequent time step as the output variable, representing the humidity.",
In this dataset N = 1461 and D = 7.,
– Boston [59]: This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass and has been used extensively throughout the literature to benchmark algorithms for regression.,
In this dataset N = 506 and D = 14.,
"– Diabetes [72]: Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of N = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.",
"– Prostatic Cancer [73]: The study examined the correlation between the level of prostate-specific antigen (PSA) and several clinical measures, in N = 97 men who were about to receive radical prostatectomy.",
The goal is to predict the log of PSA (lpsa) from given measurements of D = 4 features.,
"28 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker – Liver [53]: It is a regression problem where the first 5 variables are all blood tests that are thought to be sensitive to liver disorders that might arise from excessive alcohol consumption.",
Each line in the dataset constitutes the record of a single male individual.,
"There are D = 5 features, N = 345 data points.",
"– Plasma [57]: A cross-sectional study has been designed to investigate the relationship between personal characteristics and dietary factors, and plasma concentrations of retinol, beta-carotene, and other carotenoids.",
"Study subjects (N = 315) were patients who had an elective surgical procedure during a three-year period to biopsy or remove a lesion of the lung, colon, breast, skin, ovary, or uterus that was found to be non-cancerous.",
"– Cloud [24]: The data sets we propose to analyze are constituted of N = 1024 vectors, each vector includes D = 10 parameters.",
"Each image is divided into super-pixels 16*16 and in each super-pixel, we compute a set of parameters for the visible (mean, max, min, mean distribution, contrast, entropy, second angular momentum) and IR (mean, max, min).",
– DTMB-5415 [25]: The DTMB-5415 datasets come from a real-world naval hydrodynamics problem.,
The 21 input variables represent the design variables responsible for the shape modification of the hull while the output variable represents the corresponding total resistance coefficient of the simulated hull through a potential flow simulator.,
We propose two versions of the same problem: DTMB-54151 where all the 21 design variables are related to the output variable and DTMB-54152 where 5 of the 21 design variables are not related to the output so that in this manner we can evaluate the models also from a feature selection perspective.,
– Body Fat [65]: Estimates of the percentage of body fat are determined by underwater weighing and various body circumference measurements for N = 252 men and D = 14 different input features.,
"7.2 Numerical Set-up In this section, we present the numerical details of our experiment.",
"We evaluate the performance of the GRBFNN model for feature selection with unsupervised (GRBFNNk) and supervised (GRBFNNc) center selection, as defined in Eq.11 and Eq.",
"13, respectively.",
"To compute the centers for GRBFNNk, we use the popular k-means clustering algorithm.",
Both GRBFNNc and GRBFNNk were optimized using Adam [43] for a maximum of 10000 epochs.,
We implemented the GRBFNN in Pytorch [62].,
We perform a grid search to approximately find the best set of hyperparameters of all the models considered in these numerical experiments.,
"For the GRBFNN the grid search is composed as follows: – Number of centers: For regression problems, the number of centers M can take the following values M ∈{8, 32, 128}, where for classification M ∈{2, 4, 8, 16, 32}.",
"– Regularizers: (λw, λu) ∈{0, 10−3, 10−2, 10−1, 1, 101, 102, 103}.",
"– Adam learning rate: α ∈{10−3, 10−2}.",
"For the SVM model, we used a Gaussian kernel, and the grid search is composed as follows: – Gaussian kernel width: σ2 ∈{10−3, 10−2, 10−1, 1, 101, 102, 103}.",
"– Regularizer: C ∈{0, 10−3, 10−2, 10−1, 1, 101, 102, 103}.",
"For the RF model, the grid search is composed as follows: – Depth of the tree: dt ∈{2, 4, 8, 16, 32, 64, 128}.",
"– Minimum number of samples required to be a leaf node: st ∈{1, 5, 10, 20}.",
"– Number of decision trees: nt ∈{10, 20, 50, 100, 200, 400, 800}.",
"For the XGB model, the grid search is composed as follows: – Learning rate: lb ∈{10−3, 10−2, 10−1, 1}.",
"29 – Number of boosting stages: nb ∈{10, 20, 50, 100, 200, 400, 800}.",
"– Maximum depth of the individual regression estimators: db ∈{2, 4, 8, 16, 32, 64, 128}.",
"For the MLP the grid search is composed as follows: – Regularizer (L2-norm): λ ∈{0, 10−3, 10−2, 10−1, 1, 101, 102, 103} – Network architecture: A two-hidden-layer architecture with the following combinations of the number of neurons in the two layers is considered {(D, ⌈D/2⌉)), (2D, D), (2D, ⌈D/2⌉)} with rectifiers activation functions [30].",
"– Adam learning rate: α ∈{10−3, 10−2}.",
For the DFS we use the same grid search hyperparameters as in the MLP case.,
"This is because the DFS is the same as an MLP but with an additional sparse one-to-one layer added between the input and the first hidden layer, where each input feature is weighted.",
We use the implementation available on the following link 2.,
"For the FIDL, we use the author’s implementation of the algorithm available at the following link 3 and the following grid search hyperparameters: – Network architecture: A two-hidden-layer architecture with the following combinations of the number of neurons in the two layers is considered {(D, ⌈D/2⌉)), (2D, D), (2D, ⌈D/2⌉)}.",
– Number of important features: s = ⌈D/2⌉.,
"Always referring to FIDL, for datasets that are used also in their paper, we use the optimal set of hyperparameters found by them.",
"In this method, the user has to choose in advance, and before training the model, the number of important features s that the problem might have.",
We fix this parameter to s = ⌈D/2⌉as used in their paper for some datasets.,
We fix all the other hyperparameters to their default values provided by the authors.,
"The FT-T model has several hyperparameters together with the AdamW [49] used for the optimization: – Number of blocks: {2, 3, 4} – Embedding size of each feature: {64, 128} – Number of attention heads: {4, 8} – Attention drop-out: {0.1, 0.3} – Hidden representation size: 4/3 * Embedding size – Hidden representation drop-out: {0.1, 0.3} – AdamW learning rate: {10−4, 50−5} – AdamW weight decay:{10−5, 10−6} We evaluated the model picking 20 random combinations of the aforementioned hyperparameters.",
We used the author’s implementation of the FT-T model freely available at the following link 4.,
"For the SVM, MLP, and RF, we used the Python package [64] while for XGB [17].",
"2 https://github.com/cyustcer/Deep-Feature-Selection 3 https://github.com/maksym33/FeatureImportanceDL 4 https://github.com/yandex-research/rtdl-revisiting-models 30 Danny D’Agostino , Ilija Ilievski, Christine Annette Shoemaker Digits Iris Breast Cancer Wine Training None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy Test None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy Australian Credit-g Glass Blood Training None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy Test None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy Heart Disease Vowel Delhi Weather Boston Housing Training None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw 0.0 1.0 RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Test None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High Accuracy None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw 0.0 1.0 RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Diabetes Prostatic Cancer Liver Plasma Training None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Test None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Cloud DTMB- 5415(1) DTMB- 5415(2) Body Fat Training None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Test None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE None 10−3 10−2 10−1 100 101 102 103 λu None 10−3 10−2 10−1 100 101 102 103 λw Low High RMSE Fig.",
13: GRBFNN’s regularization behavior: dark color for lower RMSE (regression) and lighter for higher accuracy (classification).,
The red frame indicates the best regularization combination.,
31 Digits Iris Breast cancer Wine Active subspace z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Eigenvalues decay 0 8 16243240485664 k 0.0 0.5 1.0 γk PK k γk 1 2 3 4 k 0.0 0.5 1.0 γk PK k γk 0 4 8 12 16 20 24 28 k 0.0 0.5 1.0 γk PK k γk 2 4 6 8 10 12 k 0.0 0.2 0.5 γk PK k γk Australian Credit-g Glass Blood Active subspace z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Eigenvalues decay 2 4 6 8 10 12 14 k 0.0 0.2 γk PK k γk 3 6 9 12 15 18 k 0.0 0.2 0.4 γk PK k γk 1 2 3 4 5 6 7 8 9 k 0.0 0.2 0.5 γk PK k γk 1 2 3 4 k 0.0 0.5 γk PK k γk Heart Disease Vowel Delhi Weather Boston Housing Active subspace z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Eigenvalues decay 2 4 6 8 10 12 k 0.0 0.5 1.0 γk PK k γk 2 4 6 8 10 12 k 0.0 0.2 γk PK k γk 1 2 3 4 5 6 7 k 0.0 0.5 γk PK k γk 2 4 6 8 10 12 k 0.0 0.2 γk PK k γk Diabetes Prostate cancer Liver Plasma Active subspace z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Eigenvalues decay 1 2 3 4 5 6 7 8 9 10 k 0.0 0.5 1.0 γk PK k γk 1 2 3 4 5 6 7 8 k 0.0 0.5 1.0 γk PK k γk 1 2 3 4 5 k 0.0 0.2 0.5 γk PK k γk 2 4 6 8 10 12 k 0.0 0.5 1.0 γk PK k γk Cloud DTMB- 5415(1) DTMB- 5415(2) Body Fat Active subspace z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) z1 z2 0.0 0.2 0.4 0.6 0.8 1.0 f(x) Eigenvalues decay 1 2 3 4 5 k 0.0 0.5 1.0 γk PK k γk 0 3 6 9 12 15 18 21 k 0.0 0.5 1.0 γk PK k γk 0 3 6 9 12 15 18 21 k 0.0 0.5 1.0 γk PK k γk 2 4 6 8 10 12 14 k 0.0 0.5 γk PK k γk Fig.,
14: Graphical interpretation of the active subspace in two dimensions in the contour plots and corresponding eigenvalues decay.,
Function values are normalized.,
"Hierarchical Attentional Hybrid Neural Networks for Document Classiﬁcation Jader Abreu⋆, Luis Fred⋆, David Macˆedo, and Cleber Zanchettin Centro de Inform´atica Universidade Federal de Pernambuco 50.740-560, Recife, PE, Brazil {jaoa,lfgs,dlm,cz}@cin.ufpe.br Abstract.",
Document classiﬁcation is a challenging task with important applications.,
The deep learning approaches to the problem have gained much attention recently.,
"Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architec- ture eﬃciently and not take into account the contexting importance of words and sentences.",
"In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classiﬁcation tasks.",
"The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical rep- resentation.",
The proposed method in this paper improves the results of the current attention-based approaches.,
Keywords: Text classiﬁcation · Attention mechanisms · Document clas- siﬁcation · Convolutional Neural Networks.,
1 Introduction Text classiﬁcation is one of the most classical and important tasks in the ma- chine learning ﬁeld.,
"The document classiﬁcation, which is essential to organize documents for retrieval, analysis, and curation, is traditionally performed by classiﬁers such as Support Vector Machines or Random Forests.",
"As in diﬀerent areas, the deep learning methods are presenting a performance quite superior to traditional approaches in this ﬁeld [5].",
Deep learning is also playing a central role in Natural Language Processing (NLP) through learned word vector repre- sentations.,
"It aims to represent words in terms of ﬁxed-length, continuous and dense feature vectors, capturing semantic word relations: similar words are close to each other in the vector space.",
"In most NLP tasks for document classiﬁcation, the proposed models do not incorporate the knowledge of the document structure in the architecture eﬃ- ciently and not take into account the contexting importance of words and sen- tences.",
Much of these approaches do not select qualitative or informative words ⋆Authors contributed equally and are both ﬁrst authors.,
"arXiv:1901.06610v2 [cs.CL] 28 Jun 2019 2 Abreu, J., Fred, L., et al.",
and sentences since some words are more informative than others in a docu- ment.,
"Moreover, these models are frequently based on recurrent neural networks only [6].",
"Since CNN has leveraged strong performance on deep learning models by extracting more abundant features and reducing the number of parameters, we guess it not only improves computational performance but also yields better generalization on neural models for document classiﬁcation.",
A recent trend in NLP is to use attentional mechanisms to modeling informa- tion dependencies without regard to their distance between words in the input sequences.,
"In [6] is proposed a hierarchical neural architecture for document classiﬁcation, which employs attentional mechanisms, trying to mirror the hier- archical structure of the document.",
The intuition underlying the model is that not all parts of a text are equally relevant to represent it.,
"Further, determining the relevant sections involves modeling the interactions and importance among the words and not just their presence in the text.",
"In this paper, we propose a new approach for document classiﬁcation based on CNN, GRU [4] hidden units and attentional mechanisms to improve the model performance by selectively focusing the network on essential parts of the text sentences during the model training.",
"Inspired by [6], we have used the hi- erarchical concept to better representation of document structure.",
We call our model as Hierarchical Attentional Hybrid Neural Networks (HAHNN).,
"We also used temporal convolutions [2], which give us more ﬂexible receptive ﬁeld sizes.",
We evaluate the proposed approach comparing its results with state-of-the-art models and the model shows an improved accuracy.,
"2 Hierarchical Attentional Hybrid Neural Networks The HAHNN model combines convolutional layers, Gated Recurrent Units, and attention mechanisms.",
Figure 1 shows the proposed architecture.,
The ﬁrst layer of HAHNN is a pre-processed word embedding layer (black circles in the Fig- ure 1).,
The second layer contains a stack of CNN layers that consist of con- volutional layers with multiple ﬁlters (varying window sizes) and feature maps.,
We also have performed some trials with temporal convolutional layers with di- lated convolutions and gotten promising results.,
"Besides, we used Dropout for regularization.",
"In the next layers, we use a word encoder applying the attention mechanism on word level context vector.",
"In sequence, a sentence encoder apply- ing the attention on sentence-level context vector.",
The last layer uses a Softmax function to generate the output probability distribution over the classes.,
"We use CNN to extract more meaningful, generalizable and abstract features by the hierarchical representation.",
Combining convolutional layers in diﬀerent ﬁlter sizes with both word and sentence encoder in a hierarchical architecture let our model extract more rich features and improves generalization performance in document classiﬁcation.,
"To obtain representations of more rare words, by taking into account subwords information, we used FastText [3] in the word embedding initialization.",
We investigate two variants of the proposed architecture.,
"There is a basic ver- sion, as described in Figure 1, and there is another which implements a TCN [2] Title Suppressed Due to Excessive Length 3 layer.",
The goal is to simulate RNNs with very long memory size by adopting a combination of dilated and regular convolutions with residual connections.,
Di- lated convolutions are considered beneﬁcial in longer sequences as they enable an exponentially larger receptive ﬁeld in convolutional layers.,
"More formally, for a 1-D sequence input x ∈Rn and a ﬁlter f : {0, ..., k −1} →R, the dilated convolution operation F on element s of the sequence is deﬁned as F(s) = (x ∗d f)(s) = k−1 X i=o f(i) · xs−d·i (1) where d is the dilatation factor, k is the ﬁlter size, and s −d · i accounts for the past information direction.",
Dilation is thus equivalent to introducing a ﬁxed step between every two adjacent ﬁlter maps.,
"When d = 1, a dilated convolution reduces to a regular convolution.",
"The use of larger dilation enables an output at the top level to represent a wider range of inputs, expanding the receptive ﬁeld.",
1: Our HAHNN Architecture include an CNN layer after the embedding layer.,
"In addition, we have created a variant which includes a temporal convo- lutional layer [2] after the embedding layer.",
The proposed model takes into account that the diﬀerent parts of a docu- ment have no similar relevant information.,
"Moreover, determining the relevant sections involves modeling the interactions among the words, not just their iso- lated presence in the text.",
"Therefore, to consider this aspect, the model includes two levels of attention mechanisms [1].",
"One structure at the word level and other at the sentence level, which let the model pay more or less attention to individual words and sentences when constructing the document representation.",
"4 Abreu, J., Fred, L., et al.",
The strategy consists of diﬀerent parts: 1) A word sequence encoder and a word-level attention layer; and 2) A sentence encoder and a sentence-level attention layer.,
"In the word encoder, the model uses bidirectional GRU [1] to produce annotations of words by summarizing information from both directions.",
"Therefore, it incorporates the contextual information in the annotation.",
The attention levels let the model pay more or less attention to individual words and sentences when constructing the representation of the document [6].,
"Given a sentence with words wit, t ∈[0, T] and an embedding matrix We, a bidirectional GRU contains the forward GRU−→f which reads the sentence si from wi1 to wiT and a backward GRU←−f which reads from wiT to wi1: xit = Wewit, t ∈[1, T], (2) −→ hit = −−−→ GRU(xit), t ∈[1, T], (3) ←− hit = ←−−− GRU(xit), t ∈[T, 1].",
"(4) An annotation for a given word wit is obtained by concatenating the forward hidden state and backward hidden state, i.e., hit = [−→ hit, ←− hit], which summarizes the information of the whole sentence.",
We use the attention mechanism to eval- uates words that are important to the meaning of the sentence and to aggregate the representation of those informative words into a sentence vector.,
"Speciﬁcally, uit = tanh(Wwhit + bw) (5) αit = exp(u⊤ ituw) P t exp(u⊤ ituw) (6) si = X αithit (7) The model measures the importance of a word as the similarity of uit with a word level context vector uw and learns a normalized importance weight αit through a softmax function.",
"After that, the architecture computes the sentence vector si as a weighted sum of the word annotations based on the weights.",
The word context vector uw is randomly initialized and jointly learned during the training process.,
"Given the sentence vectors si, and the document vector, the sentence atten- tion is obtained as: −→ hit = −−−→ GRU(si), i ∈[1, L], (8) ←− hit = ←−−− GRU(si), i ∈[L, 1].",
"(9) The proposed solution concatenates hi = [−→ hi, ←− hi] hi which summarizes the neighbor sentences around sentence i but still focus on sentence i.",
"To reward sentences that are relevant to correctly classify a document, the solution again use attention mechanism and introduce a sentence level context vector us using it to measure the importance of the sentences: uit = tanh(Wshi + bs) (10) αit = exp(u⊤ i us) P i exp(u⊤ i us) (11) v = X αihi (12) In the above equation, v is the document vector that summarizes all the in- formation of sentences in a document.",
"Similarly, the sentence level context vector us can be randomly initialized and jointly learned during the training process.",
The output of the sentence attention layer feeds a fully connected softmax layer.,
Title Suppressed Due to Excessive Length 5 It gives us a probability distribution over the classes.,
The proposed method is openly available in the github repository 1.,
3 Experiments and Results We evaluate the proposed model on two document classiﬁcation datasets using 90% of the data for training and the remaining 10% for tests.,
We split docu- ments into sentences and tokenize each sentence.,
The word embeddings have dimension 200 and we use Adam optimizer with a learning rate of 0.001.,
The datasets used are the IMDb Movie Reviews 2 and Yelp 2018 3.,
"The former con- tains a set of 25k highly polar movie reviews for training and 25k for testing, whereas the classiﬁcation involves detecting positive/negative reviews.",
"The lat- ter include users ratings and write reviews about stores and services on Yelp, being a dataset for multiclass classiﬁcation (ratings from 0-5 stars).",
"Yelp 2018 contains around 5M full review text data, but we ﬁx in 500k the number of used samples for computational purposes.",
Table 1: Results in classiﬁcation accuracies.,
Method Accuracy on test set Yelp 2018 (ﬁve classes) IMDb (two classes) VDNN [7] 62.14 79.47 HN-ATT [6] 72.73 89.02 CNN [5] 71.81 91.34 Our model with CNN 73.28 92.26 Our model with TCN 72.63 95.17 Table 1 shows the experiment results comparing our results with related works.,
"Note that HN-ATT [6] obtained an accuracy of 72,73% in the Yelp test set, whereas the proposed model obtained an accuracy of 73,28%.",
Our results also outperformed CNN [6] and VDNN [7].,
We can see an improvement of the results in Yelp with our approach using CNN and varying window sizes in ﬁlters.,
The model also performs better in the results with IMDb using both CNN and TCN.,
"3.1 Attention Weights Visualizations To validate the model performance in select informative words and sentences, we present the visualizations of attention weights in Figure 2.",
There is an example of the attention visualizations for a positive and negative class in test reviews.,
Every line is a sentence.,
"Blue color denotes the sentence weight, and red denotes the word weight in determining the sentence meaning.",
There is a greater focus on more important features despite some exceptions.,
"For example, the word “loving” and “amazed” in Figure 2 (a) and “disappointment” in Figure 2 (b).",
"1 https://github.com/luisfredgs/cnn-hierarchical-network-for-document-classiﬁcation 2 http://ai.stanford.edu/ amaas/data/sentiment/ 3 https://www.yelp.com/dataset/challenge 6 Abreu, J., Fred, L., et al.",
(a) A positive example of visualization of a strong word in the sentence.,
(b) A negative example of visualization of a strong word in the sentence.,
"2: Visualization of attention weights computed by the proposed model Occasionally, we have found issues in some sentences, where fewer important words are getting higher importance.",
"For example, in Figure 2 (a) notes that the word “translate” has received high importance even though it represents a neutral word.",
These drawbacks will be taken into account in future works.,
"4 Final Remarks In this paper, we have presented the HAHNN architecture for document classi- ﬁcation.",
The method combines CNN with attention mechanisms in both word and sentence level.,
HAHNN improves accuracy in document classiﬁcation by incorporate the document structure in the model and employing CNN’s for the extraction of more abundant features.,
References 1.,
"BAHDANAU, Dzmitry; CHO, Kyunghyun; BENGIO, Yoshua.",
Neural ma- chine translation by jointly learning to align and translate.,
"arXiv preprint arXiv:1409.0473, 2014.",
"BAI, Shaojie; KOLTER, J. Zico; KOLTUN, Vladlen.",
An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.,
"arXiv preprint arXiv:1803.01271, 2018.",
"BOJANOWSKI, Piotr et al.",
Enriching word vectors with subword information.,
"arXiv preprint arXiv:1607.04606, 2016.",
"CHO, Kyunghyun et al.",
Learning phrase representations using RNN encoder- decoder for statistical machine translation.,
"arXiv preprint arXiv:1406.1078, 2014.",
Convolutional neural networks for sentence classiﬁcation.,
"arXiv preprint arXiv:1408.5882, 2014.",
"YANG, Zichao et al.",
Hierarchical attention networks for document classiﬁcation.,
Chapter of the Assoc.,
"2016. p.1480-1489, San Diego, CA, USA.",
"Conneau, Alexis, et al.",
”Very deep convolutional networks for text classiﬁcation.” arXiv preprint arXiv:1606.01781 (2016).,
"A Neural Network-Evolutionary Computational Framework for Remaining Useful Life Estimation of Mechanical Systems David Laredo1, Zhaoyin Chen1, Oliver Sch¨utze2 and Jian-Qiao Sun1 1Department of Mechanical Engineering School of Engineering, University of California Merced, CA 95343, USA 2Department of Computer Science, CINVESTAV Mexico City, Mexico Corresponding author.",
Email: davidlaredo1@gmail.com Abstract This paper presents a framework for estimating the remaining useful life (RUL) of mechanical systems.,
The framework consists of a multi-layer perceptron and an evolutionary algorithm for optimizing the data-related parameters.,
The framework makes use of a strided time window to estimate the RUL for mechanical components.,
Tuning the data-related parameters can become a very time consuming task.,
The framework presented here automatically reshapes the data such that the eﬃciency of the model is increased.,
"Furthermore, the complexity of the model is kept low, e.g.",
neural networks with few hidden layers and few neurons at each layer.,
Having simple models has several advantages like short training times and the capacity of being in environments with limited computational resources such as embedded systems.,
"The proposed method is evaluated on the publicly available C-MAPSS dataset [1], its accuracy is compared against other state-of-the art methods for the same dataset.",
"Keywords: artiﬁcial neural networks, moving time window, RUL estimation, prognostics, evolutionary algorithms 1.",
"Introduction Traditionally, maintenance of mechanical systems has been carried out based on scheduling strategies.",
"Such strategies are often costly and less capable of meeting the increasing demand of eﬃciency and reliability [2, 3].",
"Condition based maintenance (CBM) also known as intelligent prognostics and health management (PHM) allows for maintenance based on the current health of the system, thus cutting down the Preprint submitted to Elsevier May 16, 2019 arXiv:1905.05918v1 [cs.LG] 15 May 2019 costs and increasing the reliability of the system [4].",
"Here, we refer to prognostics as the estimation of remaining useful life of a system.",
The remaining useful life (RUL) of the system can be estimated based on the historical data.,
This data-driven approach can help optimize maintenance schedules to avoid engineering failures and to save costs [5].,
"The existing PHM methods can be grouped into three diﬀerent categories: model- based [6], data-driven [7, 8] and hybrid approaches [9, 10].",
Model-based approaches attempt to incorporate physical models of the system into the estimation of the RUL.,
"If the system degradation is modeled precisely, model-based approaches usually exhibit better performance than data-driven approaches [11].",
"This comes at the expense of having extensive a priori knowledge of the underlying system and having a ﬁne-grained model of the system, which can involve expensive computations.",
"On the other hand, data-driven approaches use pattern recognition to detect changes in system states.",
"Data-driven approaches are appropriate when the understanding of the ﬁrst principles of the system dynamics is not comprehensive or when the system is suﬃciently complex such as jet engines, car engines and complex machineries, for which it is prohibitively diﬃcult to develop an accurate model.",
Common disadvantages for the data-driven approaches are that they usually ex- hibit wider conﬁdence intervals than model-based approaches and that a fair amount of data is required for training.,
Many data-driven algorithms have been proposed.,
Good prognostics results have been achieved.,
"Among the most popular algorithms we can ﬁnd artiﬁcial neural networks (ANNs) [2], support vector machine (SVM) [12], Markov hidden chains (MHC) [13] and so on.",
"Over the past few years, data- driven approaches have gained more attention in the PHM community.",
"A number of machine learning techniques, especially neural networks, have been applied success- fully to estimate the RUL of diverse mechanical systems.",
"ANNs have demonstrated good performance in modeling highly nonlinear, complex, multi-dimensional systems without any prior knowledge on the system behavior [14].",
"While the conﬁdence lim- its for the RUL predictions cannot be analytically provided [15], the neural network approaches are promising for prognostic problems.",
Neural networks for estimating the RUL of jet engines have been previously ex- plored in [16] where the authors propose a multi-layer perceptron (MLP) coupled with a feature extraction (FE) method and a time window for the generation of the features for the MLP.,
"In the publication, the authors demonstrate that a moving window combined with a suitable feature extractor can improve the RUL prediction as compared with the studies with other similar methods in the literature.",
"In [14], the authors explore a deep learning ANN architecture, the so-called convolutional neural networks (CNNs), where they demonstrate that by using a CNN without any 2 pooling layers coupled with a time window, the predicted RUL is further improved.",
In this paper we propose a novel framework for estimating the RUL of complex mechanical systems.,
"The framework consists of a MLP to estimate the RUL of the system, coupled with an evolutionary algorithm for the ﬁne tuning of data-related parameters.",
"In this paper we refer to data-related parameters as parameters that deﬁne the shape, deﬁned in terms of window size and window stride, and quality of the data, measured with respect to some performance indicators, used by the MLP.",
"Please note that while this speciﬁc framework makes use of a MLP, the framework can in principle use several other learning algorithms, our main objective is the treatment of the data instead of the choice of a particular learning algorithm.",
"The publicly available NASA C-MAPSS dataset [1] is used to assess the eﬃciency and reliability of the proposed framework, by eﬃciency we mean the complexity of the used regressor and reliability refers to the accuracy of the predictions made by the regressor.",
This approach allows for a simple and small MLP to obtain better results than those reported in the current literature while using less computing power.,
The remainder of this paper is organized as follows.,
The C-MAPSS dataset is presented in Section 2.,
The framework and its components are thoroughly reviewed in Section 3.,
The method is evaluated using the C-MAPSS dataset in Section 4.,
A comparison with the state-of-the-art is also provided.,
"Finally, the conclusions are presented in Section 5.",
NASA C-MAPSS Dataset The NASA C-MAPSS dataset is used to evaluate performance of the proposed method [1].,
The C-MAPSS dataset contains simulated data produced using a model based simulation program developed by NASA.,
The dataset is further divided into 4 subsets composed of multi-variate temporal data obtained from 21 sensors.,
"For each of the 4 subsets, a training and a test set are provided.",
The training sets include run-to-failure sensor records of multiple aero-engines collected under diﬀerent operational conditions and fault modes as described in Table 1.,
3 C-MAPSS Dataset FD001 FD002 FD003 FD004 Training Trajectories 100 260 100 248 Test Trajectories 100 259 100 248 Operating Conditions 1 6 1 6 Fault Modes 1 1 2 2 Table 1: C-MAPSS dataset details.,
The data is arranged in an N × 26 matrix where N is the number of data points in each subset.,
"The ﬁrst two variables represent the engine and cycle numbers, respectively.",
The following three variables are operational settings which correspond to the conditions in Table 1 and have a substantial eﬀect on the engine performance.,
The remaining variables represent the 21 sensor readings that contain the information about the engine degradation over time.,
Each trajectory within the training and test sets represents the life cycles of the engine.,
"Each engine is simulated with diﬀerent initial health conditions, i.e.",
no initial faults.,
For each trajectory of an engine the last data entry corresponds to the cycle at which the engine is found faulty.,
"On the other hand, the trajectories of the test sets terminate at some point prior to failure, hence the need to predict the remaining useful life.",
The aim of the MLP model is to predict the RUL of each engine in the test set.,
The actual RUL values of test trajectories are also included in the dataset for veriﬁcation.,
Further discussions of the dataset and details on how the data is generated can be found in [17].,
"Performance Metrics To evaluate the performance of the proposed approach on the C-MAPSS dataset, we make use of two scoring indicators, namely the Root Mean Squared Error (RMSE) denoted as ERMS(e) and a score proposed in [17] which we refer as the RUL Health Score (RHS) denoted as ERH(e).",
"The two scores are deﬁned as follows, ERMS = v u u t1 n N X i=1 e2 i (1) 4 ERH = 1 n N X i=1 si si = ( exp(−ei 13) −1, ei < 0 exp( ei 10) −1, ei ≥0, (2) where n is the total number of samples in the test set and e = ˆy −y is the error between the estimated RUL values ˆy, and the actual RUL values y for each engine within the test set.",
It is important to note that ERH(e) penalizes late predic- tions more than early predictions since usually late predictions lead to more severe consequences in ﬁelds such as aerospace.,
"Framework Description n this section, the proposed ANN-EA based method for prognostics is presented.",
The method makes use of a multi-layer perceptron (MLP) as the main regressor for estimating the RUL of the engines in the C-MAPSS dataset.,
"The choice of a MLP as the learning algorithm instead of any of the other choices (SVM, RNN, CNN, Least-Squares, etc) obeys to the fact that MLPs are in general good for nonlinear data like the one exhibited by the C-MAPSS dataset, but at the same time are less computationally expensive than some of the more sophisticated algorithms as the CNN or the RNN.",
"Indeed, the RNN may be a more suitable choice for this particular problem since it involves time-sequenced data, nevertheless, we will show that by doing a ﬁne tuning of the data-related parameters (and thus data processing), the inference power of a simple MLP can be competitive even when compared against that of an RNN.",
"For the training sets, the feature vectors are generated by using a moving time window while a label vector is generated with the RUL of the engine.",
"The label has a constant RUL for the early cycles of the simulation, and becomes a linearly decreasing function of the cycle in the remaining cycles.",
This is the so-called piece-wise linear degradation model [18].,
"For the test set, a time window is taken from the last sensor readings of the engine.",
The data of the test set is used to predict the RUL of the engine.,
"The window-size nw, window-stride ns, and early-RUL Re are data-related pa- rameters, which for the sake of clarity and formalism in this study, form a vector v ∈Z3 such that v = (nw, ns, Re).",
The vector v has a considerable impact on the quality of the predictions by the regressor.,
It is computationally intensive to ﬁnd the best parameters of v given the search space inherent to these parameters.,
We 5 propose an evolutionary algorithm to optimize the data-related parameters v. The optimized parameter set v allows the use of a simple neural network architecture while attaining better results in terms of the quality of the predictions compared with the results by other methods in the literature.,
"The Network Architecture After careful examinations of the C-MAPSS dataset, we propose to use a rather simple MLP architecture for all the four subsets of the data.",
"The choice of a simple architecture over a more complex one follows the fact that simpler neural networks are less computationally expensive to train, furthermore, the inference is done also faster since less operations are involved.",
"To measure the simplicity/complexity of a neural network we use the number of trainable parameters (weights) of the neural network, usually the more trainable parameters in a network the more computations that need be done, thus increasing the computational burden of the training/inference process.",
The implementations are done in Python using the Keras/Tensorﬂow environment.,
The source code is publicly available at the git repository https://github.com/ dlaredo/NASA_RUL_-CMAPS- [19].,
"The choice of the network architecture is made by following an iterative process, our goal was to ﬁnd a good compromise between eﬃciency (simple neural network models) and reliability (scores obtained by the model using the RMSE metric): We compared 6 diﬀerent architectures (see Appendix Appendix A), training each for 100 epochs using a mini-batch size of 512 and averaging their results on a cross-validation set for 10 diﬀerent runs.",
L1 (Lasso) and L2 regularization (Ridge) [20] are used to prevent over-ﬁtting.,
"L1 regularization penalizes the sum of the absolute value of the weights and biases of the networks, while L2 regularization penalizes the sum of the squared value of the weights and biases.",
"The data-related parameters v used for this experiment are v = (30, 1, 140).",
Two objectives are pursued during the iterations: the architecture must be minimal in terms of the number of trainable parameters and the performance indicators must be minimized.,
Table 2 summarizes the results for each tested architecture.,
6 RMSE RHS Tested Architecture Min.,
"STD Architecture 1 15.51 17.15 16.22 0.49 4.60 7.66 5.98 0.91 Architecture 2 15.24 16.46 15.87 0.47 4.07 6.26 5.29 0.82 Architecture 3 15.77 17.27 16.15 0.45 5.11 8.25 5.93 0.94 Architecture 4 15.13 17.01 15.97 0.47 3.90 7.54 5.65 1.2 Architecture 5 16.39 17.14 16.81 0.23 5.19 6.58 5.98 0.42 Architecture 6 16.42 17.36 16.87 0.30 5.15 7.09 6.12 0.62 Table 2: Results for diﬀerent architectures for subset 1, 100 epochs.",
Table 3 presents the architecture chosen for the remainder of this work (which provides the best compromise between compactness and performance among the tested architectures).,
"Each row in the table represents a neural network layer while each column describes each one of the key parameters of the layer such as the type of layer, number of neurons in the layer, activation function of the layer and whether regularization is used, where L1 denotes the L1 regularization factor and L2 denotes the L2 regularization factor, the order in which the layers are appended from the table is top-bottom.",
From here on we refer to this neural network model as φ(.).,
"Layer Neurons Activation Additional Information Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 Table 3: Proposed neural network architecture φ(.).",
Shaping the Data This section covers the data pre-processing applied to the raw sensor readings in each of the datasets.,
"Although the original datasets contain 21 diﬀerent sen- sor readings, some of the sensors do not present much variance or convey redun- dant information, our choice of the sensors is based on previous studies such as [16, 14] where it was discovered that some sensor values do not vary at all through- out the entire engine life cycle while some others are redundant according to PCA or clustering analysis.",
These sensors are therefore discarded.,
"In the end, only 14 sensor readings out of the 21 are considered for this study.",
"Their indices are {2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21}.",
The raw measurements are then used to 7 create the strided time windows with window-size nw and window-stride ns.,
"For the training labels, Re is used at the early stages and then the RUL is linearly decreased.",
"Assuming x ∈Rm is the vector whose components are the sensor readings at each time stamp, then the min-max normalized vector ˆx can be computed by means of the following formula: ˆxi = 2 ∗ xi −min(xi) max(xi) −min(xi) −1.",
"Time Window and Stride In multivariate time-series problems such as RUL, more information can be gen- erally obtained from the temporal sequence of the data as compared with the multi- variate data point at a single time stamp.",
"For a time window of size nw with a stride ns = 1, all the sensor readings in the time window form a feature vector x ∈Rs∗nw, where s denotes the number of sensors being read.",
Stacking together mw of this time windows forms feature vector X ∈Rmw×s∗nw while its corresponding RUL values are deﬁned as y ∈Zm.,
"It is important to mention that the shape of X deﬁnes the number of input neurons for the neural network, therefore changing the shape of X eﬀectively changes the number of inputs to the neural network.",
"This approach has successfully been tested in [14, 16] where the authors propose the use of a moving window with sizes ranging from 20 to 30.",
"We propose not only the use of a moving time window, but also a strided time window that updates more than one element (ns > 1) at the time.",
A graphical depiction of the strided time window is shown in Figure 1.,
"For Figure 1 the numbers and time-stamps are just illustrative, the window size exempliﬁed is of 30 time-stamps while the stride is of 3 time-stamps.",
Figure 1: Graphical depiction of the time window used in this framework.,
"Figure 2 shows an example of how to form a sample vector, in this example s = 14, nw = 5 and ns = 4.",
Each one of the plotted lines denotes the readings for 8 each of the fourteen chosen sensors.,
"The dashed vertical lines (black lines) represent the size of the window, in this case we depict a window size of 5 cycles.",
"For the next window (red dashed lines) the time window is advanced by a stride of 4 cycles, note that some sensor readings may be overlapped for diﬀerent moving windows.",
"For every window, the sensor readings are appended one after another to form a vector of 14 ∗5 features, for this speciﬁc case the unrolled vector will be of 70 features.",
Figure 2: Window size and stride example.,
"The use of a strided time window allows for the regressor to take advantage not only of the previous information, but also to control the ratio at which the algorithm is fed with new information.",
"With the usual time window approach, only one point is updated for every new time window.",
The strided time window considered in this study allows for updating more than one point at the time for the algorithm to make use of the new information with less iterations.,
Our choice of the strided time window is inspired by the use of strided convolutions in Convolutional Neural Networks [21].,
Further studies of the impact of the stride on the prediction should be done in the future.,
"Piecewise Linear Degradation Model Diﬀerent from common regression problems, the desired output value of the input data is diﬃcult to determine for a RUL problem.",
It is usually impossible to evaluate the precise health condition and estimate the RUL of the system at each time step without an accurate physics based model.,
"For this popular dataset, a piece-wise linear degradation model has been proposed in [18].",
"The model assumes that the engines have a constant RUL label in the early cycles, and then the RUL starts degrading linearly until it reaches 0 as shown in Figure 3.",
The piece-wise linear degradation assumption is used in this work.,
We denote the value of the RUL in the early cycles as Re.,
"Initially, Re is randomly chosen between 90 and 140 cycles which is a reasonable range of values for this particular application.",
"When the diﬀerence between the cycle count in the time window and the terminating cycle of the training data is less than the initial value of Re, Re begins the linear descent toward the terminating cycle.",
Figure 3: Piece-wise linear degradation for RUL.,
Optimal Data Parameters As mentioned in the previous sections the choice of the data-related parameters v has a large impact on the performance of the regressor.,
"In this section, we present a method for picking the optimal combination of the data-related parameters nw, ns and Re while being computationally eﬃcient.",
"10 Vector v = (nw, ns, Re) components speciﬁc to the C-MAPSS dataset are bounded such that nw ∈[1, b], ns ∈[1, 10], and Re ∈[90, 140], where all the variables are in- teger.",
"The value of b is diﬀerent for diﬀerent subsets of the data, Table 4 shows the diﬀerent values of b for each subset.",
FD001 FD002 FD003 FD004 b 30 20 30 18 Table 4: Allowed values for b per subset.,
Let X(v) be the training/cross-validate/test sets parametrized by v and ˆy(v) = φ(X(v); θ) be the predicted RUL values of our model φ(.),
"Thus, ERMS depends directly on the choice of v since φ(.)",
is ﬁxed (See Section 2.1).,
Here we propose to solve the following optimization problem min v∈Z3 ERMS(v).,
(4) The problem to ﬁnd optimal data-related parameters has no analytic descriptions.,
"Therefore, no gradient information is available.",
An evolutionary algorithm is the natural choice for this optimization problem.,
"Nevertheless, since the computation of the error ERMS(v) requires re-training φ(.)",
an strategy to make the optimization process computationally eﬃcient must be devised.,
True Optimal Data Parameters The ﬁnite size of C-MAPSS dataset and ﬁnite search space of v allow an exhaus- tive search to be performed in order to ﬁnd the true optimal data-related parameters.,
"We would like to emphasize that although exhaustive search is possible for the C- MAPSS dataset, it is in no way a possibility in a more general setting.",
"Nevertheless, the possibility to perform exhaustive search on the C-MAPSS dataset can be ex- ploited to demonstrate the accuracy of the chosen EA and of the framework overall.",
"In the following studies, we use the results and computational eﬀorts of the exhaus- tive search as benchmarks to examine the accuracy and eﬃciency of the proposed approach.",
We should note that the subsets of the data FD001 and FD003 have similar features and that the subsets FD002 and FD004 have similar features.,
"Because of this, we have decided to just optimize the data-related parameters by considering the subsets FD001 and FD002 only.",
An exhaustive search is performed to ﬁnd the true optimal values for v. The MLP is only trained for 20 epochs.,
Table 5 shows the 11 optimal as well as the worst combinations of data-related parameters and the total number of function evaluations used by the exhaustive search.,
It is important to notice that for this experiment the window size is limited to be larger than or equal to 15.,
Dataset argmin v min ERMS(v) argmax v max ERMS(v) Function evals.,
"FD001 [24, 1, 127] 15.11 [25, 10, 94] 85.19 8160 FD002 [16, 1, 138] 30.93 [17, 10, 99] 59.78 3060 Table 5: Exhaustive search results for subsets FD001 and F002.",
"Numerical experiments seem to suggest that, at least for CMAPSS dataset, the window size plays a big role in terms of the meaningful information used for prediction by the MLP.",
It also reﬂects the history-dependent nature of the aircraft engine degradation process.,
"Furthermore, overlapping in the generated time windows seems to beneﬁt the generated sequences of sensors.",
Evolutionary Algorithm for Optimal Data Parameters Evolutionary algorithms (EAs) are a family of methods for optimization problems.,
"The methods do not make any assumptions about the problem, treating it as a black box that merely provides a measure of quality given a candidate solution.",
"Furthermore, EAs do not require the gradient when searching for optimal solutions, making them very suitable for applications such as neural networks.",
"For the current application, the diﬀerential evolution (DE) method is chosen as the optimization algorithm [22].",
"Though other meta-heuristic algorithms may also be suitable for this application, the DE has established as one of the most reliable, robust and easy to use EAs.",
"Furthermore, a ready to use Python implementation is available through the scipy package [23].",
"Although the DE method does not have special operators for treating integer variables, a very simple modiﬁcation to the algorithm, i.e.",
"rounding every component of a candidate solution to its nearest integer, is used for this work.",
"As mentioned earlier, evolutionary algorithms such as the DE use several function evaluations when searching for the optimal solutions.",
"It is important to consider that, for this application, one function evaluation requires retraining the neural network from scratch.",
"This is not a desirable scenario, as obtaining the optimal data-related parameters would entail an extensive computational eﬀort.",
"Instead of running the DE for several iterations and with a large population size, we propose to run it just for 30 iterations, i.e.",
"the generations in the literature of evolutionary computation, 12 with a population size of 12, which seems reasonable given the size of the search space of v. During the optimization, the MLP is trained for only 20 epochs.",
"The small number of epochs of training the MLP is reasonable in this case because a small batch of data is used in the training, because we only look for the trend of the scoring indicators.",
"Furthermore, it is common to observe that the parameters leading to lower score values in the early stages of the training are more likely to provide better performance after more epochs of training.",
The settings of the DE algorithm to ﬁnd the optimal data-related parameters are listed in Table 6.,
Population Size Generations Strategy MLP epochs 12 30 Best1Bin [24] 20 Table 6: Diﬀerential evolution hyper-parameters.,
The optimal data-related parameters for the subsets FD001 and FD002 found by the DE algorithm are listed in Table 7.,
"As can be observed, the results are in fact very close to the true optimal ones in Table 5 for both the subsets of the data.",
The computational eﬀort is reduced by one order of magnitude when using the DE method as compared to the exhaustive search for the true optimal parameters.,
"From the results in Table 7, it can be observed that the maximum allowable time window is always preferred while, on the other hand, small window strides yield better results.",
"For the case of early RUL, it can be observed that larger values of Re are favored.",
Dataset argmin v min ERMS(v) Function evals.,
"FD001 [24, 1, 129] 15.24 372 FD002 [17, 1, 139] 30.95 372 Table 7: Data-related parameters for each subset obtained with diﬀerential evolution.",
"The Estimation Algorithm Having described the major building blocks of the proposed method, we now introduce the complete framework in the form of Algorithm 1.",
Evaluation of the Proposed Method 4.1.,
"Experimental settings In this section, we evaluate the performance of the proposed method.",
The archi- tecture of the MLP is described in Table 3.,
We deﬁne an experiment as the process 13 Algorithm 1: ANN-EA RUL estimation framework.,
"Data: Training/testing data X ∈Rmw×s∗nw Input : Initial set of data-related parameters v ∈Z3, , training labels y ∈Zm and number of training epochs for each evaluation of φ(v).",
Output: Optimal set of data-related parameters v∗∈Z3.,
"Choose regressor architecture (ANN, SVM, linear/logistic regression, etc).",
Deﬁne φ(v) as in Section 3.3.,
"Optimize φ(v), by means of an evolutionary algorithm, using the proposed guidelines from Section 3.3.2.",
Use v∗to train the regressor for as many epochs as needed.,
of training the MLP on any of the subsets (FD001 to FD004) and evaluate its per- formance using the subset’s test set.,
"The combinations of the optimal window size nw, window stride ns and early RUL Re are presented in Table 8.",
"We perform 10 diﬀerent experiments for each data subset, the MLP is trained for 200 epochs using the train set for the corresponding data subset and evaluated using the subset’s test set.",
The results for each dataset subset are averaged and presented in Table 9.,
"Fur- thermore, the best model is saved and later used to generate the results presented in Section 4.2.",
"All of the experiments were run using the Keras/Tensorﬂow framework, an NVIDIA GeForce 1080Ti GPU was used to speed up the training process.",
Dataset nw ns Re Input size (neurons) FD001 24 1 129 336 FD002 17 1 139 238 FD003 24 1 129 336 FD004 17 1 139 238 Table 8: Data-related parameters for each subset as obtained by DE.,
Experimental results The obtained results for φ(v) using the above setting are presented in Table 9.,
Notice that the performances obtained for datasets FD001 and FD002 are improved as compared with the results in Table 7.,
"This is due to the fact that the MLP is trained for more epochs, thus obtaining better results.",
14 ERMS ERH Data Subset min max avg STD min max avg STD FD001 14.24 14.57 14.39 0.11 3.25 3.58 3.37 0.11 FD002 28.90 29.23 29.09 0.11 45.99 53.90 50.69 2.17 FD003 14.74 16.18 15.42 0.50 4.36 6.85 5.33 0.95 FD004 33.25 35.10 34.74 0.53 58.52 78.62 74.77 5.88 Table 9: Scores for each dataset using the data-related parameters obtained by DE.,
We now compare the predicted RUL values versus the real RUL values for each of the datasets.,
For Figures 4 to 7 we plot on the top sub-ﬁgure the predicted RUL values (red lines) vs the real RUL values (green lines) while on the bottom sub-ﬁgure we plot the error between real RUL and predicted RUL.,
"Figure 4 shows the comparison for subset FD001, it can be observed that the predicted RUL values closely follow the real RUL values with the exception of a pair of engines.",
The error remains small for most of the engines.,
"Meanwhile, for FD002 it can be observed in Figure 5 that the regressor overshoots several of the RUL predictions, especially in the positive spectrum.",
"That is, the method predicts a RUL when in reality the real RUL is less than the predicted value.",
This is more evident in the second subplot where the maximum error is 138 at the magenta peak in the leftmost part of the plot.,
Figure 6 shows that for FD003 the predictions follow closely the real RUL values.,
"The behavior for FD004 is similar to FD002 as depicted in Figure 7, with most of the error such that the predictions of the RUL are larger than the real value.",
"15 Figure 4: Comparison of predicted RUL values vs real RUL values for dataset FD001 Figure 5: Comparison of predicted RUL values vs real RUL values for dataset FD002 16 Figure 6: Comparison of predicted RUL values vs real RUL values for dataset FD003 Figure 7: Comparison of predicted RUL values vs real RUL values for dataset FD004 17 Finally, Figure 8 shows that most of the RUL predictions by the method are highly accurate.",
"In the case of FD001, the predictions of 50% of the engines is smaller than 6 cycles.",
"In the case of FD002, the predictions are acceptable with the error of ﬁrst quartile being lower than 6 cycles and the error for 50% of the engines being less than 19 cycles.",
"The cases for FD003 and FD004 are similar to FD001 and FD002, respectively.",
Figure 8: Prediction error of the MLP for each dataset.,
Two conclusions can be drawn from the previous discussions.,
"First, it can be ob- served that the number of operating conditions has a larger impact on the complexity of the data than the number of fault modes.",
"This is because the subsets FD002 and FD004 exhibit larger errors than the subsets FD001 and FD003, although the general trend of prediction errors is similar among those two groups of data sets.",
"Second, for the subsets FD002 and FD004, most of the error is related to the predictions larger than the real RUL values.",
Another observation is that larger window sizes usually lead to better predictions.,
This may be related to the history-dependent nature of the physical problem.,
Comparison with other approaches The performance of the proposed method is also compared against other state- of-the-art methods.,
The methods chosen for the comparison obey to two criteria: 1) that the method used is a machine learning (or statistical) method and 2) that the method is recent (6 years old at most).,
Most of the methods chosen here have only reported results on the test set FD001 in terms of Erms.,
The results are shown in 18 Table 10.,
The Erms value of the proposed method in Table 10 is the mean value of 10 independent runs.,
The values of other methods are identical to those reported in their respective original papers.,
"Method Erms ESN trained by Kalman Filter [25] 63.45 Support Vector Machine Classiﬁer [26] 29.82 Time Window Neural Network [16] 15.16 Multi-objective deep belief networks ensemble [27] 15.04 Deep Convolutional Neural Network [28] 18.45 Proposed method with nw = 30, ns = 1 and Re = 128 14.39 Modiﬁed approach with classiﬁed sub-models of the ESN [25].",
7.021 Only 80 of 100 engines predicted Deep CNN with time window [14].,
13.32 RNN-Encoder-Decoder [29].,
12.93 Table 10: Performance comparison of the proposed method and the latest related papers on the C-MAPSS dataset.,
"From the comparison studies, we can conclude that the proposed method per- forms better than the majority of the chosen methods when taking into consideration the whole dataset FD001.",
"Two existing methods come close to the performance of the proposed approach here, namely the time window ANN [16] and the Networks Ensemble [27].",
"While the performance of these two methods comes close to the results of the proposed method, our method is computationally more eﬃcient.",
We believe that the use of the time window with a proper size makes the diﬀerence.,
"Notice that three of the methods presented in Table 10 perform better than the proposed method, namely, the Convolutional Neural Network [14], the RNN-Encoder-Decoder [29] and the modiﬁed ESN [25].",
"Nevertheless in the case of [25], the method can only predict 80 out of the 100 total engines while the deep CNN [14] and RNN [29] approaches are much more computationally expensive than the MLP used in this work.",
"In the case of the method proposed in [14], the neural network model has four layers of convolutions and two more layers of fully connected layers.",
"On the other hand, the RNN-Encoder-Decoder [29] makes use of a much more complicated scheme of two RNNs, one for encoding the sequences and one for decoding them.",
"While specialized libraries such as TensorFlow or Keras make RNNs easier to use, they still remain up to date as some of the most computationally expensive architec- ture to train given their sequential nature.",
"Finally, we would like to emphasize that 19 the used MLP for this approach is one of the simplest ones in the reviewed litera- ture.",
"Furthermore, the framework proposed is simple to understand and implement, robust, generic and light-weight.",
These are the features important to highlight when comparing the proposed method against other state-of-the-art approaches.,
Conclusions We have presented a novel framework for predicting the RUL of mechanical com- ponents.,
"While the method has been tested on the jet-engine dataset C-MAPSS, the method is general enough such that it can be applied to other similar systems.",
The framework makes use of a strided moving time window to generate the training and test records.,
A shallow MLP to make the predictions of the RUL has been found to be suﬃcient for the C-MAPSS dataset.,
The evolutionary algorithm DE needs to be run just once to ﬁnd the best data-related parameters that optimize the scoring functions.,
"The resulting model of the application of the framework presented in this paper demonstrated to be accurate and computationally eﬃcient, specially when ap- plied to large datasets in real applications.",
The compactness of the resulting model makes it suitable for applications that have limited computational resources such as embedded systems.,
"It is important to note that the framework itself does involve some computations, nevertheless such computations are done oﬀ-line.",
"Furthermore, the comparison with other state-of-the-art methods has shown that the proposed method is the best overall performer.",
Two major features of the proposed framework are its generality and scalabil- ity.,
"While for this study, speciﬁc regressors and evolutionary algorithms are chosen, many other combinations are possible and may be more suitable for diﬀerent applica- tions.",
"Furthermore, the framework can, in principle, be used for model-construction, i.e.",
generating the best possible neural network architecture tailored to a speciﬁc application.,
"Thus, future work will consider extensions to the framework to make it applicable to tasks such as model selection.",
An analysis of the inﬂuence of the window stride parameter ns will also be considered for future work.,
Acknowledgement The authors acknowledge the funding from Conacyt Project No.,
285599 and a grant (11572215) from the National Natural Science Foundation of China.,
"20 References References [1] A. Saxena, K. Goebel, PHM08 challenge data set, https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/ (2008).",
"[2] N. Z. Gebraeel, M. A. Lawley, R. Li, V. Parmeshwaran, Residual-life distribu- tions from component degradation signals: A neural-network approach, IEEE Transactions on Industrial Electronics 51 (3) (2005) 150–172.",
"[3] M. Zaidan, A.",
"Mills, R. Harrison, Bayesian framework for aerospace gas turbine engine prognostics, in: IEEE (Ed.",
"), Aerospace Conference, 2013, pp.",
"[4] Z. Zhao, L. Bin, X. Wang, W. Lu, Remaining useful life prediction of aircraft engine based on degradation pattern learning, Reliability Engineering & System Safety 164 (2017) 74–83.",
"[5] J. Lee, F. Wu, W. Zhao, M. Ghaﬀari, L. Liao, D. Siegel, Prognostics and health management design for rotary machinery systems - reviews, methodology and applications, Mechanical Systems and Signal Processing 42 (12) (2014) 314–334.",
"[6] W. Yu, H. Kuﬃ, A new stress-based fatigue life model for ball bearings, Tribol- ogy Transactions 44 (1) (2001) 11–18.",
"[7] J. Liu, G. Wang, A multi-state predictor with a variable input pattern for system state forecasting, Mechanical Systems and Signal Processing 23 (5) (2009) 1586– 1599.",
"[8] A. Mosallam, K. Medjaher, N. Zerhouni, Nonparametric time series modelling for industrial prognostics and health management, The International Journal of Advanced Manufacturing Technology 69 (5) (2013) 1685–1699.",
"[9] M. Pecht, Jaai, A prognostics and health management roadmap for information and electronics rich-systems, Microelectronics Reliability 50 (3) (2010) 317–323.",
"[10] J. Liu, M. Wang, Y. Yang, A data-model-fusion prognostic framework for dy- namic system state forecasting, Engineering Applications of Artiﬁcial Intelli- gence 25 (4) (2012) 814–823.",
"21 [11] Y. Qian, R. Yan, R. X. Gao, A multi-time scale approach to remaining useful life prediction in rolling bearing, Mechanical Systems and Signal Processing 83 (2017) 549–567.",
"[12] T. Benkedjouh, K. Medjaher, N. Zerhouni, S. Rechak, Remaining useful life estimation based on nonlinear feature reduction and support vector regression, Engineering Applications of Artiﬁcial Intelligence 26 (7) (2013) 1751–1760.",
"[13] M. Dong, D. He, A segmental hidden semi-markov model (HSMM)-based diag- nostics and prognostics framework and methodology, Mechanical Systems and Signal Processing 21 (5) (2007) 2248–2266.",
"[14] X. Li, Q. Ding, J.",
"Sun, Remaining useful life estimation in prognostics using deep convolution neural networks, Reliability Engineering and System Safety 172 (2018) 1–11.",
"Z. Sikorska, M. Hodkiewicz, L. Ma, Prognostic modelling options for remain- ing useful life estimation by industry, Mechanical Systems and Signal Processing 25 (5) (2011) 1803–1836.",
"[16] P. Lim, C. K. Goh, K. C. Tan, A time-window neural networks based frame- work for remaining useful life estimation, in: Proceedings International Joint Conference on Neural Networks, 2016, pp.",
"[17] A. Saxena, K. Goebel, D. Simon, N. Eklund, Damage propagation modeling for aircraft engine run-to-failure simulation, in: International Conference On Prognostics and Health Management, IEEE, 2008, pp.",
"[18] E. Ramasso, Investigating computational geometry for failure prognostics, In- ternational Journal of Prognostics and Health Management 5 (1) (2014) 1–18.",
"[19] D. Laredo, X. Chen, O. Sch¨utze, J. Q.",
"Sun, ANN-EA for RUL estimation, source code, [Online] (2018).",
"URL https://github.com/dlaredo/NASA_RUL_-CMAPS- [20] P. B¨uhlmann, G. S., Statistics for High Dimensional Data.",
"Methods, Theory and Applications, Springer, 2011.",
"[21] C. Kong, S. Lucey, Take it in your stride: Do we need striding in CNNs?, arXiv:1712.02502 (2017).",
"22 [22] R. Storn, K. Price, Diﬀerential evolution: A simple and eﬃcient heuristic for global optimization over continuous spaces, Journal of Global Optimization 11 (4) (1997) 341–359.",
"[23] E. Jones, T. Oliphant, P. Peterson, et al., SciPy: Open source scientiﬁc tools for Python, [Online; accessed 06/2018] (2001).",
"URL http://www.scipy.org/ [24] D. Engelbrecht, Computational Intelligence: An Introduction, Willey, 2007.",
"[25] Y. Peng, H. Wang, J. Wang, D. Liu, X. Peng, A modiﬁed echo state network based remaining useful life estimation approach, in: IEEE Conference on Prog- nostics and Health Management, 2012, pp.",
"[26] C. Louen, S. X. Ding, C. Kandler, A new framework for remaining useful life estimation using support vector machine classiﬁer, in: Conference on Control and Fault-Tolerant Systems, 2013, pp.",
"[27] C. Zhang, P. Lim, A. Qin, K. Tan, Multiobjective deep belief networks ensemble for remaining useful life estimation in prognostics, IEEE Transactions on Neural Networks and Learning Systems 99 (2016) 1–13.",
"[28] G. S. Babu, P. Zhao, X. Li, Deep convolutional neural network based regression approach for estimation of remaining useful life, in: S. I.",
Publishing (Ed.,
"), 21st International Conference on Database Systems for Advanced Applications, 2016, pp.",
"[29] P. Malhotra, V. TV, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, G. Shroﬀ, Multi-sensor prognostics using an unsupervised health index based on lstm encoder-decoder, arXiv:1608.06154 (2016).",
23 Appendix A.,
Tested Neural Network Architectures In this appendix we present the tested neural network architectures.,
Each table represents a neural network model.,
"Each row in the table represents a neural network layer while each column describes each one of the key parameters of the layer such as the type of layer, number of neurons in the layer, activation function of the layer and whether regularization is used, where L1 denotes the L1 regularization factor and L2 denotes the L2 regularization factor, the order in which the layers are appended from the table is top-bottom.",
Table A.1: Proposed neural network architecture 1.,
"Layer Neurons Activation Additional Information Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 Table A.2: Proposed neural network architecture 2.",
"Layer Neurons Activation Additional Information Fully connected 50 ReLU L1 = 0.1, L2 = 0.2 Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 Table A.3: Proposed neural network architecture 3.",
"Layer Neurons Activation Additional Information Fully connected 100 ReLU L1 = 0.1, L2 = 0.2 Fully connected 50 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 Table A.4: Proposed neural network architecture 4.",
"Layer Neurons Activation Additional Information Fully connected 250 ReLU L1 = 0.1, L2 = 0.2 Fully connected 50 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 24 Table A.5: Proposed neural network architecture 5.",
"Layer Neurons Activation Additional Information Fully connected 20 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 Table A.6: Proposed neural network architecture 6.",
"Layer Neurons Activation Additional Information Fully connected 10 ReLU L1 = 0.1, L2 = 0.2 Fully connected 1 Linear L1 = 0.1, L2 = 0.2 25 Appendix B. Activation functions deﬁnitions Here we deﬁne some of the activation functions used in our neural network models.",
For the following deﬁnitions assume a vector x ∈Rn.,
Appendix B.1.,
ReLU activation function Figure B.9: ReLU activation function Appendix B.2.,
Linear activation function Figure B.10: Linear activation function 26,
"A Review on Neural Network Models of Schizophrenia and Autism Spectrum Disorder Pablo Lanillosa,∗, Daniel Olivaa,∗, Anja Philippsenb,∗, Yuichi Yamashitac, Yukie Nagaib, Gordon Chenga aInstitute for Cognitive Systems, Technical University of Munich, Arcisstraße 21, Munich, Germany bInternational Research Center for Neurointelligence, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan cDepartment of Functional Brain Research, National Center of Neurology and Psychiatry, 4-1-1 Ogawa-Higashi, Kodaira, Tokyo, Japan Abstract This survey presents the most relevant neural network models of autism spec- trum disorder and schizophrenia, from the ﬁrst connectionist models to recent deep network architectures.",
"We analyzed and compared the most represen- tative symptoms with its neural model counterpart, detailing the alteration introduced in the network that generates each of the symptoms, and identify- ing their strengths and weaknesses.",
"We additionally cross-compared Bayesian and free-energy approaches, as they are widely applied to modeling psychi- atric disorders and share basic mechanisms with neural networks.",
Models of schizophrenia mainly focused on hallucinations and delusional thoughts using neural dysconnections or inhibitory imbalance as the predominating alteration.,
"Models of autism rather focused on perceptual diﬃculties, mainly excessive at- tention to environment details, implemented as excessive inhibitory connections or increased sensory precision.",
"We found an excessive tight view of the psy- chopathologies around one speciﬁc and simpliﬁed eﬀect, usually constrained to the technical idiosyncrasy of the used network architecture.",
Recent theories and evidence on sensorimotor integration and body perception combined with modern neural network architectures could oﬀer a broader and novel spectrum to approach these psychopathologies.,
This review emphasizes the power of arti- ﬁcial neural networks for modeling some symptoms of neurological disorders but also calls for further developing these techniques in the ﬁeld of computational psychiatry.,
"Keywords: Neural Networks, Schizophrenia, Autism Spectrum Disorder, Computational Psychiatry, Predictive Coding ∗-authors contributed equally ∗∗Some ﬁgures are under copyright.",
"Email addresses: p.lanillos@tum.de (Pablo Lanillos), daniel.oliva@tum.de (Daniel Oliva), anja@ircn.jp (Anja Philippsen), nagai.yukie@mail.u-tokyo.ac.jp (Yukie Nagai) Preprint submitted to Journal of Neural Networks October 24, 2019 arXiv:1906.10015v2 [q-bio.NC] 23 Oct 2019 Contents 1 Introduction 3 1.1 Artiﬁcial neural network modeling of psychopathologies .",
4 1.2 Purpose and content overview .,
5 2 Pathologies and their symptoms 6 2.1 Schizophrenia .,
6 2.2 Autism spectrum disorder .,
7 3 Modeling approaches and hypotheses 8 3.1 Dysconnection hypotheses .,
9 3.2 Excitation/inhibition imbalance .,
9 3.3 Hypo-prior theory and aberrant precision account .,
10 3.4 Alternative modeling approaches .,
11 4 ANN models of schizophrenia 11 4.1 Hopﬁeld networks: memory .,
12 4.1.1 Memory overload .,
12 4.1.2 Memory model with disconnections .,
12 4.1.3 Memory model hippocampal region .,
13 4.2 Feed-forward networks: context and language .,
16 4.2.1 Attention and context representation .,
16 4.2.2 Auditory processing .,
18 4.2.3 Language processing .,
19 4.3 Bayesian approaches .,
20 4.3.1 Free-energy model of schizophrenia .,
21 4.3.2 Circular inference in Bayesian graphical models .,
24 4.4 Recurrent neural networks .,
25 5 ANN models of autistic spectrum disorder 27 5.1 Feed-forward and simple recurrent neural networks .,
27 5.1.1 Generalization deﬁcits through overﬁtting .,
28 5.1.2 Precision of memory representations .,
29 5.1.3 Generalization and categorization abilities in visual per- ception .,
29 5.2 Self-organizing maps .,
31 5.2.1 Increased lateral feedback inhibition .,
31 5.2.2 Familiarity preference .,
32 5.2.3 Unfolding of feature maps and stimuli coverage .,
33 5.3 Convolutional neural networks and inhibition imbalance .,
33 5.4 Spiking neural networks and local over-connectivity .,
35 5.5 Bayesian approaches .,
37 5.6 Recurrent neural networks .,
38 2 5.6.1 Freezing and repetitive behavior in a robotics experiment 38 5.6.2 Impairment in internal network representations .,
39 5.6.3 Generalization ability in a variational Bayes recurrent neu- ral network .,
41 5.7 Other approaches .,
41 6 Discussion and future directions 43 6.1 Models quality: Empirical ﬁndings and predictability .,
"44 6.2 Multiﬁnality, equiﬁnality and heterogenity .",
45 6.3 Models validation on real robotic systems .,
46 6.4 New directions .,
46 6.4.1 Developmental factors .,
46 6.4.2 SZ and ASD as disorders of the self .,
46 6.4.3 ANN novel architectures for psychopathologies .,
"Introduction In the world, there is a prevalence of schizophrenia (SZ) that ranges between four and seven per 1000 individuals (between three and ﬁve million people) [1] and a prevalence of Autism Spectrum Disorder (ASD) that ranges between six and 16 per 1000 children (between 1 of 150 and 1 of 59 children) [2].",
SZ and ASD have in common that they both cause deﬁcits in social interaction and are characterized by perceptual peculiarities.,
"While ASD has its onset in early childhood, SZ is typically diagnosed in adults, although in very rare cases, ap- pears during development [3].",
"Similar neural bases have been observed for both disorders [4], which has even led to the suggestion that some SZ cases might be part of the autism spectrum [5].",
"In fact, there are similarities such that both pathologies show atypical sensorimotor integration and perceptual interpreta- tion.",
"However, there are also striking diﬀerences between these disorders.",
"A common symptom of SZ is the occurrence of hallucinations or delusions, in con- trast to ASD which is characterized by atypical non-verbal communication and emotional reciprocity.",
"Furthermore, a few savant syndrome cases were reported in ASD individuals with extraordinary skills like painting [6].",
"1 depicts, in an artistic way, the reality perceived by two individuals in the spectrum of these disorders.",
"For both disorders, neurological, genetic and environmental factors have been suggested, but to date the actual causes and underlying cognitive pro- cesses remain unclear.",
A major challenge in diagnosis is their heterogeneity and non-speciﬁcity.,
"Heterogeneity means that symptoms, prognosis and treatment responses vary signiﬁcantly between diﬀerent subjects.",
Non-speciﬁcity expresses that a single biological basis can be underlying diﬀerent phenotypes (multiﬁnal- ity) and diﬀerent biological bases can result in a single phenotype (equiﬁnality).,
"Non-speciﬁcity, as a biological abnormality related to a psychiatric disorder, can be found in many other neurological disorders [8, 9].",
3 (a) (b) Fig.,
1: Artistic pieces representing diﬀerent perceptions of the world.,
"(a) Hunted, c⃝2019 Henry Cockburn, a SZ diagnosed artist.",
"(b) Drawing by Nadia Chomyn at the age of 5, a gifted ASD diagnosed child, reprinted from [7], c⃝2012 Lorna Selfe.",
"Computational modeling of psychopathologies or Computational Psychiatry is one of the potential key players [10, 11, 9] to tackle heterogeneity and non- speciﬁcity, and to better understand the cognitive processes underlying these disorders.",
"Eventually, computational models might help to obtain a deeper understanding of theoretical models, generate new hypothesis or even suggest new treatments.",
"There are diﬀerent levels of descriptions or units of analysis to study these disorders, which encompass from genes to molecules, to cells, to circuits, to physiology, and then to behaviour.",
“Computational Psychiatry provides some of the tools to link these levels” [12].,
"In particular, neural network models serve, due to their analogy to biological neurons, as a tool to test and generate hypotheses on possible neurological causes [13].",
"Artiﬁcial neural networks cannot only be useful from the data- driven point of view (e.g., ﬁtting a model to fMRI1 data), but can also be used as a simpliﬁed model of the human brain to replicate and predict human behavior and to investigate which modiﬁcations in the connectionist models cause a speciﬁc alteration in the behavior.",
Artiﬁcial neural network modeling of psychopathologies Artiﬁcial Neural Networks (ANNs or NNs) were ﬁrst introduced in the 1950’s as an attempt to provide a computational model of the inner processes of the human brain [14].,
"Nevertheless, their potential was not fully unraveled until the last decades because of limited computational power and data shortage [15].",
"Due to the inspiration from biological processes of our brain and their connec- tionist nature, these technologies have also opened a door to new research ﬁelds that combine disciplines, such as neuroscience and psychology with artiﬁcial intelligence and robotics.",
"Within the ﬁeld of cognitive neuroscience, neural net- works are already used as a tool for getting insights into the complex structures of our brain and gaining a better understanding of how learning, memory or visual perception might work on a neural level [16, 17].",
"1fMRI: functional magnetic resonance imaging 4 In the late 80’s and early 90’s, neural networks were used for the ﬁrst time related to psychiatry, trying to imitate psychological disorders [18, 19].",
"Early eﬀorts in compiling ANN models for cognitive disorders can be found in [20] and in [21], in particular, for autism.",
"Due to immense advances in computational power, 20 years later, computational modeling using ANNs and deep learning is becoming a powerful asset to aid the investigation of this type of disorders.",
The challenge is to translate ﬁndings from behavioral or neurological studies at dif- ferent levels of description in a coherent way into a mathematical connectionist model.,
"ANN models can process a vast amount of information, cope with non- linearities in the data, and the structure of ANNs makes it possible to system- atically test which parameter modiﬁcations cause eﬀects similar to the symp- toms of psychiatric disorders.",
"Furthermore, these ANN models and their al- terations may be directly implemented in artiﬁcial agents (e.g., robots) ﬁlling the last level: comparing the behavior of such agents with behaviors observed in patients [22, 23].",
"In this way, existing hypotheses from neuroscience and psychology could be tested, and new hypotheses on potential causes could be formulated.",
"Purpose and content overview This historical review aims at serving as a reference for computational neuro- science, robotics, psychology and psychiatry researchers interested in modeling psychopathologies with neural networks.",
"This work extends general computa- tional modeling reviews [20, 21, 24, 25, 26] by focusing on neural network models for SZ and ASD with detailed explanation of the alterations on a neural level and their associated symptoms, including their technical architectures as well as their mathematical formulation.",
"For completeness, we also included Bayesian and predictive processing models due to their similarities to ANNs and their relevance inside the neuroscience community.",
"Actually, conceptually, ANN and Bayesian models often take similar approaches to model psychiatric disorders (see Section 4.3 and Section 5.5).",
"We start in Section 2 with an introduction to the mentioned disorders, listing their main characteristics and symptoms based on the latest Diagnostic and Statistical Manual of Mental Disorders (DSM-5) descriptions.",
"For readability and due to the heterogeneity of the reviewed methods, in Section 3, we ﬁrst summarize and discuss the main modeling approaches and hypotheses which are referenced in the literature.",
"Afterwards, Section 4 and Section 5 present a comprehensive review of models of SZ and ASD, respectively, organized by the type of modeling approach.",
"To help the reader, we summarized the content of Section 4 and Section 5 into two tables: Tab.",
1 (page 12) for SZ and Tab.,
2 (page 27) for ASD.,
"Finally, in Section 6 we discuss the reviewed works and compile recommendations for future research on ANNs for computational psychiatry, in particular for ASD and SZ.",
Pathologies and their symptoms SZ and ASD are disorders that change the way we perceive and act in the world.,
"Atypicalities in perception and in cognitive process cause diﬃculties in connecting with the world, in particular for social interaction.",
"Since the ﬁrst reports of autistic symptoms [27], both conditions have been closely related.",
"Before ASD was recognized as a separate disorder, subjects with ASD were often diagnosed as schizophrenic instead [27].",
"Also nowadays, these two pathologies remain strongly connected as both are associated with atypicalities in sensory processing and information processing, and due to their strong heritability [28, 29, 30].",
"Schizophrenia SZ is a serious psychiatric disorder that aﬀects a person’s feelings, social behavior and perception of reality.",
"Its biological causes are still unknown, but genetic and environmental factors, i.e., prenatal stress, traumatic experiences or drug use, can be key factors for the development of this disorder.",
Its symptoms are usually divided into positive symptoms and negative symptoms [31].,
"Posi- tive symptoms correspond to the presence of abnormal functions, for instance, hallucinations and delusions.",
"Negative symptoms, corresponding to decreased function, are a lack of the normal function such as diminished emotional ex- pression.",
Positive symptoms are more apparent and generally respond better to medication.,
Negative symptoms are more subtle and less responsive to phar- macological treatment.,
Below some of the most characteristic symptoms of SZ taken from the DSM-5 [32] are listed.,
Positive symptoms: 1.,
"Delusions: have convinced beliefs that are not real, and cannot be changed despite clear evidence.",
"Hallucinations: perceive things that do not exist as real, without an ex- ternal stimulus.",
"Disorganized thinking: diﬃculty to keep track of thoughts, drift between unrelated ideas during speech.",
"Disorganized or abnormal movements: diﬃculties to perform goal-directed tasks, catatonic (stopping movement in unconventional posture) or stereo- typed (repetitive) movements.",
Negative symptoms: 1.,
"Diminished emotional expression: reduced expression of emotions through speech, facial expressions or movements.",
"Avolition: lack of interests, inaction.",
Alogia: diminished speech output.,
Anhedonia: diminished ability to experience pleasure.,
Asociality: lack of interest in social interaction.,
6 Multiple reports have also associated self-other disturbances to SZ.,
"This means that schizophrenic patients can perceive own and external actions or feelings, but may have problems diﬀerentiating them.",
This could be part of the explanation for auditory hallucinations and struggles during social interaction.,
Van der Weiden and colleagues published an extensive review [33] on possible causes for this disorder.,
"Finally, in more severe cases, motor disorders have been reported [34], such as stereotypical and catatonic behavior.",
"SZ is investigated by many researchers because of its prevalence and its devastating eﬀects on patients, which can have life-changing consequences on the patient’s relationships and social situation.",
"Moreover, its close relation with the inner workings of self-perception and self-other distinction, raises the interest of researchers from multiple areas such as psychology, neuroscience, cognitive science and even developmental robotics.",
Autism spectrum disorder ASD is a prevalent developmental disorder that has a behavior-based diag- nosis due to its still unclear biological causes.,
"It was ﬁrst introduced in the 1940s by Kanner [27], who presented the cases of eleven children “whose condi- tion [diﬀered] so markedly and uniquely from anything reported so far”, some of them being previously diagnosed as schizophrenic.",
"Actually, the term autis- tic was originally used for describing symptoms in schizophrenic patients.",
"This kind of disorder mainly aﬀects individual’s social interaction, communication, interests and motor abilities.",
"It is often referred to as a heterogeneous group (spectrum) of disorders, as individuals typically show distinct combinations of symptoms with varying severity.",
"Nevertheless, there are some characteristic at- tributes that are commonly associated with ASD, which we have listed from the DSM-5 [32].",
Deﬁcits in social communication and interaction: 1.,
"Impairment in socio-emotional reciprocity: struggle to share common in- terests and emotions, reduced response or interest in social interaction, 2.",
"Deﬁcits in non-verbal communication: problems integrating verbal and nonverbal communication, and using and understanding gestures or facial expressions, 3.",
Problems to maintain relationships: problems or absence of interest in understanding relationships and adjusting behavior.,
"Abnormal behavior patterns, interests or activities: 1.",
"Stereotyped movements or behavior: repetitive motor movements or speech, 2.",
"Attention to sameness: adherence to routines, distress because of small changes, 3.",
"Fixated and restricted interests: strong attachment to certain objects, activities or topics, 4.",
"Hyper- or hyporeactivity to sensory input: indiﬀerence to pain, repulsive response to certain sounds or textures, visual fascination.",
7 Deﬁcits in social interaction are often the most obvious symptoms of ASD.,
"Hence, for a long time, ASD was mainly considered as a disorder of theory of mind, suggesting that individuals with ASD are characterized by absence or weakening of their ability to reason about the beliefs and mental states of others in social contexts [35].",
"Actually, early identiﬁcation of individuals with ASD has focused on non-verbal communication interaction, mainly observing attention and gaze behaviours using standardized tests, such as the Autism Diagnostic Observation Schedule (ADOS) [36].",
"Whereas this explanation could account for a vast amount of symptoms that become obvious in development and socialization of children with ASD, it was mainly criticized due to its failure to explain similarly prominent non-social symptoms such as restricted interests, desire for sameness or excellent performance in speciﬁc areas.",
"An alternative was suggested in the 90’s with the weak central coherence theory [37, 38].",
"It sees the underlying causes of ASD in the perceptual domain, namely in diﬃculties to integrate low-level information with higher-level con- structs.",
"This “inability to integrate pieces of information into coherent wholes (central coherence)”, stated in [39], could oﬀer explanations for the aforemen- tioned deﬁcits and also be extended to an explanation of social deﬁcits.",
"An even broader view is provided by the Bayesian brain hypothesis which suggests general deﬁcits in the processing of predictions and sensory information, and can be applied to non-visual perception as well as motor abilities.",
ASD is thought to be caused by genetic disorders and environmental fac- tors and evidence points at high heritability [30].,
"Furthermore, recent studies, using a computer model of the human fetus, have also highlighted the impor- tance of intrauterine embodied interaction on the development of the human brain and in particular cortical representation of body parts [40].",
Some authors have suggested that preterm infants might have a higher risk of enduring such developmental disorders.,
Modeling approaches and hypotheses ASD and SZ are among the psychiatric disorders which are most commonly investigated using computational modeling.,
A reason might be the unclear un- derlying cognitive mechanisms of these disorders which computational models might help to unravel.,
The studies we discuss in this review often take similar approaches for modeling ASD and SZ.,
"In fact, these two disorders share cer- tain symptoms, such as deﬁcits in social communication and motor impairments manifesting as decreased response or repetitive and stereotyped movements.",
"Al- though, perceptual atypicalities in both disorders are usually diﬀerentiated in that SZ involves perceptual experiences that occur without an external stimulus (e.g., hallucinations) whereas ASD is more typically characterized by hypersen- sitivity to certain stimuli from the environment, there is some overlap.",
"For instance, hypersensitivity can be also found in SZ patients [41].",
"Furthermore, both disorders present less sensitivity to some visual illusions [42, 43].",
"Despite of all these similarities, it is still under debate how these two disorders relate to each other [44].",
"8 In computational modeling, similarities between modeling approaches are not primarily motivated by the similarities in symptoms.",
"In fact, studies mod- eling SZ focused mainly on delusions and hallucinations which are not predom- inant in ASD.",
"Similarities, instead, can be found in the suggested biological causes and in the type of altered neural network parameters.",
"There are three main biological causes that are commonly employed in com- putational models: neural dysconnections2, imbalance of excitation and inhibi- tion, and alterations of the precision of predictions or sensory information.",
"Dysconnection hypotheses Especially for SZ, one of the most discussed theories is the idea of functional disconnections [45, 46].",
"The main motivation is that SZ cannot be explained by an impairment of a single brain region, but only by a (decreased) interac- tion between multiple brain regions [45].",
"Disconnections or underconnectivity are also discussed as a potentional cause of ASD [47, 48, 49], but more recent evidence also points at increased connectivity [50, 51] or a distortion of patterns of functional connectivity [52].",
"In the discussed studies for SZ, dysconnection is primarily implemented by an increased pruning of synapses [53, 54, 55].",
Such a pruning is a normal devel- opmental process between adolescence and early adulthood [56].,
"Computational models using Hopﬁeld networks [53] or feed-forward networks [54, 55] demon- strate that too strong pruning can cause fragmented recall or the recall of new patterns, which can be related to the symptom of hallucinations in SZ.",
"Notably, the SZ symptoms replicated with connection pruning focus solely on hallucinations or delusions and might not be appropriate for modeling ASD.",
"In fact, in a biological context, it might be more appropriate to disturb connections between neurons instead of simply cutting them.",
This idea was followed by Yamashita and Tani [57] who induced noise between diﬀerent hierarchies of neurons (suggested by [58]).,
"They demonstrated in a robotic experiment that this leads to the emergence of inﬂexible, repetitive motor behavior similar to catatonic symptoms in SZ.",
This motor behaviour could also be present in ASD.,
Just a single study focused on dysconnection in ASD.,
"Park and colleagues [59, 60] showed, using a spiking neural network, that local over-connectivity, especially locally in the prefrontal cortex [61], can account for the emergence of aberrant frequency patterns of neural connections in patients with ASD.",
"Excitation/inhibition imbalance An excitation/inhibition (E/I) imbalance is among the most commonly ref- erenced biological evidence for SZ as well as for ASD [62, 63, 64, 65].",
E/I im- balance was found in many neurobiological studies on SZ and ASD.,
Although it is not clear how exactly E/I imbalance translates to changes in cognition and 2Note that disconnection usually refers to a lack of connection whereas dysconnection describes atypical connectivity which might include decreased as well as increased connectivity.,
"9 behavior [65], it seems to be linked to core symptoms of both disorders such as hallucinations [66] and social interaction deﬁcits [67].",
An unanswered question is also of which quality this imbalance is.,
A recent review of studies regarding ASD found evidence for increased inhibition as well as for increased excitation [68].,
Conﬂicting results in various brain regions might arise by diﬀerences in measurements and their reliability.,
"The most commonly used mechanisms are magnetic resonance spectroscopy which allows to measure the cortical levels of glutamate or GABA, measurements of gamma-band activ- ity (which is hypothesized to be connected to inhibition) or the analysis of the number of glutamate or GABA receptors in post-mortem studies [68].",
"Another possible interpretation of these conﬂicting results is that both, increases and decreases, in inhibition and excitation are present in ASD.",
This hypothesis was put forward by Nagai et al.,
"[69], suggesting that both impairments share a common underlying mechanism.",
"Their model could show that increased inhibi- tion and increased excitation can simulate the local or global processing bias of ASD, respectively.",
"Furthermore, Gustafsson [70] also connected E/I imbalance to the local pro- cessing style of ASD.",
"He implemented increased inhibition in a self-organizing map, in particular, stronger inhibition in the surrounding of receptive ﬁelds which led to over-discrimination.",
"For SZ, although E/I imbalance is commonly associated to SZ in the liter- ature, only the approach from Jardri et al.",
[71] explored E/I imbalance as a modeling mechanism.,
"In their model, a stronger excitation or insuﬃcient inhi- bition caused circular belief propagation: bottom-up and top-down information are confused with each other which might cause hallucinations and delusions (see page 24).",
This model was recently supported by some experimental evidence [72].,
Hypo-prior theory and aberrant precision account The increasing popularity of the Bayesian view on the brain in recent years resulted in a trend of explaining psychiatric disorders as a cause of the fail- ure of correctly integrating perceived low-level sensory information (bottom-up information) with high-level prior expectations (top-down information).,
"These approaches are inspired by diminished susceptibility of subjects with psychiatric disorders to visual illusions [43] and the well-known symptom of hypersensitivity to certain stimuli (e.g., [73]).",
Problems in the integration of top-down and bottom-up information can be explained by an inadequate estimation of the precision of these signals.,
"A decreased precision of the prior causes a weaker reliance on predictions and, hence, a relatively stronger reliance on sensory input.",
This so-called hypo- prior theory was ﬁrst suggested by Pellicano and Burr for ASD in 2012 [74].,
"Similarly, an increased precision of the bottom-up signal can account for the same consequences [75].",
"Despite some initial evidence in favor of an overrating of sensory information [76], it cannot be decided to date which of these theories is more compelling than the other.",
"Possibly, both contribute to the observed phenomena.",
"10 For both, ASD and SZ, typically a weaker inﬂuence of predictions and a higher inﬂuence of sensory information is suggested [74, 75, 76].",
Lawson and colleagues substantiated aberrant precision for ASD by basing it on hierarchical predictive coding.,
"They argued that both hypo-priors and increased sensory noise might inﬂuence the perception on diﬀerent levels of the cortical hierarchy, leaving open both hypotheses.",
"In an endeavor to clarify how such theories diﬀer for ASD and SZ, Karvelis et al.",
"[76] recently investigated how healthy individuals, scored for traits of ASD and SZ, use prior information in a visual motion perception task.",
"ASD traits were associated with increased sensory precision, whereas SZ traits did not correlate.",
"However, it might be intuitively plausible that also an overrating of top-down information can account for the occurrence of hallucinations [77].",
"In a recent review, Sterzer et al.",
[78] noticed that too strong as well as too weak priors explain psychosis.,
"They suggested that the way that priors are processed might diﬀer depending on the sensory modality or the hierarchical level of processing, yielding inconsistent theories and ﬁndings.",
"In line with this idea, computational models for ASD often suggest that an impairment might be present in both extremes [79, 80].",
"In [79], repetitive move- ment could be replicated by an aberrant estimation of sensory precision, leading to inﬂexible behavior, either due to sameness of intentional states (increased sen- sory variance) or due to high error signals and misrecognition (decreased sensory variance).",
"Similarly, [80] suggests that too strong as well as too weak reliance on the sensory signal may impair the internal representation of recurrent neu- ral networks.",
"Thus, for SZ as well as for ASD, too strong as well as too weak reliance on priors or sensory information seem to be valid modeling approaches.",
Alternative modeling approaches There are alternative theories used in the discussed computational models.,
"Synaptic gain, for instance, has been evaluated for SZ [19] as well as for ASD [81].",
"In fact, a reduction of synaptic gain might be related to reduced precision of prior beliefs as discussed in [82].",
Less biologically inspired approaches can also be found in the literature and focus more on replicating behavioural data using known engineering techniques in ANN.,
"For instance, deﬁcits in generalization capabilities are modeled in neu- ral networks by modifying the number of neurons [83], changing the training time [81] or introducing regularization factors [81, 84].",
"ANN models of schizophrenia In the following section, we present a comprehensive description of the most important ANN models of SZ.",
"The majority of approaches focuses on positive symptoms of SZ, such as hallucinations and delusional behavior, e.g., [85] and [54].",
"Nevertheless, there have been also approaches targeting other symptoms, for instance attention characteristics [19] and movement disorders [57].",
An overview of the most important models is presented in Tab.,
"1: Overview of neural network models of schizophrenia Model type Paper Disorder Characteristic Biological Evidence Approach Hopﬁeld Networks R. E. Hoﬀman, T. H. Mc- Glashan (1987)[18] Delusions, sense of mind being controlled by outside force - Storing of an excessive number of memories (memory overload) R. E. Hoﬀman, T. H. Mc- Glashan (1989)[53] Hallucinations, delusions, sense of mind being controlled by out- side force Reduced connectivity in pre- frontal cortex and other regions Excessive connection pruning D. Horn, E. Ruppin (1995) [85] Delusions and hallucinations Reactive synaptic regeneration in frontal cortex Weakening of external input pro- jections, increase of internal pro- jections and noise levels, addi- tional Hebbian component Feed-forward NNs J. D. Cohen, D. Servan- Schreiber (1992) [19] Disturbances of attention, repre- sentation of context Abnormal dopamine activity in prefrontal cortex Reduction of activation function gain in context-neurons R. E. Hoﬀman, T. H. Mc- Glashan (1997) [54] Auditory hallucinations Reduced connectivity in pre- frontal cortex and other regions Excessive connection pruning R. E. Hoﬀman et al.",
"(2011) [55] Delusionary story reconstuction Abnormal dopamine activity, cortical disconnections Increased BP learning rates, ex- cessive connection pruning in working memory Predictive processing Adams et al.",
"(2013) [86] Delusions and hallucinations, abnormal smooth pursuit eye movement Abnormal neuromodulation of superﬁcial pyramidal cells in high hierarchical levels Abnormal precision computation in the free energy minimization scheme Circular inference Jardri and Denve (2013) [71] Hallucinations and delusions Disruption in the neural excita- tory to inhibitory balance Increased excitation / reduced inhibition in belief propagation Recurrent NNs Y. Yamashita, J. Tani (2012) [57] Disturbance of self, feeling of be- ing controlled by outside force, disorganized movements Disconnectivities in hierachical networks of prefrontal and pos- terior brain regions Noise between context neuron hi- erarchies in MTRNN 4.1.",
Hopﬁeld networks: memory 4.1.1.,
"Memory overload In 1987, Ralph E. Hoﬀman, professor of psychiatry from Yale, presented the earliest neural network model of SZ [18], inspired by the suggestions of [16], who explored the function of dreams using a neural network model.",
"Hoﬀman tried to explain the causes of schizophrenic and maniac disorders with simulations using a Hopﬁeld Network, an associative memory ANN that is usually employed to simulate the inner functioning of human memory [87] and to store binary memory patterns.",
It is a recurrent neural network that converges to ﬁxed-point attractors.,
"As a learning mechanism, the famous Hebbian rule, “cells that ﬁre together wire together”, is applied.",
"In other words, connections between neurons that get activated with temporal causality are increased [88].",
"In order to model SZ, the author inspected the behavior of the network attractors after storing an increasing number of binary memories.",
"Results showed that by increasing the number of binary memory pat- terns stored, the network reaches “parasitic” states that do not correspond to previously stored memories.",
"With higher numbers of memories or decreased storage capacity, the network’s internal energy minima, that correspond to the stored memories, might inﬂuence each other and create additional deep minima (attractors) that do not correspond to any previously learned pattern.",
"These minima might inﬂuence either only the information processing course (mind being controlled by outside force) or lead to convergence to “parasitic states”, which are compared to hallucinations and delusional thoughts.",
This study did not use biological evidence to support its main thesis that SZ might be caused by memory overload and only compared behavioral observations.,
"However, this model served as a stepping stone for a successor model (see Section 4.1.2).",
Memory model with disconnections Observations that show diminished metabolism in the prefrontal cortex (hy- pofrontality) of individuals with SZ led to the theory that excessive synaptic 12 Fig.,
2: Pruning rule used for the Hopﬁeld Network in [53].,
The connections are pruned depending on the connection weight and the distance between the connected neurons.,
A: Connections before pruning.,
B: Connections after pruning.,
Reprinted from [53].,
"pruning might be the reason for the appearance of SZ between adolescence and early adulthood [89, 90].",
"A decline in synaptic density is a normal develop- mental process [56, 91] which might have gone too far in the case of SZ.",
"In 1989, Hoﬀman and Dobscha used a Hopﬁeld network, arranged as a 2D grid, as a content-addressable memory to retrieve previously stored memories giving a similar input [53].",
"A “neural Darwinism” principle was applied, which is a pruning rule that erases connections depending on their weights and length (proximity of neurons in the grid).",
The concrete pruning rule is shown in Eq.,
"(1), with |Txy| being the weight of the connection between neu- rons in coordinates (x, y) and (i, j), and ˆp the pruning coeﬃcient.",
The pruning coeﬃcient determines the number of connections which are discarded.,
2 illustrates a possible scenario for this pruning process.,
"|Txy| = ˆp · [(i −x)2 + (j −y)2]0.5 (1) For a moderate level of pruning, the network is still able to perform the memory-retrieval task, but for connection reductions of 80% the network shows fragmented retrieval.",
"This fragmentation was compared to thought disorders observed in SZ, which lead to incoherence, attention deﬁcits or the feeling that one’s mind is being controlled by an outside force.",
"Furthermore, sometimes over- pruned areas converged to patterns not included in any of the stored memories.",
These were denominated as “parasitic foci”.,
The authors compared these to hallucinations in SZ because they contained decodable information that does not belong to any stored memory.,
"Occasionally, these parasitic regions extended on a larger area and persisted independently of the input, which was compared to delusional thoughts observed in patients.",
"Memory model hippocampal region In 1995, Horn and Ruppin [85, 92] also introduced a Hopﬁeld-based network to replicate the positive symptoms of SZ.",
"This model was based on the hy- 13 pothesis by J. R. Stevens [93] that schizophrenic symptoms might be caused by “reactive anomalous sprouting and synaptic reorganization taking place at the frontal lobes, subsequent to the degeneration of temporal neurons projecting at these areas”.",
"The hypothesis takes into account observations that showed atrophic changes in the temporal lobe, and at the same time increased dendritic branching in the frontal lobe of a signiﬁcant number of schizophrenic patients.",
"Essentially, the idea is that degenerations in temporal lobe regions that are connected to the frontal lobe regions might produce a compensatory reaction in that area, namely increased receptor bindings (frontal lobe connections) and anomalous dendritic sprouting (increased inﬂuence from other cortical areas).",
The work by Hoﬀman explained in the previous section suggested that hal- lucinations should always appear in combination with memory problems in pa- tients because pruning clearly aﬀects the network’s memory retrieval perfor- mance.,
"However, this is not always the case in patients.",
"Following the hypoth- esis from Stevens, the model described in [85] would make hallucinations and intact memory capabilities compatible.",
"The model used in this paper was a Hopﬁeld network taken from [94, 95], which is more appropriate for the storage of correlated patterns.",
"This network is used for a pattern retrieval and recovery task, which means that in its original functionality, it receives an external input pattern and outputs the previously learned pattern that corresponds to it, given that a similar one was learned before.",
"Deﬁning the connection strength (weight) between neuron i and j as Wij, the learning rule is: Wij new = c Wij old , (c > 1) (2) Wij = c0 N M X µ=1 (ξµ i −p)(ξµ j −p) (3) where c is the internal projection parameter with value always > 1.",
"(3) describes the initial conﬁguration of the network weights, with c0 = 1, p being the probability that a memory pattern is chosen to be 1, and ξµ i one of the M = αN memory patterns.",
"The input of each neuron i at time step t is expressed as: hi(t) = X j WijSj(t −1) + e · ξ1 i (4) where e is the network input parameter with value 1 in normal conditions, which weights the incoming memory pattern, and Sj is the neuron output deﬁned by a sigmoid function with noise level T and a ﬁxed uniform threshold of all N neurons θ: Si(t) = ( 1, with probability 1 1+exp(−(hi(t)−θ)/T ) 0, otherwise (5) 14 In order to simulate degenerated temporal lobe projections to the frontal lobe, the input is scaled down by decreasing parameter e < 1 in Eq.",
In order to model increased receptor bindings and dendritic sprouting the parameter c in Eq.,
(3) and noise level T in Eq.,
(5) are increased.,
The parameter c scales the internal weights of the network and T inﬂuences the neuron activation.,
"After performing these modiﬁcations, the network is still able to retrieve previously stored memories, but spontaneously converges to certain memories without a speciﬁc input stimulus.",
An additional Hebbian learning rule during pattern retrieval on a lower time scale is used to account for increased dopamine levels observed in patients with SZ: Wij(t) = Wij(t −1) + γ N ( ¯Si −p)( ¯Sj −p) (6) where ¯Si is a variable that only becomes 1 if the neuron in question has been active during the last τ iterations.,
"There are studies that have observed that dopamine activity increases may enhance Hebbian-like activity-dependent synap- tic changes in the brain, and a high synaptic modiﬁcation rate γ is used to repli- cate this eﬀect, as this parameter inﬂuences how much the network’s weights are changed during learning.",
This modiﬁcation is used to imitate high dopamine levels observed in schizophrenia.,
"3: Schematic illustration of the proposed model: An ANN models the frontal module, receiving input from internal connections c, external connections from the medial temporal lobe e and connections T from distant cortical modules modeled as external noise.",
"Highlighted in red are the modiﬁcations made on the Hopﬁeld network to imitate schizophrenic behavior: Decrease of external input projections, and increase of internal projections and external noise.",
Adapted from [85] .,
"In total, four network modiﬁcations were tested on the presented architecture (Fig.",
"3): (1) weakening of the network input parameter e, (2) increase of 15 internal projections c, (3) increase of noise levels T , and (4) additional Hebbian learning rule (Eq.",
"Combining the reactive modiﬁcations to a decrease of e (internal connections and external noise) with the described Hebbian rule (even with a small γ of 0.0025), the spontaneous retrievals are enhanced and get continuously triggered without a concrete retrieval input.",
This behavior is compared to long-term hallucinations or delusional beliefs characteristic of schizophrenic patients.,
"This results would also ﬁt with the eﬀect of dopaminergic blocking agents (equivalent to reducing the eﬀect of the Hebbian learning rule), which are used to reduce hallucinations in patients.",
Feed-forward networks: context and language 4.2.1.,
Attention and context representation In 1992 the ﬁrst model based on feed-forward neural networks was intro- duced.,
The psychology professor Jonathan D. Cohen and neuroscientist David Servan-Schreiber [19] presented an extensive analysis of a possible explanation for negative symptoms in SZ.,
"More concretely, they focused on disturbances of attention and contextualization problems in schizophrenics, which were for in- stance reported in [96] and [97].",
Their main hypothesis was that schizophrenics fail to make an internal representation of context and that an abnormal amount of dopamine in the prefrontal cortex is the main cause (cf.,
Section 4.1.3 as a com- parison).,
"The authors refer to previous studies suggesting that the prefrontal cortex is the brain region responsible for maintaining an internal representations of context, and that patients with SZ show dysfunctions and abnormal dopamine levels in this area.",
"In order to test the dopamine-theory of SZ, three experi- mental tasks were compared to three neural network models, obtaining similar results to empirical observations.",
"They simulated reduced dopamine activity by decreasing the gain of the activation function (the activation function’s slope), described by Eq.",
"(7), in the neurons responsible for context representations.",
"In this equation, we used the same nomenclature as in the original paper, where net is the added activation of all incoming connections, bias the neuron bias and gain the parameter that is modiﬁed.",
The mentioned idea of modifying the activation function’s gain was based on studies that suggest that high dopamine levels potentiate the neurons’ activation (inhibitory and excitatory) in the pre- frontal cortex.,
"The modiﬁcation of the gain has a similar eﬀect because higher gain values increase the activation function’s slope, which means that even small neuron input values produce either very low neuron activations (equivalent to inhibitory signals) or high activations (equivalent to excitatory signals).",
"f(net) = 1 1 + exp(gain · net + bias) (7) The ﬁrst experiment, depicted in Fig.",
"4, was the Stroop task [99], which consists of color words printed in diﬀerent color inks that are presented to the participants.",
"These words have either congruent stimuli (color and word are the same), conﬂicting stimuli (color and word contradict each other) or control 16 (a) Stroop test (b) Network model Fig.",
4: Attention and context.,
"(a) Stroop card test used for SZ, reprinted from [98] (b) Neural network model used for the Stroop task in [19].",
Highlighted in red are the neurons with modiﬁed gain.,
stimuli (color words printed in black ink or the letters “XXX” printed in a certain color).,
The subjects must then either always name the letter’s ink color or the written word.,
"This exercise is used to test the participant’s attention capacities, and schizophrenic subjects show overall slower reaction times and perform even worse when conﬂicting stimuli are shown [98].",
"In order to feed the information in the network, the printed word’s ink color and meaning were numerically coded.",
"By reducing the gain on the color naming and word reading units from 1.0 (normal gain) to 0.6 they observed a delay in the response time of the network to properly produce a correct answer, similar to what it was observed in schizophrenic diagnosed individuals.",
"The second experiment, shown in Fig.",
"5, implemented the Continuous Per- formance Test (CPT) [100] identical pair version [101].",
It measures participant’s ability to detect repeated pattern of symbols in a longer sequence.,
"Symbols are presented sequentially and the volunteers must detect when the pattern appears consecutively, words or numbers, e.g., “9903”.",
"In this experiment, schizophren- ics usually struggle with the detection of longer patterns where previous symbols need to be taken into account.",
Prior stimulus module neurons were used to save the information about previous sequence symbols.,
"To simulate schizophrenic behavior, the authors reduced the gain of the activation-function of the task context yielding to a higher miss-rate in concordance with schizophrenic empirical observations.",
"Finally, a lexical disambiguation task depending on context was modeled based on the original work from Chapman et al.",
[102] (see Fig.,
"Participants had to solve homonym conﬂicts (words with more than one meaning), taking into account the context of the sentence.",
"In this case, schizophrenics show worse performances when the needed context to resolve ambiguity comes before the word in question.",
A similar approach than in the CPT experiment was taken: context neurons gain was manually reduced to 0.6 like in the previous experiments.,
It resulted in low performance for the schizophrenic model when the sentence context that was needed to interpret the ambiguous word was 17 (a) CPT test (b) Network model Fig.,
5: Continuous performance test.,
(a) Simpliﬁed CPT Identical Pair test used (b) Neural network model for the CPT adapted from [19].,
Highlighted in red are the neurons whose gain was decreased to model disturbed processing in the prior stimulus module.,
(a) Task (b) Network model Fig.,
6: Lexical disambiguation.,
(a) Task with context dependent meaning word.,
(b) Neural network model reprinted from [19].,
Highlighted in red are the (context) neurons whose gain was reduced to 0.6. located at the beginning of the sentence.,
"Auditory processing During a person’s life, the number of neurons in the brain peaks during childhood and then decreases by a 30% to 40% in adolescence, which is also the period of time where SZ appears most frequently (adolescence/early adulthood) [56].",
"Based on this observation and post-mortem ﬁndings which suggest neural deﬁcits in the schizophrenic’s cerebral cortex [90, 103], Hoﬀman and McGlashan designed a feed-forward neural network capable of translating phonetic inputs into words [54].",
This model was inspired by Elman’s (1990) model [104].,
As illustrated in Fig.,
7(a) it consists of one hidden layer and a temporal storage layer that saves a copy of the hidden layer from the previous processing step.,
A pruning rule was used to set the value of all connections below a cer- tain threshold to zero.,
"After pruning approximately 30% of the connections, 18 the word detection capabilities of the used network improved3.",
"However, with excessive pruning the network starts to struggle with detection tasks and shows spontaneous responses during periods without input (shown in Fig.",
This last observation was associated to auditory hallucinations reported in patients with severe SZ.,
"Furthermore, it supported the common theory that auditory hallucinations might be caused by false identiﬁcation of own inner speech as externally generated.",
(a) Network model (b) Word detection results Fig.,
7: Auditory hallucinations (a) Neural network model used in [105].,
"Input of the network are simulated phonetic codes, output are semantic features of the input word.",
Highlighted in red are the connections the pruning rule was applied on to imitate schizophrenic symptoms.,
(b) Word detection results depending on connection pruning.,
Spontaneous detections are observed for excessive pruning.,
Reprinted from [105] with permission.,
"In posterior tests with healthy patients, schizophrenics with auditory halluci- nations showed reduced word detection capabilities compared to schizophrenics without such hallucinations, which ﬁts with the previous simulations.",
"Further- more, a later review of this paper [105] highlighted that by applying active repet- itive transcranial magnetic simulation (active rTMS) on the left temporoparietal cortex, a brain region usually associated to speech perception, hallucinations seem to be reduced.",
This further supports the hypothesis of a possible correla- tion between speech-processing disorders and auditory hallucinations.,
"Language processing Another feed-forward model of SZ introduced by R. E. Hoﬀman and col- laborators [55] uses a network called DISCERN [106, 107, 108] that is able to learn narrative language and reproduce learned content, e.g., learn a story and reproduce it after feeding it with a fraction of the story.",
"Based on previous studies about SZ, eight diﬀerent network modiﬁcations were tested: (1) Working Memory (WM) disconnections by pruning of connec- tions with a weight below a certain threshold, (2) Noise addition in working memory by adding of Gaussian noise to WM neuron outputs, (3) WM net- 3Pruning is a bioinspired standard technique for improving generalization of the network.",
"However, nowadays, dropout approaches have gained popularity over pruning.",
"19 work gain reduction by reducing the activation function’s gain, (4) WM neuron bias shifts by increasing neuron bias and inducing an increased overall activa- tion, (5) Semantic network distortions by adding noise to word representations in semantic memory, (6) Excessive activation semantic networks by increas- ing neuron outputs in semantic network, (7) Increased semantic priming by blurring semantic network outputs, (8) Exaggerated prediction-error signaling (hyperlearning) by increasing back-propagation learning rates.",
"The resulting network behaviors were compared to empirical results using a goodness-of-ﬁt measure (GOF), which compared factors such as story recall suc- cess (successfully retelling story), agent confusions (switching of certain story characters), lexical errors and derailed clauses (false interpretation of certain sentences).",
The authors concluded that (1) WM disconnections with prun- ing and (8) hyperlearning best explain real-world data.,
These results for WM disconnections further reinforce the previously presented theory by Hoﬀ- man and McGlashan in [54] that excessive connection pruning during human’s adolescence might be one of the causes for this disorder.,
"Moreover, the authors also suggested that over-learning in schizophrenic brains might cause modiﬁca- tions in previously stored memories, which might lead to delusional or erroneous convictions.",
Bayesian approaches Several important models of psychiatric disorders are based on the idea that the brain uses Bayesian inference as a basic principle.,
The Bayesian brain hy- pothesis describes the human brain as a generative model of the world that makes predictions about its environment and adapts its internal model depend- ing on the observation provided by the senses.,
For SZ as well as for ASD it is suggested that patients might diﬀer in the way they combine sensory inputs with prior information.,
The idea was highly inﬂuenced by Hermann Helmholtz’s work in experimental psychology [109] that dealt with the brain’s capacity to process ambiguous sensory information.,
In his words: “Visual perception is mediated by unconscious inferences”.,
(a) Arcimboldo’s painting (b) Tacher’s illusion (c) Ocampo’s painting (d) Dallenbach’s illusion Fig.,
8: Visual illusions where the brain infers diﬀerent interpretations depending on the prior information or context.,
(a) Ortaggi in una ciotola o l’Ortolano.,
G. c⃝Arcimboldo 1590.,
(b) Tacher illusion [110].,
"(c) Forever Allways, c⃝Octavio Ocampo 1976.",
(d) Dallenbach’s illusion 1952 [111].,
8 shows puzzle images that stress that perception depends on prior knowledge as well as sensory input.,
"For instance, if we rotate Arcimboldo’s painting by 180 degree instead of vegetables we will see a human face with a hat.",
"Tacher’s illusion can be broken by also rotating the upside down images, and we will see that both faces are diﬀerent.",
"In particular, mouth and eyes are inverted.",
"In Ocampo’s painting, we can see two old people from a larger distance but two mariachis when viewing the picture from close range.",
"Finally, Dallenbach’s illusion shows that even if you know that there is an animal looking at you in the picture, it is impossible to see it until the shape of the cow is highlighted.",
Afterwards you cannot stop seeing it.,
"In essence, what we perceive not only depends on the raw sensory information, but also on our prior knowledge and predictions we have about the world.",
The classical concept of Bayesian inference presents perception as computing the posterior belief from the sensory input (likelihood) and from the model prediction (prior belief) depending on their relevance.,
"For instance, in the case of a very imprecise (highly variable) prior, the perception would shift more strongly to the direction of the sensory input.",
9 illustrates these concepts assuming that the world is one-dimensional and can be described via Gaussian distributions.,
9: Illustration of Bayesian inference: The posterior belief is generated by inference of prior belief and sensory evidence.,
"Depending on the variance (precision) of prior and sensory evidence, the posterior belief will be inﬂuenced more by one of the previous.",
Adapted from [86].,
Free-energy model of schizophrenia Friston’s free-energy model [112] describes the brain functionality as a dy- namical inference network.,
It combined the Helmholtz machine ideas [113] with the hierarchical prediction error message passing [114] and the Bayesian math- ematical framework.,
"Despite not being implemented as an ANN model, we included it in this review because it is considered one of the most relevant mod- els in the computational neuroscience community.",
"Furthermore, it serves for comparative purposes with predictive coding neural network implementations of psychiatric disorders [115, 57, 80].",
"Under the free-energy principle, the brain is seen as a prediction machine that progressively constructs an internal model of the world which is constantly 21 improved, based on the received sensory feedback and the resulting prediction error.",
Perception (posterior belief) then results from combining the brain’s predictions (prior) with the sensory evidence (likelihood) as shown in Fig.,
"If the prior’s precision is relatively higher than the precision of sensory evidence, the posterior will be more similar to the prior.",
"In the opposite case, the posterior will be more close to sensory input.",
"Therefore, precision weights the inﬂuence of prior and sensory evidence on the posterior belief.",
"Mathematically, the internal model is updated by minimizing the negative free energy F a lower bound on the KL-divergence that quantiﬁes the diﬀerence between the internal belief about the world and reality.",
"Assuming that ⃗µ are the dynamical internal states of the brain, perception is then described as the adaption of ⃗µ given the sensory observations by minimizing the free energy using the gradient descent method described in Eq.",
"(8): ˙⃗µ(t) = D⃗µ(t) −∂F(⃗s, ⃗µ) ∂⃗µ = D⃗µ(t) −∂ϵ ∂⃗µΠϵ (8) where D is a diﬀerential matrix operator that computes the currently expected hidden state, ϵ is the error between the predicted (sensory) input from the higher layer and the real input (observation) and Π is the inverse variance (precision) of the information.",
"For instance, in humans, visual information would typically have higher precision than proprioceptive sensing for body localization [116].",
"Based on these concepts, Adams et al.",
"[86] built a computational model of SZ and analysed in three diﬀerent experiments: auditory pattern recognition (using the example of a bird recognizing its own song), a object eye-tracking task and a simulation of force-matching illusion.",
"One of the core ideas was that a reduction of the precision at higher levels of the cortical hierarchy (i.e., reduced precision of prior beliefs) inﬂuenced the responses of the model.",
"More concretely, decreases in prior precision (or, for the force-matching illusion, failure to reduce sensory precision) led to struggles in auditory pattern recognition, problems with eye-tracking with occlusion and attribution of agency.",
"Furthermore, with an additional compensatory decrease of sensory precision (for the force-matching illusion, increase of prior precision), the model showed hallucination-like behav- ior during the auditory pattern recognition task and diﬃculties to distinguish self-touch and touch from others in the force-matching illusion.",
"10 shows the experiment of auditory pattern recognition of a birdsong, showing how the precision in diﬀerent cortical levels changes the response to sur- prising events.",
The ﬁrst row describes a normal behavior to surprising events (the belief precision is high).,
"In this case, when a chirp of the bird is omitted, the posterior perception contains an illusory (weakened) response at the point in the signal where sensory input is missing (white arrow at left plot).",
This eﬀect might correspond to omission-related responses found in electrophysiolog- ical recordings of the brain [117].,
"The middle and bottom rows correspond to abnormal behaviours in line with SZ ﬁndings, such as attenuation of omission- related responses and auditory hallucinations respectively.",
"10: Prediction sonograms of the auditory signal of a birdsong (left), prediction error with respect to stimulus (middle) and used model (right), when last three chirps are omitted.",
"Top row: Unmodiﬁed model generates prediction error increases with the ﬁrst missing chirp, which corresponds to normal behavior.",
"Middle row: With reduced precision at second level the model is unable to predict the third chirp, and the prediction error for missing chirps is reduced.",
"Bottom row: With compensatory sensory precision reduction in ﬁrst level, there is a complete failure of perceptual inference.",
"Despite the wrong predictions, almost no prediction error is generated due to missing precise sensory information.",
This behavior is compared to auditory hallucinations.,
Reprinted from [86] with kind permission.,
"Circular inference in Bayesian graphical models In [71], Jardri and Den´eve investigated how excitatory to inhibitory imbal- ance may relate to psychotic symptoms in schizophrenia, using belief propaga- tion in a hierarchical Bayesian graphical model.",
"In particular, it is shown that a dominance of excitation causes circular belief propagation: bottom-up sensory information and top-down predictions are reverberated, and therefore, may be confused with each other or taken into account multiple times.",
The model can account for the occurrance of erroneous percepts (hallucinations) and ﬁxed false beliefs (delusions) in SZ.,
"In the graphical model, low hierarchical levels correspond to sensory expe- rience and high levels to top-down predictions.",
Messages are passed between nodes in diﬀerent hierarchical levels from lower to higher levels (bottom-up pro- cessing) and from higher to lower levels (top-down processing).,
The fact that connections exist in both directions raises an important challenge: to diﬀer- entiate between real sensory information and sensory information which were simply inferred from top-down expectations.,
The authors suggest that such cir- cular belief propagation in the Bayesian network is avoided if a careful balance between excitation and inhibition is maintained.,
A disruption of this balance can account for the appearance of schizophrenic symptoms.,
"Concretely, information between higher and lower levels are exchanged in the form of messages.",
"For belief propagation, messages are passed recursively until convergence: M n+1 ji = ( Wij(Bn i −αdM n ij) if i is above j Wij(Bn i −αcM n ij) if j is above i, (9) where the term above means that the node i is in a higher hierarchical level than j. M n ij is the message sent from i to j at step n. Wij is the connection strength, and αd and αc are the parameters that scale the inhibitory loops in upward and downward direction, respectively.",
Bn i is the computed belief expressed as a log-odd ratio4 and updated as: Bn+1 i = X i M n+1 ji .,
"(10) The authors experimented with the two α parameters in this framework, adjusting them between 1 (normal level of inhibition) and 0 (no inhibition).",
Simulated results show that equally impaired loops (same α below 1) are still able to arrive at a proper inference.,
"Conversely, with unbalanced impaired upward loops (αu < 1) “over-estimation of the strength of sensory evidence and an underweighting of the prior” is produced.",
This is compatible with over- interpretation of sensory evidence and the reduced inﬂuence to illusions observed in schizophrenic patients.,
"4Log-odd ratio: computed as the log of the ration between the probability that a cause is present and that the cause is absent, thus, values around 0 describe uncertain states, positive values correspond to belief in presence, negative values to belief in absence.",
24 The authors recently demonstrated in [72] that the circular inference model nicely ﬁts decisions of SZ diagnosed patients using the Fisher task as the ex- perimental paradigm.,
The Fisher task permits the manipulation of the prior and the likelihood allowing comparisons with the Bayesian model predictions.,
Participants have to decide whether the ﬁsh captured comes from the left or the right lake.,
"First, two boxes (left, right) with ﬁsh and diﬀerent sizes are pre- sented (prior): bigger box express higher probability.",
"Secondly, the two lakes (left, right) are presented with ﬁshes inside with two colors (red and black).",
The proportion of red ﬁshes represent the likelihood of the observation.,
"Fi- nally, participants have to decide if the red ﬁsh comes from the left or the right.",
"According to the participant’s data and their proposed model, descending and ascending loops correlated with negative and positive SZ symptoms respectively.",
"Recurrent neural networks In 2012, Yamashita and Tani presented a model of SZ using a recurrent neural network (RNN) [115] such as they are commonly used for the recognition and generation of time series.",
"Speciﬁcally, in this study, the RNN is applied to the task of sensorimotor sequence learning in a humanoid robot: the robot learns to predict visual information and own motor movements in a scenario where it moves a cube on a surface.",
"The type of RNN they used is the Multiple Timescale Recurrent Neural Net- work (MTRNN), a special type of RNN that mimics the hierarchical structure of biological motor control systems.",
Human and animal motor movements are commonly suggested to be segmented into so-called “primitives” [118].,
These primitives can then be reused and combined to more complex motor sequences.,
"The MTRNN contains neurons working at diﬀerent timescales: fast context neurons (corresponding to the lower level of the hierarchy) learn the motion primitives and slow context units (corresponding to higher, more abstract lev- els) control the sequence of the primitives (see Fig.",
"This network is trained to perform prediction error minimization, i.e., to build an internal model of the world following the Bayesian brain idea.",
"Training the network using the Backpropagation Through Time algorithm (BPTT), the robot learns multiple motions (grasping and moving an object) adapting to diﬀerent object positions.",
It is also able to combine these actions into new action sequences by only train- ing the slow context units.,
The trained network works as a predictor where the sensory input modulates the changes on the slow context units (goals) depending on the error5.,
"Equation 11 describes the dynamics of each neuron at each layer: τ ˙ui,t = −ui,j + X j wi,j · xj,t.",
(11) 5There is a strong parallelism between Multiple Timescale RNNs and the hierarchical model proposed by Friston.,
"11: (A) Tasks to be performed by the robot: when the object is on the Right move the object backward and forward, when the object is on the Left move the object up and down.",
(B) MTRNN network architecture.,
Highlighted with a red ellipse are the connections between fast and slow context units that are degraded with noise to imitate schizophrenic behavior.,
Adapted from [57].,
"In this formula, the membrane potential ui,t of neuron i at time step t is updated with the neural state xj,t of neuron j scaled with the (learnable) connection weights wi,j.",
The time constant τ determines the update frequency of the neuron.,
"A small time constant is used for fast context units, and a large time constant for slow context units.",
"Schizophrenics can have trouble to distinguish self-generated actions from others’ actions and, in severe cases of SZ, patients can even have problems per- forming movements, and show repetitive or stereotypical behavior [33].",
"Based on observations that suggest that SZ may be caused by disconnections in hierar- chical brain regions, mainly between prefrontal and posterior regions [58, 119], uniformly distributed random noise was added in the connections be- tween fast and slow context units highlighted with the red circle in Fig.",
For the evaluation of the model a humanoid robot was used.,
"It had the task of locating an object on a table in front of it and performed diﬀerent actions depending on the object’s position: if the object was located to the right, the robot was supposed to grab the object and move it back and forth three times.",
"Otherwise, if the object was located to the left, the robot had to grab the object and move it up and down three times.",
They showed that for a small degree of disconnection (small noise addi- tion) the robot had no problems to perform the mentioned task.,
"Nevertheless, increases of spontaneous prediction error were observed and abnormal state switching appeared in the intention-network (slow units).",
The authors compared these prediction errors to patient’s problems in attribution of agency (when own movements are perceived as being executed by someone else).,
Schizophrenics 26 might want to perform an action and have an internal prediction of the upcom- ing proprioceptive and external states.,
"The increases of prediction error could be seen as incongruences between the intended actions and the results, which can give a person the feeling of not being able to control the consequences of its own actions or it may have problems to perceive these actions as self-generated.",
"For more severe disconnections, the humanoid robot clearly struggled to perform the given task and showed disorganized sequences of movements.",
"These obser- vations were compared to more severe cases of SZ, where cataleptic (stopping) and stereotypical (repetitive) behaviors have been observed.",
ANN models of autistic spectrum disorder This section describes the most important ANN models of ASD.,
They fo- cused on the atypical processing style suggested by the weak central coherence theory which could be summarized as excessive attention to detail.,
"They repli- cated deﬁcits in perception [83, 81, 70, 69].",
"Some also addressed atypicalities in memory structure and internal representations [120, 80] and inﬂexibility in mo- tor behavior [79].",
"Although most studies suggested connections to social deﬁcits in an indirect way, only one of the models made a direct connection to theory of mind, by modeling weak central coherence on the level of logical reasoning [121].",
An overview of the reviewed approaches is given in Tab.,
"2: Overview of neural network models of ASD Model type Paper Disorder Characteristic Biological Evidence Approach Feed-forward and simple recurrent NNs I. L. Cohen [83, 122] (1994, 1998) Generalization deﬁcits due to ex- cessive attention to detail Abnormal neural density in var- ious brain regions Excessive or reduced number of neurons, increased training dura- tion J. L. McClelland (2000) [120] Hyperspeciﬁcity of memory con- cepts – Excessive conjunctive coding Dovgopoly & Mercado [81] (2013) Deﬁcits in visual categorization and generalization Abnormalities in synaptic plas- ticity Reduced learning rate, neg- ative weight decay (anti- regularization) Self-Organizing Maps L. Gustafsson (1997) [70] Excessive attention to detail Lateral inhibition enhances sen- sory perception Excessive inhibitory lateral feed- back L. Gustafsson et al.",
"(2004) [123] Avoidance of novelty – Familiarity preference, higher weighting of close data points G. Noriega (2007) [124] Domain-based hypersensitivity Early brain overgrowth in chil- dren with ASD Variable (increasing) number of neurons, stronger/weaker atten- tion to stimuli G. Noriega (2008) [125] Domain-based hypersensitivity Early brain overgrowth in chil- dren with ASD Propagation delays in neural weight updates Convolutional NN Y. Nagai et al.",
(2015) [69] Local/global processing bias Excitation/inhibition imbalance Excitation/inhibition imbalance in visual processing Spiking NNs J.,
Park et al.,
"(2019) [59] Atypical neural activity: High power in higher frequency bands and decreased signal complexity Increased short-range connectiv- ity in frontal cortex and atypical- ities in resting-state EEG Local over-connectivity Predictive coding Pellicano & Burr (2012) [74] Excessive attention to detail – Hypo-prior: lower precision of prior, stronger focus on sensory input Lawson et al.",
(2014) [75] Excessive attention to detail Stronger activation in visual cor- tex than in prefrontal cortex in ASD Hypo-prior or hyper sensory in- put: Precision imbalance that leads to excessive reliance on in- put Recurrent NNs H. Idei et al.,
"(2017) [79] Stereotypical behaviors – Modiﬁcation of variance estima- tion (sensory precision) Philippsen & Nagai (2018) [80] Reduced generalization capabil- ity, heterogeneity among sub- jects – Modiﬁcation of reliance on exter- nal signal and of variance estima- tion (sensory precision) Ahmadi & Tani (2017) [84] Generalization deﬁcits – Regularization Other approaches O’Loughlin and Thagard (2000) [121] Weak coherence, Theory of Mind impairment – Impairment of coherence opti- mization in logical reasoning due to strong inhibition 5.1.",
"Feed-forward and simple recurrent neural networks First, we describe approaches using simple connectionist models, typically feed-forward networks for classiﬁcation tasks.",
"Recurrent connections might be 27 included at a structural level, but networks are not supposed to learn temporal sequences, which is why we refer to them as simple recurrent NN.",
These ap- proaches mainly explored parameters of the network such as number of neurons or learning rate.,
Generalization deﬁcits through overﬁtting The ﬁrst neural network model of ASD to our knowledge was proposed by Ira L. Cohen in 1994 [83].,
It was a feed-forward neural network trained with back-propagation and investigated basic properties of neural networks.,
"Based on studies that suggested that individuals with autism have either too few or too many neurons and neuronal connections (e.g., [126]), the inﬂuence of increased or reduced number of hidden neurons was analyzed.",
"The evaluated task was to classify children with ASD and children with mental retardation into two groups, using features obtained via a diagnostic interview [127].",
"Note that although the considered task was related to ASD, the chosen task is just taken as an example and is not crucial for the ﬁndings of this paper.",
A training and a test set were used to analyze the network’s accuracy and generalization abilities.,
The results were compared for an increasing number of hidden units and through diﬀerent number of trials.,
The results showed that a small number of hidden neurons translates into low accuracy (high training error) and bad generalization (high testing error) and an increased number of hidden neurons improved the network’s learning accuracy and generalization.,
"When the number of hidden neurons was largely increased, its general- ization ability decreased: the network learned too much details of the input data and was not able to adapt to new input data.",
An increased number of train- ing trials (longer training duration) had a similar eﬀect.,
"For the training set, the network accuracy increased with longer training duration.",
"However, with the test set, the network again showed signs of overﬁtting, as the accuracy decreased signiﬁcantly.",
Cohen compared these results qualitatively to the learning and behavioral characteristics of children with ASD.,
"In particular, many individuals with ASD show great discrimination capabilities and have no problems with already learned routines, but have problems when trying to abstract information or when con- fronted with new situations.",
Cohen extended this approach in 1998 [122] to the generalization capability in the presence of extraneous inputs to the network (set to random values).,
"In the task of classifying happy and sad expressions of a simpliﬁed cartoon face, generalization was strongly impaired in the presence of extraneous inputs.",
"This might suggest that networks trained for too long tend to attend more to non- relevant input information, instead of focusing on the more informative input neurons.",
"Note that although increased number of hidden neurons may replicate autis- tic traits as shown in [83], this parameter did not cause generalization deﬁcits neither in Cohen’s follow-up work [122] nor in a similar modeling study [81] (see discussion on p. 31).",
"Precision of memory representations In [120], James L. McClelland addressed the tendency of children with ASD to represent concepts in a too speciﬁc way, which results in diﬃculties to recog- nize two diﬀerent instances of an object as the same category.",
"He suggested that in neural networks, this could be explained with the con- cept of excessive conjunctive coding.",
"Typically, similar inputs to a neural net- work lead to similar neuron activation patterns.",
Such pattern overlaps can be useful for sharing existing knowledge and establishing associations.,
"However, too strong associations can also cause interference.",
Conjunctive coding describes the reduction of such overlap by recoding the input patterns with neurons which only become active for particular combinations of elements.,
"Assuming that what characterizes healthy human learning is a balance between generalization and discrimination, the representation of concepts in subjects with ASD could be characterized by excessive conjunctive coding.",
"This would make a neural network loose the ability to generalize, as activation pattern overlaps cannot be exploited.",
"This idea was not tested experimentally, but the author used the neural network shown in Fig.",
12 to explain his reasoning.,
"McClelland presented the example of a semantic network used in [128], as a model of organization of knowledge in memory (see Fig.",
"This model was used to associate words with their meaning, e.g., “robin” and “can” trigger the outputs “grow”, “move” and “ﬂy” because these are the actions a “robin” can perform.",
The internal layer of the network (highlighted in red in Fig.,
12) progressively learns to code the meaning of input words during learning.,
"This means that “robin” and “ca- nary” should cause a very similar activation pattern because a robin has much more in common with a canary than, for instance, a tree.",
The author suggests that hyperspeciﬁcity in perception and memory representations of ASD children might be caused by an abnormality during this process.,
"Namely, excessive con- junctive coding in the internal layer is proposed as a mechanism: an excessive reduction of overlap between representations of similar concept might cause the reported hyperspeciﬁcity which would result in generalization deﬁcits.",
"No con- crete network parameters are proposed, but it can be imagined that such an eﬀect might be achieved by increasing the number of neurons in the internal layer.",
"In this regard, the approach is similar to Cohen’s suggestion [83], but extended to learning of representations.",
Generalization and categorization abilities in visual perception Dovgopoly and Mercado [81] used an existing model of visual object per- ception [129] to replicate deﬁcits in classiﬁcation and generalization in ASD.,
"The neural network was a feed-forward network, which modeled visual input processing via two pathways: the ventral cortical pathway (for object identiﬁ- cation, including recurrent connections), and the dorsal cortical pathway (for processing of location-relevant information).",
"The authors replicated behavioral data from [130] and [131], separately on both visual pathways, which show deﬁcits in generalization and prototoype for- mation in children with high-functioning ASD.",
The experiment was the clas- 29 Fig.,
12: Semantic network used to explain the conjunctive coding hypothesis.,
"In the hid- den layers, the feed-forward neural network generates internal representations of the inputs (highlighted in red).",
Words describing similar concepts should produce similar internal repre- sentations that overlap with each other.,
"The author suggests that excessive conjunctive coding to avoid these overlaps could produce excessive discrimination, such as in autistic perception.",
Adapted from [120].,
"siﬁcation of random dot patterns as category or non-category stimuli [130], or as category A or category B stimuli [131].",
"After adjusting the parameters for replicating typical behavior, four diﬀerent parameter modiﬁcations were tested individually to replicate the data from ASD children.",
"Following evidence for abnormalities in synaptic plasticity in individuals with ASD (e.g., [132, 133]), the ﬁrst two parameters modiﬁed how weights in the network were updated.",
"First, the learning rate was decreased, which corresponds to reduced synaptic plasticity in biological neurons.",
"As a result, network training takes longer and is more prone to lead to exhibit overﬁtting.",
"Second, generaliza- tion of the network was impaired by suppressing regularization using negative weight decay.",
Weight decay is a method for regularizing neural net- works and improving their generalization abilities by keeping the connection weights small [134].,
"Typically, weight decay punishes large weights by adding a term λ⃗w′ ⃗w to the error function.",
"With a negative weight decay factor λ instead, anti-regularization is performed, encouraging the increase of weight magnitudes, and thus, over-complex classiﬁcation rules.",
"Third, they tested the inﬂuence of increasing and decreasing the number of hidden neurons similar to [83, 122], based on neurological evidence of an increased number of cortical minicolumns in the brain of individuals with ASD [135].",
"Finally, the authors adjusted the gain of the neuron’s activation function, to model the increased level of noise that is hypothesized to underlie the relative 30 increase in cortical excitation observed in ASD subjects [62, 67].",
"The gain G of the activation function, as displayed in Eq.",
"(12), manipulates the slope of the activation function.",
"A smaller gain reduces the slope, and makes the network more prone to pass noise instead of signal information to the next processing layers: s(x) = 1 1 + exp(−(G · x + b)) (12) where x represents the input to the activation function and b is a bias term.",
Good replications of the behavioral data were achieved with a decrease of learning rate and a negative weight decay.,
"A negative weight decay also caused a high variability of generalization abilities, depending on the initial network weights, providing a potential explanation for the heterogeneity of ﬁndings be- tween diﬀerent studies.",
The gain of the activation function could not fully account for the generalization deﬁcit.,
"Also an increased number of neurons did not replicate the generalization deﬁcit in ASD children, which contradicts pre- vious ﬁndings from [83].",
"In fact, an increased number of hidden units seems to lead to generalization problems only under certain training circumstances [136], indicating that it is not a good candidate for explaining generalization diﬃculties in general.",
Self-organizing maps Self-organizing maps (SOMs) are ANNs that are usually used for unsuper- vised learning and clustering tasks.,
"They model the functionality of cortical feature maps, which are spatially organized neurons that respond to stimuli and self-organize according to the features in stimuli.",
They are able to learn the relation of diﬀerent input data such as diﬀerent sensory inputs.,
Approaches for modeling ASD with SOMs typically investigate the formation of higher-level representations from sensory input.,
Increased lateral feedback inhibition Lennart Gustafsson presented two models of ASD using SOMs in [70] and [123].,
"Inspired by ﬁndings on weak central coherence in subjects with ASD and an enhanced ability to discriminate sensory stimuli [37], he suggested that alterations in the lateral feedback weights between the SOM neurons could result in atypicalities in perception [137].",
"In a SOM, each neuron typically has excitatory connections to close neigh- bors and inhibitory connections to more distant neighbors.",
They tuned the Mexican-hat curve (Fig.,
13) to induce stronger lateral feedback inhibition.,
Such activation patterns are similar to receptive ﬁelds in biological cortices and have been used to model center-surround operators in the visual cortex.,
Manipulating the lateral connections to achieve a stronger inhibition (such that the integral of the function in Fig.,
"13 becomes negative), the sensory discrimination ability of the network is increased.",
Neural columns focus on more narrow features during learning which slows down convergence and might lead to a fragmented feature 31 Fig.,
13: Mexican-hat function of the SOM.,
It deﬁnes the strength of lateral connections depending on distance to current neuron.,
The red arrows point to the part that is modiﬁed to simulate autistic perception (excessive lateral feedback inhibition).,
Adapted from [70].,
"However, excessive lateral inhibition will degrade discriminatory power and cause instabilities in information processing.",
"This behavior is compared to autistic over-discrimination and may also explain fascination or fright of moving objects, due to the instability of its cortical feature maps.",
"Familiarity preference In [123], Gustafsson and Papli´nski evaluated the eﬀect of attention-shift im- pairment and avoidance of novelty on the formation of cortical feature maps.",
"The used SOM received input stimuli from two sources (compared to two “di- alects of a language”), each of which produces 30 diﬀerent stimuli (“speech sounds”) grouped in three clusters (“phonemes”).",
The computational model was run in four diﬀerent modes.,
"In the ﬁrst mode, attention was always shifted to the source producing novel input (considered as normal learning).",
"In the second mode, an attention-shift impairment was mod- eled by shifting attention to novel sources with a very low probability.",
The third mode implements familiarity preference: attention is shifted to novel sources only if the map is familiar with that source (measured as mean distance of the current stimulus to the map nodes).,
This map develops a preference over learning to the more familiar source.,
"Finally, a model with both familiarity preference and attention-shift impairment was applied.",
The simulation results showed that familiarity preference leads to precise learning of the stimuli from one of the sources (the source with lower variability) in expense of the other source.,
"This might remind of ASD individuals’ character- istic of learning in great detail a narrow ﬁeld, which leads to increased discrim- ination and poor generalization.",
"The authors also showed that this impairment can be counteracted by modifying the probabilities of stimuli presentation in re- 32 sponse to the system, similar to early intervention in children’s learning process.",
"Maps learned with attention-shift were not impaired, whereas a combination of both mechanisms only sometimes led to an impairment.",
"The authors concluded that, in contrast to speculations in previous work [138], familiarity preference, rather than attention-shift is a more likely cause for ASD.",
"Unfolding of feature maps and stimuli coverage In 2007, Gerardo Noriega [124] modeled abnormalities in the feature coverage and the unfolding of feature maps in SOMs.",
"Neurological evidence suggests ab- normal brain development in children with ASD [139], typically reporting larger growth in young children, which gets reduced later in life [140, 141].",
These ab- normalities were modeled by manipulating the number of network nodes during the training of the SOM where the structure emerges.,
"Thus, the network di- mension is temporarily increased.",
"Results showed that such disturbance in the physical structure of a SOM does not aﬀect stimuli coverage, but impairs the unfolding of feature maps which might result in sub-optimal representations.",
"Furthermore, the author models hyper- and hyposensitivity to stimuli in a similar way like [70] using lateral interactions between neurons.",
"Hyper- or hyposensitivity was mod- eled by adjusting the neuron weights toward the winner neuron, either with a positive factor (attraction, or hypersensitivity) or with a negative factor (re- pulsion, or hyposensitivity).",
This factor converges exponentially toward zero (normal sensitivity) during map formation.,
"The authors showed that hyper- sensitivity to one of the input domains (stronger attention to this domain, i.e., restricted interests), improves the coverage of stimuli in this domain, but too strong hypersensitivity or a hyposensitivity to stimuli reduces coverage 6.",
"One year later, Noriega extended his approach in [125], investigating prop- agation delays between neurons.",
"Unlike in normal SOMs where all neurons propagate the information instantaneously to all neighboring neurons, Noriega presented a biologically more realistic approach by introducing delays in the up- date.",
He shows that decreased propagation speed has a negative eﬀect on stimuli coverage.,
"As the delayed propagation causes the arrival of competing stimuli at the same time at a neuron, he also altered the way in which these competing stimuli are handled.",
"In his experiments, a high dilution factor, meaning that incoming stimuli are averaged instead of being handled separately, decreased the stimuli coverage and also impaired the topological structure of the map.",
"Convolutional neural networks and inhibition imbalance In 2015, Y. Nagai and colleagues presented an ANN network based on Fukushima’s neocognitron ([142], [143], [144]), seen as the basis for convolu- tional neural networks, to model visual processing in ASD [69].",
"The hypoth- 6Hypersensitivity in [70] was implemented as increased inhibition in the neighborhood of neurons (higher speciﬁcity of perception), whereas this approach interprets hypersensitivity as a stronger attraction of neighboring signals to signals from a speciﬁc domain.",
"33 esis considered was that there is an excitation/inhibition imbalance in ASD [63, 64, 67].",
The structure of the neocognitron for visual processing is illustrated in Fig.,
The network is trained to recognize patterns by adjusting the weights between UC and US layers.,
The S-cells in the US layers perform feature extrac- tion.,
"They receive excitatory input from the C-cells in the preceding layer, and inhibitory connections from the V-cells in the same layer.",
"During training, the excitatory connections aSl are updated and the inhibitory connections bSl are calculated accordingly.",
The network was trained for the recognition of numbers “0” to “9” in large or small size at diﬀerent positions.,
"After training, the model was tested with compound numbers (cf.",
15 left) where a larger number is created from multiple smaller numbers.,
"The trained network is able to detect both global (large number, here “2”) and local (small numbers, here “3”) patterns for α = 1 and 0.9, but shows a preference for the global pattern, characteristics that correspond to observations with healthy individuals [145].",
14: Left: Overview of the neocognitron’s structure.,
Right: Detailed view of the connec- tions between C-cell layers UC and S-cell layers US.,
Highlighted in red are the inhibitory con- nections that are modiﬁed to inﬂuence the ratio between inhibition and excitation.,
Adapted from [69].,
"It is known that people with ASD perform diﬀerently in such a task, primar- ily focusing their attention on the details (i.e., the smaller number instead of the larger one).",
"In order to simulate this local processing bias, an imbalance of excitatory and inhibitory connections was simulated by scaling the inhibitory weight bSl with a factor α.",
"The results show that a moderate increase of α, which corresponds to in- creasing inhibition, causes the network to rather detect local patterns, replicat- ing the local processing bias in ASD.",
"When reducing α (increasing excitation), the network does not show any processing bias, rather it looses its ability to diﬀerentiate patterns.",
These results ﬁt with ASD symptoms of hyperesthesia (increased focus on detail) and hypoesthesia (no bias and general diﬃculty in pattern recognition) and suggest that excitation/inhibition imbalance could ac- count for these symptoms.,
"15: The neocognitron is fed with a visual stimulus consisting of local patterns (here 3) and global patterns (here 2), which are incongruent.",
In normal conditions the network should be able to detect both local and global patterns.,
"Spiking neural networks and local over-connectivity In [60] and a follow-up study in [59], it was proposed to use spiking neural network as computational models to investigate the consequences of local over- connectivity, which was found in the prefrontal cortex of ASD brains [61].",
The hypothesis considered was that local over-connectivity aﬀects frequency patterns of neural activations.,
A spiking neural network is more closely inspired by natural neural networks [146].,
"Whereas in standard artiﬁcial neural networks each neuron ﬁres at every time step, neurons in a spiking network only ﬁre if their potential (similar to the membrane potential of biological neurons) exceeds a certain threshold.",
"There- fore, more complex ﬁring patterns can occur ranging over various frequency bands, comparable to patterns visible in EEG7.",
A number of studies found evidence that EEG signals of ASD brains tend to exhibit higher power in low-frequency and high-frequency bands of EEG [147] and that EEG resting-state activity has lower complexity [148].,
"The authors suggest that these atypical EEG data might be explained by diﬀerences in how ASD brains, as opposed to TD brains, are connected.",
"In particular, it has been found that the brains of people with ASD have an increased local connectivity, especially in the frontal cortex [61].",
The authors investigated this hypothesis with a spiking neural network by modifying the network’s connection patterns and observing how the connec- tivity aﬀected the emerged activation patterns.,
"To manipulate the degree of local over-connectivity in the network, a parameter based on the small- world paradigm from [149] was used.",
"By default, neurons are connected to six neighboring neurons in a ring lattice as displayed in Fig.",
A pa- rameter pW S expresses the probability for each of the connections to rewire to other neurons.,
"Thus, pW S determines the randomness of the network (Fig.",
"16), ranging from regular lattice structure (pW S = 0) to random wiring (pW S = 1).",
7EEG: Electroencephalography 35 Fig.,
16: Three diﬀerent networks with diﬀerent degrees of randomness.,
"(a) is a locally over- connected network (corresponding to ASD individuals), (b) is a small-world network with many local clusters and a few longer connections (corresponding to typically developed in- dividuals), (c) is a random network including many wide-range connections.",
(d) shows the structure of each single neuron group with excitatory (red) and inhibitory (blue) connections.,
"Note that the number of nodes and edges in (a), (b) and (c) remains the same.",
"Reprinted with permission from [59], originally based on [149].",
Medium values of pW S around 0.2 describe “typically developed networks” with local clusters and some short-range connections between the clusters.,
"Notably, the parameter from [149] keeps the overall number of connections in the net- work intact, such that diﬀerences emerge only due to diﬀerences in the network structure, not by the total number of neurons or neural connections.",
"Networks are formed by generating 100 groups of neurons, corresponding to the black nodes in Fig.",
"Each group contains 1000 spiking neurons: 800 excitatory and 200 inhibitory neurons, which have an increasing or decreasing eﬀect on the ﬁring probability of postsynaptic neurons, respectively.",
"Neurons are mainly connected to neurons of the same neuron group (intra-group con- nections), and have connections to six neighboring groups according to Fig.",
16 (inter-group connections).,
Diﬀerent rewiring probabilities pW S between 0 and 1 are used to determine the initial inter-group connectivity of the network.,
"After initialization, the network updates its connections according to the rules of spike-time-dependent plasticity [150]: the update of connection weights occurs depending on the timing of ﬁring of the pre- and postsynaptic neurons.",
"If the postsynaptic neuron ﬁres within a certain time window after the presynaptic neuron, the weight of the connection is increased (corresponding to the biological process of long term potentiation).",
"If the presynaptic neuron ﬁres within a time window after the postsynaptic neuron, the connection weight is weakened (long term depression).",
During this learning period the connection weights self- organize.,
Tonic random input is presented to the network.,
"After learning, the spontaneous activity of the neurons was recorded (in the absence of input), and compared to the graph-theoretical properties of the network.",
"The activation patterns were evaluated according to their frequency spec- trum and the complexity of the time series, as measured by the multiscale en- tropy [151].",
This measure rates the informative content of time series at diﬀerent 36 temporal scales.,
"High complexity corresponds to the presence of long-range cor- relations on multiple scales in space and time, low complexity is computed for time-series with perfect regularity or randomness.",
The evaluation suggested that networks exhibiting local over-connectivity generate more oscillations in high-frequency bands and exhibit lower complexity in the signals than small- world networks.,
"Findings of atypical resting-state EEG for people with ASD, thus, might be explained by local over-connectivity in their brains.",
"Bayesian approaches There are promising models in the literature interpreting ASD on the ba- sis of the Bayesian framework (for an introduction see Schizophrenia section, p. 20).",
"However, most of these approaches are only conceptual and still lack an implementation.",
"Nevertheless, these approaches are able to explain a wide range of diﬀerent symptoms which might be caused by an atypical integration of prediction and sensory information [75, 74].",
The ﬁrst approach utilizing the Bayesian brain hypothesis for explaining the non-social symptoms of ASD was proposed by Pellicano and Burr in 2012 [74].,
Their hypo-prior hypothesis8 suggests that broader or less precise priors cause people with ASD to rely less on their predictions and stronger on sensory input which could explain the hypersensitivity of people with ASD.,
"J. Brock broadened this idea [153] by proposing that hypersensitivity cannot only be caused by a reduced precision of the prior, but also by an increased precision of sensory input.",
Lawson et al.,
"[75] summarized these ideas, arguing that both modiﬁcations reduced prior precision or increased sensory precision, can cause the same functional consequences.",
They suggest that the cause could be aberrant precision in general: Expected precision of a signal is an important source of information that helps us to decide whether to rely on this signal or not.,
"Aberrant precision of sensory input or prior predictions, thus, would alter the way in which we integrate these signals.",
"The precision of the signals also can be considered as a weighting term of the prediction error: For a signal that is expected to be imprecise, a prediction error does not need to be corrected while a prediction error arising between signals that are expected to be very precise would need correction.",
People with ASD might have problems to accurately estimate this precision.,
"Thus, they might, at the one extreme, try to minimize the prediction error too strongly, or, at the other extreme, fail to minimize the prediction error.",
"Finally, in [154], Lawson and colleagues suggested that subjects with ASD overestimate the volatility of the environment.",
They conducted a behavioral experiment which demonstrated that ASD subjects are less surprised when en- countering environmental changes.,
"Using Hierarchical Gaussian Filters, they 8In this article, we stick to the original deﬁnition of hypo-priors as a belief in low precision of priors and hyper-priors as a belief in high precision of priors.",
"Note, however, that due to the hierarchical structure of the brain and the role of precision as a hyperparameter for the inference process it might be more appropriate to talk of hypo-priors as attenuated hyperpriors as argued in [152].",
37 modeled the experimental ﬁndings computationally.,
The model parameter that best accounts for the diﬀerences found in ASD and neurotypical subjects was a meta-parameter which controlled learning about volatility of the environment.,
"These results suggest that ASD subjects overestimate the probability of a change in the environmental conditions, and build less stable expectations.",
"As a result, they might misinterpret an event with low probability which occurred by chance as an event that signiﬁes a change in environmental conditions.",
"Therefore, in- stead of being surprised in the case of an extraordinary event, they would be mildly surprised at all times.",
Recurrent neural networks The studies presented here follow the idea of predictive coding which can be seen as an implementation of the Bayesian brain idea: an RNN is used as an internal model of the world and its learning corresponds to the process of adapt- ing network weights in order to perform prediction error minimization.,
"The role of the network is to learn to predict sensory consequences, and integrates these predictions with the perceived sensory information.",
"Freezing and repetitive behavior in a robotics experiment Idei and colleagues [79, 155] used the stochastic continuous-time recurrent neural network (S-CTRNN) [156] model with parametric bias (PB) [157] to teach a robot to interact with a human in a ball-playing game (similar to the schizophrenia model [57]).",
The S-CTRNN with PB learns to predict a time series of proprioceptive (joint angles) and vision features.,
"From the current input, the network estimates the next time step (output) and its predicted precision (variance) as shown in Fig.",
"The state of the PB units reﬂect the intention of the network, i.e., the ball-playing pattern that the robot believes that they are currently engaged in.",
"The S-CTRNN was trained oﬄine to perform certain tasks depending on a yellow ball’s position, as depicted in Fig.",
"Synaptic weights and biases of the network, as well as the internal states of the PB units are updated via the backpropagation through time (BPTT) algorithm in order to maximize the likelihood in Eq.",
"This equation describes that at time step t of training sequence s, the network output of the i-th neuron (a normal distribution deﬁned by the estimated mean (output) y and estimated variance v) properly reﬂects the desired input data ˆy.",
"L(s) t,i = − ln (2πv(s) t,i ) 2 − (ˆy(s) t,i −y(s) t,i )2 2v(s) t,i (13) After training, a recognition mechanism (via adaptation of the PB units, while keeping weights and biases ﬁxed) enables the network to switch its behavior depending on the current situation.",
"To model ASD behavior, the estimated variance (sensory precision) is modiﬁed in the activation function of the variance units with the constant K 38 Fig.",
17: Left: Overview of the interactive tasks the robot must perform.,
Right: Overview of the ANN model used for the experiments.,
Highlighted in red are the variance units where a constant K is added to increase or decrease the sensory precision in order to imitate autistic behavior.,
Adapted from [79].,
"(14), where ϵ is the minimum value and u(s) t,i is the output of the i-th context unit time step t for movement sequence s. v(s) t,i = exp(u(s) t,i + K) + ϵ (14) Experimental results with a humanoid NAO robot showed that for K = 0 the robot behaved normally.",
"For increased variance (reduced precision), the robot seemed to ignore prediction error and performed stopping and stereotypic movements.",
"For decreased variance (increased precision), the robot performed incorrect movement changes or concentrated on certain movements, which also led to sudden freezing and repetitive movements.",
"These results ﬁt with the dis- ordered motor system reported in ASD [158], but add the surprising insight that increased and decreased sensory precision may cause the same consequences.",
Impairment in internal network representations Another study using the S-CTRNN to model ASD characteristics is [80].,
"Us- ing an S-CTRNN [156], the authors modify two parameters which control how the network makes predictions.",
"In contrast to the other RNN model which con- centrates on replicating behavioral patterns, this study investigates “invisible” features characterizing the network’s learning process.",
"More speciﬁcally, the authors evaluate how attention to sensory input and deﬁcits in the pre- diction of trajectory noise inﬂuence the internal representation that a network acquires during learning.",
"Internal representations are informative as they reﬂect the network’s generalization capabilities [159, 115]: similar input pattern should cause an overlap in the corresponding context neuron activations (attractors in the RNN), whereas diﬀerent patterns should be diﬀerentiated.",
The network as displayed in Fig.,
"18 is trained to recognize and draw ellipses and “eight” shapes, located at four diﬀerent (overlapping) positions of the input 39 Fig.",
18: The S-CTRNN used in [80] with two parameter modiﬁcations.,
Adapted from [80].,
Inputs and outputs are two-dimensional trajectories and the recurrent context layer comprises 70 neurons.,
"Learning is modiﬁed in two ways: The parameter χ determines how much the network relies on external input, as opposed to its own prediction, i.e., χ gradually switches between open- loop (χ = 1) and near-closed-loop (χ ≈0) control.",
The second parameter K is deﬁned analogous to [79] (see Eq.,
(14)) and manipulates the estimated variance such that networks with K ̸= 0 over- or underestimate noisy variations in the signal.,
"Unlike its usage in [79], this manipulation is not performed after training, but already during the training process, to account for the developmental nature of ASD.",
"After training, the network’s behavior is evaluated as the network’s ability to reproduce the trained trajectories.",
The internal representations are evaluated by collecting the time course of activations of the context layer neurons while generating the trajectories.,
A visualization of how the high-dimensional space (time steps × number of context neurons) is structured can be achieved by principal component analysis (PCA).,
The results indicated that networks tend to reuse internal representation structure for patterns located at the same position in the input space.,
Such an overlap is advantageous as similarities between patterns are coded.,
"However, too strong overlap of the context activations indicates missing diﬀerentiation between the patterns which might lead to worse diﬀerentiation in a recognition task.",
"Thus, the authors deﬁne “good” internal network representations as repre- sentations which strongly reﬂect the characteristics of the input data.",
19(b) shows an example of how task performance (top) and internal representation quality (bottom) change depending on the external contribution parameter.,
"The best internal representation quality is achieved with χ = 0.5 (moderately integrating input and predictions), as the internal representation reuses acti- vations but clearly diﬀerentiates trajectories at diﬀerent input space positions.",
"However, the performance in reproducing the trained behavior is comparable 40 between χ = 0.5 and χ = 1 (relying stronger on input).",
These qualitative observations were also quantitatively veriﬁed in the high-dimensional space of neurons.,
"How well the network is able to reproduce the learned patterns, thus, is not always reﬂected in the internal representation quality.",
"Interestingly, for the parameter χ, both extremes lead to an ASD-like im- pairment, as schematically depicted in Fig.",
Typical development could correspond to the middle.,
"Whereas the right-hand side would express high- functioning ASD where the performance in speciﬁc tasks might be intact, but representations might be too speciﬁc (overﬁtting).",
The left-hand side describes ASD with severe impairments also at a behavioral level.,
"It can be, thus, imag- ined that heterogeneity in the ASD population, comprising opposite symptoms such as hyper- and hyposensitivity, does not necessarily be caused by diﬀerent underlying mechanisms, but that a continuous modiﬁcation of parameters could account for the variability.",
"Generalization ability in a variational Bayes recurrent neural network In [84], a novel recurrent network type is introduced, the variational Bayes predictive coding RNN (VBP-RNN).",
"It diﬀers from the S-CTRNN in that vari- ance is not only coded on the output level, but also in the network’s context neurons to enhance the network’s ability to represent uncertainty in the data.",
"We do not discuss it in detail here, as this study is not focusing on modeling ASD, but on representing deterministic as well as probabilistic behavior in an RNN in a coherent way.",
The analogy to ASD is made in terms of the meta- parameter W that performs a trade-oﬀbetween reconstruction and regularization in the optimization (loss) function.,
"W switches between the typically minimized reconstruction error term (W = 0) and a regularization term that keeps the posterior distribution of the latent variables (i.e., the context units) similar to its prior.",
"If the network is trained with W = 0, it develops deterministic dynamics and exhibits poor generalization capabilities.",
"Values of W > 0 lead to more randomness in the network and improve generalization, but too high values result in a performance drop.",
"W could therefore model the spectrum of ASD: W = 0 is one extreme where the network solely relies on its top-down intentionality and fails to generalize, whereas too high values of W reﬂect performance impairment due to excessive randomness in the network.",
"As this parameter controls how much regularization is performed, the approach is similar to [81] where regularization was intention- ally impaired.",
"Other approaches In 2000, O’Loughlin and Thagard [121] used a connectionist model to sim- ulate weak coherence, and to demonstrate how a failure of maximizing global coherence can cause deﬁcits in theory of mind [35].",
"Their network model, a so-called constraint network, is hand-designed according to the task and does not strictly ﬁt into an existing network category.",
"The network performs logi- cal reasoning and consists of a set of neurons, each of which corresponds to a 41 (a) Hypothesis internal representation behavioral output (b) Experimental results Fig.",
19: Eﬀect of changing the external contribution parameter of the S-CTRNN from Fig.,
"18 on behavioral output (top) and on internal representation quality, evaluated in the two- dimenisonal principal components (PC) space (bottom).",
Adapted from [80].,
42 logical element such as a belief (expressed as a sentence).,
"Connections between them are set as excitatory and inhibitory, depending on whether two arguments support each other or are contradicting.",
"Weights remain ﬁxed, but the activa- tions of neurons get updated depending on the connections to neighboring cells which can be excitatory (positive) on inhibitory (negative).",
A decaying factor lets the network’s activation converge to a state after a certain amount of time.,
"Positive activations are then interpreted as an acceptance of this belief, negative activations as a denial.",
"The authors showed that a high level of inhibition, compared to ex- citation, causes early activated association nodes in the network to suppress concurring hypotheses.",
"The network, therefore, prefers more direct solutions, and makes wrong predictions.",
"The overall coherence of the network, deﬁned as the satisfaction of most constraints in the complete network, is not optimized, which can be considered as weak coherence.",
Discussion and future directions Artiﬁcial neural network models of SZ and ASD have been presented as a useful tool to ﬁll the gap between theoretical models and biological evidence.,
"Early works were biased by technical restrictions, but recent models are able to capture the same complexity as conceptual models, such as hierarchical Bayesian models.",
"However, designing ANN architectures that are able to predict novel ﬁndings and through computational simulations contribute to clinical applica- tions (e.g., diagnosis or therapy) remains a challenging task.",
"In this sense, the model should i) reproduce empirical behavioural ﬁndings, preferably in more than one domain, ii) be supported by a process theory in which the abnormal- ity used to reproduce empirical ﬁndings is realistic from the point of view of known neuropathology, and iii) predict novel ﬁndings.",
"Furthermore, addressing heterogeneity and non-speciﬁcity is still one of the most important challenges of these two psychiatric disorders.",
"Due to the large overlap in SZ and ASD regarding biological evidence (e.g., E/I imbalance), similar hypotheses were discussed as a potential cause for both disorders.",
"Computational models, however, still tend to focus on speciﬁc im- pairments of a speciﬁc disorder.",
"To help the community, it is crucial that over- arching neural network models are developed which connect ideas and results across diﬀerent contexts (ASD, SZ or even other mental disorders).",
"In this section, we ﬁrst discuss the quality of the discussed models in terms of how well they ﬁt and predict empirical ﬁndings (Section 6.1).",
"Secondly, we discuss the approaches from the point of view of multiﬁnality and equiﬁnality (Section 6.2).",
"Thirdly, we emphasize the importance of testing the models in an embodied system (Section 6.3).",
"Finally, we describe new promising directions to address with ANN models: developmental factors (Section 6.4.1); disorders of the self (Section 6.4.2); and state-of-the-art ANN architectures for future models of psychopathologies (Section 6.4.3).",
"Models quality: Empirical ﬁndings and predictability Early SZ modelling works from [53, 160] on Hopﬁeld networks as well as the feed-forward approaches from [19] and [54] lack the capabilities to generalize to a broader context: every experiment required a diﬀerent ANN architecture.",
"Hence, in terms of predictability of other symptoms, these approaches are not powerful enough.",
"In particular, the work on auditory hallucinations [54] is far from replicating the brain mechanism and does not account for deﬁcits in distin- guishing self-produced sounds observed in SZ patients.",
"However, the underlying discussion presented in those papers still provides valuable insights.",
They high- lighted the connectivity factor between diﬀerent cortical areas of the brain (ei- ther by gain reduction or pruning) specially in the context ones.,
"Later works on RNN, such as [57], revisited this idea with hierarchical networks, with the same capability to generate parasitic states due to dynamic attractors.",
Pruning was substituted by noise injection.,
"Interestingly, there are conceptual similarities be- tween noise injection and precision reduction used in Bayesian approaches.",
"Due to the more general architecture regarding sensorimotor integration, this RNN might be able to replicate other ﬁndings in earlier works such as hallucinations or performance in the Stroop task, however, this has not been experimentally demonstrated yet.",
"Bayesian approaches, such as predictive processing [86] and circular inference [71] have shown better quality in terms of predictability of new empirical ﬁnd- ings.",
Their mathematical abstraction is more powerful and may be applicable to diﬀerent types of experiments.,
"For instance, within the free-energy optimization framework, eye-tracking deﬁcits with occlusion and agency attribution disorders were investigated.",
"The circular inference model with E/I imbalance predicted ﬁndings in decision-making tasks involving likelihoods (e.g., Fisher task).",
"How- ever, due to the conceptual design, their scalability is really poor for handling real sensory information.",
"Here we ﬁnd that ANNs, such as convolutional net- work approaches [69] or Variational-Bayes RNN [84] could better account for real sensory data input.",
"Just as Hopﬁeld networks were applied for modeling SZ, some early models of ASD focused on SOM approaches.",
"These models [70, 123, 124, 125] could account for strong speciﬁcity in cortical representations or novelty avoidance.",
"Despite of that, they were highly linked to the speciﬁc network architecture, and thus, it is diﬃcult to use these mechanisms to predict performance in other types of tasks.",
"More general approaches were suggested using simple parameter modiﬁcations of feed-forward neural networks [83, 122, 81].",
"These parameters rather utilize general engineering mechanisms of neural networks and, thus, are also applicable to diﬀerent architectures (e.g., regularization was also used in a recent approach using RNNs [84]).",
"These studies mostly focused on replicating the speciﬁc symptom of generalization deﬁcits, but may not be applicable to explaining a broader range of symptoms.",
"The reviewed models of SZ only addressed positive symptoms mainly hal- lucinations, delusions and abnormal movements.",
Self-other disturbances have been only discussed in the free-energy models and negative symptoms have 44 been set aside.,
Within the ASD models only repetitive motor movements and hyper/hyporeactivity to sensory input were properly discussed.,
"Furthermore, social communication and interaction deﬁcits have been minimally addressed.",
"Interestingly, for ASD [74, 79, 80, 84] as well as for SZ [86, 71, 57], the majority of recent approaches incorporate the idea of predictive coding [114].",
"In particular, Pellicano and Burr’s paper [74] and novel hypotheses based on their theory [75, 154] signiﬁcantly inﬂuence the recent developments.",
"In terms of ﬁnding a general account for cognition, predictive coding and related approaches are the most promising candidates right now.",
"Therefore, predictive coding based approaches can be considered a useful abstraction in developing a broader model that is able to integrate typical and atypical development in a coherent whole.",
"Multiﬁnality, equiﬁnality and heterogenity A challenge in modeling psychopathologies is the non-speciﬁcity of these disorders.",
Diﬀerent biological bases may lead to the same symptom (equiﬁ- nality).,
"Therefore, many modeling mechanisms might be valid for modeling a single symptom.",
"Accordingly, the studies reviewed here cover a wide range of approaches, using various pieces of biological evidence.",
"This variety has its drawback: even if a model can explain some symptoms, we can not judge whether this mechanism actually is comparable to what happens in the human brain or not.",
The non-speciﬁcity of psychopathologies also means that a single biological basis can cause diﬀerent symptoms (multiﬁnality).,
"Thus, instead of targeting single symptoms, it is important to develop models which explain several symp- toms of a disorder.",
A good starting point is to ﬁrst model typical behavior.,
One possible basis could be ANN models of sensorimotor integration.,
"According to the majority of the computational models discussed in this manuscript, SZ and ASD are presented as disorders of sensory information fusion or interpretation.",
"Thus, general ANN sensorimotor integration models that are able to ﬁt human- like data (control and patient data) in diﬀerent experimental paradigms such as body perceptual tests or decision making task could be extended to model psychopathologies.",
"Additionally, modeling mechanisms should not only cover various symptoms of a single disorder, but they may also be used for modeling similar symptoms in diﬀerent disorders.",
"For instance, hallucinations are present in several disorders but researchers used diﬀerent ANN approaches to model them.",
"Hallucinations produced by a loss of sensory input, like in the Charles Bonnet syndrome, were studied by modeling homeostasis in a Deep Boltzmann machine (DBM) for visual [161] and tactile inputs [162].",
"However, homeostasis or DBMs were never studied for hallucinations in SZ, or discussed within circular inference or free- energy approaches [86].",
"Regarding heterogeneity, recent studies modeling ASD already acknowledge the nature of ASD as a spectrum.",
"Instead of distinguishing between impaired and intact behavior as two categories, a continuous change in symptoms is sug- gested, leading to impairments of diﬀerent severeness [163, 79] or even opposite 45 types of impairments [80].",
This oﬀers a potentially more sophisticated view on heterogeneity in ASD.,
Models validation on real robotic systems We presented some works that employ robotics systems’ validation as a useful servant for the behaviour unit/level of analysis [57].,
The relevant aspect of these approaches is that the internal mechanism of the behaviour is visible [23].,
"Furthermore, a connection can be made from rather perceptual or mechanistic impairments inside the system to diﬃculties in real interaction scenarios.",
"For instance, [156] replicated freezing and repetitive behaviors on a robot.",
"Most of the discussed models, however, are solely data models.",
"Closing the gap to real world embodied models could, therefore, help to validate how these models extend to other tasks.",
ANN approaches can also focus on solving scalability to raw stimuli in other brain-inspired mathematical abstractions.,
"For instance, [164] and [165] pre- sented free-energy-based perception and action algorithms working on humanoid robots.",
They can be used to evaluate atypical behaviours related to body per- ception in SZ and ASD.,
New directions We identiﬁed the following three research directions that are still underrep- resented in the discussed studies.,
"Developmental factors Developmental factors are especially relevant for ASD as a developmental disorder, but also for SZ.",
"Specially, to explain why many cases of SZ emerge during adolescence and early adulthood [56, 89, 90] and to investigate develop- mental factors which might contribute to the onset of SZ [166].",
Current models only partially take the developmental process into account and focus more on modeling existing deﬁcits in adult subjects with ASD.,
"For instance, existing models assume an aberrant number of neurons [83, 124] or diﬀerences in the neural connections [60, 59] during the development, or they change the way that learning proceeds by altering network regularization [81, 84] or how infor- mation are integrated during learning [80].",
"However, these studies still cannot answer the question of which initial causes promote the appearance of ASD during the development.",
"It might be beneﬁcial to take even one step more back in development, back to the development of the human fetus.",
"For instance, a recent study [40] suggests that disordered intrauterine embodied interaction during fetal period is a possible factor for neuro-developmental disorders like ASD.",
"SZ and ASD as disorders of the self One of the aspects not properly addressed in ANN computational modeling, neither for SZ nor for ASD, is how diagnosed individuals experience their body and self in comparison with control subjects.",
"For instance, SZ patients have 46 troubles diﬀerentiating self-produced actions.",
"In fact, modeling the spectrum of diﬀerences in body experience could make several psychopathologies compara- ble.",
"In addition to already described visual illusions, also body illusions can be investigated.",
"Recently, Noel et al.",
"[167] discussed how body perception diﬀers between ASD and SZ individuals, suggesting a sharper boundary between self and other in ASD and a weaker boundary in SZ.",
"This suggestion is based on ex- perimental ﬁndings, for example, on peripersonal space in body illusions where “opposite” results were found: whereas individuals with SZ were more prone to have body illusions [168], individuals with ASD showed a reduced illusionary ef- fect [169].",
"Hence, the causes of these psychopathologies have a direct impact on the perception of our body and the self.",
"In the case of patients diagnosed with SZ, this relation has been more intensively studied [170] and some treatments include embodiment therapies.",
"Hence, models of the bodily or sensorimotor self [171, 116] that are able to explain body illusions would help to validate the hy- pothesis in a common framework.",
Behavioural measures like the proprioceptive drift or peripersonal space should be also predicted by the model.,
"For instance, in [116], they used the perceptual drift as a measure to evaluate the validity of a predictive coding model for typical individuals.",
"ANN novel architectures for psychopathologies In terms of neural network architectures, there is a further need of trans- ferring the knowledge from state-of-the-art recurrent neural networks and deep learning to neurological disorders as it was performed, for instance, with the Neocognitron model of ASD [69] or the MTRNN model of SZ [57].",
"Theoretical ANN studies, computational psychiatry and neuroscience should be always be in contact to boost the feedback of those disciplines.",
"In opposition to Bayesian models that are implemented on a high abstraction level of the task, modern ANN approaches [15] are able to cope with real sensor data such as visual information.",
"For instance, cross-modal learning architectures combined with hierarchical representation learning provide an interesting follow- up to early ANN studies on SZ and ASD.",
"Furthermore, ANN models of Bayesian brain such as predictive coding [115] and circular inference are a basis for uniting both communities.",
"In fact, recent advances in probabilistic NNs like Variational Autoencoders [172] and Variational-RNN [173, 84], provide the mathematical framework to deploy ANN versions of prominent plausible models of the brain such as the free-energy principle [112].",
"In this review, we showed the power of ANNs for modeling symptoms of neurological disorders.",
"However, these techniques need to be further developed and reﬁned in the future to play a key role in computational psychiatry and to contribute in clinical applications.",
Acknowledgements This work was supported by SELFCEPTION project (www.selfception.eu) European Union Horizon 2020 Programme (MSCA-IF-2016) under grant agree- 47 ment no.,
"741941, JST CREST Cognitive Mirroring (grant no.",
"JPMJCR16E2), and by JSPS KAKENHI grant no.",
"JP17H06039, JP18K07597 and JP18KT0021.",
"References References [1] S. Saha, D. Chant, J. Welham, J. McGrath, A systematic review of the prevalence of schizophrenia, PLoS medicine 2 (5) (2005) e141.",
"[2] J. Baio, L. Wiggins, D. L. Christensen, M. J. Maenner, J. Daniels, Z. War- ren, M. Kurzius-Spencer, W. Zahorodny, C. R. Rosenberg, T. White, et al., Prevalence of autism spectrum disorder among children aged 8 years – autism and developmental disabilities monitoring network, 11 sites, united states, 2014, MMWR Surveillance Summaries 67 (6) (2018) 1.",
"[3] J. Rapoport, A. Chavez, D. Greenstein, A. Addington, N. Gogtay, Autism spectrum disorders and childhood-onset schizophrenia: clinical and biolog- ical contributions to a relation revisited, Journal of the American Academy of Child & Adolescent Psychiatry 48 (1) (2009) 10–18.",
"[4] A. E. Pinkham, J.",
"B. Hopﬁnger, K. A. Pelphrey, J. Piven, D. L. Penn, Neural bases for impaired social cognition in schizophrenia and autism spectrum disorders, Schizophrenia research 99 (1-3) (2008) 164–175.",
"[5] B. H. King, C. Lord, Is schizophrenia on the autism spectrum?, Brain research 1380 (2011) 34–41.",
"[6] D. A. Treﬀert, The savant syndrome: an extraordinary condition.",
"a synop- sis: past, present, future, Philosophical Transactions of the Royal Society of London B: Biological Sciences 364 (1522) (2009) 1351–1357.",
"[7] L. Selfe, Nadia revisited: A longitudinal study of an autistic savant, Psy- chology Press, 2012.",
"[8] C.-D. G. of the Psychiatric Genomics Consortium, et al., Identiﬁcation of risk loci with shared eﬀects on ﬁve major psychiatric disorders: a genome- wide analysis, The Lancet 381 (9875) (2013) 1371–1379.",
"[9] D. Redish, J. Gordon, Computational Psychiatry: New Perspectives on Mental Illness (Strungmann Forum Reports), Cambridge, MA: MIT Press, 2016.",
"Wang, J. H. Krystal, Computational psychiatry, Neuron 84 (3) (2014) 638–654.",
"[11] P. R. Montague, R. J. Dolan, K. J. Friston, P. Dayan, Computational psychiatry, Trends in cognitive sciences 16 (1) (2012) 72–80.",
"48 [12] R. A. Adams, Q. J. Huys, J. P. Roiser, Computational psychiatry: towards a mathematically informed understanding of mental illness, J Neurol Neu- rosurg Psychiatry 87 (1) (2016) 53–63.",
"[13] Q. J. Huys, M. Moutoussis, J. Williams, Are computational models of any use to psychiatry?, Neural Networks 24 (6) (2011) 544–551.",
"[14] F. Rosenblatt, The perceptron: a probabilistic model for information stor- age and organization in the brain., Psychological review 65 (6) (1958) 386.",
"[15] J. Schmidhuber, Deep learning in neural networks: An overview, Neural networks 61 (2015) 85–117.",
"[16] F. Crick, G. Mitchison, et al., The function of dream sleep, Nature 304 (5922) (1983) 111–114.",
"[17] M. Spitzer, A neurocomputational approach to delusions, Comprehensive Psychiatry 36.",
"[18] R. E. Hoﬀman, Computer simulations of neural information processing and the schizophrenia-mania dichotomy, Archives of General Psychiatry 44 (2) (1987) 178–188.",
"[19] J. D. Cohen, D. Servan-Schreiber, Context, cortex, and dopamine: a con- nectionist approach to behavior and biology in schizophrenia., Psycholog- ical review 99 (1) (1992) 45.",
"A. Reggia, E. Ruppin, R. S. Berndt, Neural modeling of brain and cognitive disorders, Vol.",
"6, World Scientiﬁc, 1996.",
"[21] L. Gustafsson, A. P. Paplinski, Neural network modelling of autism, Re- cent developments in autism research (2004) 100–134.",
"[22] R. Pfeifer, J. Bongard, How the body shapes the way we think: a new view of intelligence, MIT press, 2006.",
"[23] G. Cheng, S.-H. Hyon, J. Morimoto, A. Ude, J. G. Hale, G. Colvin, W. Scroggin, S. C. Jacobsen, Cb: A humanoid research platform for ex- ploring neuroscience, Advanced Robotics 21 (10) (2007) 1097–1114.",
"[24] A. Anticevic, J. D. Murray, D. M. Barch, Bridging levels of understanding in schizophrenia through computational modeling, Clinical psychological science 3 (3) (2015) 433–459.",
"[25] V. Valton, L. Romaniuk, J. D. Steele, S. Lawrie, P. Seri`es, Comprehen- sive review: computational modelling of schizophrenia, Neuroscience & Biobehavioral Reviews 83 (2017) 631–646.",
"A. Moustafa, B. Misiak, D. Frydecka, Neurocomputational models of schizophrenia, Computational Models of Brain and Behavior (2017) 73.",
"49 [27] L. Kanner, et al., Autistic disturbances of aﬀective contact, Nervous child 2 (3) (1943) 217–250.",
"[28] J. L. Daniels, U. Forssen, C. M. Hultman, S. Cnattingius, D. A. Savitz, M. Feychting, P. Sparen, Parental psychiatric disorders associated with autism spectrum disorders in the oﬀspring, Pediatrics 121 (5) (2008) e1357–e1362.",
"[29] M. F. Aukes, B.",
"Z. Alizadeh, M. M. Sitskoorn, J.-P. Selten, R. J. Sinke, C. Kemner, R. A. Ophoﬀ, R. S. Kahn, Finding suitable phenotypes for genetic studies of schizophrenia: heritability and segregation analysis, Bi- ological psychiatry 64 (2) (2008) 128–136.",
"[30] S. Sandin, P. Lichtenstein, R. Kuja-Halkola, C. Hultman, H. Larsson, A. Reichenberg, The heritability of autism spectrum disorder, Jama 318 (12) (2017) 1182–1184.",
"[31] A. Sims, Symptoms in the mind: An introduction to descriptive psy- chopathology., Bailliere Tindall Publishers, 1988.",
"[32] A. P. Association, et al., Diagnostic and statistical manual of mental dis- orders (DSM-5 R⃝), American Psychiatric Pub, 2013.",
"[33] A. van der Weiden, M. Prikken, N. E. van Haren, Self–other integration and distinction in schizophrenia: A theoretical analysis and a review of the evidence, Neuroscience & Biobehavioral Reviews 57 (2015) 220–237.",
"[34] M. Morrens, W. Hulstijn, P. J. Lewi, M. De Hert, B. G. Sabbe, Stereotypy in schizophrenia, Schizophrenia research 84 (2-3) (2006) 397–404.",
"[35] S. Baron-Cohen, Mindblindness: An essay on autism and theory of mind, MIT press, 1997.",
"[36] C. Lord, M. Rutter, P. C. DiLavore, S. Risi, K. Gotham, S. Bishop, et al., Autism diagnostic observation schedule: ADOS, Western Psychological Services Los Angeles, CA, 2012.",
"[37] U. Frith, F. Happ´e, Autism: Beyond theory of mind, Cognition 50 (1-3) (1994) 115–132.",
"[38] F. Happ´e, U. Frith, The weak coherence account: detail-focused cognitive style in autism spectrum disorders, Journal of autism and developmental disorders 36 (1) (2006) 5–25.",
"[39] U. Frith, Autism: Explaining the enigma, Blackwell Publishing, 2003.",
"[40] Y. Yamada, H. Kanazawa, S. Iwasaki, Y. Tsukahara, O. Iwata, S. Yamada, Y. Kuniyoshi, An embodied brain model of the human foetus, Scientiﬁc reports 6 (2016) 27893.",
"50 [41] M. Robbins, Experiences of schizophrenia: An integration of the personal, scientiﬁc, and therapeutic., Guilford Press, 1993.",
"[42] F. G. Happ´e, Studying weak central coherence at low levels: children with autism do not succumb to visual illusions.",
"a research note, Journal of Child Psychology and Psychiatry 37 (7) (1996) 873–877.",
"[43] C.-E. Notredame, D. Pins, S. Deneve, R. Jardri, What visual illusions teach us about schizophrenia, Frontiers in integrative neuroscience 8 (2014) 63.",
"Wood, Autism and schizophrenia: one, two or many disorders?, The British Journal of Psychiatry 210 (4) (2017) 241–242.",
"[45] K. J. Friston, The disconnection hypothesis, Schizophrenia research 30 (2) (1998) 115–125.",
"[46] M.-E. Lynall, D. S. Bassett, R. Kerwin, P. J. McKenna, M. Kitzbichler, U. Muller, E. Bullmore, Functional connectivity and brain networks in schizophrenia, Journal of Neuroscience 30 (28) (2010) 9477–9487.",
"[47] C. Frith, Is autism a disconnection disorder?, The Lancet Neurology 3 (10) (2004) 577.",
"Just, V. L. Cherkassky, T. A. Keller, N. J. Minshew, Cortical activation and synchronization during sentence comprehension in high- functioning autism: evidence of underconnectivity, Brain 127 (8) (2004) 1811–1821.",
"[49] J. S. Anderson, T. J. Druzgal, A. Froehlich, M. B. DuBray, N. Lange, A. L. Alexander, T. Abildskov, J.",
"A. Nielsen, A. N. Cariello, J. R. Cooperrider, et al., Decreased interhemispheric functional connectivity in autism, Cere- bral cortex 21 (5) (2010) 1134–1146.",
"[50] C. L. Keown, P. Shih, A. Nair, N. Peterson, M. E. Mulvey, R.-A.",
"M¨uller, Local functional overconnectivity in posterior brain regions is associated with symptom severity in autism spectrum disorders, Cell reports 5 (3) (2013) 567–572.",
"[51] K. Supekar, L. Q. Uddin, A. Khouzam, J. Phillips, W. D. Gaillard, L. E. Kenworthy, B. E. Yerys, C. J. Vaidya, V. Menon, Brain hyperconnectivity in children with autism and its links to social deﬁcits, Cell reports 5 (3) (2013) 738–747.",
"[52] A. Hahamy, M. Behrmann, R. Malach, The idiosyncratic brain: distortion of spontaneous connectivity patterns in autism spectrum disorder, Nature neuroscience 18 (2) (2015) 302.",
"[53] R. E. Hoﬀman, S. K. Dobscha, Cortical pruning and the development of schizophrenia: a computer model, Schizophrenia bulletin 15 (3) (1989) 477–490.",
"51 [54] R. E. Hoﬀman, T. H. McGlashan, Synaptic elimination, neurodevelop- ment, and the mechanism of hallucinated voices in schizophrenia, Ameri- can Journal of Psychiatry 154 (12) (1997) 1683–1689.",
"[55] R. E. Hoﬀman, U. Grasemann, R. Gueorguieva, D. Quinlan, D. Lane, R. Miikkulainen, Using computational patients to evaluate illness mecha- nisms in schizophrenia, Biological psychiatry 69 (10) (2011) 997–1005.",
"[56] P. R. Huttenlocher, et al., Synaptic density in human frontal cortex – developmental changes and eﬀects of aging, Brain Res 163 (2) (1979) 195–205.",
"[57] Y. Yamashita, J. Tani, Spontaneous prediction error generation in schizophrenia, PLoS One 7 (5) (2012) e37843.",
"[58] K. J. Friston, C. D. Frith, Schizophrenia: a disconnection syndrome, Clin Neurosci 3 (2) (1995) 89–97.",
"Park, K. Ichinose, Y. Kawai, J. Suzuki, M. Asada, H. Mori, Macroscopic cluster organizations change the complexity of neural activity, Entropy 21 (2) (2019) 214.",
"[60] K. Ichinose, J.",
"Park, Y. Kawai, J. Suzuki, M. Asada, H. Mori, Local over- connectivity reduces the complexity of neural activity: Toward a construc- tive understanding of brain networks in patients with autism spectrum disorder, in: 2017 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2017, pp.",
doi:10.1109/DEVLRN.2017.8329813.,
"[61] E. Courchesne, K. Pierce, Why the frontal cortex in autism might be talk- ing only to itself: local over-connectivity but long-distance disconnection, Current opinion in neurobiology 15 (2) (2005) 225–230.",
"[62] J. Rubenstein, M. M. Merzenich, Model of autism: increased ratio of excitation/inhibition in key neural systems, Genes, Brain and Behavior 2 (5) (2003) 255–267.",
"[63] L. Sun, C. Gr¨utzner, S. B¨olte, M. Wibral, T. Tozman, S. Schlitt, F. Poustka, W. Singer, C. M. Freitag, P. J. Uhlhaas, Impaired gamma- band activity during perceptual organization in adults with autism spec- trum disorders: evidence for dysfunctional network activity in frontal- posterior cortices, Journal of Neuroscience 32 (28) (2012) 9563–9573.",
"[64] T. M. Snijders, B. Milivojevic, C. Kemner, Atypical excitation–inhibition balance in autism captured by the gamma response to contextual modu- lation, NeuroImage: clinical 3 (2013) 65–72.",
"[65] R. Canitano, M. Pallagrosi, Autism spectrum disorders and schizophrenia spectrum disorders: excitation/inhibition imbalance and developmental trajectories, Frontiers in psychiatry 8 (2017) 69.",
"52 [66] R. Jardri, K. Hugdahl, M. Hughes, J. Brunelin, F. Waters, B. Alderson- Day, D. Smailes, P. Sterzer, P. R. Corlett, P. Leptourgos, et al., Are hallucinations due to an imbalance between excitatory and inhibitory in- ﬂuences on the brain?, Schizophrenia bulletin 42 (5) (2016) 1124–1134.",
"[67] O. Yizhar, L. E. Fenno, M. Prigge, F. Schneider, T. J. Davidson, D. J. Oshea, V. S. Sohal, I. Goshen, J. Finkelstein, J. T. Paz, et al., Neocor- tical excitation/inhibition balance in information processing and social dysfunction, Nature 477 (7363) (2011) 171.",
"[68] A. Dickinson, M. Jones, E. Milne, Measuring neural excitation and in- hibition in autism: diﬀerent approaches, diﬀerent ﬁndings and diﬀerent interpretations, Brain research 1648 (2016) 277–289.",
"[69] Y. Nagai, T. Moriwaki, M. Asada, Inﬂuence of excitation/inhibition im- balance on local processing bias in autism spectrum disorder, in: Proc.",
"of the 37th Annual Meeting of the Cognitive Science Society, 2015, pp.",
"[70] L. Gustafsson, Inadequate cortical feature maps: A neural circuit theory of autism, Biological Psychiatry 42 (12) (1997) 1138–1147.",
"[71] R. Jardri, S. Deneve, Circular inferences in schizophrenia, Brain 136 (11) (2013) 3227–3241.",
"[72] R. Jardri, S. Duverne, A. S. Litvinova, S. Den`eve, Experimental evidence for circular inference in schizophrenia, Nature communications 8 (2017) 14218.",
"[73] J. R. Lucker, Auditory hypersensitivity in children with autism spectrum disorders, Focus on Autism and Other Developmental Disabilities 28 (3) (2013) 184–191.",
"[74] E. Pellicano, D. Burr, When the world becomes too real: a bayesian expla- nation of autistic perception, Trends in cognitive sciences 16 (10) (2012) 504–510.",
"[75] R. P. Lawson, G. Rees, K. J. Friston, An aberrant precision account of autism, Frontiers in human neuroscience 8 (2014) 302.",
"[76] P. Karvelis, A. R. Seitz, S. M. Lawrie, P. Seri`es, Autistic traits, but not schizotypy, predict increased weighting of sensory information in bayesian visual integration, eLife 7 (2018) e34115.",
"[77] A. R. Powers III, M. Kelley, P. R. Corlett, Hallucinations as top-down eﬀects on perception, Biological Psychiatry: Cognitive Neuroscience and Neuroimaging 1 (5) (2016) 393–400.",
"[78] P. Sterzer, R. A. Adams, P. Fletcher, C. Frith, S. M. Lawrie, L. Muckli, P. Petrovic, P. Uhlhaas, M. Voss, P. R. Corlett, The predictive coding account of psychosis, Biological psychiatry 84 (9) (2018) 634–643.",
"53 [79] H. Idei, S. Murata, Y. Chen, Y. Yamashita, J. Tani, T. Ogata, Reduced behavioral ﬂexibility by aberrant sensory precision in autism spectrum disorder: A neurorobotics experiment, in: Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2017 Joint IEEE International Conference on, IEEE, 2017, pp.",
"[80] A. Philippsen, Y. Nagai, Understanding the cognitive mechanisms under- lying autistic behavior: a recurrent neural network study, in: Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2018 Joint IEEE International Conference on, IEEE, 2018, pp.",
"[81] A. Dovgopoly, E. Mercado, A connectionist model of category learning by individuals with high-functioning autism spectrum disorder, Cognitive, Aﬀective, & Behavioral Neuroscience 13 (2) (2013) 371–389.",
"[82] R. A. Adams, Bayesian inference, predictive coding, and computational models of psychosis, in: Computational Psychiatry, Elsevier, 2018, pp.",
"[83] I. L. Cohen, An artiﬁcial neural network analogue of learning in autism, Biological Psychiatry 36 (1) (1994) 5–20.",
"[84] A. Ahmadi, J. Tani, Bridging the gap between probabilistic and determin- istic models: a simulation study on a variational bayes predictive coding recurrent neural network model, in: International Conference on Neural Information Processing, Springer, 2017, pp.",
"[85] D. Horn, E. Ruppin, Compensatory mechanisms in an attractor neural network model of schizophrenia, Neural Computation 7 (1) (1995) 182– 205.",
"[86] R. A. Adams, K. E. Stephan, H. R. Brown, C. D. Frith, K. J. Friston, The computational anatomy of psychosis, Frontiers in psychiatry 4 (2013) 47.",
"[87] J. J. Hopﬁeld, Neural networks and physical systems with emergent col- lective computational abilities, Proceedings of the national academy of sciences 79 (8) (1982) 2554–2558.",
"[88] D. O. Hebb, The organization of behavior: A neuropsychological theory, John Wiley, 1949.",
"[89] I. Feinberg, Schizophrenia: caused by a fault in programmed synaptic elimination during adolescence?, Journal of psychiatric research 17 (4) (1982) 319–334.",
"[90] M. S. Keshavan, S. Anderson, J. W. Pettergrew, Is schizophrenia due to excessive synaptic pruning in the prefrontal cortex?",
"the feinberg hypoth- esis revisited, Journal of psychiatric research 28 (3) (1994) 239–265.",
"54 [91] P. R. Huttenlocher, C. de Courten, L. J. Garey, H. Van der Loos, Synap- togenesis in human visual cortexevidence for synapse elimination during normal development, Neuroscience letters 33 (3) (1982) 247–252.",
"[92] E. Ruppin, J.",
"A. Reggia, D. Horn, Pathogenesis of schizophrenic delusions and hallucinations: a neural model, Schizophrenia Bulletin 22 (1) (1996) 105–121.",
"[93] J. R. Stevens, Abnormal reinnervation as a basis for schizophrenia: A hypothesis 49 (1992) 238–43.",
"[94] M. Tsodyks, Associative memory in asymmetric diluted network with low level of activity, EPL (Europhysics Letters) 7 (3) (1988) 203.",
"[95] M. V. Tsodyks, M. V. Feigel’man, The enhanced storage capacity in neural networks with low activity level, EPL (Europhysics Letters) 6 (2) (1988) 101.",
"[96] N. Garmezy, The psychology and psychopathology of attention., Schizophrenia Bulletin 3 (3) (1977) 360.",
"[97] P. J. Lang, A. H. Buss, Psychological deﬁcit in schizophrenia: Ii.",
"inter- ference and activation., Journal of Abnormal Psychology 70 (2) (1965) 77.",
"[98] A. Henik, R. Salo, Schizophrenia and the stroop eﬀect, Behavioral and cognitive neuroscience reviews 3 (1) (2004) 42–59.",
"[99] J. R. Stroop, Studies of interference in serial verbal reactions., Journal of experimental psychology 18 (6) (1935) 643.",
"[100] H. E. Rosvold, A. F. Mirsky, I. Sarason, E. D. Bransome Jr, L. H. Beck, A continuous performance test of brain damage., Journal of consulting psychology 20 (5) (1956) 343.",
"A. Cornblatt, M. F. Lenzenweger, L. Erlenmeyer-Kimling, The contin- uous performance test, identical pairs version: Ii.",
"contrasting attentional proﬁles in schizophrenic and depressed patients, Psychiatry research 29 (1) (1989) 65–85.",
"[102] L. Chapman, J. P. Chapman, G. A. Miller, A theory of verbal behavior in schizophrenia., Progress in experimental personality research 72 (1964) 49.",
"[103] R. L. Margolis, D.-M. Chuang, R. M. Post, Programmed cell death: impli- cations for neuropsychiatric disorders, Biological psychiatry 35 (12) (1994) 946–956.",
"[104] J. L. Elman, Finding structure in time, Cognitive science 14 (2) (1990) 179–211.",
"55 [105] R. E. Hoﬀman, T. H. McGlashan, Book review: Neural network models of schizophrenia, The Neuroscientist 7 (5) (2001) 441–454.",
"[106] R. Miikkulainen, M. G. Dyer, Natural language processing with modular pdp networks and distributed lexicon, Cognitive Science 15 (3) (1991) 343–399.",
"[107] R. Miikkulainen, Subsymbolic natural language processing: An integrated model of scripts, lexicon, and memory, MIT press, 1993.",
"[108] U. Grasemann, R. Miikulainen, R. Hoﬀman, A subsymbolic model of lan- guage pathology in schizophrenia, in: Proceedings of the Annual Meeting of the Cognitive Science Society, Vol.",
"[109] H. Von Helmholtz, Handbuch der physiologischen Optik, Vol.",
"9, Voss, 1867.",
"[110] P. Thompson, Margaret thatcher: a new illusion., Perception 9 (4) (1980) 483–484.",
"[111] K. M. Dallenbach, A puzzle-picture with a new principle of concealment, The American journal of psychology (1951) 431–433.",
"[112] K. Friston, The free-energy principle: a uniﬁed brain theory?, Nature reviews neuroscience 11 (2) (2010) 127.",
"[113] P. Dayan, G. E. Hinton, R. M. Neal, R. S. Zemel, The helmholtz machine, Neural computation 7 (5) (1995) 889–904.",
"[114] R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: a func- tional interpretation of some extra-classical receptive-ﬁeld eﬀects, Nature neuroscience 2 (1) (1999) 79.",
"[115] Y. Yamashita, J. Tani, Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment, PLoS computational biology 4 (11) (2008) e1000220.",
[116] N.-A.,
"Hinz, P. Lanillos, H. Mueller, G. Cheng, Drifting perceptual patterns suggest prediction errors fusion rather than hypothesis selec- tion: replicating the rubber-hand illusion on a robot, arXiv preprint arXiv:1806.06809.",
"[117] H. Nordby, D. Hammerborg, W. T. Roth, K. Hugdahl, Erps for infrequent omissions and inclusions of stimulus elements, Psychophysiology 31 (6) (1994) 544–552.",
"[118] S. Schaal, S. Kotosaka, D. Sternad, Nonlinear dynamical systems as move- ment primitives, in: IEEE international conference on humanoid robotics, 2000, pp.",
"56 [119] M. B´anyai, V. A. Diwadkar, P. ´Erdi, Model-based dynamical analysis of functional disconnection in schizophrenia, Neuroimage 58 (3) (2011) 870– 877.",
"[120] J. L. McClelland, The basis of hyperspeciﬁcity in autism: A preliminary suggestion based on properties of neural nets, Journal of Autism and De- velopmental Disorders 30 (5) (2000) 497–502.",
"[121] C. OLaughlin, P. Thagard, Autism and coherence: A computational model, Mind & Language 15 (4) (2000) 375–392.",
"[122] I. Cohen, Neural network analysis of learning in autism, Neural networks and psychopathology (1998) 274–315.",
"[123] L. Gustafsson, A. P. Papli´nski, Self-organization of an artiﬁcial neural net- work subjected to attention shift impairments and familiarity preference, characteristics studied in autism, Journal of autism and developmental disorders 34 (2) (2004) 189–198.",
"[124] G. Noriega, Self-organizing maps as a model of brain mechanisms poten- tially linked to autism, IEEE Transactions on neural systems and rehabil- itation engineering 15 (2) (2007) 217–226.",
"[125] G. Noriega, Modeling propagation delays in the development of somsa parallel with abnormal brain growth in autism, Neural Networks 21 (2-3) (2008) 130–139.",
"[126] M. L. Bauman, Microscopic neuroanatomic abnormalities in autism, Pe- diatrics 87 (5) (1991) 791–796.",
"[127] I. L. Cohen, V. Sudhalter, D. Landon-Jimenez, M. Keogh, A neural net- work approach to the classiﬁcation of autism, Journal of autism and de- velopmental disorders 23 (3) (1993) 443–466.",
"[128] J. L. McClelland, B. L. McNaughton, R. C. O’reilly, Why there are com- plementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory., Psychological review 102 (3) (1995) 419.",
"[129] C. M. Henderson, J. L. McClelland, A pdp model of the simultaneous perception of multiple objects, Connection Science 23 (2) (2011) 161–172.",
"Church, M. S. Krauss, C. Lopata, J.",
"A. Toomey, M. L. Thomeer, M. V. Coutinho, M. A. Volker, E. Mercado, Atypical categorization in children with high-functioning autism spectrum disorder, Psychonomic Bulletin & Review 17 (6) (2010) 862–868.",
"[131] T. Vladusich, O. Olu-Lafe, D.-S. Kim, H. Tager-Flusberg, S. Grossberg, Prototypical category learning in high-functioning autism, Autism Re- search 3 (5) (2010) 226–236.",
"57 [132] T. Bourgeron, A synaptic trek to autism, Current opinion in neurobiology 19 (2) (2009) 231–234.",
"[133] B. D. Auerbach, E. K. Osterweil, M. F. Bear, Mutations causing syndromic autism deﬁne an axis of synaptic pathophysiology, Nature 480 (7375) (2011) 63.",
"[134] A. Krogh, J.",
"A. Hertz, A simple weight decay can improve generalization, in: Advances in neural information processing systems, 1992, pp.",
"[135] M. F. Casanova, I.",
"A. van Kooten, A. E. Switala, H. van Engeland, H. Heinsen, H. W. Steinbusch, P. R. Hof, J. Trippe, J.",
"Stone, C. Schmitz, Minicolumnar abnormalities in autism, Acta neuropathologica 112 (3) (2006) 287.",
"[136] R. Caruana, S. Lawrence, C. L. Giles, Overﬁtting in neural nets: Back- propagation, conjugate gradient, and early stopping, in: Advances in neu- ral information processing systems, 2001, pp.",
"[137] V. B. Mountcastle, Modality and topographic properties of single neurons of cat’s somatic sensory cortex, Journal of neurophysiology 20 (4) (1957) 408–434.",
"[138] E. Courchesne, J. Townsend, N. A. Akshoomoﬀ, O. Saitoh, R. Yeung- Courchesne, A. J. Lincoln, H. E. James, R. H. Haas, L. Schreibman, L. Lau, Impairment in shifting attention in autistic and cerebellar pa- tients., Behavioral neuroscience 108 (5) (1994) 848.",
"[139] M. L. Bauman, T. L. Kemper, Neuroanatomic observations of the brain in autism: a review and future directions, International journal of devel- opmental neuroscience 23 (2-3) (2005) 183–187.",
"[140] E. Courchesne, C. Karns, H. Davis, R. Ziccardi, R. Carper, Z. Tigue, H. Chisum, P. Moses, K. Pierce, C. Lord, et al., Unusual brain growth patterns in early life in patients with autistic disorder an mri study, Neu- rology 57 (2) (2001) 245–254.",
"[141] E. H. Aylward, N. J. Minshew, K. Field, B. Sparks, N. Singh, Eﬀects of age on brain volume and head circumference in autism, Neurology 59 (2) (2002) 175–183.",
"[142] K. Fukushima, S. Miyake, Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition, in: Competition and cooperation in neural nets, Springer, 1982, pp.",
"[143] K. Fukushima, Neocognitron: A hierarchical neural network capable of visual pattern recognition., Neural networks 1 (2) (1988) 119–130.",
"[144] K. Fukushima, Neocognitron for handwritten digit recognition, Neuro- computing 51 (2003) 161–180.",
"58 [145] M. Behrmann, G. Avidan, G. L. Leonard, R. Kimchi, B. Luna, K. Humphreys, N. Minshew, Conﬁgural processing in autism and its re- lationship to face processing, Neuropsychologia 44 (1) (2006) 110–129.",
"[146] E. M. Izhikevich, Simple model of spiking neurons, IEEE Transactions on neural networks 14 (6) (2003) 1569–1572.",
"[147] J. Wang, J. Barstein, L. E. Ethridge, M. W. Mosconi, Y. Takarae, J.",
"A. Sweeney, Resting state eeg abnormalities in autism spectrum disorders, Journal of neurodevelopmental disorders 5 (1) (2013) 24.",
"[148] W. Bosl, A. Tierney, H. Tager-Flusberg, C. Nelson, Eeg complexity as a biomarker for autism spectrum disorder risk, BMC medicine 9 (1) (2011) 18.",
"[149] D. J. Watts, S. H. Strogatz, Collective dynamics of small-worldnetworks, nature 393 (6684) (1998) 440.",
"[150] E. M. Izhikevich, N. S. Desai, Relating stdp to bcm, Neural computation 15 (7) (2003) 1511–1523.",
"[151] M. Costa, A. L. Goldberger, C.-K. Peng, Multiscale entropy analysis of biological signals, Physical review E 71 (2) (2005) 021906.",
"[152] K. J. Friston, R. Lawson, C. D. Frith, On hyperpriors and hypopriors: comment on pellicano and burr, Trends in cognitive sciences 17 (1) (2013) 1.",
"[153] J. Brock, Alternative bayesian accounts of autistic perception: comment on pellicano and burr, Trends in cognitive sciences 16 (12) (2012) 573–574.",
"[154] R. P. Lawson, C. Mathys, G. Rees, Adults with autism overestimate the volatility of the sensory environment, Nature neuroscience 20 (9) (2017) 1293.",
"[155] H. Idei, S. Murata, Y. Chen, Y. Yamashita, J. Tani, T. Ogata, A neuro- robotics simulation of autistic behavior induced by unusual sensory pre- cision, Computational Psychiatry 2 (2018) 164–182.",
"[156] S. Murata, J. Namikawa, H. Arie, S. Sugano, J. Tani, Learning to repro- duce ﬂuctuating time series by inferring their time-dependent stochastic properties: Application in robot learning via tutoring, IEEE Transactions on Autonomous Mental Development 5 (4) (2013) 298–310.",
"[157] J. Tani, Learning to generate articulated behavior through the bottom-up and the top-down interaction processes, Neural Networks 16 (1) (2003) 11–23.",
"[158] E. Gowen, A. Hamilton, Motor abilities in autism: a review using a com- putational context, Journal of autism and developmental disorders 43 (2) (2013) 323–344.",
"59 [159] M. Boden, A guide to recurrent neural networks and backpropagation, the Dallas project.",
"[160] E. Ruppin, J.",
"A. Reggia, D. Horn, A neural model of delusions and hallu- cinations in schizophrenia, in: Advances in Neural Information Processing Systems, 1995, pp.",
"[161] P. Series, D. P. Reichert, A. J. Storkey, Hallucinations in charles bonnet syndrome induced by homeostasis: a deep boltzmann machine model, in: Advances in Neural Information Processing Systems, 2010, pp.",
"[162] M. Deistler, Y. Yener, F. Bergner, P. Lanillos, G. Cheng, Tactile hallu- cinations on artiﬁcial skin induced by homeostasis in a deep boltzmann machine, arXiv preprint arXiv:1906.10592.",
"[163] Y. Nagai, Predictive learning: its key role in early cognitive develop- ment, Philosophical Transactions of the Royal Society B 374 (1771) (2019) 20180030.",
"[164] P. Lanillos, G. Cheng, Adaptive robot body learning and estimation through predictive coding, arXiv preprint arXiv:1805.03104.",
"[165] G. Oliver, P. Lanillos, G. Cheng, Active inference body perception and action for humanoid robots, arXiv preprint arXiv:1906.03022.",
"[166] T. D. Cannon, How schizophrenia develops: cognitive and brain mecha- nisms underlying onset of psychosis, Trends in cognitive sciences 19 (12) (2015) 744–756.",
"[167] J.-P. Noel, C. J. Cascio, M. T. Wallace, S. Park, The spatial self in schizophrenia and autism spectrum disorder, Schizophrenia research 179 (2017) 8–12.",
"[168] K. N. Thakkar, H. S. Nichols, L. G. McIntosh, S. Park, Disturbances in body ownership in schizophrenia: evidence from the rubber hand illusion and case study of a spontaneous out-of-body experience, PloS one 6 (10) (2011) e27089.",
"[169] C. J. Cascio, J. H. Foss-Feig, C. P. Burnette, J. L. Heacock, A.",
"A. Cosby, The rubber hand illusion in children with autism spectrum disorders: de- layed inﬂuence of combined tactile and visual input on proprioception, Autism 16 (4) (2012) 406–419.",
"[170] G. Stanghellini, Embodiment and schizophrenia, World Psychiatry 8 (1) (2009) 56–59.",
"[171] P. Lanillos, E. Dean-Leon, G. Cheng, Enactive self: a study of engi- neering perspectives to obtain the sensorimotor self through enaction, in: IEEE International Conference on Developmental Learning and Epige- netic Robotics, 2017.",
"60 [172] D. P. Kingma, M. Welling, Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114.",
"[173] O. Fabius, J. R. van Amersfoort, Variational recurrent auto-encoders, arXiv preprint arXiv:1412.6581.",
"Continual Learning for Recurrent Neural Networks: an Empirical Evaluation Andrea Cossua,b,1,∗, Antonio Cartaa,1, Vincenzo Lomonacoa, Davide Bacciua aUniversity of Pisa, Largo B. Pontecorvo, 3, 56127, Pisa, Italy bScuola Normale Superiore, Piazza dei Cavalieri, 7, 56126, Pisa, Italy Abstract Learning continuously during all model lifetime is fundamental to deploy ma- chine learning solutions robust to drifts in the data distribution.",
"Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like nat- ural language processing and robotics.",
"However, the existing body of work on the topic is still fragmented, with approaches which are application-speciﬁc and whose assessment is based on heterogeneous learning protocols and datasets.",
"In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks.",
"We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications.",
"We also provide a broad empirical evaluation of CL and Recurrent Neural Net- works in class-incremental scenario, by testing their ability to mitigate forgetting with a number of diﬀerent strategies which are not speciﬁc to sequential data processing.",
Our results highlight the key role played by the sequence length and the importance of a clear speciﬁcation of the CL scenario.,
"Keywords: continual learning, recurrent neural networks, benchmarks, evaluation 1.",
Introduction Continual Learning (CL) refers to the ability “to learn over time by accom- modating new knowledge while retaining previously learned experiences” [91].,
"Traditionally, Machine Learning (ML) models are trained with a large amount of data to solve a ﬁxed task.",
"The assumption is that, at test time, the model will encounter similar data.",
"Unfortunately, real world scenarios do not satisfy this assumption.",
"Non stationary processes produce gradual or abrupt drifts in the data distribution [31, 45] and the models must be continuously adapted to ∗Corresponding author 1Equal contribution Preprint submitted to Elsevier August 3, 2021 arXiv:2103.07492v4 [cs.LG] 2 Aug 2021 the new changes.",
"The objective of incrementally learn new concepts [116] may result in the Catastrophic Forgetting (CF) of previous knowledge [84, 83, 51].",
"CF causes a deterioration of the performance on old tasks when acquiring new knowledge and it is a direct consequence of the stability-plasticity dilemma [51, 43]: the need to maintain adaptive parameters both stable enough to pre- serve information and plastic enough to adapt to new tasks.",
"Today, CF is considered one of the main problems faced by CL algorithms [108, 87, 30].",
"Machine Learning models endowed with CL capabilities would radically change the way a model is deployed, removing the need for a separate oﬄine training phase and allowing continuous training during the entire model lifetime without forgetting previous knowledge.",
In this paper we focus on continual learning with Recurrent Neural Networks (RNNs) [99].,
These are adaptive models that can capture the input history within an internal hidden state which is updated iteratively.,
"These models are widely used for Sequential Data Processing (SDP) tasks [48], where the input is a sequence of items.",
"Recurrent models are not the only solution for sequential data processing: time- delayed neural networks [114] and, more recently, Transformers [112] are feed- forward neural networks able to handle sequential patterns.",
"Transformers have not been investigated much in the context of Continual Learning (with one no- table exception [105]), while feedforward models use a ﬁxed context (e.g.",
sliding windows) to emulate temporal coherence.,
"This may be challenging to imple- ment in Continual Learning, since there may not be the opportunity to collect enough patterns to build the context before making a prediction.",
"Ultimately, recurrent models show important diﬀerences with respect to both convolutional and feedforward approaches (including Transformers).",
"Recurrent models implement a variable number of hidden layers, due to the unrolling through the entire length of the input sequence.",
"This is not the case for feed- forward and convolutional models, whose number of layers is usually ﬁxed a priori.",
"Also, recurrent models leverage weight sharing across time steps.",
This inﬂuences the trajectory of the learning path of recurrent models when training with backpropagation through time.,
We hypothesize that these two diﬀerences may impact on the application of CL strategies on recurrent models.,
"Sequential data is widespread in fundamental machine learning applications such as Natural Language Processing [121], Human Activity Recognition [7, 68], Speech recognition [48], and Robotics [21].",
All these environments are highly non stationary and represent perfect examples of real world CL applications where data comes in the form of sequences.,
"For example, a robot that learns to walk on diﬀerent terrains or to grasp objects with diﬀerent shapes will re- ceive input patterns from its sensors as time series.",
"During its deployment, the robot will encounter novel environments/objects and will need to adapt to these novel settings continuously.",
It is impossible to pretrain the robot in advance on all possible environments/objects.,
"What if, once deployed, it is necessary to adapt to a new terrain or to grasp a new object?",
"Retraining from scratch is an expensive, unnecessary solution since most of the knowledge needed is already 2 contained in the model.",
Continual Learning represents a better solution which can be combined with SDP techniques to appropriately address sequential data.,
Human activity recognition [55] is another application in which sequential data processing and continual learning interact.,
The recognition of new classes of activities is typically performed from sequential data (e.g.,
"videos, sensors ob- servations).",
"Therefore, it is fundamental to study how this setting may impact on existing CL strategies.",
"As the examples show, sequential data processing may have a strong impact on Continual Learning, both at the methodological and application level.",
"A true continual learner should be able to deal with temporally correlated patterns, since this is the case in many real world scenarios.",
This advocates for a more thorough understanding of the behavior of recurrent models in Continual Learn- ing.,
"Currently, most of the Continual Learning literature focuses on Computer Vision and Reinforcement Learning problems [91, 71], while the study of se- quential data processing remains under-documented.",
The few existing works introduce new approaches tailored to speciﬁc tasks and use a diverse set of learning scenarios and experimental protocols.,
Therefore: i) it remains unclear whether previously existing CL strategies could still work well in SDP environ- ments and ii) the heterogeneity of learning scenarios in the literature makes it diﬃcult to compare experimental results among diﬀerent papers.,
"In fact, spe- ciﬁc continual settings can have diﬀerent complexity or require domain-speciﬁc solutions.",
"In this paper, we provide a systematization and categorization of current litera- ture to highlight the diﬀerent characteristic of each speciﬁc CL strategy and the peculiarities of the data and the continual environment.",
We adopt the class- incremental scenario [111] as a common test-bed for CL strategies.,
This sce- nario involves a stream of data where new steps gradually introduce new classes.,
Class-incremental scenarios allow to develop and test generic CL strategies for recurrent networks.,
"In our experiments, we verify whether or not CL strate- gies which are not speciﬁcally tailored for recurrent models are still suﬃcient to endow them with continual learning capabilities.",
"To summarize, our main contributions of this work are: a) the ﬁrst comprehensive review of the literature on continual learning in RNNs, including a precise categorization of current strategies, datasets and continual environments adopted by the literature (Section 3); b) the proposal of two novel benchmarks in the class-incremental setting: Syn- thetic Speech Recognition, based on speech recognition data, and Quickdraw, based on hand-drawn sketches (Section 4.2); c) an extensive experimental evaluation of 6 continual learning strategies on sev- eral CL environments for sequence classiﬁcation.",
"To the best of our knowl- edge, this is currently the most extensive evaluation of CL strategies for recurrent models (Section 5); d) an experimental comparison that highlights the eﬀect of the recurrence and 3 Step 1 Task 1 Step 2 Task 1 Step 3 Task 1 (a) Single Incremental Task Step 1 Task 1 Step 2 Task 2 Step 3 Task 3 (b) Multi Task Figure 1: In Single Incremental Task, each step belongs to the same task label (which is the same as not providing a task label).",
"In Multi Task, each step belongs to a diﬀerent task label.",
Such information can be used to select diﬀerent heads for diﬀerent tasks both at training and test time.,
Best viewed in color.,
sequence length on catastrophic forgetting (Section 6).,
"Also, a comparison between single-head and multi-head models (Section 6.3) which justiﬁes our choice of class-incremental scenarios.",
"Continual Learning framework In the rest of the paper, we will refer to the notation and the Continual Learning framework deﬁned in this section.",
Notation and fundamental concepts For a formal deﬁnition of continual learning environments we follow the framework proposed in [71].,
"For simplicity, here we focus on supervised con- tinual learning on a sequence of steps S = (S1, S2, ...), where each step Si has its own data distribution Di and belongs to a speciﬁc task with label ti.",
"The patterns drawn from each Di are input-target pairs (xj, yj).",
We study sequence classiﬁcation problems in which yj is the target class corresponding to the input sequence xj.,
"Each sequence xj is composed of T ordered vectors (x1 j , ..., xT j ), xi j ∈Rd.",
"In sequence classiﬁcation, the target is provided only at the end of the entire sequence, that is after seeing the last vector xT j .",
"Also, the sequence length T may vary from one sequence to another, depending on the speciﬁc application.",
Figure 2 provides a representation of a sequence classiﬁca- tion task.,
"At step i, a new batch of data becomes available.",
"A CL algorithm A executed at step i leverages the current model hi to learn the new training data TRi, drawn from Di.",
The CL algorithm may also use a memory buﬀer Mi and a task label ti associated to the training patterns.,
"The model is then updated using the new data: Ai : < hi−1, TRi, Mi−1, ti > →< hi, Mi >, ∀Di ∈(D1, D2, ...).",
(1) Notice that the buﬀer Mi and task label ti are optional and not always available.,
"For example, it may be impossible to store previous data due to privacy con- cerns, while the task labels may not be available in many real-world scenarios.",
Notice that the following deﬁnition does not pose any computational constraint to continual learning algorithms.,
"However, we are often interested in eﬃcient 4 algorithms.",
"As a results, most algorithms assume to have bounded memory and computational resources.",
"Trivial strategies, such as retraining from scratch using Si i=1 TRi as training data, are unfeasible due to their excessive compu- tational cost and bad scaling properties with respect to the number of steps.",
"Unfortunately, training sequentially on TR1, .",
", TRn will suﬀer from catas- trophic forgetting.",
Figure 2: Sequence classiﬁcation example.,
The input sequence xj is composed by T vectors (circles).,
The target label (square) is provided only after the last vector xT j .,
Best viewed in color.,
"A Continual Learning scenario de- ﬁnes the properties of the data stream, such as the distinction be- tween diﬀerent tasks and the proper- ties of new data samples.",
"The work of [82] and, subsequently, [71] introduces a classiﬁcation for continual learn- ing scenarios according to two prop- erties: the task label sequence and the content of incoming data.",
"The authors distinguished between Multi Task (MT) scenarios, where each incoming task is diﬀerent from the previous ones, Single Incremental Task (SIT) in which the task is always the same, and Multi Incremental Task (MIT) in which new and old tasks may be interleaved and presented again to the model.",
"Each task, identiﬁed by its task label, can be composed by one or more steps.",
Figure 1 compares SIT and MT scenarios with respect to the task label information.,
"In addition, data from new steps may provide New Classes (NC), New Instances (NI) of previously seen classes or a mix of both, that is New Instances and Classes (NIC).",
"A previous categorization, proposed in [111], focused on three main scenarios: Task incremental, Domain incremental and Class incremental which can ﬁnd their place in the framework of [82, 71].",
Table 1 summarizes and compare these diﬀerent classiﬁcations.,
"Task-incremental (MT, MIT) scenarios assume the availability of task labels for each sample.",
"Unfortunately, in many real-world applications it is diﬃcult to obtain explicit task labels for each sample.",
"While sometimes it may be possible to label the data for training, at test time task labels are often not available.",
This distinction is fundamental since the availability of task labels simpliﬁes the learning problem (see Section 6.3).,
"For example, in multi-task scenarios it is possible to use a multi-head model, in which each task has its own output layer (head).",
"At test time, the model uses the task label to choose the appropriate head and to compute the output.",
"Most CL systems deployed in the real world operate without task labels, and therefore it is important to develop algorithms that do not require them.",
"Single incremental task scenarios, where the task label is always the same, re- quire to use single-head output layers [38].",
"Single-head models do not partition the output space, thus they are more susceptible to catastrophic forgetting (see Section 6.3 for an experimental comparison).",
"Even without the availability of task label, a model could still leverage a multi-head output layer, but then it 5 Multi Task Single Incremental Task Multi Incremental Task New Classes Task incremental Class incremental New Instances — Domain incremental New Instances and Classes — Table 1: Comparison of diﬀerent CL scenarios classiﬁcation.",
The table highlights the fact that the three scenarios for CL introduced by [111] are not the only possible ones in CL.,
"In fact, the scenario classiﬁcation in [82] provides 7 scenarios.",
"Dashes indicate unrealistic scenarios: Multi Task is only compatible with New Classes, since new instances would belong to previously encountered tasks.",
Empty cells indicate scenarios which could occur but are not included in the classiﬁcation of [111].,
would have to infer at test time which head to use [27].,
Figure 3 shows the diﬀerence between multi and single headed models.,
Head 1 MODEL Head 2 Head 3 MODEL Single Head Figure 3: Multi-head model (left) and single- head model (right).,
"In the multi-head model, the output layer allocates a diﬀerent set of out- put units (head) for each task.",
The single-head model use the same output layer for each task.,
Best viewed in color.,
The classiﬁcation proposed above assumes task boundaries are exact and provided by the environment.,
"Task-free or online CL [101, 53, 5, 123] removes such assumption by fo- cusing on a stream of data in which a task label is not provided (SIT) and examples are seen one (or a very small number) at a time.",
Incre- mental or online learning (see [31] for a review) shares similarities with continual learning.,
"However, online learning usually focuses less on catas- trophic forgetting and more on fast learning of new data.",
"In this scenario, old knowledge must be preserved to help with the new step.",
"However, the old steps are not necessarily revisited.",
"This settings is common to many time series applications, such as stock price prediction, or weather and energy consumption prediction [31].",
"Taxonomy of Continual Learning Strategies Continual learning strategies are traditionally divided into three main fam- ilies [91]: regularization strategies, architectural strategies and replay (or re- hearsal) strategies.",
6 Regularization strategies balance plasticity and stability by adding a reg- ularization term to the loss function.,
"For example, Elastic Weight Consolidation (EWC) [64] and Synaptic Intelligence [122] estimate the parameters importance and foster the model stability by penalizing large changes in important parame- ters.",
"Importance values can be updated after each step, as in EWC and its more eﬃcient version [102], or online after each minibatch, as in Synaptic Intelligence, Memory Aware Synapses [3] and EWC++ [19].",
The eﬀectiveness of importance-based regularization approaches has been ques- tioned in [72] for class incremental scenarios.,
The authors showed that this family of strategies suﬀers from complete forgetting.,
Learning without Forgetting (LwF) [75] is a regularization strategy which is not based on importance values.,
LwF retains the knowledge of previous steps by using knowledge distillation [58] to encourage the current model to behave similarly to the models trained on the previous steps.,
"Architectural strategies increment the model plasticity with dynamic mod- iﬁcations to its architecture, for example by adding new components [98, 120].",
"Forgetting may be mitigated by freezing previous components [98], by reorga- nizing the architecture via pruning [61] or by using sparse connections [104].",
"Due to the very general idea behind this family of strategies, it is quite chal- lenging to organize the existing body of works.",
"Nonetheless, most members of the architectural family share some commonalities: an increasing computational cost with the number of steps due to the incremental model expansion and the need to choose a technique to prevent forgetting on the old model components.",
"Replay strategies leverage an external memory to store previous examples by selecting them randomly or with more ad-hoc criteria [97, 109, 4].",
"Since the amount of storage required for the memory could grow without bounds, several approaches use generative replay to avoid the need to explicitly store patterns [110, 115].",
"A generative model replaces the memory, thus bounding the mem- ory cost to the storage of the generative model weights.",
"However, catastrophic forgetting must be still taken into consideration when training the generative model.",
The three aforementioned families of CL strategies do not capture the en- tirety of the proposals in CL.,
"Bayesian approaches are increasingly used to mitigate CF [123, 86, 73, 37] and they have been successfully combined with regularization [33, 2], architectural approaches [85] and replay [67].",
Bayesian methods in CL consider an initial distribution on the model parameters and iteratively approximate its posterior when new tasks arrive.,
"The approximation usually includes components able to mitigate forgetting of previous tasks: as an example, the learning trajectory can be controlled by the uncertainty as- sociated to each model parameter, computed from its probability distribution.",
"If a parameter has a large uncertainty value, it is deemed not to be relevant for the current task.",
"Therefore, to protect important parameters and mitigate forgetting, the allowed magnitude in parameter change is set to be proportional to its uncertainty [2, 33].",
Another line of research explores the use of Sparse distributed represen- 7 tations [1].,
"Machine learning models often produce dense, entangled activa- tions, where a slight change in one parameter may aﬀect the entire represen- tation, causing catastrophic forgetting.",
"In contrast, sparse distributed repre- sentations naturally create independent substructures which do not aﬀect each other and are less prone to CF [44].",
"As a result, methods that encourage spar- sity produce disentangled representations which do not interfere with each other [47, 90, 25, 6].",
The main problem is that sparsity by design is not suﬃcient: the model has to learn how to use sparse connections during training.,
"Since one of the objectives of CL is to quickly learn new information, it seems natural to combine CL and Meta Learning [60].",
"There are already eﬀorts in this direction [12, 62, 16], with a focus on online and task-free scenarios [39, 57, 54].",
"Finally, CL has also been studied through the lens of graph- structured data: expressing similarities in the input patterns through graphs may help in mitigating forgetting [106].",
"Also, deep graph networks have been successfully combined with CL approaches on a number of class-incremental graph benchmarks [17].",
"In the remainder of this section, we present in more details the methods that we decided to use in our experiments.",
We focus on regularization and replay approaches for recurrent networks.,
Architectural approaches are a promising avenue of research but they cannot be easily adapted to diﬀerent model archi- tectures.,
"As a result, none of them has emerged as a standard in the literature.",
"Therefore, we decided to focus on model-agnostic methods that can be easily adapted to a wide range of models and CL scenarios.",
Elastic Weight Consolidation.,
Elastic Weight Consolidation (EWC) [64] is one of the most popular CL strategies.,
It belongs to the regularization family and it is based on the estimation of parameters importance.,
"At the end of each step, EWC computes an importance vector Ωn Θ for parameters Θ.",
"The importance vector is computed on the training set D at step n by approximating the diagonal elements of the Fisher Information Matrix: Ωn Θ = E(x,y)∈D  diag((∇Θ log pΘ(y|x)) (∇Θ log pΘ(y|x))T )  = E(x,y)∈D  (∇Θ log pΘ(y|x))2 , (2) where pθ is the output of the model parameterized by Θ and (x, y) is the input- target pair.",
By strictly following Eq.,
"2, the expectation operator would require to compute the squared gradient on each data sample and then average the result.",
"Since this procedure may be quite slow, a minibatch approach is usually taken.",
No major diﬀerence is experienced between these two versions.,
"During training, the loss function is augmented with a regularization term R which keeps important parameters close to their previous value: R(Θ, Ω) = λ n−1 X t=1 X θ∈Θ Ωt θ(θt −θn)2.",
(3) The hyperparameter λ controls the amount of regularization.,
8 Memory Aware Synapses.,
Memory Aware Synapses (MAS) [3] is similar to EWC since it is an importance-based regularization method.,
"Unlike EWC, MAS computes importances online in an unsupervised manner.",
"Therefore, MAS keeps only a single importance vector ΩΘ and updates it with a running average computed after each pattern: ΩΘN+1 = NΩΘN + ∇Θ∥pΘ(xk)∥2 2 N + 1 , (4) where N indexes the number of patterns seen so far.",
"To make the update faster, the running average in Eq.",
4 can be computed after each minibatch.,
The penalization during training is the same of EWC (Eq.,
Learning without Forgetting.,
Learning without Forgetting (LwF) [75] is a regu- larization approach based on knowledge distillation [58].,
"At the end of each step, a copy of the current model is saved.",
"During training, the previous model pro- duces its outputs on the current step input.",
The learning signal for the current model is regularized by a function measuring the distance between the current model output and the previous model output.,
The objective is to keep the two output distributions close to each other.,
"Formally, when training on step t, the total loss Lt is the sum of the classiﬁcation loss CE (e.g.",
"cross entropy) and the distillation penalization: Lt(xt, yt; Θt, Θt−1) = CEΘt(xt, yt) + + λ KL[pΘt(xt)||pΘt−1(xt)], (5) where KL[p || q] is the KL-divergence between p and q.",
The softmax temperature T used in the ﬁnal layer of the previous model can be tuned to control the prediction conﬁdence.,
Gradient Episodic Memory.,
Gradient Episodic Memory (GEM) [79] is a CL strategy which mitigates forgetting by projecting the gradient update along a direction which does not interfere with previously acquired knowledge.,
The av- erage gradient for each step is computed using a small buﬀer of samples collected from previous steps.,
The steps gradients are used to deﬁne a set of constraints which cannot be violated by the update direction.,
"Finding the optimal direction requires solving a quadratic programming problem: min z 1 2 ∥g −z∥2 2 (6) subject to Gz ≥γ, (7) where G is a matrix in which each column is the gradient computed step-wise on the memory samples (one column per previous step).",
The vector g is the gradient update and γ is the margin within which the constraints must be respected.,
GEM can work also in online scenarios [79].,
9 Averaged Gradient Episodic Memory.,
"Average-GEM (A-GEM) [20] is a more eﬃcient version of GEM where the quadratic program solution is approximated over a random sample from the memory, thus removing the need to compute a constraint for each previous step.",
"In this way the constraints may be broken for some step, resulting in worse performance.",
"The projected gradient ˆg is computed by ˆg = g −gT gref gT refgref gref, (8) where gref is the gradient computed on the random memory sample and g is the proposed gradient update.",
A-GEM has also been tested in online scenarios [20].,
Replay strategies store a subset of input patterns from previous steps in a separate replay memory.,
"To support the generality of our experiments, in this paper we selected patterns at random from the training set, without using any speciﬁc selection procedure.",
The memory accepts up to K patterns per class.,
"During training, each minibatch is augmented with P patterns per previous class, selected at random from the replay memory.",
We keep P to very small values in order to make the approach reasonable in a real CL environment.,
The CL strategies presented above belong to diﬀerent families and have dif- ferent characteristics.,
"In order to choose which strategy is more suitable to a speciﬁc application, one has to understand their advantages and limitations.",
"Regularization strategies like EWC, LwF and MAS are very eﬃcient since they operate without keeping a memory of previous pattern.",
"However, their per- formance usually deteriorates when observing a large number of steps [72].",
"In contrast, Replay remains eﬀective even on long streams of steps [56], even if the storage of previous patterns may be unfeasible for applications with strict memory bounds.",
"Finally, GEM and A-GEM share the storage problems of re- play, since they keep a buﬀer of previous samples.",
"However, their performance is usually superior to the regularization strategies in class-incremental settings [79].",
"Continual Learning with Recurrent Models The literature on continual learning focuses on feedforward and convolutional models, with experiments in Computer Vision and, to a lesser extent, Reinforce- ment Learning applications.",
"Recently, there has been a growing interest towards continual learning applications with sequential data.",
"In this section, we provide the ﬁrst comprehensive review of the literature on recurrent neural networks in continual learning settings.",
"In addition, we describe the main datasets and benchmarks for sequential data processing used in Continual Learning.",
"10 Paper Deep RNN Application CL scenario Large Comparison seminal [95] × - - × [42] × - SIT × [9] × - SIT × [8] × - SIT × [26] × - SIT × NLP [10] ✓ domain adaptation SIT × [74] ✓ instruction learning, NMT SIT × [118] ✓ language modeling - × [66] × language modeling Online × [81] ✓ sentiment analysis - × [107] ✓ NMT - × bio-inspired [28] × - Online × [88] × - Online, SIT × [92] × vision - × [65] × motor control MT × [89] × - Online × deep learning [119] ✓ ASR - × [103] ✓ - - × [100] ✓ synthetic - × [27] ✓ generic SIT × [32] ✓ neuroscience - × [34] ✓ - MT ✓ this work ✓ - SIT ✓ Table 2: Overview of the literature based on our categorization.",
Deep RNN refers to the use of a learning model attributable to the family of deep recurrent networks (e.g.,
Application-agnostic papers have a dash in the application column.,
A dash in the CL scenario indicates that the paper does not provide its clear indication.,
Large comparison refers to experiments with at least 3 CL baselines on two diﬀerent application domains.,
"Survey of Continual Learning in Recurrent Models We propose a new classiﬁcation in which each paper is assigned to one of 4 macro-areas: seminal work, natural language processing applications, bio- inspired and alternative recurrent models, and deep learning models.",
Table 2 provides a ﬁne-grained description of our review.,
"For each paper we highlight 5 diﬀerent properties: the group to which the paper belongs to, the use of popular deep learning architectures, the application domain, the type of CL scenarios and if the paper provides a large experimental comparison.",
Table 2 shows that we are the only one to provide a large scale evaluation of recurrent models in SIT+NC scenario.,
The details of our categorization are discussed in the following.,
11 Seminal Work.,
"Historically, interest in CL and Sequential Data Processing traces back to [95] and [42].",
The CHILD model [95] represents one of the ﬁrst attempts to deal with sequential data in reinforcement learning environments like mazes.,
"Although CHILD lacks an explicit recurrent structure, the author discussed about the possibility to include a more powerful memory component into the model (i.e.",
a recurrent neural network) to address tasks with longer temporal dependencies.,
"However, the author also recognizes the diﬃculties in this process, due to the challenge of learning very long sequences.",
"Moreover, the ﬁxed structure of a RNN compared with a growing model like CHILD high- lighted the need for approaches based on model expansion.",
"In the same period, French introduced the pseudo-recurrent connectionist net- work [42, 41], a model which makes use of pseudo replay [96] (replay based on random, but ﬁxed, input patterns), but did not address sequential data process- ing tasks.",
"Later on, the pseudo recurrent network together with pseudo-replay inspired the Reverberating Simple Recurrent Network (RSRN) [9, 8].",
This is a dual model composed by two auto-associative recurrent networks which ex- change information by means of pseudo patterns.,
"In the awake state, the per- formance network learns a new pattern through backpropagation.",
"The storage network generates a random pseudo pattern and presents it to the performance network, interleaved with the real one.",
"Once the loss falls below a certain threshold, the system enters the sleep state, in which the performance network generates a pseudo pattern and the storage network learns from it.",
"In this way, pseudo patterns produced in the awake state carry information about previous sequences to the performance network, while pseudo patterns produced in the sleep state carry information about the recently acquired knowledge to the stor- age network.",
The authors showed the presence of forgetting on toy sequences and the beneﬁcial eﬀect of pseudo patterns in mitigating it.,
"However, from their experiments it is not possible to assess the eﬀectiveness of RSRN on more realistic benchmarks.",
"From the very beginning, sparsity has been a recurring theme in Continual Learning [43, 40].",
"The Fixed Expansion Layer [25] introduced the use of a large, sparse layer to disentangle the model activations.",
"Subsequently, the Fixed Ex- pansion Layer has been applied to recurrent models [26].",
"However, in order to build the sparse layer in an optimal way, the model requires to solve a quadratic optimization problem (feature-sign search algorithm) which can be problematic in real world problems (as we discuss in Section 6).",
Natural Language Processing.,
Natural Language Processing is becoming one of the main test-beds for continual and online settings in sequential data processing [13].,
Most proposals in this area used modern recurrent architectures and fo- cus on speciﬁc problems and strategies.,
"Moreover, they rarely compare against existing CL techniques.",
It is therefore challenging to draw general conclusions on recurrent networks in CL from this kind of experiments.,
"Examples of ap- plications are online learning of language models where new words are added incrementally [74, 118, 66], continual learning in neural machine translation on multiple languages [107] and sentiment analysis on multiple domains [81].",
"12 The use of attention mechanisms [11], now widespread in NLP, may provide ap- proaches which are widely applicable, since there is no assumptions on the type of information to attend to.",
"As an example, the Progressive Memory Banks [10] augments a recurrent neural network with an external memory.",
"The memory grows in time to accommodate for incoming information, while at the same time freezing previous memory cells successfully prevents forgetting.",
"In addition, the author showed that ﬁnetuning old cells, instead of freezing them, increases for- ward transfer on future data.",
"Their experiments are executed on incremental domain adaptation tasks, where the distribution of words shifts as new domains are introduced.",
Bio-inspired and Alternative Recurrent Models.,
There are a number of con- tributions that propose customized architectures to address continual learning problems and mitigate catastrophic forgetting.,
"Spiking models for CL, such as the Hierarchical Temporal Memory [28] and the Spiking Neural Coding Net- work [88], are naturally designed to tackle sequential data processing tasks.",
"More importantly, they adopt learning algorithms which allow to control more eﬃciently the stability-plasticity trade-oﬀ.",
"While promising, these approaches are still missing a thorough empirical validation on real world CL benchmarks.",
"Echo State Networks [80] are RNNs whose recurrent component, called reser- voir, is not trained.",
"Therefore, they are appealing for CL since the reservoir cannot suﬀer from forgetting.",
One of the drawbacks may be the fact that the static connections must be able to learn diﬀerent tasks without adapting their values.,
The work in [65] tries to overcome the problem by employing a frac- tal reservoir combined with an external task vector (based on the task label) representing the current task.,
Diﬀerent reservoir chunks process diﬀerent pat- terns based on their associated task vector.,
"While the ﬁnal performance on diﬀerent motor commands for reinforcement learning environments validated the approach, the requirement of multi task scenarios limits its applicability.",
The Backpropagation Through Time (BPTT) is the most used algorithm to train recurrent networks.,
The Parallel Temporal Neural Coding Network [89] introduced a new learning algorithm which is less susceptible to forgetting than BPTT and other variants in sequential benchmarks like MNIST and language modelling.,
Temporal information may also arrive from videos.,
This is particularly impor- tant since it allows to exploit the vast literature on CL and Computer Vision.,
"However, it is also possible to develop speciﬁc solutions, as it has been done with recurrent Self Organizing Maps (SOM) [92].",
The authors incorporate temporal information into the recurrent SOM and perform object detection from short videos with CORe50 dataset [77].,
Deep Learning Models.,
"Recently, there have been a number of papers that stud- ied CL applications in sequential domains using recurrent architectures widely used in the deep learning literature, such as Elman RNNs [36] and LSTMs [59].",
The advantage of this generic approach is that it can be easily adapted to spe- 13 cialized models to solve any sequential problem.,
"As expected, vanilla recurrent models such as Elman RNNs and LSTMs suﬀer from catastrophic forgetting in CL scenarios [103, 100].",
"The combination of existing CL strategies, like Gradient Episodic Memory [79] and Net2Net [22], with RNNs has already showed promising results [103].",
"Con- trary to vanilla LSTM networks, their model was able to mitigate forgetting in three simple benchmarks.",
This important result supports the need for an exten- sive evaluation of RNNs and CL strategies not speciﬁcally tailored to sequential data processing problems.,
"Recurrent networks are also inclined to be combined with architectural strate- gies, since most of them are model agnostic.",
"The idea behind Progressive net- works [98] has been applied to recurrent models [27] and also improved by removing the need for task labels at test time with a separate set of LSTM au- toencoders, able to recognize the distribution from which the pattern is coming from.",
The resulting model is multi-headed but it is able to automatically select the appropriate head at test time in class-incremental scenarios.,
Hypernetworks (previously used for CL in [113]) are able to mitigate forgetting when combined with RNNs [34]: this work was the ﬁrst to provide an extensive comparison of traditional CL techniques in several sequential domains.,
"Diﬀerently from this paper, they use multi-task scenarios with a multi-head (see Section 6.3 for a comparison between single-head and multi-head models).",
Preserving the space spanned by the connections from being corrupted by the weight update appears to be beneﬁcial also to CL [32].,
Finance [94] and Automatic Speech Recogni- tion [119] applications have been explored as candidate application domains for online and continual learning strategies.,
"Sequential Data Processing Datasets for Continual Learning Due to the diﬀerent application domains and diﬀerent research communities interested in continual learning for SDP domains, there are no standard bench- marks used to evaluate CL strategies.",
Existing benchmarks vary greatly in terms of complexity.,
"Furthermore, diﬀerent application domains use a slightly diﬀerent language.",
"In this section, we provide a review of the diﬀerent datasets and continual learning scenarios, following the classiﬁcation described in Table 1.",
We believe that this review can favor the cross-pollination between classic CL techniques and sequential domains and between diﬀerent sequential domains.,
Table 3 provides an overview of the diﬀerent datasets in literature.,
"For each dataset we highlight previous work that used the datasets, the application domain of the data, and the CL scenario according to Table 1.",
"Clearly, most datasets in literature are used by few, or even just one, paper.",
"This is due to the diﬀerent research questions each paper tries to answer: since few works are spread over very diﬀerent areas, it is natural to ﬁnd diﬀerent benchmarks and evaluation protocols.",
"Unfortunately, these diﬀerences in the experimental setups make it impossible to compare diﬀerent models in the literature or to deduce general and task-independent conclusions.",
Diﬀerent synthetic benchmarks have been adapted to continual scenar- ios.,
"The Copy Task [50], a benchmark used to test the short-term memory of 14 Dataset Application Scenario Copy Task [103, 34] synthetic MT+NI Delay/Memory Pro/Anti [32] synthetic, neuroscience MT+NI Seq.",
"Stroke MNIST [103, 34] stroke classiﬁcation SIT+(NI/NC) Quick, Draw!",
"† stroke classiﬁcation SIT+NC MNIST-like [27] [26] † object classiﬁcation SIT+(NI/NC) CORe50 [92] object recognition SIT+(NI/NC) MNLI [10] domain adaptation SIT+NI MDSD [81] sentiment analysis SIT+NI WMT17 [14] NMT MT+NC OpenSubtitles18 [76] NMT MT+NC WIPO COPPA-V2 [63] [107] NMT MT+NC CALM [66] language modeling Online WikiText-2 [118] language modeling SIT+NI/NC Audioset [27, 34] sound classiﬁcation SIT+NC LibriSpeech, Switchboard [119] speech recognition (SIT/MT)+NC Synthetic Speech Commands † sound classiﬁcation SIT+NC Acrobot [65] reinforcement learning MT+NI Table 3: Datasets used in continual learning for sequential data processing.",
The scenario column indicates in which scenario the dataset has been used (or could be used when the related paper does not specify this information).,
Datasets used on this paper are marked with †.,
"recurrent models, incrementally increases the sequence lengths in the continual setting (SIT+NI).",
"However, the data generating distribution remains constant, which means that the drift between the diﬀerent steps is limited.",
"Pixel-MNIST is another popular benchmark for RNN models [69] where MNIST digits are presented one pixel at a time, either with the original order or using a ﬁxed permutation.",
Continual learning scenarios based on pixel-MNIST include new classes (SIT+NC in [27] and this paper or MT+NC in [35]) or new permutations (SIT+NI in this work).,
"Sequential Stroke MNIST [29] represents MNIST digits [70] as a sequence of pen strokes, with pen displacements as features (plus pen up/down bit).",
"The dataset is adapted to a continual data stream by increasing the sequence length (SIT+NI) or by creating new classes (SIT+NC in [27] and this paper, or MT+NC in [35]).",
"More realistic CL benchmarks for computer vision, like CORe50 [77], can be used also in sequential contexts to leverage temporal correlated information from videos.",
"In the Natural Language Processing domain a common scenario is the domain-incremental setting, where new instances from diﬀerent topics are grad- ually introduced (SIT+NI).",
"Examples of applications are natural language in- ference [117, 10], sentiment analysis [81] and machine translation in a MT+NC scenario [107].",
"Alternatively, [118] studies the problem of online learning a lan- guage model.",
"In this work, EWC is used to keep the previous knowledge when the recurrent model is trained with a single sequence.",
CALM is a benchmark 15 speciﬁcally designed for online CL in language modeling [66] which provides a realistic environment in which to test NLP models.,
"The most common CL scenario in the audio signal processing domain is the incremental classiﬁcation (MT/SIT+NC), where new classes are gradually introduced [27].",
"For example, AudioSet [46] is a dataset of annotated audio events.",
"The raw audio signals are already preprocessed, generating sequences of 10 timesteps with 128 features each.",
"Diﬀerently from Sequential Stroke MNIST or Copy Task, AudioSet is a real-world application.",
"However, the small sequence length may conceal possible problems when working with RNNs.",
"Other datasets in the literature refer to speciﬁc applications, like reinforcement learning [65], or neuroscience problems [32].",
"Evaluating Continual Learning Strategies In our review, we showed that literature on CL strategies for SDP domains is still in its early stages.",
The use of heterogeneous benchmarks and of diﬀerent jargon is detrimental to the comparison of the various approaches in litera- ture.,
"Most of the works focus on customized strategies, often ignoring popular techniques widely employed in other domains where CL is more mature, e.g.",
computer vision.,
"Furthermore, benchmarks have large variations in terms of complexity.",
It is for these reasons that we believe of fundamental importance to put forward a solid and articulated evaluation of existing CL strategies on RNN architectures.,
"To this end, we design an experimental setup which is easily reproducible and general enough to be used with all recurrent architectures.",
"In this section, we focus on two main points related to our evaluation pro- tocol: the choice of the CL scenarios and the application-agnostic nature of the benchmarks.",
Deﬁning the Continual Learning Scenario We rely on class-incremental classiﬁcation (SIT+NC) of sequences as a stan- dard and generic scenario to test novel CL techniques for recurrent models.,
"Incremental classiﬁcation is a challenging task - at least in the SIT scenario without explicit task labels - and it is directly applicable to many real-world problems, such as incremental audio classiﬁcation.",
"We deliberately avoid the use of Multitask scenarios (MT, MIT), in which the task label is explicitly provided for each pattern, both at training and test time.",
Although this assumption may be reasonable in very speciﬁc applications (e.g.,
"machine translation), in the vast majority of the cases such information is not available.",
We also experimented with a popular benchmark in the Domain in- cremental scenario (SIT+NI).,
This represents a simpler CL problem than class incremental.,
"Still, it is compatible with our setup since it does not use knowl- edge about task labels.",
Benchmarks We believe there is a necessity for generic CL benchmarks that measure ba- sic CL capabilities in sequential domains.,
"This paper contributes to this goal by adapting the Synthetic Speech Commands dataset [15] and Quick, Draw!",
dataset [52] to the incremental classiﬁcation scenario.,
"These are two tasks which are quite easy in an oﬄine setting (especially the Synthetic Speech Commands) but become challenging in a continual setting, making them ideal as a bench- mark.",
"Moreover, they are suitable to evaluate recurrent models since they have a sequential nature: feedforward counterparts, although applicable, are more expensive in terms of adaptive parameters since there is no weight sharing and they do not incorporate any kind of temporal knowledge.",
"In addition to the data above, our experimental protocol also includes the Permuted MNIST and Split MNIST datasets, since they are two of the most used benchmarks in the papers surveyed in our review.",
"Table 4 summarizes the main characteristics of the benchmarks used in the experiments, while Figure 4 provides a graphical representation of their patterns.",
Figure 5 highlights how our class-incremental benchmarks were created starting from the dataset classes.,
Synthetic Speech Commands (SSC).,
SSC is a dataset of audio recordings of spoken words [15].,
"Each audio represents a single word, taken from a vocabulary of 30 words, sampled at 16kHz.",
"We preprocessed each sample to extract 40 Mel coeﬃcients using a sliding window length of 25 ms, with 10 ms stride.",
The resulting sequences have a ﬁxed length of 101 timesteps.,
We did not apply any further normalization.,
"We create a Class Incremental scenario (SIT+NC) from SSC by learning two classes at a time, for a total number of 10 steps.",
"For model selection and assessment, we used a total of 16 classes out of the 30 available ones.",
Additional details on the use of SSC in the experiments are reported in Appendix B.1.,
"Quick, Draw!",
"QD is a dataset of hand-drawn sketches, grouped into 345 classes [52].",
"Each drawing is a sequence of pen strokes, where each stroke has 3 features: x and y displacements with respect to the previous stroke and a bit indicating if the pen has been lifted.",
We adopted a Class-incremental scenario (SIT+NC) by taking 2 classes at a time for a total number of 10 steps.,
"For model selection and assessment, we used a total of 16 classes out of the 345 available ones.",
Appendix B.2 provides details about the classes used in the experiments.,
"Diﬀerently from the previous datasets Quick, Draw!",
sequences have variable length.,
"Since this is often the case in real-world applications, it is important to assess the performance of recurrent models in this conﬁguration.",
Permuted MNIST (PMNIST).,
PMNIST is heavily used to train recurrent neu- ral networks on long sequences [69].,
Each MNIST image is treated as a sequence of pixels and shuﬄed according to a ﬁxed permutation.,
Each step of the data stream uses a diﬀerent permutation to preprocess the images and uses all the 10 MNIST classes.,
This allows to easily create an unlimited number of steps.,
This conﬁguration corresponds to a Domain incremental scenario (SIT+NI).,
17 SSC QD SMNIST PMNIST CL Steps 10 10 5 10 Classes per step 2 2 2 2 Sequence length 101 variable (8-211) 28/196 28/196 Total number of classes 30 345 10 10 Input size 40 3 28/4 28/4 Table 4: Benchmarks used in the experimental evaluation.,
"QuickD, Draw has patterns with variable sequence length, from a minimum of 8 steps to a maximum of 211 steps .",
"Experiments with SMNIST and PMNIST have been conducted with diﬀerent sequence lengths by taking 28 or 4 pixels at a time, resulting in sequences of length 28 and 196, respectively.",
Split MNIST (SMNIST).,
SMNIST is another popular continual adpatation of sequential MNIST.,
"In this scenario, MNIST sequences are taken two classes at a time.",
"First, all 0s and 1s are fed to the model.",
"Then, all 2s and 3s.",
And so on and so forth up to the last pair (8 and 9).,
This scenario consists of only 5 steps.,
The conﬁguration corresponds to a Class Incremental benchmark (SIT+NC).,
An Experimental Protocol for CL in SDP Domains In the following section we describe the experimental settings that we pro- pose to robustly assess CL strategies within SDP tasks and that are used in the experiments discussed in the following2.,
Model Selection.,
Hyperparameter tuning is a critical phase when building solid machine learning based predictors.,
"This process is made considerably more diﬃcult in CL scenarios, since we cannot assume to have a separate validation set comprising the entire stream of data in advance.",
"As a result, it is not possible to build the standard train-validation-split used in model selection.",
"To solve this problem, we follow the model selection procedure for CL introduced in [20].",
We separate the data stream into validation and test steps.,
"In particular, we hold out the ﬁrst 3 steps from our data stream and use them to perform model selection.",
"After selecting the best hyperparameters, we use the remainder of the stream (the test stream, composed of 10 unseen steps in our experiments), to perform model assessment.",
"Since Split MNIST has a limited number of steps (5), we decided to perform model selection and model assessment on the same data stream for this particular benchmark.",
Appendix A reports the parameters used in the model selection procedure and the selected conﬁgurations for both feedforward and recurrent models.,
"2We publish the code and conﬁguration ﬁles needed to reproduce our experiments at this link https://github.com/AndreaCossu/ContinualLearning_RecurrentNetworks 18 5 10 15 20 5 10 15 (a) SSC (b) Quick, Draw!",
(c) Row MNIST (d) Permuted Row MNIST Figure 4: Graphical representation of two patterns for each benchmark used in the experi- ments.,
4a SSC uses 40 Mel features (columns) for each of the 101 steps (rows).,
Both plots use log values.,
4b provides sketches of images (cat and airplane).,
"4c shows MNIST digits, which are provided to the model one row at a time in the Row MNIST version.",
Permuted MNIST (Fig.,
4d) permutes images row-wise.,
Best viewed in color.,
Data Preprocessing.,
Preprocessing a dataset in a CL setting suﬀers from the same limitations highlighted for the model selection phase.,
"Computing global statistics such as the mean and variance require the entire stream, which is not available beforehand.",
"As a result, we cannot operate any kind of normalization that needs access to a global statistic, since do not have access to the entire stream in advance.",
"For MNIST data we simply divided the pixel value for 255, which does not require any prior knowledge about the data stream (only about the limits of the data generating device).",
The reference recurrent model that we use in our experiments is the popular Long Short-Term Memory network (LSTM) [59] (PyTorch implemen- tation [93]).,
"LSTMs are among the most used recurrent models [24] and their performance is comparable to other alternatives, like Gated Recurrent Units [23].",
"Therefore, we decided to use LSTM as general representatives of recurrent architectures.",
"To provide a means of comparison between feedforward and recurrent models, we also consider and experiment with Multi Layer Perceptrons (MLP) with ReLU activations.",
"Notice that the feedforward models take as input the entire sequence, therefore, they can be viewed as recurrent networks which receive se- quences of length 1.",
"This alternative view over these models is useful since we 19 MODEL 1 MODEL 2 MODEL 3 Step 1 Step 2 Step 3 Figure 5: Class-incremental benchmark with 3 steps starting from the patterns of Quick, Draw!",
Each step is composed by 2 classes.,
experimented with multiple sequence lengths.,
"On PMNIST and SMNIST, we ran experiments with LSTM which takes as input 4 pixels at a time (LSTM-4), 16 pixels at a time (LSTM-16) and 28 pixels at a time (ROW-LSTM).",
Performance Metrics.,
"In order to assess the performance of the continual learn- ing strategies we chose to focus on catastrophic forgetting by measuring the av- erage accuracy over all steps at the end of training on last step T. This metric, called ACC [79], can been formalized as: ACC = 1 T T X t=1 RT,t, (9) where RT,t is the accuracy on step t after training on step T. In Appendix A.1 we also compare execution times in order to assess the computational eﬃciency of diﬀerent strategies.",
Strategies.,
"We compare the performances of our models in combination with 6 CL strategies: EWC [64], MAS [3], LwF [75], GEM [79], A-GEM [20] and Replay.",
"In addition to the continual learning strategies, we provide two baselines.",
Naive trains the model on the sequence without applying any measure against forget- ting.,
It is useful to provides a lower-bound to any reasonable CL strategy.,
Joint Training trains the model by considering the concatenation of the full stream of sequential tasks in a single batch3.,
Notice that this is not a CL strategy since it assumes access to the entire data stream.,
Joint Training can be used to highlight the eﬀect of the continuous training on the model performance.,
3Joint Training is often used as an upper bound performance for CL.,
"However, in the presence of a strong positive forward and backward transfer this may not be true.",
20 PMNIST MLP ROW-LSTM LSTM-4 EWC 0.92±0.02 0.58±0.09 0.29±0.05 LWF 0.82±0.03 0.77±0.05 0.35±0.04 MAS 0.58±0.04 0.64±0.07 0.31±0.04 GEM 0.94±0.01 0.90±0.01 0.71±0.02 A-GEM 0.80±0.03 0.58±0.08 0.20±0.03 REPLAY 0.92±0.00 0.92±0.01 0.82±0.01 NAIVE 0.31±0.04 0.61±0.06 0.30±0.03 Joint Training 0.97±0.00 0.96±0.00 0.94±0.01 SMNIST MLP ROW-LSTM LSTM-4 EWC 0.21±0.01 0.21±0.02 0.19±0.00 LWF 0.70±0.02 0.31±0.07 0.26±0.06 GEM 0.91±0.01 0.93±0.03 0.66±0.04 A-GEM 0.20±0.00 0.20±0.00 0.13±0.02 MAS 0.20±0.00 0.20±0.00 0.19±0.00 REPLAY 0.82±0.02 0.89±0.01 0.61±0.06 NAIVE 0.20±0.00 0.20±0.00 0.19±0.00 Joint Training 0.95±0.00 0.97±0.00 0.95±0.01 Table 5: Mean ACC and standard deviation over 5 runs on PMNIST and SMNIST bench- marks.,
Results We ran a set of experiments aimed at verifying 1) whether general CL strate- gies are able to mitigate forgetting in recurrent models and 2) the eﬀect of se- quence length on the catastrophic forgetting.,
Table 5 reports the mean ACC and its standard deviation (over 5 runs) computed on PMNIST and SMNIST.,
Table 6 reports the same results computed on SSC and QD.,
"Catastrophic Forgetting with CL strategies In Class-incremental scenarios, importance-based regularization strategies are subjected to a complete forgetting, as also showed by [72] for feedforward models.",
We conﬁrmed their results also for recurrent models.The eﬀectiveness of replay in SIT scenario is also conﬁrmed by our experiments.,
"GEM emerges as one of the best performing approaches not only for SM- NIST and PMNIST, but also for the more complex SSC and QD.",
"However, its computational cost remains very large (as showed in Appendix A.1), due to the quadratic optimization requested to ﬁnd a projecting direction.",
"Unfortunately, its more eﬃcient version A-GEM results in complete forgetting.",
"This is caused by the fact that A-GEM is not constrained by each previous steps, but instead, it computes an approximation of the constraints by sampling patterns randomly 21 SSC MLP LSTM EWC 0.10±0.00 0.10±0.00 LWF 0.05±0.00 0.12±0.01 MAS 0.10±0.00 0.10±0.00 GEM 0.55±0.00 0.53±0.01 A-GEM 0.05±0.00 0.09±0.01 REPLAY 0.81±0.03 0.73±0.04 NAIVE 0.10±0.00 0.10±0.00 Joint Training 0.93±0.00 0.89±0.02 QD LSTM EWC 0.12±0.02 LWF 0.12±0.01 MAS 0.10±0.00 GEM 0.47±0.03 A-GEM 0.10±0.00 REPLAY 0.49±0.02 NAIVE 0.10±0.00 Joint Training 0.96±0.00 Table 6: Mean ACC and standard deviation over 5 runs on Synthetic Speech Commands and Quick, Draw!",
benchmarks.,
from the memory.,
"While this may be suﬃcient in a MT scenario, it does not work appropriately in a SIT scenario.",
"LwF has been the most diﬃcult strategy to use, requiring a careful tuning of hyperparameters which questions its applicability to real-world environment.",
"While it works on SMNIST, its performance rapidly decreases on more complex benchmarks like SSC and QD, where it suﬀers from complete forgetting.",
Sequence Length aﬀects Catastrophic Forgetting Our experiments on PMNIST and SMNIST also allow to draw some con- clusions on the eﬀect of sequence length on forgetting.,
6 shows mean ACC results for diﬀerent sequence lengths.,
The decreasing trends highlights a clear phenomenon: long sequences cause more forgetting in recurrent networks.,
Regularization strategies suﬀer the most when increasing sequence length.,
"Figure 7 oﬀers some insights also on replay: although more robust than the other strategies in terms of ﬁnal performance, replay needs more patterns to recover from forgetting as sequences become longer.",
"The results on PMNIST with MAS may seem to contradict the sequence- length eﬀect, since ROW-LSTM performs surprisingly better than MLP.",
"How- ever, the performance has to be measured relatively to the Naive performance, in which no CL strategy is applied.",
"When compared to this value, the ROW-LSTM 22 MLP (1) ROW-LSTM (28) LSTM-16 (49) LSTM-4 (196) Model (Sequence Length) 0.0 0.2 0.4 0.6 0.8 1.0 ACC lwf gem agem mas ewc replay-10 replay-1 (a) Permuted MNIST MLP (1) ROW-LSTM (28) LSTM-16 (49) LSTM-4 (196) Model (Sequence Length) 0.0 0.2 0.4 0.6 0.8 1.0 ACC lwf gem replay-10 replay-1 (b) Split MNIST Figure 6: Average ACC on all steps for diﬀerent sequence lengths and diﬀerent CL strategies.",
Sequence length causes a decrease in performances among all strategies.,
Best viewed in color.,
does not perform any better than MLP.,
"Instead it achieves a worse accuracy with respect to the Naive, as expected.",
"The MAS-Naive ratio is 1.87 for MLP, but only 1.05 for MAS.",
"Comparison between Multi-Head and Single-Head Models Although in this work we focused on Class Incremental and Domain Incre- mental scenarios, we also provide a comparison with Task-Incremental scenarios (MT + NC).",
"Figure 8 shows results for the same experimental conﬁguration in Section 6, but with a multi-head model instead of a single-head.",
We did not test multi-head models in PMNIST since this benchmark always uses the same output units for all tasks.,
Appendix C reports Table C.13 with detailed results.,
There is a clear diﬀerence between single and multi-head performances.,
A multi-head setting easily prevents CF: even a simple Naive procedure is able to retain most of the knowledge in both class-incremental scenarios.,
The inﬂuence of sequence length in single-headed models is not reﬂected in the multi-head case.,
"In fact, the LSTM performs better than MLP on SSC: this could be related to the fact that, given the already large mitigation of forgetting with multi-head, the sequential nature of the problem favours a recurrent network.",
We concluded that MT scenarios with multi-head models should be used only when there is the need to achieve high-level performances in environments where tasks are easily distinguishable.,
"However, task label information should never be used to derive general conclusions about the behavior and performance of CL algorithms.",
23 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM-ROW LSTM-16 LSTM-4 (a) Permuted MNIST 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM-ROW LSTM-16 LSTM-4 (b) Split MNIST 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM (c) SSC 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC LSTM (d) QD Figure 7: Average ACC for diﬀerent replay patterns per minibatch.,
"The longer the input se- quence, the more the patterns needed to recover the performance.",
"Quick, Draw!",
performance does not change signiﬁcantly with respect to replay patterns in the minibatch.,
Best viewed in color.,
Research Directions The problem of learning continuously on sequential data processing tasks is far from being solved.,
"Fortunately, the recent interest in the topic is fostering the discovery of new solutions and a better understanding of the phenomena at play.",
We believe that our work could play an important role in sketching promising research directions.,
Leveraging existing CL strategies.,
"From our experiments, it is clear that some families of CL strategies are better suited to the Class-incremental scenario than others (e.g.",
GEM mitigates forgetting across all benchmarks while EWC per- forms poorly in class-incremental settings).,
"Instead of designing entirely new approaches, it could be better to start building from the existing ones.",
"Replay oﬀers a simple and eﬀective solution, but storing previous patterns may not be possible in all environments.",
It would be interesting to extend LwF with knowl- edge distillation algorithms speciﬁcally designed for recurrent networks.,
"De- pending on the eﬀectiveness of the distillation, results could drastically change.",
"Even if architectural strategies have not been part of our study, they can be an important part of the research.",
"In particular, they may be a valid option when 24 ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP multi-head LSTM-4 multi-head MLP single-head LSTM-4 single-head (a) Split MNIST ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP multi-head LSTM multi-head MLP single-head LSTM single-head (b) Synthetic Speech Commands ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC LSTM multi-head LSTM single-head (c) Quick, Draw!",
Figure 8: Bar plot comparing multi-head and single-head performances.,
Best viewed in color.,
all other approaches struggle.,
"In fact, task-speciﬁc components or freezing of previous parameters can remove forgetting by design.",
The usually large compu- tational cost can be mitigated by pruning.,
Combining pruning techniques for RNNs with dynamic architectures may open a new topic of research in CL.,
Improving recurrent models.,
The role of sequence length in continuous learning processes can be investigated under diﬀerent points of view.,
"From the point of view of dynamical systems, the learning trajectory followed by a recurrent model can be studied in terms of diﬀerent regimes (chaotic, edge-of-chaos, attractors) [18].",
Longer sequences would produce longer trajectories which could make the Continual Learning problem more diﬃcult.,
Inspecting and interpreting these trajectories could in turn foster the design of new regularization strategies to constrain the learning path.,
"Instead, from an experimental point of view, the optimization problem that has to be solved by recurrent networks is highly dependent on the sequence length.",
"For example, the Backpropagation Through Time has to propagate information from the end of the sequence to the very beginning in order to compute gradients.",
"Some of the obstacles encountered by many strategies in our experimental analysis may be overcome by alternative learning algorithms, for example Truncated Backpropagation.",
Design of realistic scenarios.,
The complexity of the CL environments is another important topic to address in future researches.,
"Providing target labels to the system each time a new pattern arrives is often too expensive, even unrealistic.",
"Instead, sequences are able to work with sparse targets [49].",
Sequential data processing may lead to more autonomous systems in which the required level of 25 supervision is drastically reduced.,
"Our proposed benchmarks could contribute to realize such scenario through a stream of realistic patterns, resembling what an agent would experience in the real world.",
"To build this setting, it could be required to concatenate multiple commands from SSC or multiple drawings from QD, possibly interleaved with noisy patterns.",
This online scenario would free the model from the necessity to learn over multiple passes over the data.,
"Instead, the environment itself would provide new instances and new classes (SIT+NIC) which can be experienced one (or few) at a time.",
"With this achievement, CL models could seamlessly be integrated in many real world applications.",
Conclusion In this work we focused on Continual Learning in Recurrent Neural Network models.,
"First, we reviewed the literature and proposed a taxonomy of contri- butions.",
We also analysed and organized the benchmarks and datasets used in CL for sequential data processing.,
The number of existing benchmarks compared to the relatively small number of contributions revealed that most papers proposed their own model and tested it on datasets which are almost never reused in subsequent works.,
"Therefore, we discussed the opportunity to build standard benchmarks and, in addition, we proposed two new CL benchmarks based on existing datasets: Synthetic Speech Commands (SSC) and Quick, Draw!",
Both datasets are speciﬁ- cally tailored to Sequential Data Processing and are general enough to be model agnostic.,
"From the literature review, we also found out that all available works focus on new CL strategies with customized architectures, often for a specialized domain.",
The diﬀerent experimental setups prevent a comparison of multiple approaches and there is little information about how recurrent networks behave with gen- eral CL strategies.,
"To ﬁll this gap, we ran an extensive experimental analysis with single-head mod- els focused on Class-Incremental scenarios.",
We compared traditional LSTMs against feedforward networks on 6 popular CL strategies.,
Our main ﬁnding shows increasing forgetting as sequences are made artiﬁcially longer without modifying their content.,
"Even replay strategies, which are the most robust among the considered methods, are subjected to this phenomenon.",
This holds also for Domain-incremental scenarios with Permuted MNIST.,
The eﬀect of sequence length on the ﬁnal performance may be questioned by results presented in [34]: the authors found the working memory (the amount of features needed to address a task) to be the main driving factor in the per- formance of recurrent models.,
"In our work, we showed that, with ﬁxed working memory, sequence length plays a role.",
"These two conclusions are not directly in contrast with each other, because the experiments of [34] have been conducted on either a MT+NC scenarios with multi-head models or on simpler scenarios like Domain-incremental.",
This diﬃculty in comparing results conﬁrms the need for our large-scale evaluation.,
"26 Finally, to justify our choice of Class-incremental scenarios, we ran additional experiments with multi-head models in Task-incremental scenarios.",
We veriﬁed that a multi-head approach greatly simpliﬁes the problem in both feedforward and recurrent architectures.,
"However, this comes at the cost of a strong as- sumption on task label knowledge.",
"Concluding, we entered into a discussion of interesting research paths inspired by our ﬁndings on the topic of Continual Learning in sequential domains.",
"We believe that addressing such challenges will make Continual Learning more ef- fective in realistic environments, where sequential data processing is the most natural way to acquire and process new information across a lifetime.",
Acknowledgments We thank the ContinualAI association and all of its members for the fruitful discussions.,
Funding This work has been partially supported by the European Community H2020 programme under project TEACHING (grant n. 871385).,
References [1] Subutai Ahmad and JeﬀHawkins.,
How do neurons operate on sparse distributed representations?,
"A mathematical theory of sparsity, neurons and active dendrites.",
"arXiv, pages 1–23, 2016.",
"[2] Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon.",
Uncertainty-based Continual Learning with Adaptive Regularization.,
"In NeurIPS, pages 4392–4402, 2019.",
"[3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.",
Memory Aware Synapses: Learning what (not) to forget.,
"In The European Conference on Computer Vision (ECCV), 2018.",
"[4] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia.",
Online Continual Learning with Maximal Interfered Retrieval.,
"In NeurIPS, pages 11849– 11860.",
"Curran Associates, Inc., 2019.",
"[5] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars.",
Task-Free Continual Learning.,
"In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.",
"[6] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars.",
Selﬂess Sequen- tial Learning.,
"In ICLR, 2019.",
"27 [7] Giuseppe Amato, Davide Bacciu, Stefano Chessa, Mauro Dragone, Clau- dio Gallicchio, Claudio Gennaro, Hector Lozano, Alessio Micheli, Gregory M. P. O’Hare, Arantxa Renteria, and Claudio Vairo.",
A benchmark dataset for human activity recognition and ambient assisted living.,
"In Helena Lindgren, Juan F. De Paz, Paulo Novais, Antonio Fern´andez-Caballero, Hyun Yoe, Andres Jim´enez Ram´ırez, and Gabriel Villarrubia, editors, Ambient Intelligence- Software and Applications – 7th International Sym- posium on Ambient Intelligence (ISAmI 2016), pages 1–9.",
"Springer Inter- national Publishing, 2016.",
"[8] Bernard Ans, St´ephane Rousset, Robert M. French, and Serban Musca.",
Self-refreshing memory in artiﬁcial neural networks: Learning temporal sequences without catastrophic forgetting.,
"Connection Science, 16(2):71– 99, 2004.",
"[9] Bernard Ans, Stephane Rousset, Robert M. French, and Serban C. Musca.",
Preventing Catastrophic Interference in MultipleSequence Learning Using Coupled Reverberating Elman Networks.,
"In Proceedings of the 24th An- nual Conference of the Cognitive Science Society, 2002.",
"[10] Nabiha Asghar, Lili Mou, Kira A Selby, Kevin D Pantasdo, Pascal Poupart, and Xin Jiang.",
Progressive Memory Banks for Incremental Do- main Adaptation.,
"In ICLR, 2019.",
"[11] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.",
Neural Machine Translation by Jointly Learning to Align and Translate.,
"ICLR, 2015.",
"[12] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, JeﬀClune, and Nick Cheney.",
Learning to Continually Learn.,
"In ECAI, 2020.",
"[13] Magdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-juss`a.",
Continual Lifelong Learning in Natural Language Processing: A Survey.,
"In Proceedings of the 28th International Conference on Computational Linguistics, pages 6523–6541, Barcelona, Spain (Online), 2020.",
Interna- tional Committee on Computational Linguistics.,
"[14] Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Ru- bino, Lucia Specia, and Marco Turchi.",
Findings of the 2017 Conference on Machine Translation (WMT17).,
"In Proceedings of the Second Con- ference on Machine Translation, pages 169–214, Copenhagen, Denmark, 2017.",
Association for Computational Linguistics.,
[15] Johannes Buchner.,
Synthetic Speech Commands: A public dataset for single-word speech recognition.,
"Kaggle Dataset, 2017.",
"28 [16] Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez, and Laurent Charlin.",
Online Fast Adaptation and Knowl- edge Accumulation: A New Approach to Continual Learning.,
"arXiv, 2020.",
"[17] Antonio Carta, Andrea Cossu, Federico Errica, and Davide Bacciu.",
Catas- trophic Forgetting in Deep Graph Networks: An Introductory Benchmark for Graph Classiﬁcation.,
"The 2021 Web Conference (WWW) Workshop on Graph Benchmarks Learning (GLB), 2021.",
"[18] Andrea Ceni, Peter Ashwin, and Lorenzo Livi.",
Interpreting Recurrent Neural Networks Behaviour via Excitable Network Attractors.,
"Cognitive Computation, 12(2):330–356, 2020.",
"[19] Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr.",
Riemannian Walk for Incremental Learning: Under- standing Forgetting and Intransigence.,
"In Proceedings of the European Conference on Computer Vision (ECCV), pages 532–547, 2018.",
"[20] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mo- hamed Elhoseiny.",
Eﬃcient Lifelong Learning with A-GEM.,
"In ICLR, 2019.",
"[21] Dechao Chen, Shuai Li, and Liefa Liao.",
A recurrent neural network ap- plied to optimal motion control of mobile robots with physical constraints.,
"Applied Soft Computing, 85:105880, December 2019.",
"[22] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.",
Net2Net: Accelerat- ing Learning via Knowledge Transfer.,
"In ICLR, 2016.",
"[23] Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio.",
On the Properties of Neural Machine Translation: En- coder–Decoder Approaches.,
"In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103– 111, Doha, Qatar, 2014.",
Association for Computational Linguistics.,
"[24] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Ben- gio.",
Empirical evaluation of gated recurrent neural networks on sequence modeling.,
"In NIPS Workshop on Deep Learning, 2014.",
[25] Robert Coop and Itamar Arel.,
Mitigation of catastrophic interference in neural networks using a ﬁxed expansion layer.,
"In 2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 726–729.",
"IEEE, 2012.",
[26] Robert Coop and Itamar Arel.,
Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer.,
"In The 2013 International Joint Conference on Neural Networks (IJCNN), pages 1–7, Dallas, TX, USA, 2013.",
"29 [27] Andrea Cossu, Antonio Carta, and Davide Bacciu.",
Continual Learning with Gated Incremental Memories for sequential data processing.,
"In Pro- ceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020), 2020.",
"[28] Yuwei Cui, Subutai Ahmad, and JeﬀHawkins.",
Continuous Online Se- quence Learning with an Unsupervised Neural Network Model.,
"Neural Computation, 28(11):2474–2504, 2016.",
[29] Edwin D. de Jong.,
Incremental Sequence Learning.,
"arXiv: 1611.03068 [cs], 2016.",
"[30] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.",
A continual learning survey: Defying forgetting in classiﬁcation tasks.,
"arXiv, 2019.",
"[31] Gregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar.",
Learn- ing in Nonstationary Environments: A Survey.,
"IEEE Computational In- telligence Magazine, 10(4):12–25, 2015.",
"[32] Lea Duncker, Laura N Driscoll, Krishna V Shenoy, Maneesh Sahani, and David Sussillo.",
Organizing recurrent network dynamics by task- computation to enable continual learning.,
"In NeurIPS, volume 33, 2020.",
"[33] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach.",
Uncertainty-guided Continual Learning with Bayesian Neu- ral Networks.,
"In ICLR, 2020.",
"[34] Benjamin Ehret, Christian Henning, Maria R Cervera, Alexander Meule- mans, Johannes von Oswald, and Benjamin F Grewe.",
Continual Learning in Recurrent Neural Networks.,
"arXiv, 2020.",
"[35] Benjamin Ehret, Christian Henning, Maria R. Cervera, Alexander Meule- mans, Johannes von Oswald, and Benjamin F. Grewe.",
Continual Learning in Recurrent Neural Networks with Hypernetworks.,
"arXiv:2006.12109 [cs, stat], 2020.",
[36] Jeﬀrey L. Elman.,
Finding Structure in Time.,
"Cognitive Science, 14(2):179–211, 1990.",
[37] Sebastian Farquhar and Yarin Gal.,
A Unifying Bayesian View of Continual Learning.,
"In NeurIPS Bayesian Deep Learning Workshop, 2018.",
[38] Sebastian Farquhar and Yarin Gal.,
Towards Robust Evaluations of Con- tinual Learning.,
"In Privacy in Machine Learning and Artiﬁcial Intelligence Workshop, ICML, 2019.",
"[39] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.",
Online Meta-Learning.,
"In ICML, 2019.",
30 [40] Robert French.,
Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks.,
"In In Proceedings of the 13th Annual Cognitive Science Society Conference, pages 173–178.",
"Erlbaum, 1991.",
[41] Robert French.,
Pseudo-recurrent Connectionist Networks: An Approach to the ’Sensitivity-Stability’ Dilemma.,
"Connection Science, 9(4):353–380, 1997.",
[42] Robert French.,
Using Pseudo-Recurrent Connectionist Networks to Solve the Problem of Sequential Learning.,
"In Proceedings of the 19th Annual Cognitive Science Society Conference, 1997.",
[43] Robert French.,
Catastrophic forgetting in connectionist networks.,
"Trends in Cognitive Sciences, 3(4):128–135, 1999.",
[44] Robert M. French.,
Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks.,
"In In Proceedings of the 13th Annual Cognitive Science Society Conference, pages 173–178.",
"Erlbaum, 1991.",
"[45] Jo˜ao Gama, Indr˙e ˇZliobait˙e, Albert Bifet, Mykola Pechenizkiy, and Ab- delhamid Bouchachia.",
A survey on concept drift adaptation.,
"ACM Com- puting Surveys (CSUR), 46(4):44:1–44:37, 2014.",
"[46] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter.",
Audio Set: An ontology and human-labeled dataset for audio events.,
"In 2017 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), pages 776– 780, 2017.",
"[47] Siavash Golkar, Michael Kagan, and Kyunghyun Cho.",
Continual Learning via Neural Pruning.,
"arXiv, 2019.",
[48] Alex Graves.,
Sequence Transduction with Recurrent Neural Networks.,
"arXiv, 2012.",
"[49] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmid- huber.",
Connectionist temporal classiﬁcation: Labelling unsegmented se- quence data with recurrent neural networks.,
"In ICML, ICML ’06, pages 369–376, New York, NY, USA, 2006.",
Association for Computing Machin- ery.,
"[50] Alex Graves, Greg Wayne, and Ivo Danihelka.",
Neural Turing Machines.,
"arXiv:1410.5401 [cs], 2014.",
[51] Stephen Grossberg.,
How does a brain build a cognitive code?,
"Psycholog- ical Review, 87(1):1–51, 1980.",
31 [52] David Ha and Douglas Eck.,
A Neural Representation of Sketch Drawings.,
"In ICLR, 2018.",
"[53] Michael Bonnell Harries, Claude Sammut, and Kim Horn.",
Extracting Hidden Context.,
"Machine Learning, 32(2):101–126, 1998.",
"[54] James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone.",
Con- tinuous meta-learning without tasks.,
"arXiv, 2019.",
[55] Mahmudul Hasan and Amit K. Roy-Chowdhury.,
A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models.,
"IEEE Transactions on Multimedia, 17(11):1909–1922, 2015.",
"[56] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan.",
Memory Ef- ﬁcient Experience Replay for Streaming Learning.,
"IEEE International Conference on Robotics and Automation (ICRA), 2018.",
"[57] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan Pascanu.",
Task Agnostic Continual Learning via Meta Learning.,
"arxiv, 2019.",
"[58] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀrey Dean.",
Distilling the Knowl- edge in a Neural Network.,
"In NIPS Deep Learning and Representation Learning Workshop, 2015.",
[59] Sepp Hochreiter and Jurgen Schmidhuber.,
Long Short-Term Memory.,
"Neural Computation, 9:1735–1780, 1997.",
"[60] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.",
Meta-Learning in Neural Networks: A Survey.,
"arXiv:2004.05439 [cs, stat], 2020.",
"[61] Steven C Y Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi- Ming Chan, and Chu-Song Chen.",
"Compacting, Picking and Growing for Unforgetting Continual Learning.",
"In NeurIPS, pages 13669–13679, 2019.",
[62] Khurram Javed and Martha White.,
Meta-Learning Representations for Continual Learning.,
"In NeurIPS, 2019.",
"[63] Marcin Junczys-Dowmunt, Bruno Pouliquen, and Christophe Mazenc.",
COPPA V2.0: Corpus of parallel patent applications.,
Building large par- allel corpora with GNU make.,
"In Proceedings of the 4th Workshop on Challenges in the Management of Large Corpora, Portoroˇz, Slovenia, May 23-28, 2016, 2016.",
"[64] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guil- laume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ra- malho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell.",
Overcoming catastrophic forget- ting in neural networks.,
"PNAS, 114(13):3521–3526, 2017.",
32 [65] Taisuke Kobayashi and Toshiki Sugino.,
Continual Learning Exploiting Structure of Fractal Reservoir Computing.,
"In Igor V Tetko, Vˇera K˚urkov´a, Pavel Karpov, and Fabian Theis, editors, Artiﬁcial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions, vol- ume 11731, pages 35–47, Cham, 2019.",
Springer International Publishing.,
"[66] Germ´an Kruszewski, Ionut-Teodor Sorodoc, and Tomas Mikolov.",
Evalu- ating Online Continual Learning with CALM.,
"arXiv, 2020.",
"[67] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan G¨unnemann.",
Continual Learning with Bayesian Neural Networks for Non-Stationary Data.,
"In ICLR, 2020.",
"[68] Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma.",
"FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network.",
"arXiv:1901.02358 [cs, stat], January 2019.",
"[69] Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton.",
A simple way to initialize recurrent networks of rectiﬁed linear units.,
"arXiv preprint arXiv:1504.00941, 2015.",
"[70] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner.",
Gradient- Based Learning Applied to Document Recognition.,
"Proceedings of the IEEE, 86(11):2278–2324, 1998.",
"[71] Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia D´ıaz-Rodr´ıguez.",
"Continual learning for robotics: Deﬁnition, framework, learning strategies, opportunities and challenges.",
"Information Fusion, 58:52–68, 2020.",
"[72] Timoth´ee Lesort, Andrei Stoian, and David Filliat.",
Regularization Short- comings for Continual Learning.,
"arXiv, 2020.",
"[73] HongLin Li, Payam Barnaghi, Shirin Enshaeifar, and Frieder Ganz.",
Con- tinual Learning Using Bayesian Neural Networks.,
"arXiv, 2019.",
"[74] Yuanpeng Li, Liang Zhao, Kenneth Church, and Mohamed Elhoseiny.",
Compositional Language Continual Learning.,
"In ICLR, 2020.",
[75] Zhizhong Li and Derek Hoiem.,
Learning without Forgetting.,
"In European Conference on Computer Vision, Springer, pages 614–629, 2016.",
"[76] Pierre Lison, J¨org Tiedemann, and Milen Kouylekov.",
"OpenSubtitles2018: Statistical Rescoring of Sentence Alignments in Large, Noisy Parallel Cor- pora.",
"In Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018), pages 1742–1748.",
"European Lan- guage Resources Association (ELRA), 2018.",
33 [77] Vincenzo Lomonaco and Davide Maltoni.,
CORe50: A New Dataset and Benchmark for Continuous Object Recognition.,
"In Sergey Levine, Vin- cent Vanhoucke, and Ken Goldberg, editors, Proceedings of the 1st An- nual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 17–26.",
"PMLR, 2017.",
"[78] Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graﬃeti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Sub- utai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni.",
Avalanche: An End-to-End Library for Continual Learning.,
"In CLVision Workshop at CVPR, 2021.",
[79] David Lopez-Paz and Marc’Aurelio Ranzato.,
Gradient Episodic Memory for Continual Learning.,
"In NIPS, 2017.",
[80] Mantas Lukoˇseviˇcius and Herbert Jaeger.,
Reservoir computing approaches to recurrent neural network training.,
"Computer Science Review, 3(3):127– 149, 2009.",
[81] Avinash Madasu and Vijjini Anvesh Rao.,
Sequential Domain Adaptation through Elastic Weight Consolidation for Sentiment Analysis.,
"arXiv, 2020.",
[82] Davide Maltoni and Vincenzo Lomonaco.,
Continuous Learning in Single- Incremental-Task Scenarios.,
"Neural Networks, 116:56–73, 2019.",
"[83] James L. McClelland, Bruce L. McNaughton, and Andrew K. Lampinen.",
Integration of new information in memory: New insights from a comple- mentary learning systems perspective.,
"Philosophical Transactions of the Royal Society B: Biological Sciences, 375(1799):20190637, 2020.",
[84] Michael McCloskey and Neal J. Cohen.,
Catastrophic Interference in Con- nectionist Networks: The Sequential Learning Problem.,
"In Gordon H. Bower, editor, Psychology of Learning and Motivation, volume 24, pages 109–165.",
"Academic Press, 1989.",
"[85] Nikhil Mehta, Kevin J Liang, and Lawrence Carin.",
Bayesian Nonpara- metric Weight Factorization for Continual Learning.,
"arXiv, pages 1–17, 2020.",
"[86] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner.",
Variational Continual Learning.,
"In ICLR, 2018.",
"[87] Hung Nguyen, Xuejian Wang, and Leman Akoglu.",
Continual Rare-Class Recognition with Emerging Novel Subclasses.,
"In ECML, 2019.",
34 [88] Alexander Ororbia.,
Spiking Neural Predictive Coding for Continual Learning from Data Streams.,
"arXiv, 2020.",
"[89] Alexander Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer.",
Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations.,
"arXiv, 2019.",
"[90] Alexander Ororbia, Ankur Mali, Daniel Kifer, and C Lee Giles.",
Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively.,
"arXiv, pages 1–11, 2019.",
"[91] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.",
Continual lifelong learning with neural networks: A review.,
"Neural Networks, 113:54–71, 2019.",
"[92] German I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter.",
Life- long Learning of Spatiotemporal Representations With Dual-Memory Re- current Self-Organization.,
"Frontiers in Neurorobotics, 12, 2018.",
"[93] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.",
"PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
"NeurIPS, 2019.",
"[94] Daniel Philps, Artur d’Avila Garcez, and Tillman Weyde.",
Making Good on LSTMs’ Unfulﬁlled Promise.,
"arXiv, 2019.",
[95] Mark B Ring.,
CHILD: A First Step Towards Continual Learning.,
"Machine Learning, 28(1):77–104, 1997.",
[96] Anthony Robins.,
Catastrophic Forgetting; Catastrophic Interference; Sta- bility; Plasticity; Rehearsal.,
"Connection Science, 7(2):123–146, 1995.",
"[97] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P Lillicrap, and Greg Wayne.",
Experience Replay for Continual Learning.,
"In NeurIPS, pages 350–360, 2019.",
"[98] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Had- sell.",
Progressive Neural Networks.,
"arXiv, 2016.",
[99] Anton Maximilian Sch¨afer and Hans Georg Zimmermann.,
Recurrent Neu- ral Networks Are Universal Approximators.,
"In Stefanos D. Kollias, An- dreas Stafylopatis, W lodzis law Duch, and Erkki Oja, editors, Artiﬁcial Neural Networks – ICANN 2006, Lecture Notes in Computer Science, pages 632–640.",
"Springer Berlin Heidelberg, 2006.",
35 [100] Monika Schak and Alexander Gepperth.,
A Study on Catastrophic For- getting in Deep LSTM Networks.,
"In Igor V Tetko, Vˇera K˚urkov´a, Pavel Karpov, and Fabian Theis, editors, Artiﬁcial Neural Networks and Ma- chine Learning – ICANN 2019: Deep Learning, Lecture Notes in Com- puter Science, pages 714–728.",
"Springer International Publishing, Cham, 2019.",
[101] Jeﬀrey C. Schlimmer and Richard H. Granger.,
Incremental learning from noisy data.,
"Machine Learning, 1(3):317–354, 1986.",
"[102] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell.",
Progress & Compress: A scalable framework for continual learning.,
"In ICML, pages 4528–4537, 2018.",
"[103] Shagun Sodhani, Sarath Chandar, and Yoshua Bengio.",
Toward Training Recurrent Neural Networks for Lifelong Learning.,
"Neural Computation, 32(1):1–35, 2019.",
"[104] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy.",
SpaceNet: Make Free Space For Continual Learning.,
"arXiv, 2020.",
"[105] Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee.",
LAMOL: LAnguage MOdeling for Lifelong Language Learning.,
"In ICLR, 2020.",
[106] Binh Tang and David S. Matteson.,
Graph-Based Continual Learning.,
"In ICLR, 2020.",
"[107] Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn.",
Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation.,
"In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2062–2068, Minneapolis, Minnesota, 2019.",
As- sociation for Computational Linguistics.,
"[108] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoﬀrey J Gordon.",
An Empirical Study of Example Forgetting during Deep Neural Network Learning.,
"In ICLR, 2019.",
"[109] Gido M. van de Ven, Hava T. Siegelmann, and Andreas S. Tolias.",
Brain- inspired replay for continual learning with artiﬁcial neural networks.,
"Na- ture Communications, 11, 2020.",
[110] Gido M. van de Ven and Andreas S. Tolias.,
Generative replay with feed- back connections as a general strategy for continual learning.,
"arXiv, 2018.",
[111] Gido M van de Ven and Andreas S Tolias.,
Three scenarios for continual learning.,
"In Continual Learning Workshop NeurIPS, 2018.",
"36 [112] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.",
Attention is All you Need.,
"In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer- gus, S. Vishwanathan, and R. Garnett, editors, NIPS, pages 5998–6008.",
"Curran Associates, Inc., 2017.",
"[113] Johannes von Oswald, Christian Henning, Jo&#xE3, O Sacramento, and Benjamin F. Grewe.",
Continual learning with hypernetworks.,
"In ICLR, 2019.",
"[114] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang.",
Phoneme recognition using time-delay neural networks.,
"IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3):328–339, 1989.",
"[115] Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis, and Lau- rent Charlin.",
Continual Learning of New Sound Classes using Generative Replay.,
"arXiv, 2019.",
[116] Gerhard Widmer and Miroslav Kubat.,
Learning in the presence of concept drift and hidden contexts.,
"Machine Learning, 23(1):69–101, 1996.",
"[117] Adina Williams, Nikita Nangia, and Samuel Bowman.",
A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.,
"In Pro- ceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana, 2018.",
Association for Computational Linguistics.,
"[118] Thomas Wolf, Julien Chaumond, and Clement Delangue.",
Continuous Learning in a Hierarchical Multiscale Neural Network.,
"In ACL, 2018.",
"[119] Jiabin Xue, Jiqing Han, Tieran Zheng, Xiang Gao, and Jiaxing Guo.",
A Multi-Task Learning Framework for Overcoming the Catastrophic Forget- ting in Automatic Speech Recognition.,
"arXiv, 2019.",
"[120] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang.",
Lifelong Learning With Dynamically Expandable Networks.,
"In ICLR, 2018.",
"[121] T Young, Devamanyu Hazarika, S Poria, and E Cambria.",
Recent Trends in Deep Learning Based Natural Language Processing.,
"IEEE Computational Intelligence Magazine, 13:55–75, 2018.",
"[122] Friedemann Zenke, Ben Poole, and Surya Ganguli.",
Continual Learning Through Synaptic Intelligence.,
"In ICML, pages 3987–3995, 2017.",
"[123] Chen Zeno, Itay Golan, Elad Hoﬀer, and Daniel Soudry.",
Task Agnostic Continual Learning Using Online Variational Bayes.,
"In NeurIPS Bayesian Deep Learning Workshop, 2018.",
37 Appendix A.,
Experiment conﬁguration Here we report additional details on the experiments conﬁguration.,
Each step is divided into a training set and a test set.,
We used the default train-test split of Torchvision for MNIST-based datasets.,
"For SSC we randomly generate a train-test split for each class, with 20% of patterns for the test set.",
"Quick, Draw!",
already provides separate splits for each class.,
We changed ran- dom seed on each run.,
"To make training stabler with RNNs, we used a ﬁxed permutation on SMNIST which does not change across steps.",
This uniformly spreads the information along the sequence.,
"Following the original paper, A-GEM concatenates a one-hot task vector to the input patterns.",
"Since we adopt class-incremental scenarios, we concatenate the vector only at training time.",
"On SSC, we leveraged the Avalanche library [78] for the GEM implementation.",
"We tested a wide range of hyperparameters for the grid search, as showed by Table A.7 for PMNIST, Table A.8 for SMNIST, Table A.9 for SSC and Table A.10 for QD.",
"The tables use the following abbreviations: number of layers is nl, number of hidden units is hs, minibatch size is mbs, optimizer is opt, learning rate is lr, patterns per step in GEM memory is pps, sample size from A-GEM memory is ss, softmax temperature in LwF is T. MLPs are with ReLU activa- tions.",
"LSTMs have 1 layer for MNIST-based benchmarks, 2 layers for SSC and Quick, Draw!.",
LSTM with QD uses 512 hidden units.,
Multi-head experiments use the same conﬁguration.,
All models use gradient clipping at norm 5.,
"The number of epochs is set to guarantee convergence except for online methods (GEM, A-GEM) which use 2 epochs and minibatches of 10 patterns.",
The default optimizer is Adam with no regularization.,
Appendix A.1.,
Computational Complexity We monitored execution times for recurrent architectures with diﬀerent CL strategies.,
"We ran experiments on a single V100 GPU, with 3 threads at a time on a Intel® Xeon® Gold 6140M CPU with 2.30GHz frequency.",
Our results clearly show that the cost of GEM is quite large and may be pro- hibitive in realistic applications.,
Appendix B. Benchmarks Description Appendix B.1.,
Synthetic Speech Commands We introduced the Synthetic Speech Commands (SSC) [15] benchmark4 and adapted it for a class-incremental setting.,
"We took 2 classes at a time and we 4The Synthetic Speec Commands Dataset is available at https://www.kaggle.com/ jbuchner/synthetic-speech-commands-dataset 38 PMNIST LSTM MLP EWC λ=(0.1, 1, 10, 100), hs=(256, 512) λ=(0.1, 1, 10, 100), nl=(1,2), hs=512 MAS λ=(0.1, 1, 10, 100), mbs=(64,128), hs=(256, 512) λ=(0.1, 1, 10), mbs=(32,64,128), lr=(1e-2, 1e-3), nl=1, hs=512 GEM pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-2, 1e-3), hs=256 pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-1, 1e-2, 1e-3), nl=(1,2), hs=512 A-GEM patterns per step i memory=(64, 128, 256), ss=(256, 512), lr=(1e-2, 1e-3), hs=256 pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), nl=(1,2), hs=5120 LwF α=(0.1, 1), hs=256, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4) α=(0.1, 1), nl=(1,2), hs=512, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay lr=(1e-3, 1e-4), hs=(256, 512) lr=(1e-3, 1e-4), hs=(128,256), nl=1 Naive hs=(256, 512), lr=(1e-3, 1e-4) hs=(256,512), nl=1 Joint Training lr=(1e-2, 1e-3), mbs=128, hs=512 hs=(256, 512), nl=1 Table A.7: Hyperparameter selection on PMNIST.",
Grid search has been performed for all the combinations in parenthesis.,
Bold notation indicates best hyperparameter value.,
See the text in Appendix A for explanation of the abbreviations used in this table.,
created sequences of 3 steps for model selection and sequences of 10 steps for model assessment.,
"We preprocessed each audio with a MelSpectrogram with 10 ms hop length, 25 ms window size and 40 MFCCs features.",
"Table B.11 following, we report statistics for all the 30 classes in the dataset.",
Appendix B.2.,
"Quick, Draw!",
"Quick, Draw!",
"dataset has been downloaded from Google Cloud Console, in the sketchrnn folder5.",
"All classes have 70, 000 patterns for training and 2, 500 5Quick, Draw!",
"details are available at https://github.com/googlecreativelab/ quickdraw-dataset 39 SMNIST LSTM MLP EWC λ=(0.1, 1, 10, 100, 1000), hs=(128, 256) λ=(0.1, 1, 10, 100, 1000), nl=(1,2) hs=128 MAS λ=(0.1, 1, 10, 100, 1000), hs=(128, 256) λ=(0.1, 1, 10, 100), lr=(1e-2, 1e-3), nl=1, hs=128 GEM pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-2, 1e-3), hs=128 pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-1, 1e-2, 1e-3), hs=128, nl=1 A-GEM pps=(128, 256, 512), ss=(256, 512), lr=(1e-2, 1e-3), hs=128 pps=(128, 256, 512), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), hs=512, nl=1 LwF α=([0, 1/2, 2*(2/3), 3*(3/4), 4*(4/5)], 1), hs=(256,512), T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4), mbs=(64,128) α=([0, 1/2, 2*(2/3), 3*(3/4), 4*(4/5)], 1), nl=(1,2), hs=256, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay lr=(1e-3, 1e-4), hs=(128, 256) hs=128, nl=(1,2) Naive hs=128, mbs=(32,64), lr=(1e-3, 1e-4) hs=(128,256), nl=1 Joint Training hs=512, mbs=128, lr=(1e-3, 1e-4) hs=128, nl=(1,2) Table A.8: Hyperparameter selection on SMNIST.",
Grid search has been performed for all the combinations in parenthesis.,
Bold notation indicates best hyperparameter value.,
See the text in Appendix A for explanation of the abbreviations used in this table.,
Table B.12 summarizes the classes used in the experiments.,
Appendix C. Additional Results We provide a set of paired plots showing the relationship between the train- ing accuracy at the end of each step and the ﬁnal test accuracy at the end of training on all steps.,
These plots are useful to compare the relative performance of diﬀerent CL strategies and diﬀerent architectures.,
Table C.13 shows the mean ACC and its standard deviation for diﬀerent CL strategies with multi-head models in MT+NC scenarios.,
A-GEM concatenates the one-hot task vector to each train and test input pattern.,
"40 SSC LSTM MLP EWC λ=(0.1, 1, 10, 100, 1000), hs=512 λ=(0.1, 1, 10, 100, 1000), lr=(1e-2, 1e-3) nl=1, hs=1024 MAS λ=(0.1, 1, 10, 100), lr=(1e-3, 1e-4), hs=512, mbs=(32, 64, 128) λ=(0.1, 1, 10, 100), mbs=(32,64,128), lr=(1e-2, 1e-3), nl=1, hs=1024 GEM pps=(256, 512), γ=(0, 0.5, 1) pps=(256, 512), γ=(0, 0.5, 1) A-GEM pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), hs=512 pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), nl=1, hs=1024 LwF α=(0.1, 1), hs=(512, 1024), T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4), mbs=(64,128) α=(0.1, 1), nl=(1,2), hs=1024, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay hs=(512, 1024) lr=(1e-2, 1e-3), hs=1024, nl=(1,2) Naive hs=(512, 1024) hs=1024, nl=(1,2,3), lr=(1e-2, 1e-3) Joint Training lr=(0.01, 0.001), hs=(512, 1024), layers=2, mbs=128 hs=1024, nl=(1,2), weight decay=(0, 1e-4), lr=1e-3, mbs=128 Table A.9: Hyperparameter selection on SSC.",
Grid search has been performed for all the combinations in parenthesis.,
Bold notation indicates best hyperparameter value.,
See the text in Appendix A for explanation of the abbreviations used in this table.,
"41 QD LSTM EWC λ=(1, 10, 100) MAS λ=(1, 10, 100) GEM pps=(32, 64), γ=(0, 0.5), lr=(1e-2, 1e-3) A-GEM pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2) LwF α=(0.1, 1), hs=512, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=1e-4, mbs=(64,128) Replay hs=512, lr=(1e-3, 1e-4) Naive hs=512, layers=(1,2), directions=(1, 2) Joint Training hs=(256, 512), layers=(1,2), bidirectional=(true, false), mbs=512, lr=1e-4 Table A.10: Hyperparameter selection on QD.",
Grid search has been performed for all the combinations in parenthesis.,
Bold notation indicates best hyperparameter value.,
See the text in Appendix A for explanation of the abbreviations used in this table.,
ewcagemgem mas lwfnaive replay-10 0 20000 40000 60000 80000 100000 120000 Time (s) LSTM-ROW LSTM-4 (a) Permuted MNIST ewcagemgem mas lwfnaive replay-10 0 250 500 750 1000 1250 1500 1750 2000 Time (s) LSTM-ROW LSTM-4 (b) Split MNIST ewc agem mas lwf naive replay-10 0 500 1000 1500 2000 2500 3000 Time (s) LSTM (c) SSC ewcagemgem mas lwf naive replay-10 0 20000 40000 60000 80000 100000 Time (s) LSTM (d) QD Figure A.9: Average execution time of recurrent models for diﬀerent CL strategies.,
"On SSC, we did not include the GEM strategy since it has been executed with a diﬀerent framework.",
Best viewed in color.,
"42 Class name patterns use Class name patterns use bed 1356 S1 oﬀ 2244 A3 bird 1346 S1 one 1276 A4 cat 1378 S2 on 2228 A4 dog 1474 S2 right 1276 A5 down 1188 S3 seven 1411 A5 eight 1113 S3 sheila 1463 A6 ﬁve 1092 - six 1485 A6 four 2400 - stop 1485 A7 go 960 - three 1188 A7 happy 1481 - tree 1188 A8 house 2382 A1 two 902 A8 left 1485 A1 up 1187 A9 marvel 1253 A2 wow 957 A9 nine 1144 A2 yes 1244 A10 no 957 A3 zero 1306 A10 Table B.11: Details of SSC dataset classes: name of the class, total number of patterns, how it has been used.",
"S for model selection, A for model assessment, the following number indicates which step uses the class.",
Dash indicates that the class has not been used in our experiments.,
"Class name use Class name use hot dog S1 octopus A4 palm tree S1 cloud A5 moon S2 bicycle A5 envelope S2 swan A6 dumbbell S3 picture frame A6 microwave S3 shorts A7 drill A1 ﬂying saucer A7 telephone A1 basketball A8 airplane A2 harp A8 dishwasher A2 beard A9 chair A3 binoculars A9 grass A3 tiger A10 rhinoceros A4 book A10 Table B.12: Details of Quick, Draw!",
"dataset classes: name of the class, how it has been used.",
"S for model selection, A for model assessment, the following number indicates which step uses the class.",
43 SMNIST MLP LSTM-4 EWC 0.99±0.00 0.64±0.16 A-GEM 0.73±0.05 0.20±0.06 NAIVE 0.97±0.02 0.72±0.11 SSC MLP LSTM EWC 0.77±0.09 0.80±0.06 A-GEM 0.57±0.02 0.62±0.07 NAIVE 0.72±0.10 0.67±0.10 QD LSTM EWC 0.98±0.00 A-GEM 0.56±0.16 NAIVE 0.71±0.09 Table C.13: Multi-head experiments showing average ACC and standard deviation.,
Naive strategy already recovers large part of the performance.,
44 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (a) PMNIST + naive MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (b) PMNIST + ewc MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (c) PMNIST + mas MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (d) PMNIST + lwf MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (e) PMNIST + gem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (f) PMNIST + agem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (g) PMNIST + replay-1 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (h) PMNIST + replay-10 45 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (i) SMNIST + naive MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (j) SMNIST + ewc MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (k) SMNIST + mas MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (l) SMNIST + lwf MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (m) SMNIST + gem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (n) SMNIST + agem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (o) SMNIST + replay-1 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (p) SMNIST + replay-10 46 MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (q) SSC + naive MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (r) SSC + ewc MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (s) SSC + mas MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (t) SSC + lwf MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (u) SSC + gem MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (v) SSC + agem MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (w) SSC + replay-1 MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (x) SSC + replay-10 47 LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (y) QD + naive LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (z) QD + ewc LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (aa) QD + mas LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ab) QD + lwf LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ac) QD + gem LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ad) QD + agem LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ae) QD + replay-1 LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (af) QD + replay-10 Figure C.10: Paired plots comparing accuracy at the end of training on each step (left point) and accuracy after training on all steps (right point).,
Each column represents a diﬀerent model.,
Red asterisk represents mean performance across all steps.,
Horizontal dotted line represents the performance of a random classiﬁer.,
"The more vertical the line, the larger the forgetting.",
Best viewed in color.,
"Towards Dropout Training for Convolutional Neural Networks Haibing Wu Department of Electronic Engineering Fudan University Shanghai 200433, China haibingwu13@fudan.edu.cn Xiaodong Gu Department of Electronic Engineering Fudan University Shanghai 200433, China xdgu@fudan.edu.cn Abstract Recently, dropout has seen increasing use in deep learning.",
"For deep convolutional neural networks, dropout is known to work well in fully-connected layers.",
"However, its effect in convolutional and pooling layers is still not clear.",
This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time.,
"In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time.",
Empirical evidence validates the superiority of probabilistic weighted pooling.,
"We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture.",
"Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation.",
"Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.",
Key words: Deep learning; Convolutional neural networks; Max-pooling dropout 1 Introduction Deep convolutional neural networks (CNNs) have recently told many success stories in visual recognition tasks and are now record holders on many challenging datasets.,
"A standard CNN consists of alternating convolutional and pooling layers, with fully-connected layers on top.",
"Compared to regular feed-forward networks with similarly-sized layers, CNNs have much fewer connections and parameters due to the local-connectivity and shared-filter architecture in convolutional layers, so they are far less prone to over-fitting.",
Another nice property of CNNs is that pooling operation provides a form of translation invariance and thus benefits generalization.,
"Despite these attractive qualities and despite the fact that CNNs are much easier to train than other regular, deep, feed-forward neural networks, big CNNs with millions or billions of parameters still easily overfit relatively small training data.",
"Dropout (Hinton et al., 2012) is a recently proposed regularizer to fight against over-fitting.",
It is a regularization method that stochastically sets to zero the activations of hidden units for each training case at training time.,
This breaks up co-adaptions of feature detectors since the dropped-out units cannot influence other retained units.,
"Another way to interpret dropout is that it yields a very efficient form of model averaging where the number of trained models is exponential in that of units, and these models share the same parameters.",
"Dropout has also inspired other stochastic model averaging methods such as stochastic pooling (Zeiler & Fergus, 2013) and DropConnect (Wan et al., 2013).",
"Although dropout is known to work well in fully-connected layers of convolutional neural nets (Hinton et al., 2012; Wan et al., 2013; Krizhevsky, Sutskever, & Hinton, 2012), its effect in convolutional and pooling layers is, however, not well studied.",
"This paper shows that using max-pooling dropout at training time is equivalent to sampling activation based on a multinomial distribution, and the distribution has a tunable parameter p (the retaining probability).",
"In light of this, probabilistic weighted pooling is proposed and employed at test time to efficiently average all possibly max-pooling dropout trained networks.",
Our empirical evidence confirms the superiority of probabilistic weighted pooling over max-pooling.,
"Like fully-connected dropout, the number of possible max-pooling dropout models also grows exponentially with the increase of the number of hidden units that are fed into pooling layers, but decreases with the increase of pooling region’s size.",
"We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture.",
"Carefully designing dropout training simultaneously in max-pooling and fully-connected layers, we report state-of-the-art results on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, in comparisons with other approaches without data augmentation.",
"As both stochastic pooling (Zeiler & Fergus, 2013) and max-pooling dropout randomly sample activation based on multinomial distributions at pooling stage, it becomes interesting to compare their performance.",
"Experimental results show that stochastic pooling performs between max-pooling dropout with different retaining probabilities, yet max-pooling dropout with typical retaining probabilities often outperforms stochastic pooling by a large margin.",
"In this paper, dropout on the input to max-pooling layers is also called max-pooling dropout for brevity.",
"Similarly, dropout on the input to convolutional (or fully-connected) layers is called convolutional (or fully-connected) dropout.",
"2 Review of Dropout Training for Convolutional Neural Networks CNNs have far been known to produce remarkable performance on MNIST (LeCun et al., 1998), but they, together with other neural network models, fell out of favor in practical machine learning as simpler models such as SVMs became the popular choices in the 1990s and 2000s.",
"With deep learning renaissance (Hinton & Salakhutdinov, 2006; Ciresan, Meier, & Schmidhuber, 2012; Bengio, Courville, & Vincent, 2013), CNNs regained attentions from machine learning and computer vision community.",
"Like other deep models, many issues can arise with deep CNNs if they are naively trained.",
Two main issues are computation time and over-fitting.,
"Regarding the former problem, GPUs help a lot by speeding up computation significantly.",
"To combat over-fitting, a wide range of regularization techniques have been developed.",
A simple but effective method is adding l2 penalty to the network weights.,
"Other common forms of regularization include early stopping, Bayesian fitting (Mackay, 1995), weight elimination (Ledoux & Talagrand, 1991) and data augmentation.",
"In practice, employing these techniques when training big neural networks provides better test performances than smaller networks trained without any regularization.",
Dropout is a new regularization technique that has been more recently employed in deep learning.,
"It is similar to bagging (Breiman, 1996), in which a set of models are trained on different subsets of the same training data.",
"At test time, different models’ predictions are averaged together.",
"In traditional bagging, each model has independent parameters, and all members would be trained explicitly.",
"In the case of dropout training, there are exponentially many possibly trained models, and these models share the same parameters, but not all of them are explicitly trained.",
"Actually, the number of explicitly trained models is not larger than me, where m is the number of training example, and e is the training epochs.",
"This is much smaller than the number of possibly trained models, n 2 ( n is number of hidden units in a feed-forward neural networks).",
"Therefore, a vast majority of models are not explicitly trained at training time.",
"At test time, bagging makes a prediction by averaging together all the sub-models’ predictions with the arithmetic mean, but it is not obvious how to do so with the exponentially many models trained by dropout.",
"Fortunately, the average prediction of exponentially many sub-models can be approximately computed simply by running the whole network with the weights scaled by retaining probability.",
"The approximation has been mathematically characterized for linear and sigmoidal networks (Baldi & Sadowski, 2014; Wager el al., 2013); for piecewise linear networks such as rectified linear networks, Warde et al.",
"(2014) empirically showed that weight-scaling approximation is a remarkable and accurate surrogate for the true geometric mean, by comparing against the true average in small enough networks that the exact computation is tractable.",
"Since dropout was thought to be far less advantageous in convolutional layers, pioneering work by Hinton et al.",
(2012) only applied it to fully-connected layers.,
It was the reason they provided that the convolutional shared-filter architecture was a drastic reduction in the number of parameters and thus reduced its possibility to overfit in convolutional layers.,
Wonderful work by Krizhevsky et al.,
"(2012) trained a very big convolutional neural net, which had 60 million parameters, to classify 1.2 million high-resolution images of ImageNet into the 1000 different categories.",
Two primary methods were used to reduce over-fitting in their experiments.,
"The first one was data augmentation, an easiest and most commonly used approach to reduce over-fitting for image data.",
Dropout was exactly the second one.,
"Also, it was only used in fully-connected layers.",
"In the ILSVRC-2012 competition, their deep convolutional neural net yielded top-5 test error rate of 15.3%, far better than the second-best entry, 26.2%, achieved by shallow learning with hand-craft feature engineering.",
This was considered as a breakthrough in computer vision.,
"From then on, the community believes that deep convolutional nets not only perform best on simple hand-written digits, but also really work on complex natural images.",
"Compared to original work on dropout, (Srivastava et al., 2014) provided more exhaustive experimental results.",
"In their experiments on CIFAR-10, using dropout in fully-connected layers reduced the test error from 15.60% to 14.32%.",
"Adding dropout to convolutional layers further reduced the error to 12.61%, revealing that applying dropout to convolutional layers aided generalization.",
Similar performance gains can be observed on CIFAR-100 and SVHN.,
"Still, they did not explore max-pooling dropout.",
"Stochastic pooling (Zeiler & Fergus, 2013) is a dropout-inspired regularization method.",
The authors replaced the conventional deterministic pooling operations with a stochastic procedure.,
"Instead of always capturing the strongest activity within each pooling region as max-pooling does, stochastic pooling randomly picks the activations according to a multinomial distribution.",
"At test time, probability weighting is used as an estimate to the average over all possible models.",
"Interestingly, stochastic pooling resembles the case of using dropout in max-pooling layers, so it is worth comparing them.",
"DropConnect (Wan et al., 2013) is a natural generalization of dropout for regularizing large feed-forward nets.",
"Instead of setting to zero the activations, it sets a randomly picked subset of weights within the network to zero with probability 1 – p. In other words, the fully-connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen stochastically during training.",
Each unit thus only receives input from a random subset of units in the previous layer.,
"DropConnect resembles dropout as it involves stochasticity within the model, but differs in that the stochasticity is on the weights, rather than the output vectors of a layer.",
Results on several visual recognition datasets showed that DropConnect often outperformed dropout.,
"Maxout network (Goodfellow et al., 2013) is another model inspired by dropout.",
The maxout unit picks the maximum value within a group of linear pieces as its activation.,
This type of nonlinearity is a generalization of rectified activation function and is capable of approximating arbitrary convex function.,
"Combining with dropout, maxout networks have been shown to achieve best results on MNIST, CIFAR-10, CIFAR-100 and SVHN.",
"However, the authors did not train maxout networks without dropout.",
"Besides, they did not train the rectified counterparts with dropout and directly compare it with maxout networks.",
"Therefore, it was not clear that which factor contributed to such remarkable results.",
3 Max-Pooling Dropout and Convolutional Dropout We now demonstrate that max-pooling dropout is equivalent to sampling activation according to a multinomial distribution at training time.,
"Basing on this interpretation, we propose to use probabilistic weighted pooling at test time.",
We also describe convolutional dropout.,
"3.1 Max-Pooling Dropout Consider a standard CNN composed of alternating convolutional and pooling layers, with ) (l ia fully-connected layers on top.",
"On each presentation of a training example, if layer l is followed by a pooling layer, the forward propagation without dropout can be described as ) ( ) ( ) ( ) ( 1 ) 1 ( ), ,..., ,..., ( l j l n l i l l j R i a a a pool a    .",
(1) Here ) (l j R is pooling region j at layer l and is the activity of each neuron within it.,
| | ) (l j R n  is the number of units in ) (l j R .,
Pool() denotes the pooling function.,
Pooling operation provides a form of spatial transformation invariance as well as reduces the computational complexity for upper layers.,
An ideal pooling method is expected to preserve task-related information while discarding irrelevant image details.,
Two popular choices are average- and max-pooling.,
Average-pooling takes all activations in a pooling region into consideration with equal contributions.,
This may downplay high activations as many low activations are averagely included.,
"Max-pooling only captures the strongest activation, and disregards all other units in the pooling region.",
We now show that employing dropout in max-pooling layers avoids both disadvantages by introducing stochasticity.,
"3.1.1 Max-Pooling Dropout at Training Time With dropout, the forward propagation becomes , ~ ˆ ) ( ) ( ) ( l l l a m a  (2) .",
"), ˆ ,..., ˆ ,..., ˆ ( ) ( ) ( ) ( ) ( 1 ) 1 ( l j l n l i l l j R i a a a pool a    (3) Here  denotes element wise product and ) (l m is a binary mask with each element ) (l i m drawn independently from a Bernoulli distribution.",
This mask is multiplied with activations ) (l a in a pooling region at layer l to produce dropout-modified activations ) (ˆ l a .,
The modified activations are then passed to pooling layers.,
1 presents a concrete example to illustrate the effect of dropout in max-pooling layers.,
"Clearly, without dropout, the strongest activation in a pooling regions is always selected as the pooled activation.",
"With dropout, it is not necessary that the strongest activation being the output.",
"Therefore, max-pooling at training time becomes a stochastic procedure.",
An illustrating example showing the procedure of max-pooling dropout.,
"The activation in the pooling region is 1, 6, 5 and 3 respectively.",
"Without dropout, the strongest activation 6 is always selected as the output.",
"With dropout, each unit in the pooling region could be possibly dropped out.",
"In this example, only 1 and 3 are retained, then 3 will be the pooled output.",
"To formulate such stochasticity, suppose the activations ) ,..., , ( ) ( ) ( 2 ) ( 1 l n l l a a a in each pooling region j are reordered in non-decreasing order, i.e., ) ( ) ( 2 ) ( 1 ... 0 l n l l a a a       .1 With dropout, each unit in the pooling region could be possibly set to zero with probability of q (q = 1 – p is the dropout probability, and p is the retaining probability).",
"As a result, ) (l ia will be selected as the pooled activation on condition that (1) ) ( ) ( 2 ) ( 1 ,..., , l n l i l i a a a      are dropped out, and (2) ) (l ia is retained.",
"This event occurs with probability of pi according to probability theory: 1 We use rectified linear unit as the activation function, so activations in each pooling region are non-negative.",
Sigmoidal and tanh nonlinearities are not adopted due to gradient vanishing effect.,
",..., 2,1 ( , ) Pr( ) ( ) 1 ( n i pq p a a i n i l i l j        (4) A special event occurring with probability of ) ( 0 0 n q p p  is that all the units in a pooling region is dropped out, and the pooled output becomes ) 0 ( ) ( 0 ) ( 0    l l a a .",
"Therefore, performing max-pooling over the dropout-modified pooling region is exactly sampling from the following multinomial distribution to select an index i, then the pooled activation is simply ) (l ia : ).",
",..., , , ( ~ where , 2 1 0 ) ( )1 ( n l i l j p p p p l Multinomia i a a    (5) Let s be the size of a feature map at layer l (with r feature maps), and t be the size of pooling regions.",
The number of pooling region is therefore t rs for non-overlapping pooling.,
"Each pooling region provides t+1 choices of the indices, then the number of possibly trained models C at layer l is     . )",
"( 1 1 rs rs t t rs t b t t C      (6) So the number of possibly max-pooling dropout trained models is exponential in the number of units that are fed into max-pooling layers, and the base b(t) ) 2 ) 1 ) ( 1(     t t t b depends on the size of pooling regions.",
"Obviously, with the increase of the size of pooling regions, the base b(t) decreases, and the number of possibly trained models becomes smaller.",
"Note that the number of possibly fully-connected dropout trained models is also exponential in the number of units that are fed into fully-connected layers, but with 2 as the base.",
"3.1.2 Probabilistic Weighted Pooling at Test Time Using dropout in fully-connected layers during training, the whole network containing all the hidden units should be used at test time, but with their outgoing weights halved (Hinton et al.",
"2012), or with their activations halved.",
"Using max-pooling dropout during training, one might intuitively pick as output the strongest activation scaled down by the retaining probability: ) ,..., ,..., max( ) ( ) ( ) ( 1 ) 1 ( l n l i l l j a a a p a    .",
(7) We call this pooling scheme scaled max-pooling.,
Instead we propose to use probabilistic weighted pooling to efficiently get a more accurate approximation of averaging all possibly trained dropout networks.,
"In this pooling scheme, the pooled activity is linear weighted summation over activations in each region: .",
1 ) ( 0 ) ( ) 1 (          n i l i i n i l i i l j a p a p a (8) Here ip is exactly the probability calculated by Eqn.,
"If a unit in a pooling region is selected as output with probability ip during training, the activation of that unit is weighted by ip at test time.",
This ensures that the pooled output at test time is the same with the expected output under the multinomial distribution used to select units at training time.,
This type of probabilistic weighted pooling can be interpreted as a form of model averaging where each selection of index i corresponds to a different model.,
"At training time, sampling from multinomial distribution to select a new index produces a new model for each presentation of each training case.",
The number of possible models is exponential in the number of units that are fed into max-pooling layers.,
"At test time, using probabilistic weighting pooling instead of sampling, we effectively get an approximation of averaging over all of these possible models without instantiating them.",
Empirical evidence will confirm that probabilistic weighted pooling is a more accurate approximation of averaging all possible dropout models than scaled max-pooling.,
"3.2 Convolutional Dropout If layer l is followed by a convolutional layer, the forward propagation with dropout is formulated as ) ( Bernoulli ~ ) ( ) ( p i m l k , (9) , ˆ ) ( ) ( ) ( l k l k l k m a a   (10)     ) ( 1 ) ( ) 1 ( ) 1 ( ) , ( conv l n k l k l j l j a W z , (11) ) ( )1 ( )1 (   l j l j z f a .",
"(12) Here ) (l ka denotes the activations of feature map k (k=1,2,…,n(l)) at layer l. The mask matrix ) (l k m consists of independent Bernoulli variables ) ( ) ( i m l k .",
"This mask is sampled and multiplied with activations in k-th feature map at layer l, to produce dropout-modified activations ) (ˆ l ka .",
These modified activations are convolved with filter ) 1 (  l j W to produce convolved features ) (l jz .,
The function f() is applied element wise to the convolved features to get the activations of convolutional layers ) 1 (  l ja .,
"Let s2 be the size of a feature map at layer l (with r feature maps), and t2 be the size of filters, then the number of convolved features is )1 ( )1 (       t s t s r .",
"Each convolved feature is the scalar product of a filter’s weights and a local region’s activations, so the number of possibly different convolved results is t t 2 when applying dropout to layer l. Therefore, the number of total possibly sets of convolved features C decided by layer l is 2 2 )1 ( )1 ( )1 ( 2 2               t s t r t s t s r t t C .",
"(13) Similarly, dropout is turned off at test time.",
"The whole network containing all the units at layer l should be used, but with the filter’s weights scaled by the retaining probability.",
This efficiently gets an estimate of averaging all possibly trained dropout networks.,
One may expect that convolutional dropout helps generalization by reducing over-fitting.,
"However, it is far less advantageous, since the shared-filter and local-connectivity architecture in convolutional layers is a drastic reduction in the number of parameters and this already reduces the possibility to overfit (Hinton et al., 2012).",
"Our empirical results will confirm this point that it does improve the generalization to test data, but is often inferior to max-pooling or fully-connected dropout.",
"4 Empirical Evaluations The purpose of our experiments is threefold: (1) to provide empirical evidence that probabilistic weighted pooling is a more accurate approximation of averaging all possibly max-pooling dropout trained models than max-pooling and scaled max-pooling, (2) to explore dropout training in different layers and (3) to compare max-pooling dropout against stochastic pooling.",
"We use rectified linear function (Krizhevsky, Sutskever, & Hinton, 2012; Vinod & Hinton, 2010) for convolutional and fully-connected layers, and softmax activation function for the output layer.",
More commonly used sigmoidal and tanh nonlinearities are not adopted due to gradient vanishing (a) MNIST (b) CIFAR-10 (c) CIFAR-100 Fig.,
"Example images from MNIST, CIFAR-10 and CIFAR-100.",
Each row corresponds to a different category.,
"For CIFAR-100, only images of ten categories are displayed.",
problem with them.,
"Vanishing gradient effect causes slow optimization convergence, yet training a deep model is already very computationally expensive.",
"Local response normalization is applied after applying rectified non-linearity in certain layers (Krizhevsky, Sutskever, & Hinton, 2012).",
"We train our models using stochastic mini-batch gradient descent with a batch size of 100, momentum of 0.95, learning rate of 0.1 to minimize the cross entropy loss.",
The weights in all layers are initialized from a zero-mean Gaussian distribution with 0.1 as standard deviation and the constant 0 as the neuron biases in all layers.,
"The heuristic (Hinton et al., 2013) which we follow is to reduce the learning rate twice by a factor of ten before terminating the training.",
We retain each unit with probability of 0.5 (i.e.,
p = 0.5) by default.,
"Specially, p is set to 0.8 for the first fully-connected layer.",
"Experiments are conducted on three widely used datasets: MNIST, CIFAR-10 and CIFAR-100.",
2 displays the example images.,
"The CNN architecture in our experiments is denoted in the following way: 2x32x32-32C5-3P2- 64C5-3P2-1000N-10N represents a CNN with 2 input image of size 32x32, a convolutional layer with 32 feature maps and 5x5 filters, a pooling layer with pooling region 3x3 and stride 2, a convolutional layer with 64 feature maps and 5x5 filters, a pooling layer with pooling region 3x3 and stride 2, a fully-connected layer with 1000 hidden units, and an output layer with 10 units (one per class).",
"4.1 MNIST We initially conduct experiments using MNIST, a widely used benchmark dataset in computer vision.",
"It consists of 28x28 pixel grayscale images, each containing a digit 0 to 9.",
"There are 60,000 training and 10,000 test examples.",
"We do not perform any pre-processing except scaling the pixel values to [0, 1].",
MNIST training and test errors for different pooling methods at test time.,
Max-pooling dropout is used during training.,
Max-pooling without dropout is presented as the baseline.,
"(a) and (b) illustrate the training and test errors produced by the smaller architecture, 1x28x28-6C5-2P2- 12C5-2P2-1000N-10N.",
"(c) and (d) illustrate the training and test errors produced by the bigger one, 1x28x28-12C5-2P2-24C5-2P2-1000N-10N.",
4.1.1 Probabilistic Weighted Pooling vs.,
"Scaled Max-Pooling To validate the superiority of probabilistic weighted pooling over max-pooling and scaled max-pooling, we first train different CNN models using two different architectures, 1x28x28-6C5- 2P2-12C5-2P2-1000N-10N and 1x28x28-12C5-2P2-24C5-2P2-1000N-10N.",
Max-pooling dropout is applied during training.,
"At test time, max-pooling, scaled max-pooling and probabilistic weighted pooling is used respectively to act as model averaging.",
3 illustrates the training and test error rates of both CNN architectures over 300 training epochs.,
"Using probabilistic weighted pooling at test time, the training errors are always lower than those produced by scaled max-pooling.",
This is a good news that probabilistic weighted pooling fits the training data better with the same model parameters.,
"In other words, probabilistic weighted pooling is a more accurate estimation of averaging all possibly trained dropout models than scaled max-pooling.",
"On test data, both probabilistic weighted pooling and scaled max-pooling produces lower errors compared to max-pooling without dropout.",
Probabilistic weighted pooling generally produces lower errors than scaled max-pooling.,
"This indicates that probabilistic weighted pooling not only fits the training data better, but also generalizes better to the test data.",
"In terms of training performance, max-pooling is about the same with probabilistic weighted pooling, but probabilistic weighted pooling clearly outperforms max-pooling on the test data.",
We then train different CNN models with different retaining probabilities for max-pooling dropout.,
The CNN architecture 1x28x28-20C5-2P2-40C5-2P2-1000N-10N is trained for 1000 epochs.,
4 compares the test performance produced by different pooling methods at test time.,
"Generally, probabilistic weighted pooling performs better than max-pooling and scaled max-pooling with different retaining probabilities.",
"For small p, max-pooling and scaled max-pooling performs very poorly; probabilistic weighted pooling is considerably better.",
"With the increase of p, the performance gap becomes smaller.",
"This is not surprising as the pooled outputs for different pooling methods are close to each other for large p. An extreme case is that when p = 1, scaled max-pooling and probabilistic weighted pooling are exactly the same with max-pooling.",
MNIST test errors for different pooling methods at test time.,
Max-pooling dropout is used to train CNN models with different retaining probabilities at training time.,
"4.1.2 Dropout Training in Different Layers To investigate the effect of using dropout in different layers, we first train different models by separately using dropout on the input to convolutional, max-pooling and fully-connected layers.",
Models are trained using two CNN architectures: 1x28x28-6C5-2P2-12C5-2P2-1000N-10N and Fig.,
"MNIST training and test errors for separately using convolutional, max-pooling and fully-connected dropout.",
The case without dropout is also presented for comparison’s purpose.,
"(a) and (b) illustrate the training and test errors produced by the smaller architecture, 1x28x28-6C5-2P2-12C5-2P2-1000N-10N.",
"(c) and (d) illustrate the training and test errors produced by the bigger one, 1x28x28-12C5-2P2-24C5-2P2-1000N-10N.",
1x28x28-12C5-2P2-24C5-2P2-1000N-10N.,
The case without dropout is used as the baseline.,
"For max-pooling dropout, only probabilistic weighted pooling is used at test time for its superiority over max-pooling and scaled max-pooling.",
5 illustrates the progression of these models’ training and test error rates over 300 epochs.,
"Without dropout, the training error falls rapidly to zero, but the test error stands at about 0.9% for both CNN architectures, revealing that both models suffer from severe over-fitting.",
"With convolutional dropout, the training error decreases slower, and the test error drops to 0.85% and 0.65% respectively for the above two CNN architectures.",
This indicates that convolutional dropout can reduce over-fitting and aid generalization to the test data.,
"With max-pooling dropout, the training error also falls down slower while the test error decreases to about 0.60% and 0.53% respectively, much better than convolutional dropout.",
"With fully-connected dropout, the training error still decreases much slower, and the performance on test data is much better compared to the case without dropout.",
Note that fully-connected dropout is the best performer on the smaller architecture.,
"This is not surprising since fully-connected layers have many units and need regularization; the convolutional and pooling component is thin enough, so regularization is relatively not so advantageous.",
"Max-pooling dropout performs well on both architectures, even though the CNN architectures are small, indicating that max-pooling dropout is a robust regularizer.",
"Convolutional dropout is not trivial compared to the case without dropout, but its performance gain is relatively small compared to max-pooling dropout and fully-connected dropout.",
"This is to be expected, since the shared-filter and local-connectivity convolutional architecture is a drastic reduction in the number of parameters and this already reduces the possibility to overfit.",
"MNIST test errors for 1x28x28-20C5-2P2-40C5-2P2-1000N-10N trained with dropout in various types of layers compared to current state-of-the-art CNN models, excluding methods that augment the training data.",
"As observed, separately using convolutional, max-pooling and fully-connected dropout reduces over-fitting and improves test performance.",
"To achieve better results, we try using dropout simultaneously on the input to different layers.",
"For max-pooling dropout, only probabilistic weighted pooling is used at test time.",
"To this end, we train various models for 1000 training epochs using architecture 1x28x28-20C5-2P2-40C5-2P2-1000N-10N.",
"For comparison’s purpose, the results of separately using dropout in different layers are also presented.",
"Table 1 records these models’ test errors, as well as recently published state-of-the-art results produced by CNNs.",
The best performing CNN model that do not use dropout yields an error rate of 0.81% on MNIST.,
"With fully-connected dropout, the test error decreases to 0.56%.",
Convolutional dropout reduces the test error to 0.60%.,
"With max-pooling dropout, the test error drops to 0.47%.",
"However, Compared to only using convolutional or max-pooling dropout, simultaneously using convolutional and pooling dropout does not decrease, but increase the error.",
"Simultaneously employing max-pooling and fully-connected dropout, we obtain test error rate of 0.39%.",
"This beats the previous state-of-the-art results to the best of our knowledge, 0.47% (Lin, Chen, & Yan, 2014), 0.45% (Goodfellow et al., 2013) and 0.47% (Zeiler & Fergus, 2013).",
"4.2 CIFAR-10 The CIFAR-10 dataset (Krizhevsky, 2009) consists of ten classes of natural images with 50,000 examples for training and 10,000 for testing.",
Each example is a 32x32 RGB image taken from the tiny images dataset collected from the web.,
"We also scale to [0, 1] for this dataset and subtract the mean value of each channel computed over the dataset for each image.",
"Compared to MNIST, the CIFAR-10 images are highly varied within each class, so we train deeper and wider nets to model the complex non-linear relationships.",
The architecture is 3x32x32-96C5-3P2-128C3-3P2-256C3-3P2-2000N-2000N-10N.,
"A notable difference from the architecture used for MNIST is that each input image for CIFAR-10 has three channels, with each channel corresponding to an R/G/B component.",
Models are trained for 1000 epochs.,
We first compares different pooling methods at test time for max-pooling dropout trained models on CIFAR-10.,
"The retaining probability is set to 0.3, 0.5 and 0.7 respectively.",
"At test time, max-pooling, scaled max-pooling and probabilistic weighted pooling are respectively used to act as model averaging.",
6 presents the test performance of these pooling methods.,
"Again, for small retaining probability p = 0.3, scaled max-pooling and probabilistic weighted pooling perform poorly.",
Probabilistic weighted pooling is the best performer with different retaining probabilities.,
The increase of p narrows different pooling methods’ performance gap.,
"Method Error % No dropout 0.81 Fully-connected dropout 0.56 Convolutional dropout 0.60 Max-pooling dropout 0.47 Convolutional and fully-connected dropout 0.50 Convolutional and max-pooling dropout 0.61 Max-pooling and fully-connected dropout 0.39 NIN + dropout (Lin, Chen, & Yan, 2014) 0.47 Maxout + dropout (Goodfellow et al., 2013) 0.45 Stochastic pooling (Zeiler & Fergus, 2013) 0.47 Fig.",
CIFAR-10 test errors for different pooling methods at test time.,
Max-pooling dropout is used to train CNN models with different retaining probabilities at training time.,
We also train various CNN models by separately and simultaneously using dropout in different layers on CIFAR-10.,
"For max-pooling dropout trained models, only probabilistic weighted pooling is used at test time.",
"Table 2 records the performance of various models, as well as recently published results produced by CNN models.",
"Without dropout, the best test error is 16.50%.",
"Separately using convolutional, max-pooling and fully-connected dropout reduces the test error.",
Convolutional dropout still underperforms max-pooling dropout.,
"Again, simultaneously using convolutional and max-pooling dropout seems to easily result in too strong regularization, and thus decreases the test performance.",
"Using max-pooling and fully-connected dropout, we obtain very competitive test error rate, 11.29%, compared to recently published results, 11.35% (Springenberg & Riedmiller, 2014), 10.41% (Lin, Chen, & Yan, 2014), 11.68% (Goodfellow et al., 2013), 15.14% (Zeiler & Fergus, 2013) and 15.60% (Hinton et al., 2012).",
"CIFAR-10 test errors for dropout training in various types of layers compared to current state-of-the-art CNN models, excluding methods that augment the training data.",
"4.3 CIFAR-100 CIFAR-100 (Krizhevsky, 2009) is just like CIFAR-10, but with 100 categories.",
"We also scale to [0, 1] for CIFAR-100 and subtract the mean value of each R/G/B channel.",
The architecture is the same with that of CIFAR-10 except that the number of output units is 100.,
Models are trained for 1000 epochs.,
"Method Error % No dropout 16.50 Fully-connected dropout 14.24 Convolutional dropout 15.78 Max-pooling dropout 13.85 Convolutional and max-pooling dropout 15.87 Convolutional and fully-connected dropout 12.93 Max-pooling and fully-connected dropout 11.29 Probout (Springenberg & Riedmiller, 2014) 11.35 NIN + dropout (Lin, Chen, & Yan, 2014) 10.41 Maxout + dropout (Goodfellow et al., 2013) 11.68 Stochastic pooling (Zeiler & Fergus, 2013) 15.14 Dropout the last hidden layer (Hinton et al., 2012) 15.60 Fig.",
CIFAR-100 test errors for different pooling methods at test time.,
Max-pooling dropout is used to train CNN models with different retaining probabilities at training time.,
7 compares different pooling methods at test time for max-pooling dropout trained models on CIFAR-100.,
"The retaining probability is set to 0.3, 0.5 and 0.7 respectively.",
"Again, probabilistic weighted pooling shows clear superiority over max-pooling and scaled max-pooling.",
We then train various CNN models by separately and simultaneously using dropout in different layers on CIFAR-100.,
"For max-pooling dropout trained models, only probabilistic weighted pooling is used at test time.",
"As shown in Table 3, it improves the generalization to test data that using dropout separately in convolutional, max-pooling and fully-connected layers.",
"Again, if dropout is employed improperly, the performance would decrease, such as simultaneously using convolutional and max- pooling dropout.",
"The best result, 37.13%, is achieved using max-pooling and fully-connected dropout.",
"CIFAR-100 test errors for dropout training in various types of layers compared to current state-of-the-art CNN models, excluding methods that augment the training data.",
"4.4 Max-Pooling Dropout and Stochastic Pooling Similar to max-pooling dropout, stochastic pooling (Zeiler & Fergus, 2013) also randomly picks activation according to a multinomial distribution at training time, and also involves probabilistic weighting at test time.",
"More concretely, at training time it first computes the probability pi for each unit within pooling region j at layer l by normalizing the activations: ).",
",..., 2,1 ( , 1 ) ( ) ( n i a a p n k l k l i i     (14) Method Error % No dropout 44.47 Fully-connected dropout 41.18 Convolutional dropout 42.23 Max-pooling dropout 40.15 Convolutional and max-pooling dropout 43.33 Convolutional and fully-connected dropout 38.56 Max-pooling and fully-connected dropout 37.13 Probout ( Springenberg & Riedmiller, 2014) 38.14 NIN + dropout (Lin, Chen, & Yan, 2014) 35.68 Maxout + dropout (Goodfellow et al., 2013) 38.57 Stochastic pooling (Zeiler & Fergus, 2013) 42.51 It then samples from a multinomial distribution based on pi to select an index i in the pooling region.",
The pooled activation is simply ) (l ia : ).,
",..., , ( ~ where , 2 1 ) ( )1 ( n l i l j p p p l Multinomia i a a   (15) At test time, probabilistic weighting is adopted to act as model averaging.",
"That is, the activations in each pooling region are weighted by the probability pi and summed: .",
"1 ) ( ) 1 (    n i l i i l j a p a (16) One may have found that stochastic pooling bears much resemblance to max-pooling dropout, as both involve stochasticity at pooling stage.",
We are therefore very interested in their performance differences.,
As presented in Sec.,
"3, for max-pooling dropout, the number of possibly trained models at layer l is rs t b ) ( , where t t t b )1 ( ) (   .",
"For stochastic pooling, since each pooling region provides t choices, the number of possibly trained models C at layer l is  . )",
"( rs rs t t rs t b t t C    (17) So the number of possibly trained models for stochastic pooling is also exponential in the number of hidden units, rs, but with a different base b(t) ) 3 ) ) ( 1( 3    t t t b .",
8 compares this function Fig.,
The base b(t) for stochastic pooling and max-pooling dropout.,
"For max-pooling dropout, t t t b  1 ) ( , while t t t b  ) ( for stochastic pooling.",
Here t is the size of pooling regions.,
The difference in b(t) decreases with the increase of t. against the base of max-pooling dropout.,
Now consider a concrete example.,
"For a certain layer l, let , 96 , 32 32    r s and .2 2  t For stochastic pooling, 4 / 1 ) 2 2 /( 1 4 ) 2 2 ( ) (     t b ; for max-pooling dropout, 4 / 1 ) 2 2 /( 1 5 ) 2 2 1( ) (      t b .",
"Max-pooling dropout provides  times larger number of possible models, and  is computed as follows: .",
"25 .1 ) 5 / 4 ( 24576 96 32 32 4 / 1       (18) Clearly, max-pooling dropout provides extremely larger number of possibly trained models than stochastic pooling, despite the small difference in the base.",
"To compare their performances, we train various models on MNIST using two CNN architectures, 1x28x28-6C5-2P2-12C5-2P2-1000N-10N and 1x28x28-12C5-2P2-24C5-2P2-1000N-10N.",
"For max-pooling dropout trained models, only probabilistic weighted pooling is used at test time.",
9 illustrates the progression of training and test errors using stochastic pooling and max-pooling dropout with different retaining probabilities over 300 training epochs.,
"Regarding training errors, stochastic Fig.",
"MNIST training and test errors for stochastic pooling, and max-pooling dropout with different retaining probabilities.",
(a) and (b) illustrate the training and test errors produced by 1x28x28-6C5-2P2-12C5-2P2-1000N-10N.,
(c) and (d) illustrate the training and test errors produced by 1x28x28-12C5-2P2-24C5-2P2-1000N-10N.,
pooling falls in between max-pooling dropout with p = 0.7 and p = 0.5 for both CNN architectures.,
"On the test data, stochastic pooling performs worst for the smaller CNN architecture.",
"For the bigger one, the test performance of stochastic pooling is between max-pooling dropout with p = 0.7 and p = 0.5.",
"For a more clear comparison, we train CNN models with more different retaining probabilities on MNIST, CIFAR-10 and CIFAR-100.",
"For max-pooling dropout trained models, only probabilistic weighted pooling is used at test time.",
10 compares the test performances of max-pooling dropout with different retaining probabilities against stochastic pooling.,
"As we can observed, the relation Fig.",
Test errors for max-pooling dropout with different retaining probabilities against stochastic pooling.,
"The architecture is 1x28x28-20C5-2P2-40C5-2P2-1000N-10N for MNIST, 3x32x32-96C5- 3P2-128C3-3P2-256C3-3P2-2000N-2000N-10N for CIFAR-10 and 3x32x32-96C5-3P2-128C3- 3P2-256C3-3P2-2000N-2000N-100N for CIFAR-100.",
between the performance of max-pooling dropout and the retaining probability p is a U-shape.,
"If p is too small or too large, max-pooling dropout performs poorer than stochastic pooling.",
Yet max-pooling dropout with typical p (around 0.5) outperforms stochastic pooling by a large margin.,
"Therefore, although stochastic pooling is hyper-parameter free and this saves the tuning of retaining probability, its performance is often inferior to max-pooling dropout.",
4.5 A Brief Summary of Experimental Results The summary of experimental results is threefold in response to the purpose of our experiments.,
"(1) For max-pooling dropout trained CNN models, probabilistic weighted pooling not only fits the training data better, but also generalizes better to the test data than max-pooling and scaled max-pooling.",
"For small retaining probability, max-pooling and scaled max-pooling produce very poor results, while probabilistic weighted pooling performs very well.",
"With the increase of the retaining probability, the performance gap becomes smaller.",
"(2) Separately using convolutional, max-pooling and fully-connected dropout improves performance, but convolutional dropout seems far less advantageous.",
"Simultaneously using dropout in different layers could further reduce the test error, but should be careful.",
"For example, with typical retaining probability 0.5, using convolutional and max-pooling dropout simultaneously does not performs well; while using max-pooling dropout and fully-connected dropout simultaneously achieves best results on MNIST and CIFAR datasets.",
"It is worth to point out that determining which layers to train with dropout is an open question, since many factors (such as network architecture, the retaining probability and even the training data) have large influence on the performance.",
"Especially, for different network architecture, the best dropout regularization strategy could be quite different.",
"While the best regularization strategy (simultaneously using max-pooling and fully-connected dropout) is consistent across three datasets in our experiments, it is possible that these networks just need lots of regularization in max-pooling and fully-connected layers.",
"(3) The retaining probability affects max-pooling’s performance, and the relation exhibits U-shape.",
Max-pooling dropout with typical retaining probabilities (around 0.5) often outperforms stochastic pooling by a large margin.,
5 Conclusions This paper mainly addresses the problem of understanding and using dropout on the input to max-pooling layers of convolutional neural nets.,
"At training time, max-pooling dropout is equivalent to randomly picking activation according to a multinomial distribution, and the number of possibly trained networks is exponential in the number of input units to the pooling layers.",
"At test time, a new pooling method, probabilistic weighted pooling, is proposed to act as model averaging.",
"Experimental evidence confirms the benefits of using max-pooling dropout, and validates the superiority of probabilistic weighted pooling over max-pooling and scaled max-pooling.",
"Using dropout simultaneously in different layers could further improve the performance, but should be careful.",
"Using max-pooling dropout and fully-connected dropout, this paper reports better results on MNIST, and comparable performance on CIFAR-10 and CIFAR-100, in comparisons with state-of-the-art results that do not use data augmentation.",
"In addition, considering that stochastic pooling is similar to max-pooling dropout, we empirically compare them and show that the performance of stochastic pooling is between those produced by max-pooling dropout with different retaining probabilities.",
Acknowledgements This work was supported in part by National Natural Science Foundation of China under grant 61371148.,
"References Baldi, P., & Sadowski, P. (2014).",
The dropout learning algorithm.,
"Artificial Intelligence, 210, 78-122.",
"Bengio, Y., Courville, A., & Vincent, P. (2013).",
Representation learning: a review and new perspectives.,
"IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 1798-1828.",
"Boureau, Y. L., Ponce J., & LeCun, Y.",
A theoretical analysis of feature pooling in visual recognition.,
In Proceedings 27th of International Conference on Machine Learning (ICML 2010).,
"Breiman, L. (1996).",
Bagging predictors.,
"Machine Learning, 24, 123-140.",
"D., Meier, U., & Schmidhuber, J.",
Multi-column deep neural networks for image classification.,
In Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2012).,
"Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y.",
Maxout networks.,
In Proceedings of 30th International Conference on Machine Learning (ICML 2013).,
"Hinton, G. E., & Salakhutdinov, R. R. (2006).",
Reducing the dimensionality of data with neural networks.,
"Science, 313, 504-507.",
"Hinton, G. E., Srivastave, N., Krizhevsky, A., Sutskever, I.",
"& Salakhutdinov, R. R. (2012).",
Improving neural networks by preventing co-adaption of feature detectors.,
arXiv 1207.0580.,
"Springenberg J. T., & Riedmiller M. (2014).",
Improving deep neural networks with probabilistic maxout units.,
In Proceedings of 3rd International Conference on Learning Representations (ICLR 2014).,
"Krizhevsky, A.",
Learning multiple layers of features from tiny images.,
"diss., Department of Computer Science, University of Toronto.",
"Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).",
ImageNet classification with deep convolutional neural networks.,
In Advances in Neural Information Processing Systems (NIPS 2012).,
"LeCun, Y., Bottou, L., Bengio, Y.",
"& Haffner, P. (1998).",
Gradient-based learning applied to document recognition.,
In Proceedings of the IEEE.,
"Ledoux, M., & Talagrand, M. (1991).",
Probability in banach spaces.,
"Lin, M., Chen, Q., & Yan S. (2014).",
Network in network.,
In Proceedings of 3rd International Conference on Learning Representations (ICLR 2014).,
"Mackay, D. C. (1995).",
Probable networks and plausible predictions: A review of practical Bayesian methods for supervised neural networks.,
In Bayesian Methods for Backpropagation Networks.,
"Scherer, D., Muller, A., & Behnke, S. (2010).",
Evaluation of pooling operations in convolutional architectures for object recognition.,
In Proceedings of 20th International Conference on Artificial Neural Networks (ICANN 2010).,
"Srivastava, N., Hinton.",
"G. E., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).",
Dropout: a simple way to prevent neural networks from overfitting.,
"Journal of Machine Learning Research, 15, 1929-1958.",
"Vinod, N., & Hinton, G. E. (2010).",
Rectified linear units improve restricted Boltzmann machines.,
In Proceedings 27th of International Conference on Machine Learning (ICML 2010).,
"Wan, L., Zeiler, M. D., Zhang, S., LeCun, Y., & Fergus, R. (2013).",
Regularization of neural networks using DropConnect.,
In Proceedings of 30th International Conference on Machine Learning (ICML 2013).,
"Warde, F. D., Goodfellow, I. J., Courville, A., & Bengio, Y.",
An empirical analysis of dropout in piecewise linear networks.,
In Proceedings of 3rd International Conference on Learning Representations (ICLR 2014).,
"Wager, S., Wang, S., & Liang, P. (2013).",
Dropout training as adaptive regularization.,
In Advances in Neural Information Processing Systems (NIPS 2013).,
"Zeiler, M. D., & Fergus R. (2013).",
Stochastic pooling for regularization of deep convolutional neural networks.,
In Proceedings of 2nd International Conference on Learning Representations (ICLR 2013).,
"arXiv:1807.02477v1 [cs.NE] 6 Jul 2018 Development of a Sensory-Neural Network for Medical Diagnosing Igor Grabec1, Eva ˇSvegl2 and Mihael Sok2 1 - Slovenian Academy of Sciences and Art, Ljubljana, Slovenia 2 - Faculty of Medicine, University of Ljubljana, Slovenia igor.grabec@fs.uni-lj.si, eva.svegl@gmail.com, miha.sok@kclj.si Abstract.",
Performance of a sensory-neural network developed for di- agnosing of diseases is described.,
Information about patient’s condition is provided by answers to the questionnaire.,
Questions correspond to sensors generating signals when patients acknowledge symptoms.,
These signals excite neurons in which characteristics of the diseases are repre- sented by synaptic weights associated with indicators of symptoms.,
The disease corresponding to the most excited neuron is proposed as the re- sult of diagnosing.,
Its reliability is estimated by the likelihood deﬁned by the ratio of excitation of the most excited neuron and the complete neural network.,
"Keywords: sensory-neural network, disease symptoms, medical diag- nosing.",
"1 Introduction Medical diagnosing can be treated as a mapping of symptoms to characteris- tics of diseases [1,2].",
"Our goal is to develop a sensory-neural network (SNN) by which this mapping could be performed automatically by a PC [3,4,5].",
With this aim we ﬁrst transform data about patient’s condition into a proper form for the numerical processing.,
This transformation corresponds to sensing of symptoms by sensors at the input layer of the SNN shown in Fig.,
"Similarly as data of patients, the characteristics of diseases are transformed and utilized to specify the synaptic weights of neurons in the next layer [6].",
At this speciﬁcation we take into account the performance of a doctor at the diagnosis assessment.,
The doctor collects symptoms of the treated patient and compares them with the properties of diseases.,
"At the comparison he considers certain symptoms as more signiﬁ- cant than others and so assesses the agreement between given symptoms and those describing diseases [6,7].",
The disease with the highest correlation is then selected as a result of diagnosis.,
We describe such treatment by characterizing the signiﬁcance of symptoms by synaptic weights connecting the sensors to neu- rons that correspond to various diseases.,
A signal from a particular sensor thus contributes diﬀerent amounts to excitation of various neurons.,
"The excitation of a particular neuron therefore represents the correlation between the symptoms 2 I. Grabec, E. ˇSvegl, M. Sok OOOOOOOOO OOOO OOOOOO OO OOO OO S N Fig.",
Schematic drawing of the sensory-neural network.,
"Bold arrows demon- strate input and output information ﬂow, o - denote symptom indicators, ∗- show synapses, ↔- represent connections between SNN elements.",
of a patient and the properties of the disease represented by the neuron.,
The disease corresponding to the most excited neuron in the NN layer determines the result of diagnosis.,
We describe the reliability of such assessment by expressing the output of the neuron relatively with respect to the mean value of all outputs.,
The corresponding estimator represents the likelihood of the diagnosis.,
At the speciﬁcation of the SNN we ﬁrst select a set of diseases and specify their symp- toms as well as synaptic weights of neurons.,
In terms of them we next deﬁne the agreement and likelihood estimators.,
"Symptoms used for characterization of diseases s Symptom name - S(s) s Symptom name - S(s) s Symptom name - S(s) 1 no symptoms 17 sleep disturb.- night walk 33 diﬃculty when speaking 2 sudden onset of illness 18 distorted vision 34 diﬃculty moving one side 3 hyperthermia 19 frequent urination 35 sudden infertility 4 malaise, tiredness 20 tinnitus 36 diﬃcult verbal expression 5 age over 70 21 bleeding 37 vomiting 6 headache 22 dark urine in last 2 weeks 38 snoring during sleep 7 sudden headache 23 thirst, dry mouth 39 smelly urine 8 cough 24 nervousness, depression 40 heart rate 9 dizziness 25 throat irritation 41 arterial hypertension 10 trouble breathing 26 pharyngeal dryness 42 pulse rhythmicity 11 pain 27 hoarseness 43 O satur.",
- pulse oxim.,
12 w. loss in last 1/2 y 28 obstruction in the throat 44 breathing frequency 13 loss of appetite 29 jaundice of skin & sclera 45 body height 14 irregular pulse 30 light feces 46 body weight 15 sweating 31 frequent infections 16 diﬃcult concentrating 32 ability to lift both arms Development of a Sensory-Neural Network for Medical Diagnosing 3 Table 2.,
"Indicators of symptoms s Indicators I(s, :) 3 {fever, y, n} 6 {y, n, in the morning, daily persistent} 8 {y-in the morning, y-with sputum, y-daily persistent, y-without sputum,n} 10 {at rest, on exertion, n} 11 {chest,ear,abdomen,pharynx,at swallowing,at speaking,at urinating,shoulder,n} 15 {daily persistent, nocturnal,n} 18 {y, n, one eye only} 19 {y, n, at night} 21 {from the nose, in the sputum, in the urine, n} 31 {urinary tract, tongue, corners of the mouth, lung, n} 40 {<50, 51-70, 71-90, >90} 41 {<100, 100-140, >140} 42 {rhythmic, arrhythmic} 43 {≤90, >90} 44 {<10, 11-15, >15} 45 {<150, 151-160, 161-170, 171-180, 181-190, >190} cm 46 {<50, 51-60, 61-70, 71-80, 81-90, 91-100, >100} kg All other possible indicators are speciﬁed by: I(s, :) = {yes, no} = {y, n}.",
"Components missing in Table 2 correspond to empty space ∅, for example: I(s, :) = {fever, y, n} ≡{fever, y, n, ∅, ∅, ∅, ∅, ∅, ∅}.",
"2 Properties of data bases 2.1 Symptoms and diseases According to the suggestion of the research initiator [5] we apply the set of 15 most frequent diseases denoted by the name and index D(d): 1) Anemia, 2) Uri- nary tract infection, 3) Diabetes-2, 4) Atrial ﬁbrillation, 5) Acute hemorrhagic stroke, 6) Obstructive sleep apnea, 7) Tuberculosis, 8) Chronic obstructive pul- monary disease, 9) Pneumonia, 10) Otitis media, 11) Leukocytosis, 12) Hepatitis- A, 13) Hypertension, 14) Throat inﬂammation, 15) No disease.",
"Their properties are characterized by Ns = 46 symptoms S(s) shown in Table 1 and used to describe the sensory layer of the SNN [3,4].",
"A symptom S(s) is described by up to Ni = 9 possible indicators I(s, i) shown in Table 2.",
A particular disease is characterized by various symptom indicators.,
"The presence or absence of a symptom indicator can be generally denoted by 0 or 1, but such indication leads to a very rough assessment of diagnosis.",
"For a bet- ter assessment we describe the importance of the symptom indicator I(s, i) of the disease D(d) by a synaptic weight W(d, s, i).",
"A non important indicator is assigned the weight W = 0, while the other weights are positive or nega- tive.",
A negative weight points to the absence of the disease.,
To avoid prob- lems related with network training we apply a set of ﬁxed weights speciﬁed by a clinical research.,
"Their values mostly lie in the interval (1, 3).",
"By the set 4 I. Grabec, E. ˇSvegl, M. Sok Table 3.",
"Statistical weights describing importance of symptom indicators d The values denote: (symptom index s, indicator index i, weight W (d, s, i)) 1 (2,2,1) (3,3,1) (4,1,1) (6,1,1) (9,1,1) (10,1,1) (14,1,1) (16,1,1) (17,1,1) (20,1,1) (31,1,1) (31,3,2) (40,4,1) (44,3,2) 2 (2,1,1) (3,1,3) (3,2,1) (11,3,2) (11,8,4) (19,1,3) (22,1,3) (39,1,3) 3 (2,2,2) (3,3,2) (4,1,1) (12,2,2) (18,1,1) (19,1,2) (31,1,2) 4 (2,1,3) (3,2,3) (5,1,3) (10,2,2) (11,1,2) (14,1,3) (41,2,1) (41,3,3) (42,2,3) 5 (2,1,2) (3,3,1) (7,1,2) (9,1,1) (18,3,2) (32,2,2) (33,1,2) (34,2,2) (35,1,2) (36,1,2) (37,1,1) 6 (2,2,1) (4,1,1) (6,3,1) (16,1,1) (17,1,3) (23,1,1) (24,1,2) (41,3,1) (46,6,2) (46,7,3) 7 (2,2,1) (3,2,1) (4,1,1) (8,3,3) (11,1,1) (12,1,1) (13,1,1) (15,2,3) (21,2,3) 8 (2,2,2) (3,3,1) (8,1,3) (8,2,3) (8,5,-3) (31,4,2) 9 (2,1,2) (3,1,3) (3,2,1) (4,1,2) (8,2,1) (8,3,1) (8,4,1) (10,1,1) (10,2,1) (11,1,1) 10 (2,1,2) (3,2,1) (4,1,1) (6,4,2) (11,2,3) (29,1,1) 11 (2,2,1) (3,2,1) (4,1,1) (9,1,1) (10,1,1) (10,2,1) (12,1,1) (13,1,1) (15,1,1) (18,1,1) 12 (2,1,2) (3,2,1) (4,1,3) (11,3,2) (13,1,2) (29,1,3) 22,1,3) (30,1,3) (37,1,2) 13 (2,2,1) (3,3,2) (5,1,1) (6,4,2) (9,1,1) (21,1,1) (41,3,6) 14 (2,1,2) (3,1,1) (11,4,3) (25,1,2) (26,1,1) (27,1,1) (28,1,1) 15 (1,1,1) (2,2,1) (3,3,1) (4,2,1) (6,2,1) (8,5,1) (11,9,1) (13,2,1) (16,2,1) (18,2,1) (21,4,1) (22,2,1) (23,2,1) (24,2,1) (25,2,1) (26,2,1) (27,2,1) (28,2,1) (29,2,1) (30,2,1) (31,5,1) (32,2,1) (33,2,1) (34,2,1) (35,2,1) (36,2,1) (37,2,1) (38,2,1) (39,2,1) Weights at other indices are 0.",
"{W(d, s, i); 1 ≤d ≤15; 1 ≤s ≤46; 1 ≤i ≤9} shown in Table 3 we describe quantitatively the properties of diseases.",
"For this purpose we treat the weights W(d, s, i) as the transmission parameters of synaptic joints on the neuron with index d. To provide for equivalent treatment of all diseases at the assessment of a diagnosis, it is reasonable to normalize the weights so that the sum of their positive values equals 1.",
"By using the Heaviside function: {H(x) = 0 for x ≤0 ; H(x) = 1 for x > 0} we include just positive weights into the total positive weight: Wt(d) = PNs s=1 PNi i=1 W(d, s, i)H(W(d, s, i)), and deﬁne the normalized weight by the fraction: Wn(d, s, i) = W(d, s, i)/Wt(d).",
"2.2 Algorithm of diagnosis At a diagnosis, the patient ﬁrst completes the questionnaire containing names of symptoms and their indicators.",
"A conﬁrmed item is represented by the value 1 in the response matrix {R(s, i); s = 1 .",
Ns; i = 1 .,
"Ni}, while all other non-conﬁrmed terms are 0.",
This matrix represents the signals of values 0/1 supplied from the sensory to neural layer of the network.,
"We next assume that the signal from I(s, i) excites the neuron(d) over the synaptic weight Wn(d, s, i) and adds to its output the amount Wn(d, s, i)R(s, i).",
"The complete output of Development of a Sensory-Neural Network for Medical Diagnosing 5 the neuron(d) is then given by the sum: A(d) = Ns X s=1 Ni X i=1 Wn(d, s, i)R(s, i) (1) The distribution {A(d); d = 1, .",
", Nd} describes the excitation of the com- plete NN, while A(d) is the estimator of the agreement between the patient’s symptoms and the symptoms of the disease D(d).",
"If all symptoms of D(d) coincide with the symptoms conﬁrmed by the patient, the neuron(d) is max- imally excited and we get A(d) = 1 (or 100%).",
"As the result of the diag- nosis assessment, we propose the disease D(do) with the maximal agreement: A(do) = max {A(d); d = 1 .",
"To support our decision we determine the mean value < A >= PNd d=1 A(d)/Nd and the standard deviation σA = p var(A) of A(d), and then determine the rel- ative deviation: ∆A(d) = (A(d)−< A >)/σA.",
In the case when ∆A(do) ≫1 we conclude that the corresponding D(do) is signiﬁcantly outstanding and can be considered as a reliable quantitative result of the diagnosis.,
Such reasoning is possible when the symptoms of a particular disease are well exhibited.,
"However, this is not always the case, since several diseases share similar symptoms.",
"This leads to similar stimulations and consequently, a question arises as how to infer when ∆A(do) is not outstanding.",
To address this possibility we divide A(d) by the sum: PNd d=1 A(d) and deﬁne the disease likelihood estimator L(d) as: L(d) = A(d) PNd d′=1 A(d′) (2) This estimator describes the relative stimulation of the neuron(d) in the layer of neurons and represents the reliability of our decision.,
A high value of L(d) corresponds to a ﬁrm argument for selecting the corresponding disease as a reliable result of the diagnosis.,
It is important that both A(d) and L(d) are quantitative in character and thus can be treated as the supplement of other quantitative data in clinical tests.,
"3 Performance of SNN 3.1 Characterization of the performance For a wide application we have developed a program that interacts with a user over a graphic interface [3,4].",
"At the start the graphic interface presents the user with three options: 1) Identifying a diagnosis based on completing the ques- tionnaire, 2) Testing the program performance based on internally stored data sets of formal symptoms for all diseases, 3) Changing the weights of symptom indicators.",
All options can be repeated.,
In the ﬁrst option a new window with instructions for the user appears to- gether with a window to enter the patient’s name.,
"After accepting the name, 6 I. Grabec, E. ˇSvegl, M. Sok the program shows sequentially 46 windows with the symptom names and their indicators.",
"Conﬁrmed data are translated into the response matrix R that is led from the sensory to the neural layer of SNN, where the distributions of the agreement A and likelihood L are determined.",
The results are transferred to the user over various channels.,
"The most informative is the displayed diagram of A(d) and L(d) distribution versus the disease index d. The lines at the levels of < A >, < A > +σA and < A > +2σA are used as references for a visual assessment of the diagnosis.",
Two examples are shown in Fig.,
"In addition to such diagram, the ﬁles with the patient’s responses and corresponding numerical data are available for printing.",
In the second option the program displays the set of diseases.,
"After one of them is selected, the program applies the corresponding formal set of symptoms and uses it instead of the patient’s symptoms in the same procedure as in the ﬁrst option.",
This step shows the optimal possibility of the selected disease diagnosis.,
The third option allows specialists to examine how a variation of synaptic weights inﬂuences the diagnosis process.,
"By adjusting the weights and further testing the diagnosis using the second option, the performance of the complete program can be gradually improved.",
"With this aim, the program allows modi- ﬁcation of weights.",
"This option provides for NN training, while the changing of the set of symptoms provides for evolution of the complete SNN.",
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Disease index d 0 10 20 30 40 50 60 70 80 90 100 % A(d) L(d) < A > <A>+ (A) <A>+2 (A) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Disease index d 0 10 20 30 40 50 60 70 80 90 100 % A(d) L(d) < A > <A>+ (A) <A>+2 (A) Fig.,
Left : Distribution of A and L in testing the program performance based on formal symptoms of D(13) - hypertension.,
Right : Distribution of A and L based on the answers to the questionnaire by a patient suﬀering from D(12) - hepatitis A.,
Development of a Sensory-Neural Network for Medical Diagnosing 7 3.2 Testing of the program performance To demonstrate the performance of developed SNN we ﬁrst present results of its testing performed with the formal symptoms of the typical disease D(13) - hypertension.,
"In this test, we get the diagram shown on the left of Fig.",
"The value of the corresponding optimal agreement estimator is in this case Ao(4) = 100%, and it surpasses the mean value < A > for 3.25 σA.",
This out- standing deviation from the mean value indicates a correct assessment of the diagnosis.,
But the value of A corresponding to several other diseases also sur- passes the mean value < A >.,
This outcome indicates that the symptoms of these diseases are in a sense similar to those of hypertension.,
"In spite of this property, the distribution of A(d) suggests selecting hypertension as the result of the di- agnosis.",
"Although the agreement with its symptom indicators is Ao = 100%, the likelihood value of hypertension disease is only Lo = 35%; the other values, however, are still appreciably smaller: L(d) ≪Lo; d ̸= do.",
Similar performance of the program as in testing the diagnosis of hypertension is observed when using formal symptoms of other diseases.,
"The corresponding optimal likelihood values {Lo(d); d = 1, .",
"15} are: Lo = (29, 32, 34, 32, 37, 33, 29, 48, 24, 32, 28, 28, 35, 48, 41)% (3) The mean value of the set of optimal values {Lo(d); d = 1, .",
", 15} is similar as in the demonstrated case of hypertension, amounting to < Lo >= 34, 6%.",
"The corresponding standard deviation is σL = 7% while the maximal and minimal values are Lo,max = Lo(8) = Lo(14) = 48% and Lo,min = Lo(9) = 24%, respec- tively.",
These data indicate that the likelihood value of Lo ≈35% yields a rather ﬁrm quantitative argument for accepting the result of the automatic diagnosing.,
To demonstrate performance of SNN in the clinical practice we perform di- agnosis of the disease D(12) - hepatitis A by data obtained from a patient.,
The diagram is shown on the right of Fig.,
In this case the value of the relative agreement is also A(12) = 100% and surpasses the mean value < A > of the agreement distribution by 2.53 σA.,
"All other values {A(d); d ̸= 12} are essen- tially below the value A(12); therefore, we accept as the rather reliable result of diagnosis the D(12) - hepatitis A.",
"This conclusion is supported by the value L(12) = 19% that is smaller as Lo(12) = 28% obtained at testing by formal indicators, but still approximately two times greater than all other values.",
Such result is obtained when all signiﬁcant symptom indicators in the questionnaire are conﬁrmed by a patient.,
"Other characteristic examples are published else- where [3,4].",
4 Conclusions Our testing has indicated that the selected sets of symptoms and synaptic weights provide a proper basis for the speciﬁcation of a sensory-neural network applicable for an automatic diagnosing of selected diseases.,
"The advantage of the developed method is the quantitative expression of the agreement between 8 I. Grabec, E. ˇSvegl, M. Sok patient symptoms and properties of diseases.",
By using this estimator various subjective errors could be avoided at the assessment of a diagnosis.,
"Moreover, its reliability can be described by the disease likelihood estimator.",
"Consequently, the developed SNN could be widely applied by medical doctors and patients out- side the professional environment.",
At the development of our SNN we have utilized synaptic weights determined by doctors.,
"However, in the interest of reﬁning the diagnosing the corresponding data could also be automatically created and even improved during the appli- cation of the corresponding computer program.",
"Various methods developed for training artiﬁcial neural networks could be applied for this purpose [1,6].",
Such an adaptation would in fact allow for the acquisition of new medical knowledge and also for its storage.,
We are aware that our procedure corresponds to a rather crude simpliﬁcation of the professional performance of doctors.,
To improve it one should take more symptoms as well as diseases into account.,
"However, making such an improve- ment requires more in depth descriptions of the corresponding sets.",
We expect that for this purpose applying a hierarchic structure could be advantageous.,
Acknowledgments.,
"This research was was supported by the Slovenian Academy of Sciences and Art, Ljubljana and Amanova Ltd, Technology Park of Ljubljana, Slovenia.",
References 1.,
"Wagholikar, K. B., Sundararajan, V., Deshpande, A. W.: Modeling paradigms for medical diagnostic decision support: a survey and future directions.",
"36(5), 3029-3049 (2012).",
"Improving Diagnosis in Health Care, Eds: Balogh, E. P. , Miller, B. T., Ball, J. R.; Committee on Diagnostic Error in Health Care; Board on Health Care Services; Institute of Medicine; The Nat.",
"The National Academic Press, Washington DC, 2015.",
"Sok, M., ˇSvegl, E., Grabec, I.: Statistical diagnosis of diseases.",
"Applied Stat., Ribno (Bled) Slovenia, Sept. 18-21, 2016, Abstracts and Program, p30.",
"Sok, M., ˇSvegl, E., Grabec, I.: A sensory-neural network for medical diagno- sis.",
In: IEEE Conf.,
"(EAIS2017), Ljubljana, Slove- nia, 31 May-2 June 2017, http://ieeexplore.ieee.org/document/7954819/; DOI: 10.1109/EAIS.2017.7954819.",
Project proposals call: https://en.wikipedia.org/wiki/Tricorder_X_Prize.,
"Grabec, I., Sachse, W. : Synergetics of Measurement, Prediction and Control.",
"Springer-Verlag, Berlin (1997).",
"Zhou, X. H., Obuchowski, N. A., McClish, D. K.: Statistical Methods in Diagnostic Medicine, 2nd Edition.",
Wiley series in probability and statistics (2011).,
IEEE Copyright Notice ©2022 IEEE.,
Personal use of this material is permitted.,
"Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
"Accepted to be Published in: Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN 2022), 18-23 July, 2022, Padua, Italy.",
"arXiv:2205.13273v1 [cs.CV] 26 May 2022 Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks 1st Guilherme Vieira Department of Applied Mathematics University of Campinas Campinas, Brazil email: vieira.g@ime.unicamp.br 2nd Marcos Eduardo Valle Department of Applied Mathematics University of Campinas Campinas, Brazil email: valle@ime.unicamp.br Abstract—This paper features convolutional neural networks deﬁned on hypercomplex algebras applied to classify lymphocytes in blood smear digital microscopic images.",
"Such classiﬁcation is helpful for the diagnosis of acute lymphoblast leukemia (ALL), a type of blood cancer.",
We perform the classiﬁcation task using eight hypercomplex-valued convolutional neural networks (HvCNNs) along with real-valued convolutional networks.,
"Our results show that HvCNNs perform better than the real-valued model, showcasing higher accuracy with a much smaller number of parameters.",
"Moreover, we found that HvCNNs based on Clifford algebras processing HSV-encoded images attained the highest observed accuracies.",
"Precisely, our HvCNN yielded an average accuracy rate of 96.6% using the ALL-IDB2 dataset with a 50% train-test split, a value extremely close to the state- of-the-art models but using a much simpler architecture with signiﬁcantly fewer parameters.",
"Index Terms—Convolutional neural network, hypercomplex algebras, Clifford algebras, Acute Lymphoblastic Leukemia, computer assisted diagnosis.",
"INTRODUCTION Over the past few decades, artiﬁcial neural networks (ANN) have been employed on various classiﬁcation tasks, many of them previously performed by humans.",
"One popular ex- ample is the computer-assisted diagnosis (CAD), in which the output of the ANN may assist doctors in making more accurate decisions [1].",
"Most applications in CAD consist of a machine learning model performing tasks that would be handmade by specialists, reducing the ﬁnancial and human costs while also avoiding possible mistakes caused by fatigue [2], [3].",
"Moreover, computers have been repeatedly appointed as outperforming unaided professionals in these tasks [3], [4].",
"CAD applications have been effectively used, for example, for leukemia identiﬁcation [5].",
Acute lymphoblastic leukemia (ALL) is a blood pathology that can be lethal in only a few weeks if left unchecked.,
"ALL is a type of blood cancer identiﬁed by immature lymphocytes, known as lymphoblasts, in the blood and bone marrow.",
"The peak incidence lies in children ages 2-5 years, and one of This work was supported in part by CNPq under grant no.",
"315820/2021-7, FAPESP under grant no.",
"2019/02278-2, and Coordenac¸˜ao de Aperfeic¸oamento de Pessoal de N´ıvel Superior - Brasil (CAPES) - Finance Code 001. the primary forms of ALL detection is through microscopic blood sample inspection.",
"Computer-aided leukemia diagnosis has been achieved using many different approaches in the past years, including deep learning models combined with transfer learning and unsharpening techniques [6], [7], [8], [9].",
"Also, it has been reported in the literature that a quaternion-valued convolutional neural network exhibited better generalization capability than its corresponding real-valued model to classify white cells as lymphoblasts [10].",
"Furthermore, the quaternion- valued CNN has about 34% of the total number of param- eters of the corresponding real-valued network.",
This paper investigates further the application of hypercomplex-valued convolutional neural networks (HvCNN) ALL detection.,
"Like complex numbers, quaternions are hypercomplex num- bers with a wide range of applications in science and engineering.",
"For example, quaternions provide an efﬁcient tool for describing 3D rotations used in computer graphics.",
"Furthermore, quaternion-valued neural networks have been effectively applied for signal processing and control [11], [12], image classiﬁcation [13], [14], and many other pattern recognition tasks [15], [16].",
"However, besides the quaternions, many other hypercomplex-valued algebras exist, including the coquaternions, the tessarines, Clifford algebras, and Cayley- Dickson algebras.",
"The tessarines, introduced by Cockle a few years after the introduction of quaternions, is a commuta- tive hypercomplex algebra that has been effectively used for signal processing and the development of neural networks [17], [18], [19], [20], [21].",
"The Clifford algebras comprise a broad family of associative hypercomplex algebras with interesting geometrical properties [22], [23], [24].",
"A family of hypercomplex algebras, obtained from the recursive process of Cayley-Dickson, can be used to develop effective HvNNs [25], [26].",
"Furthermore, four-dimensional hypercomplex algebras have been used for designing neural networks for controlling a robot manipulator [12].",
This paper considers the hypercomplex algebra framework proposed by Kantor and Solodovnikov [27].,
"This approach furnishes a broad class of hypercomplex algebras which com- prises the tessarines, the Clifford algebras, and the Cayley- Dickson algebras.",
"On the downside, the general framework by Kantor and Solodovnikov also contains many hypercomplex algebras with no attractive features.",
"Thus, we shall focus only on eight four-dimensional associative hypercomplex algebras, four of which are commutative.",
"We consider the tessarines, the bi-complex numbers, and the Klein 4-group algebra among the commutative algebras [12], [28].",
The four non-commutative hypercomplex algebras include quaternions and coquaternions.,
"Also, they are isomorphic to Clifford as well as Cayley- Dickson algebras [26], [29].",
The paper is organized as follows: Next section presents the basic concepts of hypercomplex algebras and explores notable four-dimensional algebras.,
Section III addresses hypercomplex-valued CNN models and shows how to emulate them using a real-valued convolutional network.,
Section IV describes the computational experiments conducted using the ALL-IDB dataset.,
The paper ﬁnishes with some concluding remarks in Section V. II.,
HYPERCOMPLEX NUMBERS AND SOME NOTABLE ASSOCIATIVE FOUR-DIMENSIONAL ALGEBRAS Let us present a few basic deﬁnitions regarding hypercom- plex numbers.,
"Hypercomplex algebras can be deﬁned in any ﬁeld, but we focus on algebras over the real numbers in this work.",
"As pointed out in the introduction, we consider the general framework proposed by Kantor and Solodovnikov, which includes the most used hypercomplex algebras [27].",
We ﬁnish this section by presenting eight notable associative four-dimensional hypercomplex algebras.,
"A Brief Review on Hypercomplex Number Systems A hypercomplex algebra A of dimension n + 1 over R is a set A = {x = x0 +x1i1 +x2i2 +· · ·+xnin : xµ ∈R, ∀µ}, (1) equipped with two operations, namely addition and multipli- cation [27].",
"The symbols iµ, with iµ /∈R for all µ = 1, .",
", n, denote the so-called hyperimaginary units of A.",
The addition of two hypercomplex numbers x = x0 + x1i1 + · · · + xnin and y = y0 + y1i1 + · · · + ynin is simply deﬁned as x + y = (x0 + y0) + (x1 + y1)i1 + .,
+ (xn + yn)in.,
"(2) The multiplication operation is carried out distributively and replacing the product of two hypercomplex units iµ and iν by an hypercomplex number pµν := iµiν, where pµν = (pµν)0 + (pµν)1i1 + · · · + (pµν)nin ∈A, (3) for all µ, ν = 1, .",
", n. Precisely, the multiplication of two hypercomplex numbers is given by the equation xy = x0y0 + n X µ,ν=1 xµyν(pµν)0 !",
"+ x0y1 + x1y0 + n X µ,ν=1 xµyν(pµν)1 !",
"+ x0yn + xny0 + n X µ,ν=1 xµyν(pµν)n !",
(4) Note that hyperimaginary units products characterize a hypercomplex algebra A.,
"In other words, the hypercomplex numbers pµν given by (3) determine A.",
"It is common practice to arrange the hypercomplex numbers pµν in a table, called the multiplication table.",
Examples of multiplication tables are given in Tables I and II.,
Algebraic properties of an hypercomplex algebra A can be inferred from its multipli- cation tables.,
"For example, symmetric multiplication tables represent commutative hypercomplex algebras [27].",
Note that the four multiplication tables in Table II are symmetric.,
"Thus, they represent commutative hypercomplex algebras.",
We will discuss in detail the multiplication tables provided in Tables I and II in the following subsection.,
"A scalar α ∈A can be identiﬁed with the hypercomplex number α = α+0i1 +· · ·+0in, and vice-versa.",
"Furthermore, from (4), the product by scalar in any algebra A satisﬁes αx = αx0 + αx1i1 + · · · + αxnin.",
"(5) This remark shows that a hypercomplex algebra A can be identiﬁed with an (n + 1)-dimensional vector space with the vector addition and the product by a scalar given by (2) and (5), respectively.",
"Moreover, the basis elements of such (n+1)-dimensional vector space are 1, i1, .",
"Besides the addition and the product by a scalar, a hypercomplex algebra is equipped with the multiplication of vectors given by (4).",
We would like to point out that different multiplication tables do not necessarily yield different hypercomplex alge- bras.,
"Precisely, we know from linear algebra that a vector space can be represented using different bases.",
"Similarly, a hypercomplex algebra can be obtained from different bases or, equivalently, using different hypercomplex units.",
"Since the multiplication table determines the outcome of the product of any two hypercomplex units, a change of basis results in a different multiplication table.",
"Because of this remark, we say two hypercomplex algebras A and A′ are isomorphic if they differ by a change of basis.",
"In other words, A and A′ are isomorphic hypercomplex algebras if the multiplication table of A′ can be obtained from the multiplication table of A through a change of basis.",
"B. Four-dimensional Hypercomplex Algebras Let us now focus on four-dimensional hypercomplex alge- bras, i.e., hypercomplex algebras of the form A = {x = x0 + x1i + x2j + x3k : x0, .",
", x3 ∈R}, (6) where i ≡i1, j ≡i2, and k ≡i3 are the three hy- percomplex units.",
Four-dimensional hypercomplex algebras are particularly useful for image processing because a color can be represented using a single hypercomplex number.,
"In other words, four-dimensional hypercomplex algebras allows representing a color by a single entity.",
Note that the hypercomplex numbers pµν = (pµν)0 + (pµν)1i+(pµν)2j +(pµν)3k in the multiplication table results in a large number of algebras.,
"Some of these algebras are isomorphic, and many of them have no attractive features.",
"Therefore, we present a construction rule that yields eight associative four-dimensional hypercomplex algebras in the following.",
"Like in the construction of Clifford algebras [30], we assume the product of hypercomplex units is associative.",
"Furthermore, we let the identity k = ij, (7) hold true.",
"Finally, we assume the four-dimensional hypercom- plex algebra is either commutative or anti-commutative.",
1) Anticommutative Algebras: We obtain an anticommu- tative four-dimensional hypercomplex algebra by imposing ij = −ji.,
"From the associativity and (7), we obtain k2 = (ij)(ij) = i(ji)j = i(−ij)j = −i2j2, (8) ik = i(ij) = i2j, (9) jk = j(ij) = j(−ji) = −j2i.",
"(10) To simplify the exposition, let i2 = γ1 and j2 = γ2.",
"From (7)-(10), we obtain an associative and anticommutative four- dimensional algebra denoted by A[γ1, γ2], whose multiplica- tion table is then A[γ1, γ2] i j k i γ1 k γ1j j −k γ2 −γ2i k −γ1j γ2i −γ1γ2 (11) By considering γ1, γ2 ∈{−1, +1}, we obtain the four- dimensional hypercomplex algebras A[−1, −1], A[−1, +1], A[+1, −1], and A[+1, +1] whose multiplication tables are depicted in Table I.",
"The hypercomplex algebra A[−1, −1] coincides with the quaternions because they have the same multipli- cation table.",
"The algebra A[−1, +1] corresponds to the co- quaternions, also known as split-quaternions [12].",
"Similarly, A[+1, +1] and A[+1, −1] can be identiﬁed with the Clifford algebras Cℓ2,0 and Cℓ1,1, respectively.",
"Furthermore, the alge- bras A[−1, +1], A[+1, −1], and A[+1, +1] are all isomorphic.",
"Finally, we would like to remark that the algebras A[−1, −1], A[−1, +1], A[+1, −1], and A[+1, +1] can be derived using the generalized Cayley-Dickson process [25], [26].",
"2) Commutative Algebras: In a similar fashion, we impose the condition ij = ji to obtain the commutative four- dimensional hypercomplex algebras.",
"Again, using associativity and (7), we are able to write the identities k2 = (ij)(ij) = i(ji)j = i(ij)j = i2j2, (12) ik = i(ij) = i2j, (13) jk = j(ij) = j(ji) = j2i.",
"(14) By expressing i2 = γ1 and j2 = γ2, we obtain a commutative hypercomplex algebra B[γ1, γ2] whose multiplication table is B[γ1, γ2] i j k i γ1 k γ1j j k γ2 γ2i k γ1j γ2i γ1γ2 (15) Analogously, we end up with the four-dimensional alge- bras B[−1, −1], B[−1, +1], B[+1, −1], B[+1, +1] by taking γ1, γ2 ∈{−1, +1}.",
Table II contains the multiplication table of these four algebras.,
"Because they have the same multiplication ta- ble, the hypercomplex algebra B[−1, +1] corresponds to the tessarines, also known as commutative quaternions [19], [17].",
"Similarly, the algebra B[−1, −1] corresponds to the bi- complex numbers [12] while B[+1, +1] is equivalent to the Klein 4-group, a commutative algebra of great interest in symmetric group theory [28].",
"Finally, we would like to point out that the algebras B[−1, −1] and B[+1, −1] can be both obtained from B[−1, +1] by a change of bases.",
"Therefore, the three algebras B[−1, −1], B[+1, −1] and B[−1, +1] are all isomorphic.",
"Concluding, we have a total of eight four-dimensional hypercomplex algebras.",
"All of them are associative, four are anticommutative and the remaining are commutative.",
"Further- more, the hypercomplex units are well structured in their multiplication table.",
"Precisely, their multiplication table can be written as follows i j k i s11 s12k s13j j s21k s22 s23i k s31j s32i s33 (16) where sij ∈{−1, +1}, for all i, j = 1, .",
", 3, depends on the parameters γ1 and γ2 as well as on the commutativity or anticommutativity of the multiplication.",
The multiplication table (16) helped us to efﬁciently implement convolutional neural networks based on the eight hypercomplex algebras presented in this section.,
We detail this remark in the following section.,
HYPERCOMPLEX-VALUED CONVOLUTIONAL NEURAL NETWORKS (HVCNN) Convolutional layers are crucial building blocks of convo- lutional neural networks.,
"The main strength of convolutional layers is their ability to process data locally and, thus, learn lo- cal patterns.",
Convolutional neural networks have been widely applied to image processing tasks [31].,
Let us brieﬂy describe a real-valued convolutional layer.,
A. Real-valued Convolutional Layers Suppose a convolutional layer is fed by a real-valued image I with C feature channels.,
"Let I(p, c) ∈R denote the intensity of the cth channel of the image I at pixel p. The neurons of a convolutional layer are parameterized structures called ﬁlters.",
The ﬁlters have the same number C of channels as the image I.,
"Let D, commonly a rectangular grid, denote the domain of the ﬁlters.",
"Also, let the weights of a convolutional layer with K real-valued ﬁlters be arranged in an array F such that F(q, c, k) denotes the value of the cth channel of the kth ﬁlter at the point q ∈D, for c = 1, .",
", C and k = 1, .",
", K. A convolutional layer with K ﬁlters yields a real-valued image J TABLE I MULTIPLICATION TABLES OF THE ANTICOMMUTATIVE ALGEBRAS.",
"Quaternions Cℓ2,0 Coquaternions Cℓ1,1 A[−1, −1] i j k i −1 k −j j −k −1 i k j −i −1 A [+1, +1] i j k i 1 k j j −k 1 −i k −j i −1 A [−1, +1] i j k i −1 k −j j −k 1 −i k j i 1 A [+1, −1] i j k i 1 k j j −k −1 i k −j −i 1 TABLE II MULTIPLICATION TABLES OF COMMUTATIVE ALGEBRAS.",
"Bicomplex numbers Tessarines Klein 4-group B[−1, −1] i j k i −1 k −j j k −1 −i k −j −i 1 B[+1, −1] i j k i 1 k j j k −1 −i k j −i −1 B[−1, +1] i j k i −1 k −j j k 1 i k −j i −1 B[+1, +1] i j k i 1 k j j k 1 i k j i 1 with K feature channels obtained by evaluating an activation function on the addition of a bias term and the convolution of the image I by each of the K ﬁlters.",
"Precisely, let (I∗F)(p, k) denote the convolution of the image I by the kth ﬁlter at pixel p. Intuitively, (I ∗F)(p, k) is the sum of the multiplication of the weights of the kth ﬁlter and the intensities of the pixels of the image in a window characterized by the translation of p by S(q), for q ∈D.",
"The term S(q), for q in the domain D of the ﬁlter, represents a translation that can take vertical and horizontal strides into account.",
"In mathematical terms, the convolution of the image I by the kth ﬁlter at pixel p is given by (I ∗F)(p, k) = C X c=1 X q∈D I  p + S(q), c  F(q, c, k).",
"(17) Moreover, the intensity of the kth feature channel of the output of a convolutional layer at pixel p is deﬁned by J(p, k) = ϕ (b(k) + (I ∗F)(p, k)) , (18) where ϕ : R →R denotes the activation function.",
"B. Hypercomplex-valued Convolutional Layers The hypercomplex-valued convolutional layer is deﬁned analogously to the real-valued convolutional layer by replacing the real numbers and operations with their corresponding hypercomplex versions in (17) and (18) [32], [33].",
"Precisely, the “intensity” of the kth channel of the hypercomplex-valued output image J(h) at pixel p is given by J(h)(p, k) = ϕA  b(h)(k) + (I(h) ∗F(h))(p, k)  , (19) where ϕA : A →A is a hypercomplex-valued activation function, b(h) k ∈A is the bias term, and (I(h) ∗F(h))(p, k) = C X c=1 X q∈D I(h)(p + S(q), c)F(h)(q, c, k), (20) is the convolution of I(h) by the kth hypercomplex-valued ﬁlter at pixel p. In this paper, we only consider split-functions deﬁned using a real-valued function ϕ : R →R as follows for all x ∈x0 + x1i1 + · · · + xnin ∈A: ϕA(x) = ϕ(x0) + ϕ(x1)i1 + .",
"(21) C. Emulating Hypercomplex-valued Convolutional Layers Since most deep neural network libraries are designed for real-valued inputs, we show how to emulate a four- dimensional hypercomplex-valued convolutional layer using a real-valued convolutional layer.",
"The reasoning is quite similar to the approaches reported in the literature for complex- and quaternion-valued deep networks [32], [33].",
"First, an image I(h) with C channels deﬁned on a four- dimensional hypercomplex algebra can be represented by I(h) = I0 + I1i + I2j + I3k, (22) where I0, I1, I2, and I3 are real-valued images with C channels.",
"Similarly, a bank of K hypercomplex-valued ﬁlters can be represented by F(h) = F0 + F1i + F2j + F3k, (23) where F0, F1, F2, and F3 are real-valued arrays representing each a bank of K ﬁlters with domain D and C feature channels.",
"From the multiplication table (16) and omitting the arguments (p + S(q), c) and (q, c, k) to simplify the notation, we obtain I(h)(p + S(q), c)F(h)(q, c, k) = (I0 + I1i + I2j + I3k)(F0 + F1i + F2j + F3k) = I0F0 + s11I1F1 + s22I2F2 + s33I3F3 + (I0F1 + I1F0 + s23I2F3 + s32I3F2)i + (I0F2 + s13I1F3 + I2F0 + s31I3F1)j + (I0F3 + s12I1F2 + s21I2F1 + I3F0)k Therefore, the real-part of the convolution given by (20) satisﬁes the equation  I(h) ∗F(h) 0(p, k) = C X c=1 X q∈D h I0(p + S(q), c)F0(q, c, k) + s11I1(p + S(q), c)F1(q, c, k) (24) + s22I2(p + S(q), c)F2(q, c, k) + s33I3(p + S(q), c)F3(q, c, k) i , Equivalently, the real-part of the convolution of the image I(h) by the kth hypercomplex-valued ﬁlter at pixel p can be computed using the real-valued convolution  I(h) ∗F(h) 0(p, k) =  I(r) ∗F(r) 0  (p, k), (25) where I(r) is the real-valued image with 4C features channels obtained by concatenating the real and imaginary parts of I(h) as follows I(r)(p, :) = [I0(p, :), I1(p, :), I2(p, :), I3(p, :)], (26) for all pixels p, and F(r) 0 is the real-valued ﬁlter deﬁned by F(r) 0 (q, 1 : C, k) = F0(q, 1 : C, k), (27) F(r) 0 (q, C + 1 : 2C, k) = s11F1(q, 1 : C, k), (28) F(r) 0 (q, 2C + 1 : 3C, k) = s22F2(q, 1 : C, k), (29) F(r) 0 (q, 3C + 1 : 4C, k) = s33F3(q, 1 : C, k), (30) for all q ∈D and k = 1, .",
", K. For short, the notation F(r) 0 = [F0, s11F1, s22F2, s33F3], (31) means that F(r) 0 is obtained by concatenating F0, s11F1, s22F2, and s33F3 as prescribed above.",
"Note from (17) that  I(r) ∗F(r) 0  (p, k) = 4C X c=1 X q∈D I(r) p + S(q), c  F(r) 0 (q, c, k) = X q∈D "" C X c=1 I(r) p + S(q), c  F(r) 0 (q, c, k) + .",
"+ 4C X c=3C+1 I(r) p + S(q), c  F(r) 0 (q, c, k) # = X q∈D "" C X c=1 I0  p + S(q), c  F0(q, c, k) + .",
"+ C X c′=1 s33I3  p + S(q), c′ F3(q, c′, k) # , which coincides with  I(h) ∗F(h) 0(p, k) given by (24).",
"Therefore, using a split-function, the real-part J0(p, k) of the output J(h)(p, k) of the hypercomplex-valued convolutional layer given by (20) can be computed using a real-valued convolutional layer as follows J0(p, k) = ϕ  b0(k) +  I(r) ∗F(r) 0  (p, k)  , (32) a) Probable lymphoblast b) Healthy cell Fig.",
Examples of candidate cells for the classiﬁcation task.,
Images from the ALL-IDB dataset [35].,
"where the bias term b0(k) corresponds to the real-part of b(k) while I(r) and F(r) 0 are given by (26) and (31), respectively.",
"In a similar vein, the three imaginary parts J1(p, k), J2(p, k), and J3(p, k) of the kth channel of the hypercomplex-valued image J(h) at pixel p can be computed using real-valued convolutional layers with bias terms b1(k), b2(k), and b3(k) and real-valued ﬁlters F(r) 1 = [F1, F0, s23F3, s32F2], (33) F(r) 2 = [F2, s13F3, F0, s31F1], (34) F(r) 3 = [F3, s12F2, s21F1, F0], (35) respectively.",
"We would like to ﬁnish this section with few remarks: First, emulating a hypercomplex-valued convolutional layer allow us to take advantage of open-source deep libraries such as Tensorflow and PyTorch for python and Flux for Julia Language.",
"Consequently, hypercomplex- valued versions of well-known deep neural networks can be implemented in current deep learning libraries.",
"On top of that, although not properly designed for hypercomplex-valued models, pooling layers, sophisticated optimizers, and speed-up training techniques such as batch normalization and weight initialization can be incorporated as the ﬁrst attempt into hypercomplex-valued deep networks.",
"In the following section, we consider a hypercomplex-valued deep neural network for the classiﬁcation of lymphocytes from smear blood images that resembles the LeNet architecture [34].",
"LYMPHOBLAST IMAGE CLASSIFICATION TASK One of the primary forms of diagnosis of acute lymphoblas- tic leukemia (ALL) is through microscopic blood smear image inspection [6], [7], [8], [9].",
"Precisely, physicians diagnose ALL by the presence of many lymphoblasts in a blood smear image in which white cells are stained with bluish-purple coloration.",
"A candidate cell image is selected, cut from the blood smear image, and fed into a machine learning classiﬁer for a computer-aided leukemia diagnosis.",
"For illustrative pur- poses, Fig.",
1 shows two candidate cell images used in such a classiﬁcation task.,
The classiﬁer predicts if the candidate image is a lymphoblast in the simplest binary case.,
"In our model, the candidate images have 100 × 100 pixels and have been saved using either the RGB (red-green-blue) TABLE III PARAMETER DISTRIBUTION PER LAYER FOR EACH ARCHITECTURE.",
"RvCNN HvCNN Conv Layer 1 (3,3) ﬁlters 32 8 Parameters 896 320 Conv Layer 2 (3,3) ﬁlters 64 16 Parameters 18,496 4,672 Conv Layer 3 (3,3) ﬁlters 128 32 Parameters 73,856 18,560 Dense Layer Units 1 1 Parameters 12,801 12,801 Total 106,049 36,353 or the HSV (hue-saturation-value) color encodings.",
"Using the RGB encoding, an image is characterized by the intensities of red (R), green (G), and blue (B), respectively.",
An RGB encoded image has been mapped into a hypercomplex-valued image I(h) RGB by means of the equation I(h) RGB = Ri + Gj + Bk.,
"(36) Similarly, a HSV-encoded image is characterized by the hue H ∈[0, 2π), the value V ∈[0, 1], and the saturation S ∈[0, 1].",
A hypercomplex-valued image I(h) HSV is derived from an HSV- encoded image as follows [10]: I(h) HSV =  cos(H) + sin(H)i  (S + V j).,
"(37) Note that, because ij = k in all the considered hypercomplex algebras, (37) is equivalent to I(h) HSV = S cos(H) + S sin(H)i + V cos(H)j + V sin(H)k. (38) We performed the lymphocyte classiﬁcation task using real- valued CNNs (RvCNNs) and the proposed HvCNNs.",
"Similar architectures are adopted for both real- and hypercomplex- valued models, as suggested in [10].",
The RvCNN features three convolutional layers with 3 × 3 ﬁlters followed by a max-pooling layer with 2 × 2 kernels.,
A dense layer with 1 unit yields the output.,
The hypercomplex-valued models have the same layer layout but a much smaller number of ﬁl- ters per convolution layer because each hypercomplex-valued channel is equivalent to four real-valued feature channels.,
The activation function adopted for all convolutional layers is the rectiﬁed linear unit (ReLU).,
The dense layer for both architectures is a single neuron that outputs the label 0 for a healthy white cell and 1 for a lymphoblast image.,
Table III shows a breakdown of total parameters per layer for each architecture.,
All deep network models in this work were imple- mented using the python libraries Keras and Tensorflow.,
The source codes are available at https://github.com/mevalle/ Hypercomplex-valued-Convolutional-Neural-Networks.,
"A. Computational Experiments Let us now describe the outcome of the computational ex- periments performed using the acute lymphoblastic leukemia image database (ALL-IDB), a popular public dataset for segmentation and classiﬁcation tasks directed at ALL detection [35].",
The ALL-image database (ALL-IDB) consists of two sets of images.,
The ALL-IDB1 contains 108 blood smear images for segmentation and classiﬁcation tasks.,
"The ALL- IDB2 contains 260 segmented images, each containing a single blood element like the images depicted in Fig.",
"1, and is aimed exclusively for the classiﬁcation task [35].",
We used all the 260 images from the ALL-IDB2 dataset in our computational experiments.,
Like Genovese et al.,
"[7], [8], we randomly split the dataset into the training and test sets containing each 50% of the total number of images.",
The training set is enlarged using horizontal and vertical ﬂips.,
We conducted 100 simulations per experiment.,
"Each simulation consists of splitting training and test set, augmenting the training set, initializing the network parameters, training for 100 epochs using ADAM optimizer and the binary cross- entropy loss function, and predicting test images.",
We evaluate the performances using the accuracy score in the test set.,
We derive a total of 18 network conﬁgurations using the real numbers and the eight hypercomplex algebras detailed in Section II.,
"Namely, each of the nine models (1 RvCNN and 8 HvCNNs) is used in the classiﬁcation tasks of both RGB and HSV encoded image sets.",
The box plots depicted in Fig.,
2 summarize the outcome of the computational experiment.,
"Note that all models performed well for the RGB encoded images, with medians close to 95% accuracy except for the HvCNN based on the algebra B[−1, +1] that yields a median accuracy rate of 93.8%.",
"In the HSV case, however, perfor- mances vary more drastically.",
"The real-valued model exhibits poor performance compared to all the hypercomplex-valued ones, with a median accuracy rate of 91.5%.",
"The HvCNNs based on the algebras A[−1, −1], B[−1, −1], and B[−1, +1] yielded all a median accuracy score of 96.2% while the HvCNN based on the algebra B[+1, −1] achieved a median accuracy score of 96.5%.",
"Moreover, the HvCNNs based on the isomorphic algebras A[−1, +1], A[+1, −1], and A[+1, +1] as well as the HvCNN based on B[+1, +1] yielded all a median accuracy score of 96.9%.",
The signiﬁcant improvement in the accuracy scores of the HvCNN models indicates they can take advantage of the locally cohesive structure of the HSV encoding.,
"To better depict the outcome of this computational experi- ment, we summarize the performance of the network classiﬁers in the Hasse diagram shown in Fig.",
This ﬁgure depicts a single HvCNN model of each isomorphic hypercomplex algebra group to simplify the exposition.,
"Precisely, Fig.",
"3 compares the performance of the HvCNNs based on the hypercomplex algebras A[−1, −1] (quaternions), A[−1, +1] (coquaternions), B[−1, +1] (tessarines), B[+1, +1] (Klein 4- group), along with the real-valued models.",
"In this diagram, a solid line connecting two models indicates that the one above achieved better performance than the one below, with a conﬁdence level of 0.99 according to a Student’s t-test.",
"In other words, models higher up in the diagram perform signiﬁcantly better than those on the lower end.",
"Furthermore, the solid lines indicate a transitive relation.",
"Thus, if model A is better than B and B is better than C, then A is better than C. First off, Fig.",
"3 conﬁrms that the HvCNN models performed better using the a) RGB-encoded images b) HSV-encoded images Real+RGB A[-1,-1]+RGB A[-1,+1]+RGB A[+1,-1]+RGB A[+1,+1]+RGB B[-1,-1]+RGB B[-1,+1]+RGB B[+1,-1]+RGB B[+1,+1]+RGB 85 87 89 91 93 95 97 99 Accuracies (%) Real+HSV A[-1,-1]+HSV A[-1,+1]+HSV A[+1,-1]+HSV A[+1,+1]+HSV B[-1,-1]+HSV B[-1,+1]+HSV B[+1,-1]+HSV B[+1,+1]+HSV 85 87 89 91 93 95 97 99 Accuracies (%) Fig.",
Boxplot of test set accuracies produced by real-valued and hypercomplex-valued neural networks.,
"Real+RGB Real+HSV A[-1,-1]+RGB (quaternions) B[-1,+1]+RGB (tessarines) A[-1,+1]+RGB (coquaternions) B[+1,+1]+RGB (Klein 4-group) A[-1,-1]+HSV (quaternions) A[-1,+1]+HSV (coquaternions) B[-1,+1]+HSV (tessarines) B[+1,+1]+HSV (Klein 4-group) Fig.",
Hasse diagram of representative experiment conﬁgurations.,
"HSV than the RGB encoding, and HvCNNs on HSV encoded images outperform the real-valued models on both encodings.",
"In addition, it shows the real-valued model on HSV encoded images as the poorest performer.",
"Moreover, coquaternion- and tessarine-based HvCNNs outperformed the HvCNN based on quaternions, the four-dimensional algebra most widely used in applications.",
"At this point, we would like to recall that superior performance of neural networks based on non-usual hypercomplex algebras has been previously reported in the literature [26], [29], despite quaternion-based neural network yielding better performance in applications like controlling a robot manipulator [12].",
"Finally, we would like to recall that Genovese et al.",
ob- tained average accuracy rates of 97.92% using a ResNet18 combined with histopathological transfer learning [7].,
The top-performing HvCNNs achieved average accuracy scores of 96.51% and 96.39%.,
"However, in contrast to the ResNet18 network with approximately 11.4M parameters, the HvC- NNs have only 36K trainable parameters.",
"In particular, the coquaternion-valued HvCNN with HSV encoding achieved 98% of the average accuracy score of the ResNet18 with transfer learning but with only 0.3% of its total number of trainable parameters.",
"V. CONCLUDING REMARKS In this work, we extended the concept of a quaternion- valued CNN to general hypercomplex algebras.",
We con- structed eight such algebras with desirable properties ac- cording to Kantor and Solodovnikov framework.,
"These alge- bras are isomorphic to well-known four-dimensional algebras: quaternions, coquaternions, tessarines, Klein 4-group, Cayley- Dickson algebras, and Clifford algebras.",
The hypercomplex- valued neural networks have been applied for lymphoblast image classiﬁcation.,
"Using the public dataset ALL-IDB2, we conducted exper- iments featuring real-valued and hypercomplex-valued mod- els.",
We observed that the HvCNNs performed similarly to the real-valued models in the RGB-encoded images while outperforming the RvCNNs with HSV-encoded images.,
"The superior performance of the HvCNN indicates that they take better advantage of the HSV color system, which is more akin to human vision and more widely used in computer vision applications [36].",
"Consequently, HvCNN models seem more efﬁcient at comprising information in its ﬁlters and four- dimensional entities than the real-valued models.",
"Moreover, coquaternion- and tessarine-valued models outperformed the quaternion-valued CNNs.",
"Lastly, the performance attained by the top-performing HvCNN models is signiﬁcant and com- parable to notable results in the literature.",
"Indeed, Genovese et al.",
"observed an average accuracy score of 97.92% with the ResNet18, a deep network containing around 11 million parameters [7].",
The coquaternion-valued CNN yielded an average accuracy of 96.51% with approximately 0.3% of the total of trainable parameters of the real-valued ResNet18.,
"REFERENCES [1] L. H. Eadie, P. Taylor, and A. P. Gibson, “A systematic review of computer-assisted diagnosis in diagnostic cancer imaging,” European journal of radiology, vol.",
"e70–e76, 2012.",
"[2] C. R. Pereira, D. R. Pereira, S. A. Weber, C. Hook, V. H. C. de Al- buquerque, and J. P. Papa, “A survey on computer-assisted parkinson’s disease diagnosis,” Artiﬁcial intelligence in medicine, vol.",
"48–63, 2019.",
"[3] W. Qian, L. P. Clarke, B. Zheng, M. Kallergi, and R. Clark, “Computer assisted diagnosis for digital mammography,” IEEE Engineering in Medicine and Biology Magazine, vol.",
"561–569, 1995.",
"[4] D. Leaper, J. C. Horrocks, J. Staniland, and F. De Dombal, “Computer- assisted diagnosis of abdominal pain using “estimates” provided by clinicians,” Br Med J, vol.",
"350–354, 1972.",
"[5] H. T. Salah, I. N. Muhsen, M. E. Salama, T. Owaidah, and S. K. Hashmi, “Machine learning applications in the diagnosis of leukemia: Current trends and future directions,” International Journal of Laboratory Hema- tology, vol.",
"717–725, 12 2019.",
"[6] N. Bibi, M. Sikandar, I. U. Din, A. Almogren, and S. Ali, “Iomt-based automated detection and classiﬁcation of leukemia using deep learning,” Journal of Healthcare Engineering, 2020.",
"[7] A. Genovese, M. S. Hosseini, V. Piuri, K. N. Plataniotis, and F. Scotti, “Histopathological transfer learning for acute lymphoblastic leukemia detection,” CIVEMSA 2021 - IEEE International Conference on Compu- tational Intelligence and Virtual Environments for Measurement Systems and Applications, Proceedings, 6 2021.",
"[8] ——, “Acute lymphoblastic leukemia detection based on adaptive un- sharpening and deep learning,” ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings, vol.",
"2021- June, pp.",
"1205–1209, 2021.",
"[9] M. Zolfaghari and H. Sajedi, “A survey on automated detection and classiﬁcation of acute leukemia and WBCs in microscopic blood cells,” Multimedia Tools and Applications, pp.",
"1–31, 1 2022.",
"Avail- able: https://link.springer.com/article/10.1007/s11042-022-12108-7 [10] M. A. Granero, C. X. Hern´andez, and M. E. Valle, “Quaternion-valued convolutional neural network applied for acute lymphoblastic leukemia diagnosis,” in Intelligent Systems, A. Britto and K. Valdivia Delgado, Eds.",
"Cham: Springer International Publishing, 2021, pp.",
"[11] S. P. Talebi, S. Werner, and D. P. Mandic, “Quaternion-Valued Dis- tributed Filtering and Control,” IEEE Transactions on Automatic Con- trol, vol.",
"4246–4257, 10 2020.",
"[12] K. Takahashi, “Comparison of high-dimensional neural networks using hypercomplex numbers in a robot manipulator control,” Artiﬁcial Life and Robotics, vol.",
"367–377, 8 2021.",
"Available: https://link.springer.com/article/10.1007/s10015-021-00687-x [13] F. Shang and A. Hirose, “Quaternion Neural-Network-Based PolSAR Land Classiﬁcation in Poincare-Sphere-Parameter Space,” IEEE Trans- actions on Geoscience and Remote Sensing, vol.",
"5693–5703, 2014.",
"[14] M. E. Valle and R. A. Lobo, “Quaternion-valued recurrent projection neural networks on unit quaternions,” Theoretical Computer Science, vol.",
"136–152, 12 2020.",
"[15] T. Parcollet, M. Morchid, and G. Linar`es, “A survey of quaternion neural networks,” Artiﬁcial Intelligence Review, vol.",
"2957–2982, 4 2020.",
"Available: https://doi.org/10.1007/ s10462-019-09752-1 [16] E. Bayro-Corrochano, S. Solis-Gamboa, G. Altamirano-Escobedo, L. Lechuga-Gutierres, and J. Lisarraga-Rodriguez, “Quaternion Spiking and Quaternion Quantum Neural Networks: Theory and Applications,” https://doi.org/10.1142/S0129065720500598, vol.",
"[17] J. Navarro-Moreno, R. M. Fern´andez-Alcal´a, J. D. Jim´enez-L´opez, and J. C. Ruiz-Molina, “Tessarine Signal Processing under the T-Properness Condition,” Journal of the Franklin Institute, 8 2020.",
"Available: https://linkinghub.elsevier.com/retrieve/pii/S0016003220305391 [18] J. Navarro-Moreno and J. C. Ruiz-Molina, “Wide-sense Markov signals on the tessarine domain.",
"A study under properness conditions,” Signal Processing, vol.",
"183, p. 108022, 6 2021.",
"[19] F. Ortolani, D. Comminiello, M. Scarpiniti, A. Uncini, F. Ortolani, D. Comminiello, M. Scarpiniti, A. Uncini, D. Comminiello, M. Scarpiniti, and A. Uncini, “On 4-Dimensional Hypercomplex Algebras in Adaptive Signal Processing,” Smart Innovation, Systems and Technologies, vol.",
"131–140, 6 2017.",
"Available: https://link.springer.com/chapter/10.1007/978-3-319-95098-3 12 [20] R. Carniello, W. Vital, and M. Valle, “Universal approximation theorem for tessarine-valued neural networks,” in Anais do XVIII Encontro Nacional de Inteligˆencia Artiﬁcial e Computacional.",
"Porto Alegre, RS, Brasil: SBC, 2021, pp.",
"Available: https://sol.sbc.org.br/index.php/eniac/article/view/18256 [21] F. Senna and M. Valle, “Tessarine and quaternion-valued deep neural networks for image classiﬁcation,” in Anais do XVIII Encontro Nacional de Inteligˆencia Artiﬁcial e Computacional.",
"Porto Alegre, RS, Brasil: SBC, 2021, pp.",
"Available: https://sol.sbc.org.br/index.php/eniac/article/view/18266 [22] D. Hestenes and G. Sobczyk, Clifford algebra to geometric calculus: a uniﬁed language for mathematics and physics.",
"Springer Science & Business Media, 2012, vol.",
"[23] E. Hitzer, T. Nitta, and Y. Kuroe, “Applications of Clifford’s Geometric Algebra,” Advances in Applied Clifford Algebras, vol.",
"377–404, 6 2013.",
"[24] J. Vaz and R. da Rocha, An Introduction to Clifford Algebras and Spinors.",
"Oxford University Press, 2016.",
"Brown, “On generalized Cayley-Dickson algebras.” Paciﬁc Journal of Mathematics, vol.",
"415–422, 1967.",
"Available: https://projecteuclid.org/euclid.pjm/1102992693 [26] G. Vieira and M. Eduardo Valle, “Extreme Learning Machines on Cayley-Dickson Algebra Applied for Color Image Auto-Encoding,” in 2020 International Joint Conference on Neural Networks (IJCNN).",
"IEEE, 7 2020, pp.",
"Available: https://ieeexplore.ieee.org/ document/9207495/ [27] I. L. Kantor and A. S. Solodovnikov, Hypercomplex Numbers: An Elementary Introduction to Algebras.",
"Springer New York, 1989.",
"[28] M. Kobayashi, “Hopﬁeld neural networks using Klein four-group,” Neurocomputing, vol.",
"123–128, 4 2020.",
"[29] G. Vieira and M. E. Valle, “A general framework for hypercomplex- valued extreme learning machines,” Journal of Computational Mathe- matics and Data Science, vol.",
"3, p. 100032, 6 2022.",
"Available: https://linkinghub.elsevier.com/retrieve/pii/S2772415822000062 [30] P. Renaud and P. R. Clifford, “CLIFFORD ALGEBRAS LECTURE NOTES ON APPLICATIONS IN PHYSICS,” ALGEBRAS LECTURE NOTES ON APPLICATIONS IN PHYSICS, 11 2020.",
"Available: https://hal.archives-ouvertes.fr/hal-03015551https: //hal.archives-ouvertes.fr/hal-03015551/document [31] Aurelien G´eron, Hands–On Machine Learning with Scikit–Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Sys- tems, 2nd ed.",
"Sebastopol, California, USA.",
": O Reilly, 10 2019.",
"[32] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J. F. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, and C. J. Pal, “Deep complex networks,” 5 2017.",
"Available: http: //github.com/ChihebTrabelsi/deep complex [33] C. Gaudet and A. Maida, “Deep Quaternion Networks,” arXiv, 12 2017.",
"Available: http://arxiv.org/abs/1712.04604 [34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol.",
"2278–2323, 1998.",
"[35] R. D. Labati, V. Piuri, and F. Scotti, “All-IDB: The acute lymphoblastic leukemia image database for image processing,” in Proceedings - International Conference on Image Processing, ICIP, 2011, pp.",
2045– 2048.,
"[36] H.-D. Cheng, X. H. Jiang, Y.",
"Sun, and J. Wang, “Color image segmen- tation: advances and prospects,” Pattern recognition, vol.",
"2259–2281, 2001.",
"CHANGING DATA SOURCES IN THE AGE OF MACHINE LEARNING FOR OFFICIAL STATISTICS PRESENTED AT UNECE MACHINE LEARNING FOR OFFICIAL STATISTICS WORKSHOP 2023 Cedric De Boom Statistics Flanders Belgium cedric.deboom@vlaanderen.be Michael Reusens Statistics Flanders Belgium michael.reusens@vlaanderen.be June 6, 2023 ABSTRACT Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data.",
"With such data science practices in place, it enables more timely, more insightful and more flexible reporting.",
"However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them.",
"In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.",
"This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics.",
"We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception.",
"Next, we highlight the repercussions of changing data sources on statistical reporting.",
"These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering.",
"We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring.",
"In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.",
"1 Introduction The field of statistics has long played a critical role in informing policy decisions, driving innovation, and advancing scientific knowledge.",
"Traditional statistical methods such as surveys and censuses have provided valuable insights into a wide range of topics, from population demographics to economic trends and public opinion.",
"However, in recent years, the increasing availability of open and large data sources has opened up new opportunities for statistical analysis.",
"In particular, the rise of machine learning has transformed the field of statistics, enabling the analysis of massive datasets, the identification of complex patterns and relationships, non-linear forecasting, etc.",
"Machine learning algorithms can be used to analyze data from a wide range of sources, providing insights that traditional survey methods may not capture.",
"The use of machine learning for official statistics has the potential to provide more timely, accurate and comprehensive insights into a wide range of societal topics [3].",
"By leveraging the vast amounts of data that are generated by individuals and entities on a daily basis, statistical agencies can gain a more nuanced understanding of trends and patterns, and respond more quickly to emerging issues.",
"However, this shift towards machine learning also presents a number of challenges.",
"In particular, there are concerns about data quality, privacy, and security, as well as the need for appropriate technical skills and infrastructure [4, 5], as arXiv:2306.04338v1 [stat.ML] 7 Jun 2023 well as challenges related to explainability, accuracy, reproducibility, timeliness, and cost effectiveness [6].",
"As statistical agencies grapple with these challenges, it is essential to ensure that the benefits of machine learning are balanced against the risks and that the resulting insights are both accurate and representative.",
"In this paper, we explore the changing data sources in the age of machine learning for official statistics, as we believe that this pervasive issue largely remains underexposed, as we will explain in Section 2.2.",
"In that respect, we highlight some of the key considerations for statistical agencies looking to incorporate machine learning into their workflows in Section 3, by zooming in on the causes and risks associated with using external data sources, the consequences on using such sources for statistical production, and, finally, a set of mitigations that should ideally be incorporated in any genuine deployment of machine learning for official statistics.",
"2 Machine learning for official statistics The data abundance in governmental, corporate, social and personal contexts, both online and offline, becomes a tantalizing source and opportunity for the improvement and expansion of official statistics.",
"For example, to inquire about the overall satisfaction with life of its citizens, a nation could organize periodic surveys.",
"But when this nation has access to its citizens’ social media posts, likes, reader’s letters, media consumption, ticket sales, (online) shopping carts, etc.",
it could use all of these data as a proxy to extract novel and innovative statistical insights [7].,
"Typically, the end result is either a new statistic, a statistic that complements an existing one, or an ersatz statistic that aims to replace one or more existing statistics.",
We will briefly zoom in on the different components of which such a novel statistic is generally comprised.,
"2.1 Machine learning To derive novel insights and innovative statistics from data, data scientists and statistical researchers often use a wide variety of powerful tools that are in the realm of machine learning1.",
In this paper we will not go into too much detail about machine learning.,
"However, the use of data sources together with machine learning models can cause unwanted effects regarding statistical production, see further in Section 3.",
"So, we deem it useful to provide a high-level overview of the typical process that involves machine learning for official statistics.",
Machine learning is a subdiscipline of artificial intelligence that enables machines to learn from data and improve their performance over time without being explicitly programmed.,
"This approach involves building algorithms that automatically learn from data to identify patterns, relationships, and structures that may be difficult or impossible for humans to discern.",
The typical process of designing a machine learning algorithm consists of two main phases: training and inference.,
"During the training phase, the parameters of a machine learning model are tuned to solve a specific task.",
"For this, a wide variety of data sources can be used, or even outputs from existing or pre-trained machine learning models.",
"After the model is trained, its parameters are kept fixed so that the model can be used to predict outcomes or identify patterns in new or previously unseen data.",
This process is called inference.,
It is important to keep in mind the distinction between training and inference.,
"After training, the model remains unchanged, and it remains unchanged until it is retrained again.",
"Later, in Section 3, we will focus on the disparities that this can cause w.r.t.",
"the inference phase and, in consequence, official statistics production.",
Supervised learning is one of the most common types of machine learning used for official statistics.,
"This method involves training a model on a labeled dataset, where each data point has a known outcome or target variable.",
"The model learns to associate features in the data with the target variable, enabling it to make predictions on new data with similar features.",
"In the context of official statistics, supervised learning can for example be used to predict the happiness of an individual based on their Twitter profile [8].",
"Unsupervised learning, on the other hand, is used when the target variable is unknown or the goal is to identify patterns or relationships within the data.",
"In this approach, the machine learning model learns to recognize similarities and differences among input data without explicit guidance from labeled data.",
"In the context of official statistics, unsupervised learning can for example be used to identify citizens, companies or events that are similar to each other on one or more aspects that could be hidden from plain sight [9].",
"Machine learning can be used to complement or even replace official statistics, and its ability to nowcast and forecast is an extremely valuable addition.",
"Modern machine learning models, tools and hardware can analyze vast amounts of data in real-time or near-real-time, providing more up-to-date and precise estimates of e.g.",
economic and social trends.,
"By 1Although originally (and technically) they imply different methods and techniques, the terms machine learning, data science, artificial intelligence, deep learning... are nowadays considered interchangeable.",
In this paper we consistently use the term machine learning to denote the scientific discipline concerned with learning the most optimal model parameters based on data.,
"It is a subdiscipline of artificial intelligence, while deep learning is a subdiscipline of machine learning.",
"Data science encompasses both machine learning as well as data preparation, analytics and visualization.",
"2 incorporating machine learning into official statistical production, one can benefit from the strengths of both approaches and make more informed decisions based on the most current and accurate data [10].",
2.2 External data sources Let’s focus on the data sources that will power such machine learning models.,
"Their nature, size, structure, frequency... can be vastly different, they must typically be gathered ‘in the wild’ and should often be combined with each other to extract meaningful insights.",
"Compared to more traditional data sources for official statistics, they may present unique and appealing characteristics such as: Broad-spectrum – Covers a wide variety of topics.",
Diversity – A large variety of sources to cover different perspectives.,
Availability – Lots of data is freely and easily accessible.,
"Size – Some datasets can be enormous, sometimes even complete.",
"Structure – Not only tabular data, but also images, video, text, audio, etc.",
Timeliness – (Near) up-to-date and real-time information.,
"Frequency – Raw data on various, even very fine-grained time scales.",
"Granularity – Raw data on various, even fine-grained levels of detail.",
Coverage – Various locations and regions can be filtered and covered.,
"On the other hand, before all this data is ready to be exerted for machine learning and official statistical production, a few challenges need to be overcome, such as: Data quality – Data may contain errors, biases, or missing values that need to be addressed to ensure accuracy and reliability.",
"Data interpretation – Understanding the context and meaning of data can be difficult, especially when dealing with unstructured data such as text or images.",
Data integration – Combining data from different sources with varying structures and formats can be challenging and time-consuming.,
"Selection bias – Proper randomization or compiling representative population samples can be challenging, and it greatly depends on the underlying data origins.",
"Operationalization bias – Reproducibility can be difficult as it depends on many implicit, hidden, and/or production- specific design choices [11, 12].",
Computational resources – Processing and analyzing large amounts of data may require significant computational resources.,
Privacy and security – Sensitive data may need to be protected and anonymized to ensure privacy and security.,
Data ethics – Data collection and use should adhere to ethical principles.,
Fairness and justness – The end solution should ideally be as neutral as possible and should not discriminate [13].,
"Cost – All of the above requires resources, budgets and a talented workforce.",
"In addition, the data source itself might need to be purchased.",
"In 2016, McKinsey reported that many companies have started to specialize in acquiring and selling data [14].",
"3 With the right tools, workforce, technological advances, mindset, and legislative support, these challenges can and should be manageable.",
"The most challenging piece of the puzzle, however – and one that is more than often ignored – is the lack of control you can exert over the data sources that are externally gathered.",
"As a national statistics agency, traditionally, survey data and administrative records that power official statistics are completely under your own control.",
"But once you start exploiting external data sources to power novel, innovative, complimentary or ersatz statistics, this lack of control of your data should never be ignored, and if possible, should be front and center on your agenda early on in the process.",
"As the popular saying goes: “With great power comes great responsibility” (from Spider-Man, 2002).",
Having control and power over your data is essential to fulfilling your responsibilities as a statistics agency.,
"However, in the world of data, the opposite is often true: with great amounts of external data comes great powerlessness.",
"Therefore, it is crucial to prioritize the issue of data control when incorporating external sources into official statistics.",
Taking the time to establish proper protocols and procedures for external data management can prevent a multitude of issues down the line and ensure that the data you rely on remain accurate and trustworthy.,
"This paper delves into the pervasive problem of powerlessness and lack of control, unraveling the multifaceted aspects, risks, and pitfalls that arise from utilizing external data sources for machine learning in official statistics.",
We will explore the concepts of ‘change’ and ‘consequence’ in their most expansive interpretations to comprehensively tackle this question.,
3 The challenge of changing data sources Relying heavily on external data sources for machine learning in official statistics comes with significant risks.,
Such a dependence can leave statistical agencies vulnerable since they have limited control over these sources.,
"This situation is similar to how our global economy, mobility, and prosperity were once highly dependent on the availability of oil.",
"Since the prices and availability of these precious resources are often beyond our control, countries can do nothing but endure price fluctuations and shortages.",
"Clive Humby proclaimed in 2006 that “data is the new oil”, given its powerful intrinsic value.",
"However, his statement keeps holding true in terms of vulnerability, powerlessness, and lack of control over external providers.",
"In the following paragraphs, we will delve into the various types and causes of data changes.",
We will then discuss the ramifications of changing data sources for machine learning in official statistics.,
"Finally, we will provide a list of best practices and tips, although it is important to remember that there is no free lunch: whenever we incorporate external data, we expose ourselves to the risk of future changes in these data sources.",
3.1 Types and Causes of Changing Data Sources 3.1.1 Data types and schemas A change in data types or schemas refers to modifications made to the data formats or the structure in which the data are stored and offered.,
"These types of changes may arise due to a need to accommodate future use cases or business requirements, to eliminate technical debt, or to improve data storage and retrieval efficiency.",
Even the most innocent changes – e.g.,
"integers becoming floats, data columns that are added or removed... – can break entire pipelines.",
In the most fortunate of cases the runtime environment will throw errors that reveal the cause of these data changes.,
"In other cases, however, the data changes remain undetected and secretly wreak havoc in the pipeline.",
"If the pipeline contains machine learning components, data type changes can e.g.",
induce feature mismatches – discrepancies between the feature distributions at train and inference time – that lead to unreliable predictions.,
"It is important to be vigilant about changes in data types or schemas, as even seemingly minor adjustments can have significant impacts further down the data pipeline.",
"To mitigate these risks, it is advisable to stay informed about data change announcements from providers and implement robust data checks during data ingestion, ranging from simple data (type) validation to full-blown automated feature analysis, outlier detection, etc.",
"Additionally, the deployment of effective monitoring systems can help catch machine learning failures quickly and prevent potentially costly errors.",
"3.1.2 Sharing and collection technology Data can be shared and collected using many different technologies, such as APIs, queues, network drives, external drives, e-mail... but also web scraping, online analytics tools, sensor networks... Changes in these technologies inevitably occur from time to time.",
"For example, API endpoints often need to be updated to improve functionality and performance.",
"Changes may be made to the API’s data structures or methods, to provide more efficient or comprehensive data access.",
"In addition, changes may be made to the API to address security vulnerabilities or to ensure compliance with new 4 regulations or standards.",
"Furthermore, changes in business requirements or strategy may also lead to changes in API endpoints.",
"For instance, a company may introduce new products or services, modify their existing offerings, or change their pricing.",
"A recent, telling example is the Twitter API.",
"In 2021, Twitter launched version 2 of its popular API that introduced many changes in endpoints, data fields, pricing... compared to version 1.1.",
"Twitter encouraged developers to migrate to this new API offering, but for many use cases such a migration would introduce breaking changes that, in their turn, would impact entire data processing pipelines, statistics production, etc.",
"For the time being, Twitter offered both version 1.1 and version 2 of their API in parallel, which caused many to bury their heads in the sand.",
The situation got even worse when Elon Musk acquired Twitter in 2022 and decided to suspend all existing API offerings.,
"Instead, in 2023 a new enterprise tier was introduced that put a price of more than 40 thousand USD per month on any reasonably effective use of the API.",
This caused great dissatisfaction in the development and research community and many initiatives were abandoned.,
"3.1.3 Concept drift Concept drift is related to changes in the data distribution between train and test time, which can have multiple causes [15, 16].",
"Changes in business logic can induce information shifts, for example, when categorical variables are expanded with additional categories or when the meaning of certain data fields is altered.",
"A particular pervasive issue is the calculation of derived data fields, especially when those calculations are not transparent or proprietary.",
"In the age of machine learning, you should always assume that derived data fields can be the result of a model prediction; when this model is updated without your knowing, the derived data fields will have a (slightly) different data distribution, which will cause issues in dependent machine learning models.",
"But even when data fields are not the result of a model’s prediction, it is important to periodically reevaluate and retrain models, since many sociological and economic processes are naturally prone to concept drift themselves.",
"3.1.4 Frequency and interruptions A change in data frequency refers to modifications made to the rate at which data is collected or updated, which can happen deliberately or randomly.",
Deliberate changes may arise due to shifts in business requirements or technological choices.,
"Random shifts are most often attributed to noisy factors such as network issues, component failures, down- time... or (human) errors.",
Such changes in frequency can impact machine learning components dramatically.,
"when periodical data is sampled every minute instead of every second, the data distribution changes on which the model was trained.",
"To mitigate these risks, data pipelines should be designed to monitor changes in incoming data frequency.",
"3.1.5 Ownership and discontinuation Worse than interruptions is downright discontinuation of the data source, which has immediate consequences on the future existence of the statistic.",
"Also, a change of ownership of the data source – e.g.",
"when acquired by another company – is not a fictional scenario, and it can trigger any of the risks that are discussed in this section.",
Building redundancy by diversifying data sources is a useful mitigating strategy to avoid single points of failures.,
"3.1.6 Legal properties Legal changes refer to modifications made to the legal landscape that governs the collection, storage, and use of data.",
"This type of change may arise due to new privacy laws, contractual obligations, or changes in the cost of data access or storage.",
"One cause of this type of change is the adoption of new regulations, such as GDPR, which require companies to comply with stricter rules for collecting and processing data.",
"Additionally, changes in the cost of data access or storage may require companies to modify their data sources or methods to reduce costs, which can have contractual consequences.",
"If possible, negotiate airtight SLAs with the data provider and make sure to attribute enough attention to future data changes.",
3.1.7 Ethics and public perception Ethical considerations and public perceptions can affect data collection methods and sources.,
"If certain data sources or variables are considered controversial or intrusive, there may be a shift towards alternative sources, which may require a refresh of the used machine learning models.",
It can also impact the way machine learning models are designed and trained.,
"If certain variables or factors are considered discriminatory or unethical, there may be a push towards eliminating or adjusting them to reduce algorithmic bias.",
Changes in ethics or public perceptions can also result in greater accountability and the need for transparency.,
"Stakeholders may demand more openness and clarity around the use of algorithms, data sources, and decision-making processes.",
"This can lead to greater scrutiny and oversight of 5 machine learning models, which may impact their performance if not adequately addressed, especially when black-box models need to be replaced by more interpretative variants [17, 18, 19].",
"Finally, public trust can be significantly affected.",
"If stakeholders perceive that machine learning is being used inappropriately or unethically, they may lose faith in the integrity and reliability of official statistics.",
This can have significant consequences for public policy and decision making.,
"3.2 Consequences of Changing Data Sources When data sources change, there will be consequences for official statistics production, especially if there are machine learning components involved.",
"We will broadly but briefly cover a variety of areas that can be impacted, some of which have already been mentioned above.",
"Concept drift – Concept drift means that the underlying patterns and relationships in the data may change over time, which can lead to model deterioration or loss of accuracy.",
"This issue can be particularly relevant when dealing with long-term trends, as changes in societal norms, technology, or other external factors can influence data over time [16].",
"Model staleness – When a model becomes outdated, it no longer reflects current trends or patterns in the data.",
This can occur if the machine learning model is not updated frequently enough to keep pace with changing data sources.,
"As a result, the model may not perform as well as it once did, leading to less accurate official statistics.",
"Bias and neutrality – Changing data sources can also introduce bias or incorrect data, which can impact the neutrality of the statistics produced, or which can lead to the phenomenon “garbage in, garbage out” [20, 21].",
"Since it is essential that official statistics remain neutral and objective, this will negatively impact the accuracy and validity of these statistics.",
"Availability – If data become unavailable (for a certain period in time or indefinitely) or are limited in scope, this may impact the ability to produce accurate and timely official statistics.",
Integration – A change in data sources can cause a domino effect when multiple statistics or models rely on this data source.,
"Especially be mindful when the output of machine learning models is used as input for other machine learning models, either directly or indirectly as part of a larger data pipeline.",
"Since the predictions of a machine learning model can become unreliable when the input data change, this prediction shift itself is a changing data source for other models.",
"Extra labor – The risk of changing data sources requires additional resources and labor to mitigate the effects of such changes, monitor the occurrence of changes, and ensure that the new data are properly integrated into existing machine learning models.",
"This has tremendous impacts on the costs and timeline of the produced statistics, and it may also require a significant team expansion.",
"Breaking changes or discontinuation – In some cases, changing data sources may cause the impossibility of producing a statistic any further.",
"If this is the case, it may be necessary to stop offering the statistic altogether or find alternative data sources that can produce accurate and reliable official statistics.",
"When alternative data sources are found, there will almost always be a mismatch with the original data source that has an impact on the resulting statistic, resulting in a breaking change.",
"In that case, it is important to overcome the mismatches as best as possible – e.g.",
"in terms of statistical properties – and, certainly, to be transparent about the breaking change, e.g.",
by indicating on a graph when exactly the data source was changed.,
"Quality metrics – Finally, changing data sources imply changes in timeliness, validity, accuracy, completeness, consistency and other quality metrics w.r.t.",
the produced official statistics [6].,
Ensuring that resulting statistic continues to meet these quality metrics remains critical.,
"3.3 Mitigating Changing Data Sources As has been illustrated above, changes in data sources can significantly impact the performance of a machine learning model.",
"The effects can be diverse, ranging from introducing biases in the data to producing incorrect results.",
"This can have serious implications, especially when the model is being used for official statistics, where accuracy and reliability are of paramount importance.",
"Therefore, it is essential to take measures to prevent and mitigate such changes.",
"This is not an easy task, as the consequences can be diverse, and the required efforts to mitigate them are often time-consuming and not straightforward.",
We do not claim to have definite answers.,
"However, we will propose several recommendations and best practices, including performing a risk analysis, monitoring, diversifying data sources, building technical robustness, using data normalization techniques, and incorporating data validation processes.",
6 Risk analysis – Performing a risk analysis before incorporating a new data source is an essential step in mitigating the impact of changes in data sources.,
"This analysis involves identifying the potential risks associated with the data source, which we have covered in Section 3.1.",
"The analysis should be comprehensive, considering both technical and non-technical aspects of the data source, and should ideally include potential solutions for the identified risks.",
This will often force you to face the hard truth and will lead you to decide that the candidate data sources are not adequate or reliable enough.,
"Trade-offs will nevertheless need to be considered, depending on the use case at hand.",
Monitoring – Monitoring everything that is relevant is another crucial step in mitigating the impact of changing data sources.,
"It involves tracking various aspects of the data sources, the machine learning models, and their outputs to detect and respond to changes promptly.",
Draft a list of variables and quantities that must be continuously tracked to ensure that the models remain reliable and accurate over time.,
"For this, inspiration can be drawn from the discussed topics in Section 3.1, but it will vary from use case to use case, as well as the nature of the models that have been used.",
"Supervised models, for example, can be tested against a reference test set or a historical reference model; if the accuracy, precision or recall starts to deviate significantly from this reference set, it should be flagged.",
"On the other hand, monitoring the performance of unsupervised models can be more challenging, because there is no clear performance measure that can be directly computed.",
One approach is to monitor the model’s ability to detect patterns and clusters in the data.,
"It is possible to use a reference test set or reference model for this, but the informative metrics – e.g.",
"cluster similarity, homogeneity, separation... – are more abstract and somewhat harder to interpret.",
"Another approach is to visualize projections of certain interesting data points in the learned latent spaces or preferably a reduction thereof, which greatly benefits interpretability but makes it harder to convert it into hard numbers.",
"As a suggestion, a good balance between interpretability and hard performance metrics is found when clusters are tested against pre-existing domain knowledge, e.g.",
by listing similar data points for given queries.,
Simply monitoring whether expected similarities emerge or not can provide powerful signals about model and data performance.,
Another effective approach is to create proxy supervised tasks that rely on the output of the unsupervised model.,
Monitoring the model’s performance on such proxy tasks can provide insights into the quality and usefulness of the unsupervised model’s output.,
"Diversification – Diversifying data sources is another important measure, but is easier said than done.",
One challenge of using multiple data sources is the potential for conflicts or inconsistencies between the sources.,
"Different data sources may have different formats, schemas, and levels of quality, which can create discrepancies and inconsistencies that must be resolved before the data can be used in the model.",
"Therefore, data normalization is key.",
"Additionally, integrating multiple data sources can be a complex and time-consuming process.",
"It can also create additional computational overhead, which may impact the model’s scalability and portability.",
"Finally, finding relevant and reliable data sources can be a challenging task, particularly for specialized or niche domains.",
It may require extensive research and communication with data providers to retrieve relevant data.,
"Again, this story is about economical, technical and practical trade-offs, and is of course highly use-case-dependent.",
Technical robustness – Building technical robustness is paramount and requires significant engineering efforts.,
"Building an automated, data-driven statistic that is resistant to changing data sources such as errors, outliers, outages, time-dependent variability, etc.",
ensures consistency in the statistical offering.,
"Using data normalization techniques and incorporating data validation into the pipeline are essential measures, but robust technical implementations also require thorough unit and integration testing, failover and deduplication, scalability solutions, security measures, etc.",
"Of course, this is an entire field of study on its own.",
"Legal robustness – Finally, we believe that agreeing on clear legal guidelines is the best mitigation strategy to counter the risk of changing data sources, for example, by closing formal data sharing agreements or SLAa with data providers.",
"Such agreements should specify the terms and conditions under which the data can be shared, as well as the legal responsibilities of each party.",
"In particular, the agreements should specify the legal consequences of non-compliance.",
4 Conclusion In this paper we have investigated the risks and consequences of changing data sources when using machine learning for official statistics.,
"The list is long and covers many different aspects, ranging from statistical issues and model inconsistencies to technical problems and ethical considerations.",
We have also looked at a few potential mitigation strategies.,
"However, we admit that these strategies do not provide all the adequate answers and might leave the reader unsatisfied or, worse still, beguiled, as the solutions require many additional resources and efforts.",
"As we have stressed a couple of times in this paper, this is a story of trade-offs.",
"Depending on the use case at hand, some trade-offs might be easier to handle than other ones.",
"However, in the context of official statistics, our advice is to not tread lightly on these matters and to minimize the risk of losing control over your data sources as much as possible.",
"This takes time, effort 7 and careful planning with a horizon of multiple years.",
"To end on a positive note, despite the challenges associated with changing data sources, machine learning offers many opportunities for official statistics.",
"By being aware of the risks and taking necessary precautions, statistical agencies can leverage these opportunities while maintaining the integrity and reliability of their data-driven products.",
We hope that our checklist of risks and mitigation strategies provides a useful starting point for statistical agencies and practitioners to ensure the robustness of their machine learning-based statistical reporting.,
References [1] Stuart J. Russell and Peter Norvig.,
Artificial Intelligence.,
"Pearson Education, 2009.",
"[2] Trevor Hastie, Jerome Friedman, and Robert Tisbshirani.",
The elements of Statistical Learning.,
"Springer, 2017.",
Machine learning for official statistics.,
"Technical report, UNECE, 2022.",
"[4] Hossein Hassani, Gilbert Saporta, and Emmanuel Sirimal Silva.",
"Data mining and official statistics: The past, the present and the future.",
"Big Data, 2(1):34–43, March 2014.",
[5] Marco Puts and Piet Daas.,
Machine learning from the perspective of official statistic.,
"The Survey Statistician, 84:12–17, July 2021.",
"[6] Wesley Yung, Siu-Ming Tam, Bart Buelens, Hugh Chipman, Florian Dumpert, Gabriele Ascari, Fabiana Rocci, Joep Burger, and InKyung Choi.",
A quality framework for statistical algorithms.,
"Statistical Journal of the IAOS, 38(1):291–308, March 2022.",
[7] Wesley Yung.,
The Evolution of Official Statistics in a Changing World.,
"Harvard Data Science Review, 3(4), oct 28 2021.",
"[8] Manon Reusens, Michael Reusens, Marc Callens, Bart Baesens, et al.",
Benchmark study for flemish twitter sentiment analysis.,
"Social Science Research Network, 2022.",
"[9] Annelien Crijns, Victor Vanhullebusch, Manon Reusens, Michael Reusens, and Bart Baesens.",
Topic modelling applied on innovation studies of flemish companies.,
"Journal of Business Analytics, pages 1–12, 2023.",
"[10] Sevgui Erman, Eric Rancourt, Yanick Beaucage, and Andre Loranger.",
The Use of Data Science in a National Statistical Office.,
"Harvard Data Science Review, 4(4), oct 27 2022.",
"[11] Matthias Haucke, Rink Hoekstra, and Don van Ravenzwaaij.",
When numbers fail: do researchers agree on operationalization of published research?,
"Royal Society Open Science, 8(9):191354, September 2021.",
[12] Nagireddy Neelakanteswar Reddy.,
Operationalization bias: A suboptimal research practice in psychology.,
December 2022.,
"[13] Matthias Kuppler, Christoph Kern, Ruben L. Bach, and Frauke Kreuter.",
From fair predictions to just decisions?,
conceptualizing algorithmic fairness and distributive justice in the context of data-driven decision-making.,
"Frontiers in Sociology, 2022.",
"[14] Nicolaus Henke, Jacques Bughin, Michael Chui, James Manyika, Tamim Saleh, Bill Wiseman, and Guru Sethupathy.",
The age of analytics: competing in a data-driven world.,
"Technical report, McKinsey & Company, 2016.",
"[15] Hanqing Hu, Mehmed Kantardzic, and Tegjyot S Sethi.",
No free lunch theorem for concept drift detection in streaming data classification: A review.,
"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(2):e1327, 2020.",
"[16] Firas Bayram, Bestoun S. Ahmed, and Andreas Kassler.",
From concept drift to model degradation: An overview on performance-aware drift detectors.,
"Knowledge-Based Systems, 245:108632, June 2022.",
"[17] Alicja Gosiewska, Anna Kozak, and Przemysław Biecek.",
Simpler is better: Lifting interpretability-performance trade-off via automated feature engineering.,
"Decision Support Systems, 150:113556, November 2021.",
[18] Cynthia Rudin.,
Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.,
"Nature Machine Intelligence, 1(5):206–215, May 2019.",
[19] Alex John London.,
Artificial intelligence and black-box medical decisions: Accuracy versus explainability.,
"Hastings Center Report, 49(1):15–21, January 2019.",
[20] Bertie Vidgen and Leon Derczynski.,
"Directions in abusive language training data, a systematic review: Garbage in, garbage out.",
"PLOS ONE, 15(12):e0243300, December 2020.",
"[21] R. Stuart Geiger, Dominique Cope, Jamie Ip, Marsha Lotosh, Aayush Shah, Jenny Weng, and Rebekah Tang.",
"“garbage in, garbage out” revisited: What do machine learning application papers report about human-labeled training data?",
"Quantitative Science Studies, 2(3):795–827, 2021.",
"DOME: Recommendations for supervised machine learning validation in biology Ian Walsh​1,*​, Dmytro Fishman​2,*​, Dario Garcia-Gasulla​3​, Tiina Titma​4​, Gianluca Pollastri​5​, The ELIXIR Machine Learning focus group​#​, Jennifer Harrow​6,+​, Fotis E. Psomopoulos​7,+​ & Silvio C.E.",
"Tosatto​8,+ 1​Bioprocessing Technology Institute, Agency for Science, Technology and Research, Singapore, ​2​Institute of Computer Science, University of Tartu, Estonia, ​3​Barcelona Supercomputing Center (BSC), Barcelona, Spain, 4​School of Information Technologies, Tallinn University of Technology, Estonia, ​5​School of Computer Science, University College Dublin, Ireland, ​6​ELIXIR HUB, South building, Wellcome Genome Campus, Hinxton, Cambridge, UK, ​7​Institute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece, ​8​Dept.",
"of Biomedical Sciences, University of Padua, Padua, Italy.",
*​contributed equally #​see list of co-authors at the end of the manuscript +​corresponding authors Abstract Modern biology frequently relies on machine learning to provide predictions and improve decision processes.,
There have been recent calls for more scrutiny on machine learning performance and possible limitations.,
Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology.,
"Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome.",
The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm.,
Answers to these questions can be easily included in the supplementary material of published papers.,
"Introduction With the steep decline in the cost of high-throughput technologies, large amounts of biological data are being generated and made accessible to researchers.",
"Machine learning (ML) has been brought into the spotlight as a very useful approach to understand cellular​1​, genomic​2​, proteomic​3​, post-translational​4​, metabolic​5 and drug discovery data​6 with the potential to result in ground-breaking medical applications​7,8​.",
"This is clearly reflected in the corresponding growth of ML publications (Figure 1), reporting a wide range of modelling techniques in biology.",
"While every novel ML method should be validated experimentally, this happens only in a fraction of the publications​9​.",
"This sharp increase in publications inherently requires a corresponding increase in the number and depth of peer-reviews to offer critical assessment​10​ and improve reproducibility​11,12​.",
"Guidelines or recommendations on how to appropriately construct ML algorithms can help to ensure correct results and predictions​13,14​.",
"In the biomedical research field, communities have defined standard guidelines and best practices for scientific data management​15 and reproducibility of computational tools​16,17​.",
"On the other hand, a demand exists in the ML community for a cohesive and combined set of 1 recommendations with respect to data, the optimization techniques, the final model, and evaluation protocols as a whole.",
​Exponential increase of ML publications in biology.,
"The number of ML publications per year is based on Web of Science from 1996 onwards using the “topic” category for “machine learning” in combination with each of the following terms: “biolog*”, “medicine”, “genom*”, “prote*”, “cell*”, “post translational”, “metabolic” and “clinical”.",
"Recently, a comment highlighted the need for standards in ML​18​, arguing for the adoption of on-submission checklists​10 as a first step towards improving publication standards.",
"Through a community-driven consensus, we propose a list of minimal requirements asked as questions to ML implementers (Box 1) that, if followed, will help to assess the quality and reliability of the proposed methods more faithfully.",
"We have focused on Data, Optimization, Model and Evaluation (DOME) as each component of an ML implementation usually falls within one of these four topics.",
"Importantly, no specific solutions are discussed, only recommendations (Table 1) and a checklist are provided (Box 1).",
"Our recommendations are made primarily for the case of supervised learning in biology in the absence of direct experimental validation, as this is the most common type of ML approach used.",
"We do not discuss how ML can be used in clinical applications​19,20​.",
"It also remains to be investigated if the DOME recommendations can be extended to other fields of ML, like unsupervised, semi-supervised and reinforcement learning.",
Development of the recommendations The recommendations outlined below were initially formulated through the ELIXIR ML focus group after the publication of a comment calling for the establishment of standards for ML in biology​18​.,
"ELIXIR, initially established in 2014, is now a mature intergovernmental European infrastructure for biological data and represents over 220 research organizations in 22 countries across many aspects of bioinformatics​21​.",
"Over 700 national experts participate in the development and operation of national services that contribute to data access, integration, training and analysis for the research community.",
Over 50 of these experts involved in the field of ML have established an ELIXIR ML focus group 2 (​https://elixir-europe.org/focus-groups/machine-learning​) which held a number of meetings to develop and refine the recommendations based on a broad consensus among them.,
"3 Broad topic Be on the lookout for Consequences Recommendation(s) Data - Data size & quality - Appropriate partitioning, dependence between train and test data.",
- Class imbalance - No access to data ●Data not representative of domain application.,
●Unreliable or biased performance evaluation.,
●Cannot check data credibility.,
Independence of optimization (training) and evaluation (testing) sets.,
"(​requirement) This is especially important for meta algorithms, where independence of multiple training sets must be shown to be independent of the evaluation (testing) sets.",
"Release data preferably using appropriate long-term repositories, including exact splits (​requirement​) Sufficient evidence of data size & distribution being representative of the domain​.",
"(​recommendation​) Optimization - Overfitting, underfitting and illegal parameter tuning - Imprecise parameters and protocols given.",
●Reported performance too optimistic or too pessimistic.,
●Models noise or miss relevant relationships.,
●Results are not reproducible.,
"Clear statement that evaluation sets were not used for feature selection, pre-processing steps or parameter tuning.",
(​requirement​) Reporting indicators on training and testing data that can aid in assessing the possibility of under/overfitting e.g.,
train vs. test error.,
"(​requirement) Release definitions of all algorithmic hyper-parameters, regularization protocols, parameters and optimization protocol.",
"(​requirement​) For neural networks, release definitions of train and learning curves.",
"(​recommendation​) Include explicit model validation techniques, such as N-fold Cross validation.",
"(​recommendation​) Model - Unclear if black box or interpretable model - No access to: resulting source code, trained models & data - Execution time is impractical ●An interpretable model shows no explainable behaviour ●Cannot cross compare methods, reproducibility, & check data credibility.",
●Model takes too much time to produce results Describe the choice of black box / interpretable model.,
If interpretable show examples of it doing so.,
(​requirement​).,
Release of: documented source code + models + executable + UI/webserver + software containers.,
(​recommendation​) Report execution time averaged across many repeats.,
If computationally tough compare to similar methods (​recommendation​) Evaluation - Performance measures inadequate - No comparisons to baselines or other methods ●Biased performance measures reported.,
●The method is falsely claimed as state-of-the-art.,
Compare with public methods & simple models (baselines).,
(​requirement​) Adoption of community validated measures and benchmark datasets for evaluation.,
(​requirement​) Comparison of related methods and alternatives on the same dataset.,
(​recommendation​) Table 1.,
"Supervised ML in Biology: concerns, the consequences they impart and recommendations/requirements (recommendations in ​italics​ and requirements in ​bold​).",
Key terms underlined.,
Box 1: Structuring a Materials and Methods Section for Supervised Machine Learning Here we suggest a list of questions that must be asked about each DOME section to ensure high quality of ML analysis.,
"● Data​: ​(this section is to be repeated separately for each dataset) ○ Provenance​: What is the source of the data (database, publication, direct experiment)?",
"If data is in classes, how many data points are available in each class e.g., total for the positive (N​pos​) and negative (N​neg​) cases?",
"If regression, how many real value points are there?",
Has the dataset been previously used by other papers and/or is it recognized by the community?,
○ Data splits​: How many data points are in the training and test sets?,
"Was a separate validation set used, and if yes, how large was it?",
Is the distribution of data types in the training and test sets different?,
Is the distribution of data types in both training and test sets plotted?,
○ Redundancy between data splits​: How were the sets split?,
Are the training and test sets independent?,
How was this enforced (e.g.,
redundancy reduction to less than X% pairwise identity)?,
How does the distribution compare to previously published ML datasets?,
"○ Availability of data​: Is the data, including the data splits used, released in a public forum?",
"If yes, where (e.g.",
"supporting material, URL) and how (license)?",
● Optimization​: ​(this section is to be repeated separately for each trained model) ○ Algorithm​: What is the ML algorithm class used?,
Is the ML algorithm new?,
"If yes, why is it not published in a ML journal, and why was it chosen over better known alternatives?",
○ Meta-predictions​: Does the model use data from other ML algorithms as input?,
"If yes, which ones?",
Is it completely clear that training data of initial predictors and meta-predictor is independent of test data for the meta-predictor?,
○ Data encoding​: How was the data encoded and pre-processed for the ML algorithm?,
○ Parameters​: How many parameters (​p)​ are used in the model?,
How was ​p​ selected?,
○ Features​: How many features (​f) are used as input?,
Was feature selection performed?,
"If yes, was it performed using the training set only?",
○ Fitting​: Is the number of parameters (​p) much larger than the number of training points and/or is the number of features (f) ​large (e.g.,
in classification is p>>(N​pos​+N​neg​) and/or f>100)?,
"If yes, how was over-fitting ruled out?",
"Conversely, if the number of training points seem very much larger than p and/or ​f is small (e.g.",
N​pos​+N​neg​>>p and/or f<5) how was under-fitting ruled out?,
○ Regularization​: were any over-fitting prevention techniques performed (e.g.,
early stopping using a validation set)?,
"If yes, which ones?",
4 - Highly variable performance.,
●Unpredictable performance in production.,
Evaluate performance on a final independent hold-out set.,
(​recommendation​) Confidence intervals/error intervals and statistical tests to gauge prediction robustness.,
"(​requirement​) ○ Availability of configuration​: Are the hyper-parameter configurations, optimization schedule, model files and optimization parameters reported available?",
"If yes, where (e.g.",
URL) and how (license)?,
● Model​: ​(this section is to be repeated separately for each trained model) ○ Interpretability​: Is the model black box or transparent?,
"If the model is transparent, can you give clear examples for this?",
○ Output: ​Is the model classification or regression?,
○ Execution time​: How much real-time does a single representative prediction require on a standard machine?,
seconds on a desktop PC or high-performance computing cluster) ○ Availability of software​: Is the source code released?,
"Is a method to run the algorithm such as executable, web server, virtual machine or container instance released?",
"If yes, where (e.g.",
URL) and how (license)?,
● Evaluation​: ○ Evaluation method​: How was the method evaluated?,
"cross-validation, independent dataset, novel experiments) ○ Performance measures​: Which performance metrics are reported?",
Is this set representative (e.g.,
compared to the literature)?,
○ Comparison​: Was a comparison to publicly available methods performed on benchmark datasets?,
Was a comparison to simpler baselines performed?,
○ Confidence​: Do the performance metrics have confidence intervals?,
Are the results statistically significant to claim that the method is superior to others and baselines?,
○ Availability of evaluation​: Are the raw evaluation files (e.g.,
"assignments for comparison and baselines, statistical code, confusion matrices) available?",
"If yes, where (e.g.",
URL) and how (license)?,
The above description is shown in table format in the Supplementary Material together with two fully worked-out examples.,
"Scope of the recommendations The recommendations cover four major aspects of supervised ML according to the “DOME” acronym: data, optimization, model and evaluation.",
The key points and rationale for each aspect of DOME is described below and summarized in Table 1.,
"More importantly, Box 1 gives an actionable checklist to implement ML methods, with the actual recommendations codified as questions.",
Data State-of-the-art ML models are often capable of memorizing all the variation in training data.,
Such models when evaluated on data that they were exposed to during training would create an illusion of mastering the task at hand.,
"However, when tested on an independent set of data (termed test or validation set), the performance would seem less impressive, suggesting low generalization power of the model.",
"In order to tackle this problem, initial data should be divided randomly into non-overlapping parts.",
The simplest approach is to have independent train and test sets (possibly a third validation set).,
"Alternatively, 5 the cross-validation or bootstrapping techniques which choose a new train/test split multiple times from the available data, is often considered a preferred solution​22​.",
Overlapping of train/test data splits are particularly troublesome to overcome in biology.,
"For example, in predictions on entire gene and protein sequences independence of train-test could be achieved by reducing the amount of homologs in the data​10,23​.",
"Modelling enhancer-promoter contacts require a different criterion, e.g., not sharing one endpoint​24​.",
Modeling protein domains might require the multi-domain sequence to be split into its constituent domains before homology reduction​25​.,
"In short, each area of biology has its own recommendations for handling overlapping data issues and previous literature is vital to put forward a strategy.",
"In Box 1, we propose a set of questions under the section ‘data splits’ that should help to evaluate potential overlap between train and test data.",
Reporting statistics on the data size and distribution of data types can help show if there is a good domain representation in all sets.,
"Simple plots and/or tables showing the number of classes (classification), histogram of real values binned (regression), the different types of biological molecules in the data are vital pieces of information for each set.",
"Further, in classification, including methods that address imbalanced classes​26,27 are also needed if the class frequencies show so.",
"Models trained on one dataset may not be successful in dealing with data coming from adjacent but not identical datasets, a phenomenon known as covariance shift.",
"The scale of this effect has been demonstrated in several recent publications, e.g.",
for prediction of disease risk from exome sequencing​28​.,
"Although up to now the covariance shift remains an open problem, several potential solutions have been proposed in the area of transfer learning​29​.",
"Moreover, the problem of training ML models that can generalize well on small training data, usually requires special models and algorithms​30​.",
"Lastly, it is important to make as much data available for the public as possible​12​.",
Having open access to the data used for experiments including precise data splits would ensure better reproducibility of published research and as a result will improve the overall quality of published ML papers.,
"If datasets are not readily available for example in public repositories, authors should be encouraged to find the most appropriate one, e.g.",
"ELIXIR Deposition databases or Zenodo, to guarantee the long-term availability of such data.",
"Optimization Optimization, also known as training, refers to the process of changing values that constitute the model (parameters and hyper-parameters), including pre-processing steps, in a way that maximizes the model’s ability to solve a given problem.",
A poor choice of optimization strategy may lead to issues such as over- or under-fitting​31​.,
"A model that has suffered severe over-fitting will show an excellent performance on training data, while performing poorly on unseen data, rendering it useless for real-life applications.",
"On the other side of the spectrum, under-fitting occurs when very simple models capable of ca​pturing only straightforward dependencies between features are applied to data of a more complex nature.",
Algorithms for feature selection​32 can be employed to reduce the chances of over-fitting.,
"However, feature selection and other pre-processing actions come with their own recommendations.",
"The ma​in one being to abstain from using non-training data for feature selection and pre-processing - a particularly hard issue to spot for meta-predictors, which may lead to an overestimation of performance.",
"Finally, the release of files showing the exact specification of the optimization protocol and the type of parameters/hyper-parameters are a vital characteristic of the final algorithm.",
"Lack of documentation, including limited accessibility to relevant records for the involved parameters, hyper-parameters and optimization protocol may further compound the understanding of the overall model performance.",
6 Figure 2.,
(Top) Classification metrics.,
"​For binary classification, true positives (tp), false positives (fp), false negatives (fn) and true negatives (tn) together form the confusion matrix.",
"As all classification measures can be calculated from combinations of these four basic values, the confusion matrix should be provided as a core metric.",
Several measures (shown as equations) and plots should be used to evaluate the ML methods.,
For descriptions on how to adapt these metrics to multi-class problems see ​35​.,
​(Bottom) Regression metrics.,
​ML regression attempts to produce predicted values (p) matching experimental values (e).,
Metrics (shown as equations) attempt to capture the difference in various ways.,
"Alternatively, a plot can provide a visual way to represent the differences.",
It is advisable to report all in any ML work.,
Model Equally important aspects related to ML models are their interpretability and reproducibility.,
Interpretable models can infer causal relationships from the data and can output logical reasoning for each of its predictions.,
They are especially relevant in areas of discovery such as drug design​6 and diagnostics​33​.,
"Conversely, black box models often give accurate predictions but do not provide insight into why they made the predictions in a way humans can understand.",
Both interpretable and black box models are discussed in more detail elsewhere​34​.,
"However, developing recommendations on the choice of black box or interpretability cannot be made as both have their merits given certain situations.",
"The main recommendation would be that there is a statement on the model type, i.e.",
"is it black box or interpretable (see Box 1), and if it is interpretable it should be a requirement to give clear examples of it doing so.",
Reproducibility is a key component to ensuring research outcomes can be further utilized and validated by the wider community.,
"Poor model reproducibility extends beyond the documentation and reporting of the involved parameters, hyper-parameters and optimization protocol.",
"Lacking access to the various components of a model (source code, model files, parameter configurations, executables), as well as having steep computational requirements to execute the trained models in the context of generating predictions based on new data, can both make reproducibility of the model either limited or impossible.",
Evaluation There are two types of evaluation scenarios in biological research.,
The first is the experimental validation of the predictions made by the ML model in the laboratory.,
This is highly desirable but beyond the possibilities of many ML studies.,
The second is a computational assessment of the model performance using established metrics.,
The following deals with the latter and there are a few possible related risks with computational evaluation.,
"Starting with the performance metrics, i.e.",
"the quantifiable indicators of a model's ability to solve the given task, there are dozens of metrics available​35 for assessing different ML classification and regression problems.",
"However, the plethora of options available, combined with the domain-specific expertise that might be required to select the appropriate metrics, can lead to the selection of inadequate performance measures.",
"Often, there are critical assessment communities advocating certain performance metrics for biological ML models, e.g.",
"CAFA​3 and CAGI​28​, and it is recommended a new algorithm should use ones from the literature and critical assessments.",
"In the absence of literature, the ones shown in Figure 2 could be a starting point.",
"Once performance metrics are decided, methods published in the same biological domain must be cross-compared using appropriate statistical tests (e.g.",
Student's t-test) and confidence intervals.,
"Additionally, to prevent the release of ML methods that appear sophisticated but perform no better than simpler algorithms, baselines should be compared to the ‘sophisticated’ method and proven to be statistically inferior (e.g.",
small vs. deep neural networks).,
Open areas and limitations of the proposed recommendations The field of Machine Learning in biology is vast.,
"As such, it would be a herculean task to identify cross-cutting recommendations that could be applicable to all branches of ML across all possible domains.",
"Therefore, this work focuses exclusively on supervised ML applications, excluding unsupervised ML, clinical applications, and individual implementations through pipelines to name a few.",
"Moreover, the primary goal of this work is to define requirements and best practices that can be of use in the reviewing process and creation of ML-related papers, while remaining agnostic as to the actual 8 underlying solutions.",
The intent is to trigger a discussion in the wider ML community leading to future work addressing possible solutions.,
Several key issues related to reproducibility (e.g.,
"data is not published, data splits are not reported and model source code with its final parameters and hyperparameters are not released) can be aided by a multitude of workflow systems that help to ensure and automate multi-step processes are completely reproducible by tracking model parameters and exact versions of the source code and libraries.",
Examples of commonly used workflows include Galaxy​36 and Nextflow​37 .,
"Another ​de facto standard practice in software engineering is using version control systems such as Github to create an online copy of the source code, which can also include parameters and documentation.",
Similar version control systems exist for datasets.,
"Public repositories can store experimental data on demand for significant amounts of time, enabling a long-term reproducibility of the experiment.",
Existing software engineering tools can be easily used to address many of the DOME recommendations.,
"Although having additional, more topic-specific recommendations in the future will undoubtedly be useful, in this work we aim to provide a first version that could be of general interest.",
"Adapting the DOME recommendations to address the unique aspects of specific topics and domains, would be a task of those particular (sub-)communities.",
"For example, having guidelines for data independence is tricky because each biological domain has its own set of guidelines for this.",
"Nonetheless, we believe it is relevant to at least have a recommendation that authors describe how they achieved data split independence.",
Discussions on the correct independence strategies are needed for all of biology.,
"​Given constructive consultation processes with ML communities, relying on our own experience, it is our belief that our manuscript can be useful as a first iteration of the recommendations for supervised ML in biology​.",
"This will have the additional benefit of kickstarting the community discussion with a coherent but rough set of goals, thus facilitating the overall engagement and involvement of key stakeholders.",
"For instance, topics to be addressed by (sub-)communities is how to adapt DOME to entire pipelines, unsupervised, semi-supervised, reinforcement, and other types of ML.",
"in unsupervised learning, the evaluation metrics shown in Figure 2 would not apply and a completely new set of definitions are needed.",
"Another considerable debate, as AI becomes more commonplace in society, is that ML algorithms differ in their ability to explain learned patterns back to humans.",
Humans naturally prefer actions or predictions to be made with reasons given.,
This is the black box vs. interpretability debate and we point those interested to excellent reviews as a starting point for thoughtful discussions​38–41​.,
"Finally, we address the governance structure by suggesting a community-managed governance model similar to the open-source initiatives​42​.",
Community managed governance has been used in initiatives such as MIAME​43 or the PSI-MI format​44​.,
"This sort of structure ensures continuous community consultation and improvement of the recommendations in collaboration with academic (CLAIRE, see: https://claire-ai.org/) and industrial networks (Pistoia Alliance, see: https://www.pistoiaalliance.org/).",
"More importantly, this can be applied in particular to ML (sub-)communities working with specific problems requiring more detailed guidelines, e.g.",
imaging or clinical applications.,
"We have set up a website (URL: https://www.dome-ml.org/) to provide a platform for governance and community involvement around the DOME recommendations, where news and upcoming events will be posted.",
"As the recommendations and minimal requirements evolve over time, a version history will be available on the website.",
The template methods section in human (e.g.,
"DOC) and machine readable format (YAML), as well as software for the automatic conversion of a YAML file into a human readable one are available from a dedicated GitHub repository (URL: https://github.com/MachineLearning-ELIXIR/dome-ml).",
"Conclusion 9 The objective of our recommendations is to increase the transparency and reproducibility of ML methods for the reader, the reviewer, the experimentalist, and the wider community.",
We recognize that these recommendations are not exhaustive and should be viewed as a consensus-based first iteration of a continuously evolving system of community self-review.,
One of the most pressing issues is to agree to a standardized data structure to describe the most relevant features of the ML methods being presented.,
"As a first step to address this issue, we recommend including an “ML summary table”, derived from Box 1, in future ML studies (see Supplementary Material).",
"We recommend including the following sentence in the methods section of all papers: “To increase the reproducibility of the machine learning method of this study, the machine learning summary table (Table X) is included in the supporting information as per consensus guidelines (with reference to this manuscript).” The development of a standardized approach for reporting ML methods has major advantages in increasing the quality of publishing ML methods.",
"First, the disparity in manuscripts of reporting key elements of the ML method can make reviewing and assessing the ML method challenging.",
"Second, certain key statistics and metrics that may affect the validity of the publication’s conclusions are sometimes not mentioned at all.",
"Third, there are unexplored opportunities associated with meta-analyses of ML datasets.",
Access to large sets of data can enhance both the comparison between methods and facilitate the development of better-performing methods while reducing unnecessary repetition of data generation.,
We believe that our recommendations to include a “machine learning summary table” and the availability of data as described above will greatly benefit the ML community and improve its standing with the intended users of these methods.,
"The ELIXIR Machine Learning focus group Emidio Capriotti (ORCID: 0000-0002-2323-0963) Department of Pharmacy and Biotechnology, University of Bologna, Bologna (Italy) Rita Casadio (ORCID: 0002-7462-7039) Biocomputing Group, University of Bologna, Italy; IBIOM-CNR,Italy Salvador Capella-Gutierrez (ORCID: 0000-0002-0309-604X) INB Coordination Unit, Life Science Department.",
"Barcelona Supercomputing Center (BSC), Barcelona, Spain Davide Cirillo (ORCID: 0000-0003-4982-4716) Life Science Department.",
"Barcelona Supercomputing Center (BSC), Barcelona, Spain Alexandros C. Dimopoulos (ORCID: 0000-0002-4602-2040) Institute for Fundamental Biomedical Science, Biomedical Sciences Research Center ""Alexander Fleming"", Athens, Greece Victoria Dominguez Del Angel (ORCID: 0000-0002-5514-6651) Centre National de Recherche Scientifique, University Paris-Saclay, IFB, France Joaquin Dopazo (ORCID: 0000-0003-3318-120X) Clinical Bioinformatics Area, Fundación Progreso y Salud, Sevilla, Spain Piero Fariselli (ORCID: 0000-0003-1811-4762) 10 Department of Medical Sciences, University of Turin, Turin, Italy José Mª Fernández (ORCID: 0000-0002-4806-5140) INB Coordination Unit, Life Sciences Department, Barcelona Supercomputing Center (BSC), Barcelona, Spain Dmytro Fishman (ORCID: 0000-0002-4644-8893) Institute of Computer Science, University of Tartu, Estonia Dario Garcia-Gasulla (ORCID: 0000-0001-6732-5641) Barcelona Supercomputing Center (BSC), Barcelona, Spain Jen Harrow (ORCID:0000-0003-0338-3070) ELIXIR HUB, South building, Wellcome Genome Campus, Hinxton, Cambridge, UK.",
"Florian Huber (ORCID: 0000-0002-3535-9406) Netherlands eScience Center, Amsterdam, the Netherlands.",
"Anna Kreshuk (ORCID:0000-0003-1334-6388) EMBL Heidelberg Tom Lenaerts (ORCID: 0000-0003-3645-1455) Machine Learning Group, Université Libre de Bruxelles, Artificial Intelligence Lab, Vrije Universiteit Brussel and Interuniversity Institute of Bioinformatics in Brussels,Brussels, Belgium.",
"Pier Luigi Martelli (ORCID: 0000-0002-0274-5669) Biocomputing Group, University of Bologna, Italy Arcadi Navarro (ORCID: 0000-0003-2162-8246) Institute of Evolutionary Biology (Department of Experimental and Health Sciences, CSIC-Universitat Pompeu Fabra), Barcelona, Spain Catalan Institution of Research and Advanced Studies (ICREA), Barcelona, Spain CRG, Centre for Genomic Regulation, Barcelona Institute of Science and Technology (BIST), Barcelona, Spain Marco Necci (ORCID: 0000-0001-9377-482X) Dept.",
"of Biomedical Sciences, University of Padua, Padua, Italy.",
"Pilib Ó Broin (ORCID: 0000-0002-6702-8564) School of Mathematics, Statistics & Applied Mathematics, National University of Ireland Galway, Ireland Janet Piñero (ORCID: 0000-0003-1244-7654) Research Programme on Biomedical Informatics (GRIB), Hospital del Mar Medical Research Institute (IMIM), Department of Experimental and Health Sciences, Pompeu Fabra University (UPF), Barcelona, Spain Damiano Piovesan (ORCID: 0000-0001-8210-2390) Dept.",
"of Biomedical Sciences, University of Padua, Padua, Italy.",
"Gianluca Pollastri (ORCID: 0000-0002-5825-4949) School of Computer Science, University College Dublin, Ireland 11 Fotis E. Psomopoulos (ORCID: 0000-0002-0222-4273) Institute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece Martin Reczko (ORCID: 0000-0002-0005-8718) Institute for Fundamental Biomedical Science, Biomedical Sciences Research Center ""Alexander Fleming"", Athens, Greece Francesco Ronzano (ORCID: 0000-0001-5037-9061) Research Programme on Biomedical Informatics (GRIB), Hospital del Mar Medical Research Institute (IMIM), Department of Experimental and Health Sciences, Pompeu Fabra University (UPF), Barcelona, Spain Venkata Satagopam (ORCID: 0000-0002-6532-5880) Luxembourg Centre For Systems Biomedicine (LCSB), University of Luxembourg and ELIXIR-Luxembourg Node Castrense Savojardo (ORCID: 0000-0002-7359-0633) Biocomputing Group, University of Bologna, Italy Vojtech Spiwok (ORCID: 0000-0001-8108-2033) Department of Biochemistry and Microbiology, University of Chemistry and Technology, Prague, ELIXIR-Czech Republic Marco Antonio Tangaro (ORCID: 0000-0003-3923-2266) Institute of Biomembranes, Bioenergetics and Molecular Biotechnologies, National Research Council (CNR), Bari, Italy Giacomo Tartari Institute of Biomembranes, Bioenergetics and Molecular Biotechnologies, National Research Council (CNR), Bari, Italy David Salgado (ORCID: 0000-0002-5905-3591) Aix Marseille Univ, INSERM, MMG UMR1251, 13005 Marseille, France.",
"Tiina Titma (ORCID: 0000-0002-4935-8914) School of Information Technologies, Tallinn University of Technology, Estonia Silvio C. E. Tosatto (ORCID: 0000-0003-4525-7793) Dept.",
"of Biomedical Sciences, University of Padua, Padua, Italy.",
"Alfonso Valencia (ORCID:0000-0002-8937-6789) Catalan Institution of Research and Advanced Studies (ICREA), Barcelona, Spain Life Science Department.",
"Barcelona Supercomputing Center (BSC), Barcelona, Spain Ian Walsh (ORCID: ​0000-0003-3994-5522​) Bioprocessing Technology Institute, Agency for Science, Technology and Research, Singapore Federico Zambelli (ORCID: 0000-0003-3487-4331) Dept.",
"of Biosciences, University of Milan, Milan, Italy 12 Author contributions IW, DF, JH, FP and SCET guided the development, writing and final edits.",
All members of the ELIXIR machine learning focus group contributed to the discussions leading to the recommendations and writing of the manuscript.,
Competing interests The authors declare no competing interests.,
"Acknowledgements The work of the ML focus group was funded by ELIXIR, the Research infrastructure for life-science data.",
IW was funded by Core Budget of Singapore Agency for Science Technology and Research (A*STAR).,
References 1.,
"Baron, C. S. ​et al.​ Cell Type Purification by Single-Cell Transcriptome-Trained Sorting.",
"​Cell ​179​, 527-542.e19 (2019).",
"Libbrecht, M. W. & Noble, W. S. Machine learning applications in genetics and genomics.",
"​16​, 321–332 (2015).",
"Radivojac, P. ​et al.​ A large-scale evaluation of computational protein function prediction.",
"Methods ​10​, 221–227 (2013).",
"Franciosa, G., Martinez-Val, A.",
"& Olsen, J. V. Deciphering the human phosphoproteome.",
Biotechnol.​ 1–2 (2020) doi:10.1038/s41587-020-0441-3.,
"Yang, J. H. ​et al.​ A White-Box Machine Learning Approach for Revealing Antibiotic Mechanisms of Action.",
"​Cell ​177​, 1649-1661.e9 (2019).",
"Vamathevan, J.",
​et al.​ Applications of machine learning in drug discovery and development.,
Drug Discov.,
"​18​, 463–477 (2019).",
"Rajkomar, A., Dean, J.",
"& Kohane, I.",
Machine Learning in Medicine.,
"​380​, 1347–1358 (2019).",
Ascent of machine learning in medicine.,
"​18​, 407 (2019).",
"Littmann, M. ​et al.​ Validity of machine learning in biology and medicine increased through collaborations across fields of expertise.",
"​2​, 18–24 (2020).",
"Walsh, I., Pollastri, G. & Tosatto, S. C. E. Correct machine learning on protein sequences: a 13 peer-reviewing perspective.",
"​17​, 831–840 (2016).",
"Bishop, D. Rein in the four horsemen of irreproducibility.",
"​Nature ​568​, 435 (2019).",
"Hutson, M. Artificial intelligence faces reproducibility crisis.",
"​Science ​359​, 725–726 (2018).",
"Schwartz, D. Prediction of lysine post-translational modifications using bioinformatic tools.",
​Essays Biochem.,
"​52​, 165–177 (2012).",
"Piovesan, D. ​et al.​ Assessing predictors for new post translational modification sites: a case study on hydroxylation.",
​bioRxiv​ (2020).,
"Wilkinson, M. D. ​et al.​ The FAIR Guiding Principles for scientific data management and stewardship.",
"Data ​3​, 160018 (2016).",
"Sandve, G. K., Nekrutenko, A., Taylor, J.",
"& Hovig, E. Ten Simple Rules for Reproducible Computational Research.",
​PLOS Comput.,
"​9​, e1003285 (2013).",
"Grüning, B.",
​et al.​ Practical Computational Reproducibility in the Life Sciences.,
​Cell Syst.,
"​6​, 631–635 (2018).",
"Jones, D. T. Setting the standards for machine learning in biology.",
"​20​, 659–660 (2019).",
"Norgeot, B.",
​et al.​ Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist.,
"​26​, 1320–1324 (2020).",
"Luo, W. ​et al.​ Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View.",
Internet Res.,
"​18​, e323 (2016).",
ELIXIR: Providing a Sustainable Infrastructure for Federated Access to Life Science Data at European Scale.,
​submitted​.,
"Kohavi, R. A study of cross-validation and bootstrap for accuracy estimation and model selection.",
"14 1137–1145 (Montreal, Canada, 1995).",
"Hobohm, U., Scharf, M., Schneider, R. & Sander, C. Selection of representative protein data sets.",
Protein Sci.,
"​1​, 409–417 (1992).",
"Xi, W. & Beer, M. A.",
Local epigenomic state cannot discriminate interacting and non-interacting enhancer-promoter pairs with high accuracy.,
​PLoS Comput.,
"​14​, e1006625 (2018).",
"Zhou, X., Hu, J., Zhang, C., Zhang, G. & Zhang, Y.",
Assembling multidomain protein structures 14 through analogous global structural alignments.,
"​116​, 15930–15938 (2019).",
"Chawla, N. V., Bowyer, K. W., Hall, L. O.",
"& Kegelmeyer, W. P. SMOTE: synthetic minority over-sampling technique.",
"​16​, 321–357 (2002).",
"He, H., Bai, Y., Garcia, E. A.",
"& Li, S. ADASYN: Adaptive synthetic sampling approach for imbalanced learning.",
"in 1322–1328 (IEEE, 2008).",
"Daneshjou, R. ​et al.​ Working toward precision medicine: Predicting phenotypes from exomes in the Critical Assessment of Genome Interpretation (CAGI) challenges.",
"​38​, 1182–1192 (2017).",
A Survey on Transfer Learning.,
​IEEE Trans.,
"​22​, 1345–1359 (2010).",
"Vinyals, O., Blundell, C., Lillicrap, T. & Wierstra, D. Matching networks for one shot learning.",
in 3630–3638 (2016).,
"Mehta, P. ​et al.​ A high-bias, low-variance introduction to machine learning for physicists.",
Rep. (2019).,
"& Elisseeff, A.",
An introduction to variable and feature selection.,
"​3​, 1157–1182 (2003).",
​et al.​ The practical implementation of artificial intelligence technologies in medicine.,
"25​, 30–36 (2019).",
"Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.",
"​1​, 206–215 (2019).",
"Baldi, P., Brunak, S., Chauvin, Y., Andersen, C. A.",
"& Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview.",
​Bioinforma.,
"​16​, 412–424 (2000).",
"Goecks, J., Nekrutenko, A., Taylor, J., & Galaxy Team.",
"Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences.",
Genome Biol.,
"​11​, R86 (2010).",
"Di Tommaso, P. ​et al.​ Nextflow enables reproducible computational workflows.",
Biotechnol.,
"​35​, 316–319 (2017).",
"Arrieta, A.",
"​et al.​ Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and 15 challenges toward responsible AI.",
"Fusion ​58​, 82–115 (2020).",
"Guidotti, R. ​et al.​ A survey of methods for explaining black box models.",
​ACM Comput.,
"CSUR 51​, 1–42 (2018).",
"& Berrada, M. Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI).",
"​IEEE Access ​6​, 52138–52160 (2018).",
"Holm, E. A.",
In defense of the black box.,
"​Science ​364​, 26–27 (2019).",
"O’Mahony, S. The governance of open source initiatives: what does it mean to be community managed?",
"​11​, 139–150 (2007).",
​et al.​ Minimum information about a microarray experiment (MIAME)—toward standards for microarray data.,
"​29​, 365–371 (2001).",
"Hermjakob, H. ​et al.​ The HUPO PSI’s Molecular Interaction format—a community standard for the representation of protein interaction data.",
Biotechnol.,
"​22​, 177–183 (2004).",
"16 Supplementary Material Machine learning summary table: 17 DOME Version 1.0 Data Provenance Source of data, data points (positive, N​pos​ / negative, N​neg​).",
Used by previous papers and/or community.,
"Dataset splits Size of N​pos​ and N​neg​ of training set, validation set (if present), test set.",
Distribution of N​pos​ and N​neg​ across sets.,
(section to be repeated for each dataset) Redundancy between data splits Independence between sets.,
Strategy used to make examples representative (e.g.,
eliminating data points more similar than X%).,
Comparison relative to other datasets.,
Availability of data Yes/no for datasets.,
"If yes: Supporting information, website URL, license.",
Optimization Algorithm ML class (e.g.,
"neural network, random forest, SVM).",
"If novel approach, reason is it not previously published.",
(section to be repeated for each trained model) Meta-predictions Yes/No.,
If yes: how other methods are used and whether the datasets are clearly independent.,
Data encoding How input data is transformed (e.g.,
"global features, sliding window on sequence).",
"Parameters Number of ML model parameters (p), e.g.",
tunable weights in neural networks.,
"Protocol used to select p. Features Number of ML input features (f), i.e.",
encoding of data points.,
"In case of feature selection: Protocol used, indicating whether it was performed on training data only.",
"Fitting Justification for excluding over- (if p >> N​pos, train​+N​neg, train​ or f > 100) and under-fitting (if p << N​pos, train​+N​neg, train​ or f < 5).",
Regularization Yes/no for overfitting prevention.,
If yes: specify details and parameters of technique used.,
Availability of configuration Yes/no for hyper-parameter configuration and model files.,
"If yes: Supporting information, website URL, license.",
Model Interpretability Black box or transparent.,
"If transparent, provide examples.",
(section to be repeated for each trained model) Output Specify whether the model produces a classification (e.g.,
binary predictions) or regression (e.g.,
probability score).,
Execution time CPU time of single representative execution on standard hardware (e.g.,
seconds on desktop PC).,
Availability of software Source code repository (e.g.,
"GitHub), software container, website URL, license.",
"Example tables for author reference: The following is an example for a primary ML summary table built from (Walsh et al., Bioinformatics 2012).",
"18 Evaluation Evaluation method Cross-validation, independent dataset or novel experiments.",
"Performance measures Accuracy, sensitivity, specificity, etc.",
"Comparison Name of other methods and, if available, definition of baselines compared to.",
Justification of representativeness.,
Confidence Confidence intervals and statistical significance.,
Justification for claiming performance differences.,
Availability of evaluation Yes/no for raw evaluation files (e.g.,
"assignments for comparison and baselines, confusion matrices).",
"If yes: Supporting information, website URL, license.",
DOME Version 1.0 Data: ​X-ray disorder Provenance Protein Data Bank (PDB) X-ray structures until May 2008 (training) and from May 2008 until September 2010 (test).,
"3,813 proteins total.",
"N​pos​ = 44,433 residues.",
"N​neg​ = 710,207 residues.",
Not previously used.,
"Dataset splits N​pos,train​ = 37,495.",
"N​neg,train​ = 622,625.",
"N​pos,test​ = 6,938.",
"N​neg,test​ = 87,582 residues.",
No separate validation set.,
5.68% positives on training set.,
7.34% positives on test set.,
Redundancy between data splits Maximum pairwise identity within and between training and test set is 25% enforced with UniqueProt tool.,
"Availability of data Yes, URL: http://protein.bio.unipd.it/espritz/.",
Free use license.,
"Data: DisProt disorder Provenance DisProt version 3.7 (January 2008) for training set, DisProt version 5.7 for test set.",
536 proteins total.,
"N​pos​ = 63,841 residues.",
"N​neg​ = 164,682 residues.",
Not previously used.,
"Dataset splits N​pos,train​ = 56,414.",
"N​neg,train​ = 163,010.",
"N​pos,test​ = 7,427.",
"N​neg,test​ = 1,672 residues.",
No separate validation set.,
25.71% positives on training set.,
41.04% positives on test set.,
Redundancy between data splits Maximum pairwise identity within and between training and test set is 40% enforced with UniqueProt tool.,
Less stringent threshold used to maintain adequate dataset size.,
"Availability of data Yes, URL: http://protein.bio.unipd.it/espritz/.",
Free use license.,
19 Data: ​NMR disorder Provenance Protein Data Bank (PDB) NMR structures until May 2008 (training) and from May 2008 until September 2010 (test) analyzed using the Mobi software.,
"2,858 proteins total.",
"N​pos​ = 40,368 residues.",
"N​neg​ = 192,170 residues.",
Not previously used.,
"Dataset splits N​pos,train​ = 29,263.",
"N​neg,train​ = 143,891.",
"N​pos,test​ = 11,105.",
"N​neg,test​ = 48,279 residues.",
No separate validation set.,
16.9% positives on training set.,
18.7% positives on test set.,
Redundancy between data splits Maximum pairwise identity within and between training and test set is 25% enforced with UniqueProt tool.,
"Availability of data Yes, URL: http://protein.bio.unipd.it/espritz/.",
Free use license.,
Optimization Algorithm BRNN (Bi-directional recurrent neural network) with ensemble averaging.,
Meta-predictions No.,
Data encoding Sliding window of length 23 residues on input sequence with “one hot” encoding (i.e.,
20 inputs per residue).,
"Parameters ESpritz p = 4,647 to 5,886 depending on model used.",
No optimization.,
Features ESpritz f = 460 for sliding window of 23 residues with 20 inputs per residue.,
No feature selection.,
"Fitting The number of training examples is between 30 and 100 times p, suggesting neither over- nor under-fitting.",
Regularization No.,
The training data is 30-100 times the number of model parameters.,
Availability of configuration No.,
"Model Interpretability Black box, as correlation between input and output is masked.",
No attempt was made to make the model transparent.,
"Output Regression, i.e.",
probability of residues being disordered.,
Execution time ESpritzS ca.,
"1 sec / protein, ESpritzP ca.",
"1,500 sec / protein on a single Intel Xeon core.",
"Availability of software Web server, URL: http://protein.bio.unipd.it/espritz/ Linux executable, URL: http://protein.bio.unipd.it/download/.",
Bespoke license free for academic use.,
Evaluation Evaluation method Independent datasets.,
"Performance measures Accuracy, sensitivity, specificity, selectivity, F-measure, MCC, AUC are standard.",
S​w​ = Sens + Spec -1.,
"Comparison Disopred, MULTICOM, DisEMBL, IUpred, PONDR-FIT, Spritz, CSpritz.",
Wide range of popular predictors used for comparison.,
"Confidence Bootstrapping was used to estimate statistical significance as in CASP-8 (Noivirt-Brik et al, Proteins 2009).",
"80% of target proteins were randomly selected 1000 times, and the standard error of the scores was The following is an example for meta-predictor ML summary built from (Necci et al., Bioinformatics 2017).",
20 calculated (i.e.,
1.96 × standard_error gives 95% confidence around mean for normal distributions).,
Availability of evaluation No.,
DOME Version 1.0 Data Provenance Protein Data Bank (PDB).,
X-ray structures missing residues.,
"N​pos​ = 339,603 residues.",
"N​neg​ = 6,168,717 residues.",
"Previously used in (Walsh et al., Bioinformatics 2015) as an independent benchmark set.",
"Dataset splits training set: N/A N​pos,test​ = 339,603 residues.",
"N​neg,test​ = 6,168,717 residues.",
No validation set.,
5.22% positives on the test set.,
Redundancy between data splits Not applicable.,
"Availability of data Yes, URL: http://protein.bio.unipd.it/mobidblite/.",
Free use license.,
Optimization Algorithm Majority-based consensus classification based on 8 primary ML methods and post-processing.,
"Meta-predictions Yes, predictor output is a binary prediction computed from the consensus of other methods; Independence of training sets of other methods with test set of meta-predictor was not tested since datasets from other methods were not available.",
Data encoding Label-wise average of 8 binary predictions.,
"Parameters p = 3 (Consensus score threshold, expansion-erosion window, length threshold).",
No optimization.,
Features Not applicable.,
Fitting Single input ML methods are used with default parameters.,
Optimization is a simple majority.,
Regularization No.,
Availability of configuration Not applicable.,
"Model Interpretability Transparent, in so far as meta-prediction is concerned.",
Consensus and post processing over other methods predictions (which are mostly balck boxes).,
No attempt was made to make the meta-prediction a black box.,
"Output Classification, i.e.",
residues thought to be disordered.,
21 Execution time ca.,
1 second per representative on a desktop PC.,
"Availability of software Yes, URL: http://protein.bio.unipd.it/mobidblite/.",
Bespoke license free for academic use.,
"Evaluation Evaluation method Independent dataset Performance measures Balanced Accuracy, Precision, Sensitivity, Specificity, F1, MCC.",
"Comparison DisEmbl-465, DisEmbl-HL, ESpritz Disprot, ESpritz NMR, ESpritz Xray, Globplot, IUPred long, IUPred short, VSL2b.",
Chosen methods are the methods from which the meta prediction is obtained.,
Confidence Not calculated.,
Availability of evaluation No.,
"Learn to Accumulate Evidence from All Training Samples: Theory and Practice Deep Pandey 1 Qi Yu 1 Abstract Evidential deep learning, built upon belief the- ory and subjective logic, offers a principled and computationally efficient way to turn a determin- istic neural network uncertainty-aware.",
The resul- tant evidential models can quantify fine-grained uncertainty using the learned evidence.,
"To en- sure theoretically sound evidential models, the ev- idence needs to be non-negative, which requires special activation functions for model training and inference.",
"This constraint often leads to infe- rior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets.",
"To unveil the real cause of this undesired behavior, we theoreti- cally investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation func- tions create zero evidence regions, which prevent the model to learn from training samples falling into such regions.",
A deeper analysis of eviden- tial activation functions based on our theoretical underpinning inspires the design of a novel regu- larizer that effectively alleviates this fundamental limitation.,
Extensive experiments over many chal- lenging real-world datasets and settings confirm our theoretical findings and demonstrate the effec- tiveness of our proposed approach.,
"Introduction Deep Learning (DL) models have found great success in many real-world applications such as speech recognition (Kamath et al., 2019), machine translation (Singh et al., 2017), and computer vision (Voulodimos et al., 2018).",
"How- ever, these highly expressive models may easily fit the noise in the training data, which leads to overconfident predictions (Nguyen et al., 2015).",
"The challenge is further compounded when learning from limited labeled data, which is common 1Rochester Institute of Technology.",
Correspondence to: Qi Yu <qi.yu@rit.edu>.,
"Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA.",
"PMLR 202, 2023.",
Copyright 2023 by the author(s).,
"for applications from specialized domain (e.g., medicine, public safety, and military operations) where data collec- tion and annotation is highly costly.",
Accurate uncertainty quantification is essential for successful application of DL models in these domains.,
"To this end, DL models have been augmented to become uncertainty-aware (Gal & Ghahra- mani, 2016; Blundell et al., 2015; Pearce et al., 2020).",
"How- ever, commonly used extensions require expensive sampling operations (Gal & Ghahramani, 2016; Blundell et al., 2015), which significantly increase the computational costs (Lak- shminarayanan et al., 2017).",
"The recently developed evidential models bring together evidential theory (Shafer, 1976; Jøsang, 2016) and deep neural architectures that turn a deterministic neural network uncertainty-aware.",
"By leveraging the learned evidence, evi- dential models are capable of quantifying fine-grained un- certainty that helps to identify the sources of ‘unknowns’.",
"Furthermore, since only lightweight modifications are intro- duced to existing DL architectures, additional computational costs remain minimum.",
"Such evidential models have been successfully extended to classification (Sensoy et al., 2018), regression (Amini et al., 2020), meta-learning (Pandey & Yu, 2022a), and open-set recognition (Bao et al., 2021) settings.",
"Cifar100 Result Despite the attractive uncertainty quantifica- tion capacity, eviden- tial models are only able to achieve a pre- dictive performance on par with standard deep architectures in rela- tively simple learning problems.",
They suffer from a sig- nificant performance drop when facing large datasets with more complex features even in the common classification setting.,
"As shown in Figure 1, an evidential model using ReLU activation and an evidential MSE loss (Sensoy et al., 2018) only achieves 36% test accuracy on Cifar100, which is almost 40% lower than a standard model trained using softmax.",
"Additionally, most evidential models can easily break down with minor architecture changes and/or have a much stronger dependency on hyperparameter tuning to achieve reasonable predictive performance.",
The experiment section provides more details on these failure cases.,
1 arXiv:2306.11113v2 [cs.LG] 24 Jun 2023 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Figure 2.,
Visualization of zero-evidence region for evidential mod- els with ReLU activation in a binary classification setting.,
Existing models fail to learn from samples that are mapped to such zero- evidence region (shared area at the bottom left quadrant).,
"To train uncertainty-aware evidential models that can also predict well, we perform a novel theoretical analysis with a focus on the standard classification setting to unveil the underlying cause of the performance gap.",
Our theoreti- cal results show that existing evidential models learn sub- optimally compared to corresponding softmax counterparts.,
Such sub-optimal training is mainly attributed to the inher- ent learning deficiency of evidential models that prevents them from learning across all training samples.,
"More specif- ically, they are incapable to acquire new knowledge from training samples mapped to “zero-evidence regions” in the evidence space, where the predicted evidence reduces to zero.",
The sub-optimal learning phenomenon is illustrated in Figure 2 (detailed discussion is presented in Section 4.2).,
We analyze different variants of evidential models present in the existing literature and observe this limitation across all the models and settings.,
Our theoretical results inspire the design of a novel Regularized Evidential model (RED) that includes positive evidence regularization in its train- ing objective to battle the learning deficiency.,
"Our major contributions can be summarized as follows: • We identify a fundamental limitation of evidential models, i.e., lack the capability to learn from any data samples that lie in the “zero-evidence” region in the evidence space.",
• We theoretically show the superiority of evidential models with exp activation over other activation functions.,
• We conduct novel evidence regularization that enables evidential models to avoid the “zero-evidence” region so that they can effectively learn from all training samples.,
"• We carry out experiments over multiple challenging real- world datasets to empirically validate the presented theory, and show the effectiveness of our proposed ideas.",
Related Works Uncertainty Quantification in Deep Learning.,
Accu- rate quantification of predictive uncertainty is essential for development of trustworthy Deep Learning (DL) models.,
"Deep ensemble techniques (Pearce et al., 2020; Lakshmi- narayanan et al., 2017) have been developed for uncer- tainty quantification.",
An ensemble of neural networks is constructed and the agreement/disagreement across the en- semble components is used to quantify different uncertain- ties.,
"Ensemble-based methods significantly increase the number of model parameters, which are computationally expensive at both training and test times.",
"Alternatively, Bayesian neural networks (Gal & Ghahramani, 2016)(Blun- dell et al., 2015)(Mobiny et al., 2021) have been devel- oped that consider a Bayesian formalism to quantify dif- ferent uncertainties.",
"For instance, (Blundell et al., 2015) use Bayes-by-backdrop to learn a distribution over neural network parameters, whereas (Gal & Ghahramani, 2016) enable dropout during inference phase to obtain predictive uncertainty.",
Bayesian methods resort to some form of ap- proximation to address the intractability issue in marginal- ization of latent variables.,
"Moreover, these methods are also computationally expensive as they require sampling for uncertainty quantification.",
Evidential Deep Learning.,
Evidential models introduce a conjugate higher-order evidential prior for the likelihood dis- tribution that enables the model to capture the fine-grained uncertainties.,
"For instance, Dirichlet prior is introduced over the multinomial likelihood for evidential classification (Bao et al., 2021; Zhao et al., 2020), and NIG prior is in- troduced over the Gaussian likelihood (Amini et al., 2020; Pandey & Yu, 2022b) for the evidential regression models.",
"Adversarial robustness (Kopetzki et al., 2021) and calibra- tion (Tomani & Buettner, 2021) of evidential models have also been well studied.",
"Usually, these models are trained with evidential losses in conjunction with heuristic evidence regularization to guide the uncertainty behavior (Pandey & Yu, 2022a; Shi et al., 2020) in addition to reasonable gen- eralization performance.",
"Some evidential models assume access to out-of-distribution data during training (Malinin & Gales, 2019; 2018) and use the OOD data to guide the un- certainty behavior.",
"A recent survey (Ulmer, 2021) provides a thorough review of the evidential deep learning field.",
"In this work, we focus on evidential classification models and consider settings where no OOD data is used during model training to make the proposed approach more broadly applicable to practical real-world situations.",
Learning Deficiency of Evidential Models 3.1.,
Preliminaries and problem setup Standard classification models use a softmax transformation on the output from the neural network FΘ for input x to ob- tain the class probabilities in K-class classification problem.,
Such models are trained with the cross-entropy based loss.,
"2 Learn to Accumulate Evidence from All Training Samples: Theory and Practice For a given training sample (x, y), the loss is given by Lcross = − K X k=1 yk log(smk) (1) where smk is the softmax output.",
These models have achieved state-of-the-art performance on many benchmark problems.,
A detailed gradient analysis shows that they can effectively learn from all training data samples (see Ap- pendix A).,
"Nevertheless, these models lack a systematic mechanism to quantify different sources of uncertainty, a highly desired property in many real-world problems.",
Graphical model for Evidential Deep Learning Evidential classification models formulate training as an evidence acquisition process and consider a higher-order Dirichlet prior Dir(p|α) over the predictive Multino- mial distribution Mult(y|p).,
"Different from a standard Bayesian formulation which optimizes Type II Maximum Likelihood to learn the Dirichlet hyperparameter (Bishop & Nasrabadi, 2006), evidential models directly predict α using data features x and then generate the prediction y by marginalizing the Multinomial parameter p. Figure 3 de- scribes this generative process.",
Such higher-order prior en- ables the model to systematically quantify different sources of uncertainty.,
"In evidential models, the softmax layer of the standard neural networks is replaced by a non-negative activation function A, where A(x) ≥0 ∀x ∈[−∞, ∞], such that for input x, the neural network model FΘ with parameters Θ can output evidence e for different classes.",
Dirichlet prior α is evaluated as α = e+1 to ensure α ≥1.,
The trained evidential model outputs Dirichlet parameters α for input x that can quantify fine-grained uncertainties in addition to the prediction y.,
"Mathematically, for K−class classification problem, Evidence(e) = A(FΘ(x)) = A(o) (2) Dirichlet Parameter(α) = e + 1 (3) Dirichlet Strength(S) = K + K X k=1 ek (4) The activation function A(·) assumes three common forms to transform the neural network output into evidence: (1) ReLU(·) = max(0, ·), (2) SoftPlus(·) = log(1 + exp(·)), and (3) exp(·).",
Evidential models assign input sample to that class for which the output evidence is greatest.,
"Moreover, they quantify the confidence in the prediction for K class classification prob- lem through vacuity ν (i.e., measure of lack of confidence in the prediction) computed as Vacuity(ν) = K S (5) For any training sample (x, y), the evidential models aim to maximize the evidence for the correct class, minimize the evidence for the incorrect classes, and output accurate confi- dence.",
"To this end, three variants of evidential loss functions have been proposed (Sensoy et al., 2018): 1) Bayes risk with sum of squares loss, 2) Bayes risk with cross-entropy loss, and 3) Type II Maximum Likelihood loss.",
"Please refer to equations (21), (22), and (23) in the Appendix for the spe- cific forms of these losses.",
"Additionally, incorrect evidence regularization terms are introduced to guide the model to output low evidence for classes other than the ground truth class (See Appendix C for discussion on the regularization).",
"With evidential training, accurate evidential deep learning models are expected to output high evidence for the correct class, low evidence for all other classes, and output very high vacuity for unseen/out-of-distribution samples.",
"Theoretical Analysis of Learning Deficiency in Evidential Learning To identify the underlying reason that causes the perfor- mance gap of evidential models as described earlier, we consider a K class classification problem and a represen- tative evidential model trained using Bayes risk with sum of squares loss given in (21).",
We first provide an important definition that is critical for our theoretical analysis.,
Definition 1 (Zero-Evidence Region).,
A Zero-evidence sample is a data sample for which the model outputs zero evidence for all classes.,
A region in the evidence space that contains zero-evidence samples is a zero-evidence region.,
"For a reasonable evidential model, novel data samples not yet seen during training, difficult data samples, and out-of- distribution samples should become zero-evidence samples.",
"Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero.",
Consider an input x with one-hot ground truth label y.,
"Let the ground truth class index be gt, i.e., ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0.",
"Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively.",
"In this evidential model, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (6) 3 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule: ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok (7) Based on the actual form of A, we have three cases: Case I: ReLU(·) to transform logits to evidence ek = ReLU(ok) =⇒∂ek ∂ok = ( 1 if ok > 0 0 otherwise (8) For a zero-evidence sample, the logits ok satisfy the rela- tionship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 Case II: SoftPlus(·) to transform logits to evidence ek = log(exp(ok) + 1) =⇒ ∂ek ∂ok = Sigmoid(ok) (9) For a zero-evidence sample, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Case III: exp(·) to transform logits to evidence ek = exp(ok) =⇒ ∂ek ∂ok = exp(ok) = αk −1 (10) For a zero-evidence sample, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient in (7) to counterbalance these zero-approaching gradients.",
"So, for zero-evidence training samples, for any node k, ∂LMSE(x, y) ∂ok = 0 (11) Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
This implies that the evidential models fail to learn from a zero-evidence data sample.,
"For completeness, we present the analysis of standard clas- sification models in Appendix A, detailed proof of the evi- dential models trained using Bayes risk with sum of squares error along with other evidential lossses in Appendix B, and impact of incorrect evidence regularization in Appendix C. Remark: Evidential models can not learn from a train- ing sample that the model has never seen and for which the model accurately outputs “I don’t know”, i.e., ek = 0 ∀k ∈[1, K].",
Such samples are expected and likely to be present during model training.,
"However, the supervised in- formation in such training data points is completely missed by evidential models so they fail to acquire any new knowl- edge from all such training data samples (i.e., data samples in zero-evidence region of the evidence space).",
Corollary 1.,
Incorrect evidence regularization can not help evidential models learn from zero-evidence samples.,
"Intuitively, the incorrect evidence regularization encourages the model to output zero evidence for all classes other than the ground truth class and the regularization does not have any impact on the evidence for the ground truth class.",
"So, the regularization updates the model parameters such that the model is likely to map input samples closer to zero- evidence region in the evidence space.",
"Thus, the regular- ization does not address the failure of evidential models to learn from zero evidence samples.",
"For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential acti- vation function leads to a larger gradident update on the model parameters than softplus and ReLu.",
"Limited by space, we present the proof of Theorem 2 along with additional analysis in the Appendix D. The proof fol- lows the gradient analysis of the exponential, Softplus, and ReLU based models.",
It implies that the the training of evidential models is most effective with the exponential activation function.,
"Intuitively, the ReLU based activation completely destroys all the information in the negative logits, and has largest region in evidence space in which training data have zero evidence.",
"Softplus activation improves over the ReLU, and compared to ReLU, has smaller region in evidence space where training data have zero evidence.",
"However, Softplus based evidential models fail to cor- rect the acquired knowledge when the model has strong wrong evidence.",
"Moreover, these models are likely to suf- fer from vanishing gradients problem when the number of classes increases (i.e., classification problem becomes more challenging).",
"Finally, exponential activation has the smallest zero-evidence region in the evidence space without suffering from the issues of SoftPlus based evidential models.",
Avoiding Zero-Evidence Regions Through Correct Evidence Regularization We now consider an evidential model with exponential func- tion to transform the logits into evidence.,
"We propose a novel vacuity-guided correct evidence regularization term Lcor(x, y) = −λcor log(αgt −1) (12) where λcor = ν = K S represents the regularization term whose value is given by the magnitude of the vacuity output by the evidential model and αgt −1 represents the predicted evidence for the ground truth class.",
The regularization term λcor determines the relative importance of the correct 4 Learn to Accumulate Evidence from All Training Samples: Theory and Practice evidence regularization term compared to the evidential loss and incorrect evidence regularization and is treated as constant during model parameter update.,
"Correct evidence regularization Lcor(x, y) can address the issue of learning from zero-evidence train- ing samples.",
"The proposed regularization term Lcor(x, y) does not contain any evidence terms other than the evidence for the ground truth node.",
"So, the gradient of the regularization for nodes other than the ground truth node will be 0 i.e.",
"∂Lcor(x,y) ∂ok k̸=gt = 0 and there will be no update on these nodes.",
"For the ground truth node gt, ygt = 1, the gradient is given by ∂Lcor(x, y) ∂ogt = ∂  −λcor log(αgt −1)  ∂ogt (13) = −λcor ∂log(αgt −1) ∂αgt × ∂αgt ∂ogt (14) = − λcor (αgt −1)(αgt −1) = −λcor (15) The gradient value equals the magnitude of the vacuity.",
"The vacuity is bounded in the range [0, 1], and zero-evidence sample, the vacuity is maximum, leading to the greatest gradient value of ∂Lcor(x,y) ∂ogt = −1.",
"In other words, the reg- ularization encourages the model to update the parameters such that the correct evidence αgt −1 increases.",
"As the model evidence increases, the vacuity decreases, and the contribution of the regularization Lcor(x, y) is minimized.",
"Thus, the proposed regularization enables the evidential model to learn from zero-evidence samples.",
Evidential Model Training We formulate an overall objective used to train the pro- posed Regularized evidential model (RED).,
"Essentially, the evidential model is trained to maximize the correct evi- dence, minimize the incorrect evidence, and avoid the zero- evidence region during training.",
"The overall loss is L(x, y) = Levid(x, y) + η1Linc(x, y) + Lcor(x, y) (16) where Levid(x, y) is the loss based on the evidential framework given by (21), (23), or (22) (See Appendix B), Linc(x, y) represents the incorrect evidence regularization (See Appendix Section C), Lcor(x, y) represents the pro- posed novel correct evidence regularization term in (12), and η1 = λ1 × min(1.0, epoch index/10) controls the impact of incorrect evidence regularization to the overall model training.",
"In this work, we consider the forward-KL based incorrect evidence regularization given in (42) based on (Sensoy et al., 2018).",
Evidence Space Visualization Figure 4.,
Evidence space visualization to demonstrate the effec- tiveness of the proposed method.,
Figure 2 visualizes the evidence space in ReLU-based ev- idential models by considering the pre-ReLU output in a binary classification setting.,
"Ideally, all samples that belong to Class 1 should be mapped to the blue region (region of high evidence for Class 1, low evidence for all other classes), all samples that belong to Class 2 should be mapped to the red region, and all out-of distribution samples should be mapped to the zero-evidence region (no evidence for all classes).",
"To realize this goal, the models are trained using the evidential loss Levid with incorrect evidence regular- ization Linc.",
"However, there is no update to the evidential model from such samples of zero-evidence region.",
Model’s prior belief of “I don’t know” for such samples does not get updated even after being exposed to the true label.,
"For the samples with high incorrect evidence and low correct evidence, evidential model aims to correct itself.",
"However, many such samples are likely to get mapped to the zero- evidence region (as shown by blue and orange arrows in Figure 2) after which there is no update to the model.",
Such fundamental limitation holds true for all evidential models.,
The evidence space visualization for RED is shown in Figure 4 to illustrate how it addresses the above limitation.,
Cor- rect evidence regularization (indicated by green arrows) is weighted by the magnitude of the vacuity and is maximum in the zero-evidence region.,
"In this problematic region, the proposed regularization fully dominates the model update as there is no update to the model from the two loss com- ponents (Levid and Linc) in (16).",
"As the sample gets far away from the zero evidence region, the vacuity decreases proportionally, the impact of the proposed regularization to model update becomes insignificant, and the evidential losses (Levid & Linc) guide the model training.",
"In this way, RED can effectively learn from all training samples irrespective of the model’s existing evidence.",
5 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 5.,
Experiments Datasets and setup.,
"We consider the standard supervised classification problem with MNIST (LeCun, 1998), Ci- far10, and Cifar100 datasets (Krizhevsky et al., 2009), and few-shot classification with mini-ImageNet dataset (Vinyals et al., 2016).",
"We employ the LeNet model for MNIST, ResNet18 model (He et al., 2016) for Cifar10/Cifar100, and ResNet12 model (He et al., 2016) for mini-ImageNet.",
We first conduct experiments to demonstrate the learning deficiency of existing evidential models to confirm our the- oretical findings.,
We then evaluate the proposed correct evidence regularization to show its effectiveness.,
We finally conduct ablation studies to investigate the impact of evi- dential losses on model generalization and the uncertainty quantification of the proposed evidential model.,
"Limited by space, additional clarifications, experiment results includ- ing few-shot classification experiments, experiments over challenging tiny-Imagenet datasett with Swin Transformer, hyperparameter details, and discussions are presented in the Appendix.",
Learning Deficiency of Evidential Models Sensitivity to the change of the architecture.,
"We first consider a toy illustrative experiment with two frameworks: 1) standard softmax, 2) evidential learning, and experiment with the LeNet (LeCun et al., 1999) model considered in EDL (Sensoy et al., 2018) with a minor modification to the architecture: no dropout in the model.",
"To construct the toy dataset, we randomly select 4 labeled data points from the MNIST training dataset as shown in the Figure 5.",
"For the evidential model, we use ReLU to transform the network outputs to evidence, and train the model with MSE-based evidential loss (Sensoy et al., 2018) given in (21) without incorrect evidence regularization.",
We train both models using only these 4 training data points.,
Figure 6 compares the training accuracy and training loss trends of the evidential model with the standard softmax model (trained with the cross-entropy loss).,
"Before any training, both models have 0% accuracy and the loss is high as expected.",
"For the evidential model, in the first few iter- ations, the model learns from the training dataset, and the model’s accuracy increases to 50%.",
"Afterward, the eviden- tial model fails to learn as the evidential model maps two of the training data samples to the zero-evidence region.",
"Even in such a trivial setting, the evidential model fails to fit the 4 training data points showing their learning deficiency that empirically verifies the conclusion in Theorem 1.",
It is also worth noting that the range of the evidential model’s loss is significantly smaller than the standard model.,
"This is mainly due to the bounded nature of the evidential MSE loss(i.e., it is bounded in the range [0, 2]) (a detailed theoretical analy- sis of the evidential losses is provided in the Appendix).",
"In contrast, the standard model trained with cross-entropy loss GT : 3 GT : 5 GT : 2 GT : 6 Figure 5.",
Toy dataset with 4 data points.,
0 2 4 6 8 Iterations (× 10) 0.00 0.25 0.50 0.75 1.00 Accuracy Standard model Evidential model (a) Training accuracy trend 0 2 4 6 8 Iterations (× 10) 0 1 2 Loss Standard Model Evidential Model (b) Training loss trend Figure 6.,
"Training of standard and evidential models easily fits the trivial dataset, obtains near 0 loss, and perfect accuracy of 100% after a few iterations of training.",
0 25 50 75 100 Iteration 0 2 4 Evidence 3 5 2 6 Figure 7.,
"Zero-evidence trend during model training Additionally, we visualize the zero-evidence data samples for the toy dataset setting.",
We plot the total evidence for each training sample as training progresses for the first 100 iterations.,
The total evidence trend as training progresses for the first 100 iterations is shown in Figure 7.,
"The ev- idential model’s predictions are correct for data samples with ground truth labels of 3 and 6, and incorrect for the remaining two data samples.",
"After few iterations of training, the remaining two samples have zero total evidence (i.e.",
"samples are mapped to zero evidence region), the model never learns from them, and the model only achieves overall 50% training accuracy even after 100 iterations.",
"Clearly, the evidential model continues to output zero evidence for two of the training examples and fails to learn from them.",
Such learning deficiency of evidential models limits their extension to challenging settings.,
"In contrast, the standard model easily overfits the 4 training examples and achieves 100% accuracy.",
Sensitivity to hyperparameter tuning.,
"In this experi- ment, evidential models are trained using evidential losses given in (21), (22), or (23) with incorrect evidence regular- ization to guide the model for accurate uncertainty quan- 6 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Figure 8.",
Impact of different incorrect evidence regularization strengths to the test set accuracy on Cifar100 dataset tification.,
We study the impact of the incorrect evidence regularization λ1 to the evidential model’s performance using Cifar100.,
The result shows that the generalization performance of evidential models is highly sensitive to λ1 values.,
"To illustrate, we consider the Type II Maximum Likelihood loss in (23) with different λ1 to control KL reg- ularization (results on other loss functions are presented in the Appendix).",
"As shown in Figure 8, when some regular- ization is introduced, evidential model’s test performance improves slightly.",
"However, when strong regularization is used, the model focuses strongly on minimizing the incor- rect evidence.",
"Such regularization causes the model to push many training samples into or close to the zero-evidence regions, which hurts the model’s learning capabilities.",
"In contrast, the proposed model can continue to learn from samples in zero-evidence regions, which shows its robust- ness to incorrect evidence regularization.",
"Moreover, our model has stable performance across all hyperparameter settings as it can effectively learn from all training samples.",
Challenging datasets and settings.,
We next consider standard classification models for the Cifar100 dataset and 1-shot classification with the mini-ImageNet dataset.,
We develop evidential extensions of the classification models using Type II Maximum Likelihood loss given in (23) with- out any incorrect evidence regularization and use ReLU to transform logits to evidence.,
"As shown in Figure 10, com- pared to the standard classification model, the evidential model’s predictive performance is sub-optimal (almost 20% lower for both classification problems).",
"This is mainly due to the fact that evidential model maps many of the training data points to zero-evidence region, which is equivalent to the model saying “I don’t know to which class this sample belongs” and stopping to learn from them.",
"Consequently, the model fails to acquire new knowledge (i.e., update itself), even after being exposed to correct supervision (the label information).",
"In these cases, instead of learning, the eviden- tial model chooses to ignore the training data on which it does not have any evidence and remains to be ignorant.",
Visualization of zero-evidence samples.,
We next show the 2-dimensional visualization of the latent representation for the randomly selected 500 training examples based on Figure 9.,
Zero-Evidence Sample Visualization (a) Cifar100 Results (b) 1-Shot Results Figure 10.,
Learning trends in complex classification problems the tSNE plot for ReLU based evidential model trained on the Cifar100 dataset with λ1 = 0.1.,
Figure 9 plot visualizes the latent embedding of zero evidence (Zero E) training sam- ples with non-zero evidence (Non-Zero E) training samples.,
"As can be seen, both zero and non-zero evidence samples ap- pear to be dispersed, overlap at different regions, and cover a large area in the embedding space.",
This further confirms the challenge of effectively learning from these samples 5.2.,
Effectiveness of the RED Evidential activation function.,
We first experiment with different activation functions for the evidential models to show the superior predictive performance and generalization capability of exp activation validating our Theorem 2.,
We consider evidential models trained with evidential log loss given by (23) in Table 1 (Additional results along with hy- perparameter details are presented in Appendix Section F).,
"As can be seen, exp activation to transform network outputs into evidence leads to superior performance compared to ReLU and Softplus based transformations.",
"Furthermore, our proposed model with correct evidence regularization further improves over the exp-based evidential models as it enables the evidential model to continue learning from zero-evidence samples.",
Classification performance comparison Model MNIST Cifar10 Cifar100 ReLU 98.19±0.08 41.43±19.60 61.27±3.79 SoftPlus 98.21±0.05 95.18±0.11 74.48±0.17 exp 98.79±0.02 95.11±0.10 76.12±0.04 RED(Ours) 99.10±0.02 95.24±0.06 76.43±0.21 We next present the test set performance change as training 7 Learn to Accumulate Evidence from All Training Samples: Theory and Practice progresses with MNIST dataset and two different evidential losses in Figure 11 where we observe similar results.,
"The exp activation shows superior performance, as it has small- est zero-evidence region, and does not suffer from many learning issues present in other activation functions.",
(a) Evidential MSE loss (b) Evidential Log loss Figure 11.,
Impact of evidential activation functions to the Test Accuracy Correct evidence regularization.,
We now study the im- pact of the proposed correct evidence regularization using the MNIST and Cifar100 classification problems.,
"We con- sider the evidential baseline model that uses exp activation to acquire evidence, and is trained with Type II Maximum Likelihood based loss with different incorrect evidence reg- ularization strengths.",
We introduce the proposed novel cor- rect evidence regularization to the model.,
"As can be seen in Figure 12, the model with correct-evidence regularization has superior generalization performance compared to the baseline evidential model.",
"This is mainly due to the fact that with proposed correct evidence regularization, the evi- dential model can also learn from the zero-evidence training samples to acquire new knowledge instead of ignoring them.",
Our proposed model considers knowledge from all the train- ing data and aims to acquire new knowledge to improve its generalization instead of ignoring the samples on which it has no knowledge.,
"Finally, even though strong incorrect evidence regularization hurts the model’s generalization, the proposed model is robust and generalizes better, empirically validating our Theorem 3.",
"Limited by space, we present additional results in Appendix F.3.2.",
Zero-evidence Sample Anaysis.,
"Similar to the toy MNIST zero-evidence analysis, we consider the Cifar100 dataset, and carry out the analysis for this complex dataset/setting.",
"Instead of focusing on a few training ex- amples, we present the average statistics of the evidence (E) for the 50,000 training samples in the 100 class classi- fication problem for a model trained for 200 epochs using a log-based evidential loss in (23) with λ1 = 1.0.",
"For ref- erence, the samples with less than 0.01 average evidence (i.e., E ≤0.01) are samples on which the model is not confident (i.e., having a high vacuity of ν ≥0.99), and are close to the ideal zero-evidence region.",
"Our proposed RED model effectively avoids such zero evidence regions, and has the lowest number of samples (i.e.",
"only 0.06% of total training dataset compared to 58.96% of SoftPlus based, (a) Trend for λ1 = 1.0 (b) Trend for λ1 = 10.0 (c) Trend for λ1 = 0.1 (d) Trend for λ1 = 1.0 Figure 12.",
"Impact of correct evidence regularization to test accu- racy: (a), (b) - MNIST Results; (c), (d) - Cifar100 Results and 100% of ReLU based evidential models) in very low evidence regions.",
Zero-Evidence Analysis for Complex Dataset-Setting Model E ≤.01 E ≤0.1 E ≤1.0 E > 1.0 ReLU 50000 50000 50000 0 SoftPlus 29483 32006 49938 62 Exp 48318 49881 49949 51 RED 30 16322 25154 24846 5.3.,
Ablation Study Impact of loss function.,
We next study the impact of the evidential loss function on the model’s performance using MNIST and CIFAR100 classification problems.,
"We consider all three activations: ReLU, SoftPlus, and exp to transform neural network outputs to evidence and carry out experiments over CIFAR100 with identical model and settings.",
"As seen in Table 3, the generalization performance of evidential model is consistently sub-optimal when trained with evidential MSE loss given by (21) compared to the two other evidential losses (22) & (23).",
This is consistent across all three evidence activation functions.,
"This is mainly due to the bounded nature of the evidential MSE loss (21): for all training samples, evidential MSE loss is bounded in the range of [0, 2].",
Type II Maximum Likelihood loss given in (23) and cross-entropy based evidential loss given in (22) show comparable empirical results.,
"Next, we consider exp activation and conduct experiments over the MNIST dataset for incorrect evidence regulariza- tion strengths of λ1 = 0&1.",
We again observe similar results where the training with the Evidential MSE loss in (21) leads to sub-optimal test performance.,
"Additional re- sults, along with theoretical analysis are presented in the Appendix.",
"In the subsequent experiments, we consider the Type II Maximum Likelihood loss (23) for evidential model training due to its simplicity and some theoretical advan- 8 Learn to Accumulate Evidence from All Training Samples: Theory and Practice tages (see Appendix E).",
We leave a thorough investigation of these two evidential losses ((22) & (23)) as future work.,
Impact of evidential losses on classification performance Loss ReLU SoftPlus exp RED(Ours) MSE(21) 31.49±0.3 15.74±0.5 42.95±0.7 75.73±0.3 CE (22) 68.62±2.4 74.44±0.1 76.23±0.1 76.35±0.1 Log(23) 61.27±3.8 74.48±0.1 76.12±0.1 76.43±0.2 (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 Figure 13.,
Impact of evidential losses on test set accuracy Figure 14.,
Accuracy-Vacuity curve Study of uncertainty information.,
We now investigate the uncertainty behavior of the proposed evidential model with Cifar100 experiments.,
We present the Accuracy- Vacuity curve for different incorrect evidence regulariza- tion strengths (λ1) in Figure 14.,
"Vacuity reflects the lack of confidence in the predictions, and the accuracy of effec- tive evidential model should increase with lower vacuity threshold.",
"Without any incorrect evidence regularization (i.e., λ1 = 0), the evidential model is highly confident on its predictions and all test samples are concentrated on the low vacuity region.",
"As the incorrect evidence regularization strength is increased, the model outputs more accurate confi- dence in the predictions.",
"Strong incorrect evidence regular- ization hurts the generalization over the test set as indicated by low accuracy when all test samples are considered (i.e., vacuity threshold of 1.0).",
"In all cases, the evidential model shows reasonable uncertainty behavior: the model’s test set accuracy increases as the vacuity threshold is decreased.",
"Next, we look at the accuracy of the evidential models on their top-K % most confident predictions over the test set.",
Table 4 shows the accuracy trend of Top-K (%) confident samples.,
Consider the most confident 20% samples (cor- responding to 2000 test samples of Cifar100 dataset).,
The proposed model leads to highest accuracy (of 99.35%) com- pared to all the models.,
Similar trend is seen for different K values where the proposed model shows comparable to superior results demonstrating its accurate uncertainty quantification capability.,
"Accuracy on Top-K% confident samples (%) Model 10% 20% 30% 50% 80% 100% ReLU 98.50 98.30 97.27 90.60 71.54 61.27 SoftPlus 99.10 98.75 98.30 95.86 85.56 74.48 exp 99.40 98.95 98.50 96.52 86.46 76.12 RED 99.60 99.35 98.83 96.24 86.38 76.43 We next consider out-of-distribution (OOD) detection ex- periments for the Cifar100-trained evidential model using SVHN dataset (as OOD) (Netzer et al., 2011).",
"As seen in Table 5, the evidential models, on average, output very high vacuity for the OOD samples, showing the potential for OOD detection.",
Out-of-Distribution sample detection Model InD Vacuity OOD Vacuity (SVHN) exp 0.3227 0.7681 RED (Ours) 0.2729 0.7552 We present the AUROC score for Cifar100 trained models with SVHN dataset test set as the OOD samples in Table 6.,
"In AUROC calculation, we use the maximum softmax score for the standard model, and predicted vacuity score for all the evidential models.",
"As can be seen, the exp-based model outperforms all other activation functions, and the proposed model RED can learn from all the training samples that leads to the best performance.",
AUROC for Cifar100-SVHN experiment Model ReLU SoftPlus Standard exp RED AUROC 0.7430 0.8058 0.8669 0.8804 0.8833 6.,
"Conclusion In this paper, we theoretically investigate the evidential mod- els to identify their learning deficiency, which makes them fail to learn from zero-evidence regions.",
We then show the superiority of the evidential model with exp evidential activation over the ReLU and SoftPlus based models.,
"We further analyze the evidential losses, and introduce a novel correct evidence regularization over the exp-based ev- idential model.",
"The proposed model effectively pushes the training samples out of the zero-evidence regions, leading to superior learning capabilities.",
We conduct extensive experi- ments that empirically validate all theoretical claims while demonstrating the effectiveness of the proposed approach.,
Acknowledgements This research was supported in part by an NSF IIS award IIS-1814450 and an ONR award N00014-18-1-2875.,
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.,
"9 Learn to Accumulate Evidence from All Training Samples: Theory and Practice References Amini, A., Schwarting, W., Soleimany, A., and Rus, D. Deep evidential regression.",
"Advances in Neural Informa- tion Processing Systems, 33:14927–14937, 2020.",
"Bao, W., Yu, Q., and Kong, Y. Evidential deep learning for open set action recognition.",
"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
"13349–13358, 2021.",
"Bishop, C. M. and Nasrabadi, N. M. Pattern recognition and machine learning, volume 4.",
"Springer, 2006.",
"Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network.",
"In International conference on machine learning, pp.",
"PMLR, 2015.",
"Charpentier, B., Z¨ugner, D., and G¨unnemann, S. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts.",
"Advances in Neural Information Processing Systems, 33:1356–1367, 2020.",
"Chen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X. Meta- baseline: Exploring simple meta-learning for few-shot learning.",
"In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.",
"9062–9071, 2021.",
"Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks.",
"In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp.",
"Gal, Y. and Ghahramani, Z.",
Dropout as a bayesian approx- imation: Representing model uncertainty in deep learn- ing.,
"In international conference on machine learning, pp.",
"PMLR, 2016.",
"He, K., Zhang, X., Ren, S., and Sun, J.",
Deep residual learn- ing for image recognition.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.",
"770–778, 2016.",
"Huynh, E. Vision transformers in 2022: An update on tiny imagenet.",
"arXiv preprint arXiv:2205.10660, 2022.",
"Jøsang, A. Subjective logic, volume 3.",
"Springer, 2016.",
"Kamath, U., Liu, J., and Whitaker, J.",
"Deep learning for NLP and speech recognition, volume 84.",
"Springer, 2019.",
"Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.",
"arXiv preprint arXiv:1412.6980, 2014.",
"Knopp, K. Weierstrass’s factor-theorem.",
"In Theory of Functions: Part II, pp.",
"Dover, 1996.",
"Kopetzki, A.-K., Charpentier, B., Z¨ugner, D., Giri, S., and G¨unnemann, S. Evaluating robustness of predictive un- certainty estimation: Are dirichlet-based models reliable?",
"In International Conference on Machine Learning, pp.",
"PMLR, 2021.",
"Krizhevsky, A., Hinton, G., et al.",
Learning multiple layers of features from tiny images.,
"Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles.",
"Advances in neural information processing systems, 30, 2017.",
The mnist database of handwritten digits.,
http://yann.,
"com/exdb/mnist/, 1998.",
"LeCun, Y., Haffner, P., Bottou, L., and Bengio, Y. Ob- ject recognition with gradient-based learning.",
"In Shape, contour and grouping in computer vision, pp.",
"Springer, 1999.",
"Malinin, A. and Gales, M. Predictive uncertainty estima- tion via prior networks.",
"Advances in neural information processing systems, 31, 2018.",
"Malinin, A. and Gales, M. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness.",
"Advances in Neural Information Processing Systems, 32, 2019.",
"Mobiny, A., Yuan, P., Moulik, S. K., Garg, N., Wu, C. C., and Van Nguyen, H. Dropconnect is effective in modeling uncertainty of bayesian deep networks.",
"Scientific reports, 11(1):1–14, 2021.",
"Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y.",
"Reading digits in natural images with unsupervised feature learning, 2011.",
"Nguyen, A., Yosinski, J., and Clune, J.",
Deep neural net- works are easily fooled: High confidence predictions for unrecognizable images.,
"In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pp.",
"427–436, 2015.",
"Pandey, D. S. and Yu, Q. Multidimensional belief quantifi- cation for label-efficient meta-learning.",
"In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pp.",
"14391–14400, June 2022a.",
"Pandey, D. S. and Yu, Q. Evidential conditional neural processes.",
"arXiv preprint arXiv:2212.00131, 2022b.",
"Pearce, T., Leibfried, F., and Brintrup, A.",
Uncertainty in neural networks: Approximately bayesian ensembling.,
"In International conference on artificial intelligence and statistics, pp.",
"PMLR, 2020.",
"10 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Sensoy, M., Kaplan, L., and Kandemir, M. Evidential deep learning to quantify classification uncertainty.",
"Advances in neural information processing systems, 31, 2018.",
"Shafer, G. A mathematical theory of evidence, volume 42.",
"Princeton university press, 1976.",
"Shi, W., Zhao, X., Chen, F., and Yu, Q. Multifaceted uncertainty estimation for label-efficient deep learning.",
"Advances in neural information processing systems, 33, 2020.",
"Singh, S. P., Kumar, A., Darbari, H., Singh, L., Rastogi, A., and Jain, S. Machine translation using deep learning: An overview.",
"In 2017 international conference on computer, communications and electronics (comptelix), pp.",
"IEEE, 2017.",
"Tomani, C. and Buettner, F. Towards trustworthy predictions from deep neural networks with fast adversarial calibra- tion.",
"In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.",
"9886–9896, 2021.",
"Ulmer, D. A survey on evidential deep learning for single-pass uncertainty estimation.",
"arXiv preprint arXiv:2110.03051, 2021.",
"Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.",
Matching networks for one shot learning.,
"Advances in neural information processing systems, 29, 2016.",
"Voulodimos, A., Doulamis, N., Doulamis, A., and Protopa- padakis, E. Deep learning for computer vision: A brief review.",
"Computational intelligence and neuroscience, 2018, 2018.",
"Zhao, X., Chen, F., Hu, S., and Cho, J.-H.",
Uncertainty aware semi-supervised learning on graph data.,
"Advances in Neural Information Processing Systems, 33:12827– 12836, 2020.",
"11 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Appendix Organization of the Appendix • In Section A, we present an analysis of standard classification models trained with cross-entropy loss to show their learning capabilities.",
"• In Section B, we present a complete proof of Theorem 1 for different evidential losses that demonstrates the inability of evidential models to learn from zero-evidence samples.",
"• In Section C, we describe different incorrect evidence regularizations used in the existing literature and carry out a gradient analysis to study their impact on evidential model learning.",
"• In Section D, we present the proof for Theorem 2 that shows the superiority of exp activation over the SoftPlus and ReLU functions to transform logits to evidence.",
"• In Section E, we analyze the evidential losses that reveals the theoretical limitation of evidential models trained using Bayes risk with sum of squares loss.",
"• In Section F, we present additional experiment results, clarifications, hyperparameter details, and discuss some limitations along with possible future works.",
The source code for the experiments carried out in this work is attached in the supplementary materials and is available at the link: https://github.com/pandeydeep9/EvidentialResearch2023 A.,
Standard Classification Model Consider a standard cross-entropy based model for K−class classification.,
Let the overall network be represented by fΘ(.,
"), and let o = fΘ(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y.",
The output after the softmax layer is given by smi = exp(oi) PK k=1 exp(ok) = exp(oi) Sce (17) Where Sce = PK i=1 exp(oi).,
The model is trained with cross-entropy loss.,
"For a given sample (x, y), the loss is given by Lcross-entropy = − K X k=1 yk log(smk) = − K X k=1 h ykok −yk log  K X i=1 exp(oi) i (18) = log Sce − K X k=1 ykok (19) Now, looking at the gradient of this loss with respect to the pre-softmax values o gradk = ∂Lcross-entropy ∂ok =  1 Sce ∂Sce ∂ok −yk  = exp(ok) Sce −yk  = smk −yk (20) Analysis of the gradients For Standard Classification Model.",
"The gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as 0 ≤smk ≤1 and yk ∈{0, 1}.",
The model is updated using gradient descent based optimization objectives.,
"For input x, the neural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y̸=gt = 0.",
"When yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value.",
"Only when smi = 0, the gradient is zero, and the model is not updated.",
"In all other cases when smi ̸= 0, there is a non-zero gradient dependent on smi, and the model is updated to minimize the smi as expected.",
"12 Learn to Accumulate Evidence from All Training Samples: Theory and Practice When yi = 1, the gradient signal is gradi = smi −1 and the model optimizes the parameters to minimize this value.",
"As smi ∈[0, 1], only when the model outputs a large logit on i (corresponding to the ground truth class) and small logit for all other nodes, smi = 1, the gradient is zero, and the model is not updated.",
"In all other cases when smi < 1, there is a non-zero gradient dependent on smi and the model is updated to maximize the smi and minimize all other sm̸=i as expected.",
The gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables learning from all the training data samples.,
"B. Evidential Classification Models Theorem 1: Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero.",
"In the main paper, we considered a K−class classification problem and a representative evidential model trained using Bayes risk with sum of squares loss (Eqn.",
21) in the proof.,
"Following 3 variants of evidential losses ((Sensoy et al., 2018)) have been commonly used in evidential classification works: 1.",
"Bayes risk with sum of squares loss (i.e., Evidential MSE loss) (Zhao et al., 2020) LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (21) 2.",
"Bayes risk with cross-entropy loss (i.e., Evidential CE loss)(Charpentier et al., 2020) LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  (22) 3.",
"Type II Maximum Likelihood loss (i.e., Evidential log loss)(Pandey & Yu, 2022a) LLog(x, y) = K X k=1 yk  log(S) −log(αk)  (23) For completeness, we consider all three loss functions used in evidential classification models and carry out their analysis.",
Gradient of Evidential Activation Functions A(.),
"Three non-linear functions are proposed and commonly used in the existing literature to transform the neural network output to evidence: 1) ReLU function, 2) SoftPlus function, and 3) Exponential function.",
"In this section, we compute the gradients of the evidence output ei from these non-linear activation functions with respect to the logit input oi 1.",
"= max(0, .)",
"ek = ReLU(ok) = max(0, ok) =⇒∂ek ∂ok = ( 0 if ok ≤0 1 otherwise (24) 2.",
= SoftPlus(.),
= log(1 + exp(.)),
ek = log(exp(ok) + 1) =⇒∂ek ∂ok = 1 1 + exp(−ok) = Sigmoid(ok) (25) 3.,
ek = exp(ok) =⇒∂ek ∂ok = exp(ok) = ek = αk −1 (26) 13 Learn to Accumulate Evidence from All Training Samples: Theory and Practice B.2.,
"Evidential Model Trained using Bayes risk with sum of squares loss (i.e., Eqn.",
Consider an input x with one-hot ground truth label of y.,
Let the ground truth class be g i.e.,
"ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0.",
"Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively.",
"In this evidential framework, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) = 1 −2αgt S + P k α2 k S2 + 2 P i P j αiαj S2(S + 1) (27) = 2 −2αgt S − 2 P i P j αiαj S(S + 1) (28) Now, consider different components of the loss and compute the gradients of the components with respect to Dirichlet parameters α, ∂αgt S ∂αgt = 1 S −αgt S2 & ∂αgt S ∂α̸=gt = −αgt S2 =⇒∂αgt S ∂αk = yk S −αgt S2 The gradient of the variance term is the same for all the K Dirichlet parameters and is given by ∂ P i P j αiαj S(S+1) ∂αk = (S −αk) S(S + 1) − (2S + 1) P i P j αiαj (S2 + S)2 Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule as ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = −  2∂αk S ∂αk −2 ∂ P i P j αiαj S(S+1) ∂αk  × ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok Case I: ReLU(.)",
"to transform logits to evidence ek = ReLU(ok) = max(0, ok) =⇒∂ek ∂ok = ( 1 if ok > 0 o otherwise (29) For zero-evidence sample with ReLU(.)",
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 Case II: SoftPlus(.)",
to transform logits to evidence ek = log(exp(ok) + 1) =⇒∂ek ∂ok = Sigmoid(ok) (30) Case II: exp(.),
to transform logits to evidence ek = exp(ok) =⇒∂ek ∂ok = exp(ok) = αk −1 (31) For zero-evidence sample with SoftPlus(.),
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
For zero-evidence sample with exp(.),
"used to transform the logits to evidence, αk → 1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
29) to counterbalance these zero-approaching gradients.,
"So, for zero-evidence samples, ∂LMSE(x, y) ∂ok = 0 (32) Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
"Thus, the evidential models fail to learn from such zero-evidence samples.",
14 Learn to Accumulate Evidence from All Training Samples: Theory and Practice B.3.,
"Evidential Model Trained using Type II Maximum Likelihood formulation of Evidential loss (i.e., Eqn.",
23) Consider a K−class evidential classification model that trains the model using Type II Maximum Likelihood formulation of the evidential loss.,
"Consider an input x with one-hot ground truth label of y, PK k=1 yk = 1.",
"For this evidential framework, the Type II Maximum Likelihood loss is given by LLog(x, y) = K X k=1 yk  log(S) −log(αk)  = log S − K X k=1 yk log αk (33) Taking the gradient of the loss with the logits o, we get gradk = ∂LLog(x, y) ∂ok = 1 S ∂S ∂ok −yk 1 αk ∂αk ∂ok =  1 S −yk αk ∂ek ∂ok (34) Case I: ReLU(.)",
to transform logits to evidence For any zero-evidence sample with ReLU(.),
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LLog(x,y) ∂ok = 0 ∀k ∈[1, K] Case II: SoftPlus(.)",
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 25, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  Sigmoid(ok) (35) Case III: exp(.)",
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 26, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (ek) =  1 S −yk αk  (αk −1) (36) For zero-evidence sample with SoftPlus(.)",
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Similarly, for zero-evidence sample with exp(.)",
"used to transform the logits to evidence, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
35 and Eqn.,
36 ) to counterbalance these zero-approaching gradient terms.,
"Since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples.",
"Thus, the evidential models trained with Type II Maximum Likelihood formulation of the evidential loss fail to learn from such zero-evidence samples.",
"Evidential Model Trained using Bayes risk with cross-entropy formulation of Evidential loss (i.e., Eqn.",
22) Consider a K−class evidential classification model that trains model using Bayes risk with cross-entropy loss for evidential learning (Eqn.,
"Consider an input x with one-hot ground truth label of y, PK k=1 yk = 1.",
"For this evidential framework, the loss is given by LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  = Ψ(S) −Ψ(αgt) (37) Where αgt represents the output Dirichlet parameter for the ground truth class i.e.",
"ygt = 1, y̸=gt = 0, and Ψ(.)",
"represents the Digamma function, and for z ≥1, is given by Ψ(z) = d dz log Γ(z) = d dz  −γz −log z + ∞ X n=1  z n −log  1 + z n  = −γ −1 z + z ∞ X n=1 1 n(n + z) 15 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Here, γ is the Euler–Mascheroni constant, and Γ(.)",
"is the gamma function, Using Weierstass’s definition of gamma function (Knopp, 1996) for values outside negative integers that is given by Γ(z) = e−γz z ∞ Y n=1  1 + z n −1 e z n Using the definition of the digamma functions, the loss updates as LCE(x, y) = Ψ(S) −Ψ(αgt) = 1 αgt −1 S + S ∞ X n=1 1 n(n + S) −αgt ∞ X n=1 1 n(n + αgt) (38) The derivative of the digamma function is bounded and is given by ∂Ψ(z) ∂z = ∂ ∂z  −γ −1 z + ∞ X n=1 1 n − 1 n + z  = 1 z2 + ∞ X n=1 1 (n + z)2 1 z2 <∂Ψ(z) ∂z < 1 z2 + π2 6 , z ≥1 With this, we can compute the gradients of the loss with respect to the logits as gradk = ∂LCE(x, y) ∂ok = ∂ ∂αk  Ψ(S) −Ψ(αgt) ∂αk ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2 ∂ek ∂ok (39) Case I: ReLU(.)",
to transform logits to evidence For any zero-evidence sample with ReLU(.),
"used to transform the logits to evidence, the logits ok satisfy the relationship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LCE(x,y) ∂ok = 0 ∀k ∈[1, K] Case II: SoftPlus(.)",
to transform logits to evidence.,
Considering Eqn.,
"25 and Eqn 39, the gradient of the loss with respect to the logits becomes gradk = ∂LCE(x, y) ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2  Sigmoid(ok) (40) Case III: exp(.)",
to transform logits to evidence.,
Considering Eqn.,
"26 and Eqn 39, the gradient of the loss with respect to the logits becomes gradk = ∂LCE(x, y) ∂ok =  1 S2 + ∞ X i=1 1 (n + S)2 −yk α2 gt − ∞ X i=1 yk (n + αgt)2  (αk −1) (41) For zero-evidence sample with SoftPlus(.)",
"used to transform the logits to evidence, the logits ok →−∞ =⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0.",
"Similarly, for zero-evidence sample with exp(.)",
"used to transform the logits to evidence, αk →1 =⇒ ∂ek ∂ok →0.",
"Moreover, there is no term in the first part of the loss gradient (see Eqn.",
29) to counterbalance these zero-approaching gradient terms.,
The gradient of the loss with respect to all the nodes is zero for all the considered cases.,
"Since the gradient of the loss with respect to all the nodes is zero for all three cases, there is no update to the model from such samples.",
"Thus, the evidential models fail to learn from such zero-evidence samples in all cases.",
"C. Regularization in the Evidential Classification Models Based on the evidence e, beliefs b, and the Dirichlet parameters α, various regularization terms have been introduced that aim to penalize the incorrect evidence/incorrect belief of the model, leading to the model with accurate uncertainty estimates.",
"Here, we briefly summarize the key regurlaizations: 16 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 1.",
"Introduce a forward KL regularization term as in EDL (Sensoy et al., 2018) that regularizes the model to output no incorrect evidence.",
"LEDL reg(x, y) = KL  Dir(p|˜α)||Dir(p|1)  = log  Γ PK k=1 ˜αk Γ(K) QK k=1 Γ˜αk  + K X k=1 (˜αk −1)  ψ(˜αk) −ψ  K X j=1 ˜αj  (42) Where ˜α = y + (1 −y) ⊙α = (˜α1, ˜α2, ...˜αN) parameterize a dirichlet distribution, ˜αi=gt = 1, ˜αi = αi∀i ̸= gt.",
"Here, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., Dir(p|˜α) to be flat which is possible when there is no incorrect evidence.",
"42, we can see that the regularization term, introduces digamma functions for the loss and may require evaluation of higher-order polygamma functions for challenging problems (e.g.",
"involving bi-level optimizations as in MAML (Finn et al., 2017)).",
"Introduce an incorrect evidence regularization term as in ADL (Shi et al., 2020) that is the sum of the incorrect evidence for a sample LADL reg(x, y) = K X k=1  e ⊙(1 −y)  k = K X k=1 ek × (1 −yk) (43) Here, ⊙represents element-wise product.",
The evidence for a class ek is only restricted to be non-negative and can take large positive values leading to large variation in the overall loss.,
"Introduce incorrect belief-based regularization as in Units-ML (Pandey & Yu, 2022a) LUnits reg (x, y) = K X k=1   e S ⊙(1 −y)  k = K X k=1 ek S × (1 −yk) (44) The regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake is.",
All three regularizations aim to guide the model such that the incorrect evidence is minimized (ideally close to zero).,
These regularizations help the evidential model acquire desired uncertainty quantification capabilities in evidential models.,
Such guidance is expected to update the model such that it maps input samples near zero-evidence regions in the evidence space.,
"Thus, the regularization does not help address the issue of learning from zero-evidence samples and is likely to hurt the model’s learning capabilities.",
Gradient Analysis of the Incorrect Evidence Regularizations The regularization terms use ground truth information to consider only the incorrect evidence.,
"Thus, the gradient of the regularization loss with respect to the ground truth node αgt is 0.",
"In this analysis, we consider the gradient with respect to non-ground truth nodes i.e.",
"αk, and ok, k ̸= gt.",
Gradient for EDL regularization (Eqn.,
"42 ) LEDL reg(x, y) = KL  Dir(p|˜α)||Dir(p|1)  = log  Γ PK k=1 ˜αk Γ(K) QK k=1 Γ˜αk  + K X k=1 (˜αk −1)  ψ(˜αk) −ψ  K X j=1 ˜αj  = log Γ(S −αgt) −log Γ(K) − K X k=1 log Γ˜αk + K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  (45) 17 Learn to Accumulate Evidence from All Training Samples: Theory and Practice ∂LEDL reg(x, y) ∂αk = ∂ ∂αk  log Γ(S −αgt) −log Γ(K) − K X k=1 log Γ˜αk + K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  = ψ(S −αgt) −ψ(αk) + ∂ ∂αk  K X k=1 (˜αk −1)  ψ(˜αk) −ψ(S −αgt)  = ψ(S −αgt) −ψ(αk) + ψ(αk) −ψ(S −αgt) + (αk −1) ∂ ∂αk  ψ(˜αk) −ψ(S −αgt)  = (αk −1) ∂ ∂αk  ψ(αk) −ψ(S −αgt)  = (αk −1)  ψ1(αk) −ψ1(S −αgt)  Where ψ1 is the trigamma function.",
"Further, using the definition of trigamma function, ∂LEDL reg(x, y) ∂αk = (αk −1)  ψ1(αk) −ψ1(S −αgt)  = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (46) Now, the gradients with respect to the logits ok becomes ∂LEDL reg(x, y) ∂ok = ∂LEDL reg(x, y) ∂αk ∂αk ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  × ∂ek ∂ok (47) Case I: ReLU(.)",
to transform logits to evidence.,
The gradients with respect to the logits ok for zero evidence is zero.,
"For all non-zero evidence, the gradient updates as ∂ek ∂ok = 1∀ek > 0 and ∂LEDL reg(x, y) ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (48) Now, when αk →∞, the value of the gradient ∂LEDL reg(x,y) ∂ok →0.",
There is close to zero model update from regularization for very large incorrect evidence.,
Case II: SoftPlus(.),
to transform logits to evidence.,
The gradients with respect to the logits ok is given by the sigmoid i.e.,
"∂ek ∂ok = sigmoid(ok) , limok→∞ ∂ek ∂ok = 1, and ∂LEDL reg(x, y) ∂ok = (αk −1)  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  σ(αk −1) (49) Now, similar to ReLU, when αk →∞, the value of the gradient ∂LEDL reg(x,y) ∂ok →0.",
There is close to zero model update from regularization for very large incorrect evidence.,
Case III: exp(.),
to transform logits to evidence.,
"When using exponential non-linearity to transform the neural network output to evidence, the αk is given by αk = exp(ok) + 1, ∂αk ∂ok = αk −1.",
"Now the gradients with respect to the neural network output ok becomes: ∂L2 reg(x, y) ∂ok = ∂L2 reg(x, y) ∂αk × ∂αk ∂ok = (αk −1)2  ∞ X n=0 1 (n + αk)2 − 1 (n + S −αgt)2  (50) Here, the gradient values increase as αk →∞, and the gradient values do not vanish.",
"Simply, as the incorrect evidence becomes very large, the model updates also become large in the accurate direction.",
"Thus, considering Case I, II, and II, we see that the incorrect evidence-based regularization with forward KL divergence is not effective in regions of incorrect evidence when using ReLu and SoftPlus functions to transform logits to evidence.",
This issue of correcting very large incorrect evidence does not appear when using exp function to transform the logits into evidence.,
18 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 2.,
"Gradient for ADL regularization ((Shi et al., 2020) ) LADL reg(x, y) = K X k=1  e ⊙(1 −y)  k = K X k=1 ek × (1 −yk) = S −K −αgt + 1 (51) Considering the gradient of the regularization with respect to the parameters αk, k ̸= gt, and corresponding logits ok, we get ∂LADL reg(x, y) ∂αk = 1 =⇒∂LADL reg(x, y) ∂ok = ∂ek ok (52) When considering the exp function to transform logits to evidence, ∂ek ok = ek = exp(ok) and the gradient value becomes very large when the model’s predicted incorrect evidence value is large.",
This may lead to exploding gradients and stability issues in the model training.,
"For ReLU and SoftPlus functions, the gradients in positive evidence regions are ∂ek ok = 1, and ∂ek ok = σ(ok) respectably.",
"Thus, the gradient and corresponding model updates for high incorrect evidence are as desired.",
"Gradient analysis of incorrect belief regularization term as in Units-ML(Pandey & Yu, 2022a) LUnits reg (x, y) = K X k=1   e S ⊙(1 −y)  k = K X k=1 ek S × (1 −yk) = 1 S  S −K −αgt + 1  (53) The regularization value is bounded to be in a range of [0, 1] for all the data samples, no matter how severe the mistake which may limit its effectiveness.",
"Next, the gradient of the regularization with respect to the parameters αk, and logits ok is given by ∂LUnits reg (x, y) ∂αk = ∂  1 S  S −K −αgt + 1  ∂αk = αgt + K −1 S2 = egt + K (K + PK k=1 ek)2 (54) ∂L3 reg(x, y) ∂ok = ∂LUnits reg (x, y) ∂αk × ∂αk ∂ok = egt + K S2 × ∂ek ∂ok (55) The gradient value decreases as the number of classes K in the classification problem increases.",
"For all three transformations: ReLU, SoftPlus, and exp to transform logits to evidence, the gradients will go to zero as the incorrect evidence increases i.e.",
"ek →∞and S →∞=⇒ ∂L3 reg(x,y) ∂ok →0.",
"So, the regularization may be ineffective when the incorrect evidence is very high.",
"D. Impact of Non-linear Transformation Theorem 2: For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential activation function leads to a larger gradident update on the model parameters than softplus and ReLu.",
"Consider an evidential loss L, which is formally defined in Eqns.",
"(21), (22), and (23), is used to train the evidential model, let o, e ∈RK denote the neural network output vector before applying the activation A, and the evidence vector, respectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, ∀k ∈[K], we have: 1.",
ReLu: ∂L1 ∂w = X k ∂L1 ∂ek ∂ek ∂ok ∂ok ∂w = 0 (see Eqn.,
SoftPlus: ∂L2 ∂w = X k ∂L2 ∂ek ∂ek ∂ok ∂ok ∂w = X k ∂L2 ∂ek ∂ok ∂w Sigmoid(ok) ( see Eqn.,
"9), 19 Learn to Accumulate Evidence from All Training Samples: Theory and Practice 3.",
Exponential: ∂L3 ∂w = X k ∂L3 ∂ek ∂ek ∂ok ∂ok ∂w = X k ∂L3 ∂ek ∂ok ∂w exp(ok) = X k ∂L3 ∂ek ∂ok ∂w {[1 + exp(ok)]Sigmoid(ok)} (see Eqn.,
"10) Thus, we have ∂L3 ∂w ≥∂L2 ∂w ≥∂L1 ∂w , which implies that A = exp leads to a larger update to the network than both Softplus and ReLu.",
This completes the proof.,
Now we carry out an analysis of the three activations.,
Analysis: Consider a representative K−class evidential classification model that trains using Type II Maximum Likelihood evidential loss.,
"Consider an input x with one-hot label of y, PK k=1 yk = 1.",
"For this evidential framework, the Type II Maximum Likelihood loss (LLog(x, y)) and its gradient with the logits o ( Eqn.",
"34) are given by LLog(x, y) = log S − K X k=1 yk log αk & gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk ∂ek ∂ok (56) Case I and II: ReLU(.)",
and SoftPlus(.),
to transform logits to evidence.,
• Zero evidence region: For ReLU(.),
"based evidential models, if the logits value for class k i.e.",
"ok is negative, then the corresponding evidence for class k i.e.",
"ek = 0, ∂ek ∂ok = 0 & gradk = ∂LLog(x,y) ∂ok = 0.",
"So, there is no update to the model through the nodes that output negative logits value.",
In the case of SoftPlus(.),
"based evidential models, there is no update to the model when training samples lie in zero-evidence regions.",
This is possible in the condition of ok →−∞.,
"In other cases, there will be some small finite small update in the accurate direction from the gradient.",
• Range of gradients: The range of gradients for both ReLU(.),
and SoftPlus(.),
based evidential models are identical.,
"Considering the gradient for the ground truth node i.e.yk = 1, the range of gradients is [ 1 K −1, 0].",
For all other nodes other than the ground truth node i.e.,
"yk = 0, the range of gradients is [0, 1 K ].",
"So, for classification problems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth class will be bounded in a small range and is likely to be very small.",
• High incorrect evidence region: If the evidence for class k is very large i.e.,
"ek →∞, then for ReLU(.",
"), ∂ek ok = 1, and for SoftPlus(.",
"), ∂ek ok = Sigmoid(ok) →1, 1 αk = 1 ek+1 →0, 1 S →0, & gradk = ∂LLog(x,y) ∂ok →0.",
"For large positive model evidence, there is no update to the corresponding node of the neural network.",
"The evidence can be further broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect evidence (corresponding to the evidence for any other class other than the ground truth class).",
"When the correct class evidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which is desired.",
"When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence.",
"However, the evidential models with ReLU and Softplus fail to minimize incorrect evidence when the incorrect evidence value is large.",
These necessities the need for incorrect evidence regularization terms.,
Case III: exp(.),
to transform logits to evidence.,
Considering Eqn.,
"34 and Eqn 26, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (ek) =  1 S −yk αk  (αk −1) (57) • Zero evidence region: In case of exp(.)",
"based evidential models, except in the extreme cases of αk →∞, there will be some signal to guide the model.",
In cases outside the zero-evidence region (i.e.,
"outside αk →∞), there will be some finite small update in the accurate direction from the gradient.",
"Moreover, for same evidence values, the gradient of exp based model is larger than the SoftPlus based evidential model by a factor of 1 + exp(ok).",
"Compared to SoftPlus models, the larger gradient is expected to help the model learn faster in low-evidence regions.",
"• Range of gradients: For the ground truth node i.e.yk = 1, the range of gradients is [−1, 0].",
For all nodes other than the ground truth node i.e.,
"yk = 0, the range of gradients is [0, 1].",
"Thus, the gradients are expected to be more expressive and accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models.",
20 Learn to Accumulate Evidence from All Training Samples: Theory and Practice • High evidence region: If the evidence for class k is very large i.e.,
"ek →∞, then αk −1 ≈αk and gradk = smk −yk.",
"In other words, the model’s gradient updates become identical to the standard classification model (see Section A) without any learning issues.",
"Due to smaller zero-evidence region, more expressive gradients, and no issue of learning in high incorrect evidence region, the exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based evidential models.",
"E. Analysis of Evidential Losses Here, we analyze the three variants of evidential loss.",
"As seen in Section D, exp function is expected to be superior to ReLU and SoftPlus functions to transform the logits to evidence.",
"Thus, in this section, we consider exp function to transform the logits into evidence.",
"However, the analysis holds true for all three functions.",
Bayes risk with the sum of squares loss (Eqn.,
"21) LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (58) The loss can be simplified as LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (59) = 1 −2αgt S + P k α2 k S2 + 2 P i P j αiαj S2(S + 1) (60) = 1 −2αgt S + P k α2 k + 2 P i P j αiαj S2 + 2 P i P j αiαj S2(S + 1) − 2 P i P j αiαj S2 (61) = 2 −2αgt S + 2 P i P j αiαj S2 h 1 (S + 1) −1 i (62) = 2 −2αgt S − 2 P i P j αiαj S(S + 1) (63) The range of the two components in the loss is 0 ≤2αgt S + 2 P i P j αiαj S(S+1) ≤2 and the loss is bounded in the range [0, 2].",
"In other words, the loss for any sample in the entire sample space is bounded in the range of [0, 2] no matter how severe the mistake is.",
Such bounded loss is expected to restrict the model’s learning capacity.,
Bayes risk with cross-entropy loss (Eqn.,
"22) LCE(x, y) = K X j=1 yk  Ψ(S) −Ψ(αk)  = Ψ(S) −Ψ(αgt) (64) Where Ψ(.)",
"is the Digamma function, and Γ is the gamma function.",
"The functions and their gradients are defined as Γ(z) = e−γz z ∞ Y n=1  1 + z n −1 e z n (65) Ψ(z) = d dz log Γ(z) = d dz  −γz −log z + ∞ X n=1  z n −log  1 + z n  (66) = −γ −1 z + ∞ X n=1 1 n − 1 n + z (67) ∂Ψ(z) ∂z = ∂ ∂z  −γ −1 z + ∞ X n=1 1 n − 1 n + z  = 1 z2 + ∞ X n=1 1 (n + z)2 (68) 21 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Now, the Bayes risk with cross-entropy loss becomes LCE(x, y) = Ψ(S) −Ψ(αgt) (69) = 1 αgt −1 S + S ∞ X n=1 1 n(n + S) −αgt ∞ X n=1 1 n(n + αgt) (70) Both the infinite sums (P∞ n=1 1 n(n+S) and P∞ n=1 1 n(n+αgt)) converge and lie in the range of 0 to π2 6 .",
The minimum possible value of this loss is 0 when αgt →∞&S ≈αgt.,
The maximum possible value is ∞when only S →∞.,
"The loss lies in the range [0, ∞] and is more expressive compared to MSE-based evidential loss.",
Considering the gradient of the loss with respect to the ground truth node (i.e.,
"αgt, ygt = 1), ∂LCE(x, y) ∂αgt = ∂ ∂αgt Ψ(S) −Ψ(αgt) = 1 S2 + ∞ X n=1 1 (n + S)2 −1 α2 gt − ∞ X n=1 1 (n + αgt)2 (71) As αgt < S, the gradient is always negative.",
"Thus, the model aims to maximize the correct evidence αgt.",
Considering the gradient of the loss with respect to nodes not corresponding to the ground truth (i.e.,
"αk, k ̸= gt, yk = 0), ∂LCEx, y) ∂αk = ∂ ∂αk Ψ(S) −Ψ(αgt) = ∂Ψ(S) ∂S ∂S ∂αk = 1 S2 + ∞ X n=1 1 (n + S)2 (72) ∂LCEx, y) ∂ok = ∂LCEx, y) ∂αk × αk ok =  1 S2 + ∞ X n=1 1 (n + S)2  (αk −1) (73) The gradient at nodes that do not correspond to ground truth is always non-negative.",
"However, this gradient is also minimum and 0 when S →∞& αk →∞.",
This is an undesired behavior as the model may be encouraged to always increase the evidence for all the classes.,
"Moreover, the gradient is zero and there is no update to the nodes when S →∞, & αk →∞.",
"So, the incorrect evidence regularization to penalize the incorrect evidence is essential for the evidential model trained with this loss.",
Type II Maximum Likelihood loss (Eqn.,
"23) LLog(x, y) = K X k=1 yk  log(S) −log(αk)  = log(S) −log(αgt) (74) The loss is bounded in the range of [0, ∞] as the loss is minimum and 0 when αgt →S →∞, and maximum loss when αgt << S & S →∞.",
"Thus, the loss is more expressive compared to MSE based evidential loss.",
"Now, the gradient of the loss is given by ∂LLog(x, y) ∂ok = 1 S ∂S ∂ok −yk 1 αk ∂αk ∂ok =  1 S −yk αk ∂ek ∂ok =  1 S −yk αk  (αk −1) (75) Here, when S →∞& αk →∞, the gradient becomes ∂LLog(x,y) ∂ok →(1−yk).",
"This is highly desirable behavior for the model as it aims to minimize the evidence for the incorrect class and there will be no update to the node corresponding to the ground truth class if αk = αgt, ygt = 1.",
"Thus, the Type II based issue is expected to be superior to the other two losses as the range of loss is optimal (i.e.",
"in the range [0, ∞]), and no learning issue arises for samples with high incorrect evidence.",
"F. Additional Experiments and Results We first present the details of the models, hyperparameter settings, clarification regarding dead neuron issue, and experiments used in the work in Section F.1.",
"We then present additional results and discussions, including Few-shot classification, and 200-class tiny-ImageNet Classification results, that show the effectiveness of the proposed model RED in Section F.3.",
Finally discuss some limitations and potential future works in Section F.4.,
22 Learn to Accumulate Evidence from All Training Samples: Theory and Practice F.1.,
"Hyperparameter details For Table 1 results, λ1 = 1.0 was used for MNIST experiments, λ1 = 0.1 was used for Cifar10 experiments, and λ1 = 0.001 was used for Cifar100 experiments.",
"Table 8, 9, and 10 present complete results across the hyperparameter values and experiment settings.",
"MNIST model was trained on the LeNet model (Sensoy et al., 2018) for 50 epochs, and Cifar10/Cifar100 models were trained on Resnet-18 based classifier (He et al., 2016) for 200 epochs.",
"Few-shot classification experiments were carried out with λ1 = 0.1 using Resnet-12 based classifier (Chen et al., 2021).",
All results presented in this work are from local reproduction.,
"MNIST models were trained with learning rate of 0.0001 and Adam optimizer (Kingma & Ba, 2014), and all remaining models were trained with learning rate of 0.1 and Stochastic Gradient Descent optimizer with momentum.",
Tabular results represent the mean and standard deviation from 3 independent runs of the model.,
"In the proposed model RED, correct evidence regularization is weighted by the parameter λcor whose value is given by the predicted vacuity ν. λcor is treated as hyperparameter, i.e., constant weighting term in the loss during model update.",
"Dead Neuron Issue Clarification Instead of using ReLU as an activation function in a standard deep neural network, evidential models introduce ReLU as non-negative transformation function in the output layer to ensure that the predicted evidence is non-negative to satisfy the requirement of evidential theory.",
"This non-negative evidence vector parameterizes a Dirichlet prior for fine-grained uncertainty quantification that covers second-order uncertainty, including vacuity and dissonance.",
We theoretically and empirically show the learning deficiency of ReLU based evidential models and justify the advantage of using an exponential function to output (non-negative) evidence.,
We further introduce a correct evidence regularization term in the loss that addresses the learning deficiency from zero-evidence samples.,
"The “dead neuron” issue in the activation functions has been studied, and ReLU variations such as Exponential Linear Unit, Parametric ReLU, and Leaky ReLU have been developed to address the issue.",
"But, these activation functions will not be theoretically sound in the evidential framework as they are can lead to negative evidences.",
"In this case, they can not serve as Dirichlet parameters that are interpreted as pseudo counts.",
Effectiveness of Regularized Evidential Model (RED) F.3.1.,
EVIDENTIAL ACTIVATION FUNCTION.,
"In this section, we present additional results (for section 5.2) with the MNIST classification problem using the LeNet model to empirically validate Theorem 2.",
"We carry out experiments for evidential models trained using all three evidential losses: Evidential MSE loss in (21), Evidential cross-entropy loss in (22), and Evidential Log loss in (23) with λ1 = {0.0, 1.0, &10.0}.",
"As can be seen in Figure 15, 16, and 17, using exp activation for transforming logits to evidence leads to superior performance in all settings compared to ReLU and Softplus based evidential models that empirically validates Theorem 2.",
(a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 15.,
Impact of Evidential Activation to the test set accuracy of the model trained with MSE based evidential loss (Eqn.,
CORRECT EVIDENCE REGULARIZATION We introduce the novel correct evidence regularization term to train the evidential model (Section 4.1).,
"In this section, we present additional results for the evidential model that uses exp activation.",
"We trained the model using evidential losses with different incorrect evidence regularization strengths ( λ1 = 0, 1.0 & 10.0).",
"As can be seen( Figure 18, and 19), the model with proposed correct-evidence regularization leads to improved generalization compared to the baseline model 23 Learn to Accumulate Evidence from All Training Samples: Theory and Practice (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 16.",
Impact of Evidential Activation to test set accuracy of the model trained with cross-entropy based evidential loss (Eqn.,
22) (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 17.,
Impact of Evidential Activation to the test set accuracy of the model trained with Type II based evidential loss (Eqn.,
23) as the proposed correct-evidence regularization term enables the evidential model to learn from zero-evidence samples instead of ignoring them.,
"Moreover, even though strong incorrect evidence regularization hurts both model’s generalization, the proposed regularization leads to a more robust model that generalizes better.",
"Finally, the MSE-based evidential model is hurt the most with strong incorrect evidence regularization as thee MSE based evidential loss is bounded in the range [0, 2], and the incorrect evidence-regularization term may easily dominate the overall loss compared to other evidential losses.",
This can be seen in Figure 18(c) where the incorrect evidence regularization strength is large i.e.,
λ1 = 10.0 and the evidential model fails to train.,
"Due to strong incorrect evidence regularization, the model may have learned to map all training samples to zero-evidence region.",
"However, with the proposed regularization, the model continues to learn and achieves good generalization performance.",
(a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 18.,
Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidential model( Trained with Eqn.,
FEW-SHOT CLASSIFICATION EXPERIMENTS Ideas presented in this work address the fundamental limitation of evidential classification framework that enables the evidential model to acquire knowledge from all the training samples.,
"Using these ideas, evidential framework can be extended to challenging classification problems to the reasonable predictive performance.",
"To this end, we experiment with few-shot classification using 1-shot and 5-shot classification for the mini-ImageNet dataset (Vinyals et al., 2016).",
We 24 Learn to Accumulate Evidence from All Training Samples: Theory and Practice (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 (c) Trend for λ1 = 10.0 Figure 19.,
Impact of proposed Correct Evidence Regularization to the test set accuracy of the evidnetial model (Trained with Eqn.,
"22) consider the ResNet-12 backbone, classifier-baseline model (Chen et al., 2021), and its evidential extension.",
Table 7 shows the results for 1-shot and 5-shot classification experiments.,
"As can be seen, the ReLU and Softplus based evidential models have suboptimal performance as they avoid many training samples of the zero-evidence region.",
"In contrast, the exp model has a better learning capacity that leads to superior performance.",
"Finally, the proposed model RED can learn from all training samples, which leads to the best generalization performance among all the evidential models.",
Few-Shot Classification Accuracy comparison: mini-ImageNet dataset Standard CE Model: 1 Shot: 57.9±0.2%; 5-Shot: 76.9±0.2% 1-Shot Experiments Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 38.78±3.75 51.60±0.40 57.11±0.09 56.27±0.15 λ1 = 0.100 31.15±1.69 48.87±0.21 56.43±0.03 58.03±0.39 λ1 = 1.000 20.00±0.00 43.81±0.56 27.43±0.88 54.68±0.45 5-Shot Experiments Regularization ReLU SoftPlus exp Ours λ1 = 0.000 52.66±5.32 67.22±0.17 75.87±0.09 75.31±0.13 λ1 = 0.100 43.95±3.72 66.14±0.05 74.08±0.13 76.05±0.17 λ1 = 1.000 20.00±0.00 61.96±0.61 34.01±1.46 72.32±0.20 F.3.4.,
"COMPLEX DATASET/MODEL EXPERIMENTS We also carry out experiment for a challenging 200-class classification problem over Tiny-ImageNet based on (Huynh, 2022).",
"We adapt the Swin Transformer to be evidential, and train all the models for 20 epochs with Evidential log loss (Eqn.",
"In this setting, ReLU based evidential model achieves 85.25% accuracy, softplus based model achieves 85.15 % accuracy, the exponential model improves over both to achieve 89.93 % accuracy, and our proposed model RED outperforms all the evidential models to achieve the greatest accuracy of 90.14%, empirically validating our theoretical analysis.",
Limitations and Future works We carried out a theoretical investigation of the Evidential Classification models to identify their fundamental limitation: their inability to learn from zero evidence regions.,
The empirical study in this work is based on classification problems.,
We next plan to extend the ideas to develop Evidential Segmentation and Evidential Object Detection models.,
"Moreover, this work identifies limitations of Evidential MSE loss in (21), and we plan to carry out a thorough theoretical analysis to analyze other evidential losses given in (23) and (22)).",
"The proposed evidential model, similar to existing evidential classification models, requires hyperparameter tuning for λ1 i.e.",
the incorrect evidence regularization hyperparameter.,
"In addition, extending evidential models to noisy and incomplete data settings and investigating the benefits of leveraging uncertainty information could be interesting future work.",
"Finally, It will be an interesting future work to extend the analysis and evidential models to tasks beyond classification, for instance to build effective evidential segmentation and object detection models.",
25 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Table 8.,
Classification performance comparison: MNIST dataset Standard CE Model: 99.21±0.03% Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 97.06±0.19 97.07±0.24 98.85±0.03 98.82±0.04 λ1 = 1.000 98.19±0.08 98.21±0.05 98.79±0.02 99.10±0.02 λ1 = 10.000 83.17±4.54 80.37±18.70 98.14±0.07 98.84±0.03 Evidential CE loss λ1 = 0.000 97.03±0.21 97.09±0.21 98.84±0.02 98.81±0.01 λ1 = 1.000 98.27±0.02 98.36±0.02 98.87±0.03 99.12±0.02 λ1 = 10.000 97.46±1.02 97.14±1.42 98.31±0.07 98.84±0.04 Evidential MSE loss λ1 = 0.000 96.18±0.02 96.20±0.03 98.42±0.03 98.41±0.06 λ1 = 1.000 97.41±0.22 97.45±0.16 98.35±0.05 99.02±0.00 λ1 = 10.000 19.93±6.98 27.14±6.37 27.17±3.72 98.76±0.03 Table 9.,
Classification performance comparison: Cifar10 Dataset Standard CE Model: 95.43±0.02% Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 43.83±14.60 95.19±0.10 95.35±0.02 95.03±0.14 λ1 = 0.100 41.43±19.60 95.18±0.11 95.11±0.10 95.24±0.06 λ1 = 1.000 38.42±15.64 94.94±0.22 93.95±0.06 94.78±0.17 λ1 = 10.000 10.00±0.00 32.42±6.99 23.29±5.24 90.96±0.35 λ1 = 50.000 10.00±0.00 10.00±0.00 12.47±3.49 65.09±0.74 Evidential CE loss λ1 = 0.000 79.19±16.06 95.32±0.17 95.38±0.10 95.40±0.14 λ1 = 0.100 75.97±20.56 95.12±0.05 95.33±0.03 95.08±0.07 λ1 = 1.000 75.83±20.74 94.99±0.08 94.65±0.04 94.74±0.11 λ1 = 10.000 10.00±0.00 89.63±0.38 56.54±4.80 91.71±0.23 λ1 = 50.000 10.00±0.00 27.03±2.62 25.33±6.66 62.98±0.84 Evidential MSE loss λ1 = 0.000 95.43±0.05 95.35±0.15 95.10±0.04 94.92±0.12 λ1 = 0.100 95.15±0.10 95.04±0.05 95.14±0.03 95.03±0.13 λ1 = 1.000 49.68±29.48 93.51±0.03 18.98±1.82 94.90±0.20 λ1 = 10.000 10.00±0.00 10.00±0.00 10.00±0.00 90.15±0.71 λ1 = 50.000 10.00±0.00 10.00±0.00 10.00±0.00 27.11±24.20 26 Learn to Accumulate Evidence from All Training Samples: Theory and Practice Table 10.,
Classification performance comparison: Cifar100 dataset Standard CE Model: 75.67 ± 0.11 Log loss Regularization ReLU SoftPlus exp RED (Ours) λ1 = 0.000 56.69±5.83 73.85±0.20 76.25±0.16 76.26±0.27 λ1 = 0.001 61.27±3.79 74.48±0.17 76.12±0.04 76.43±0.21 λ1 = 0.010 54.20±5.93 75.56±0.43 76.02±0.16 76.14±0.09 λ1 = 0.100 20.29±4.54 75.67±0.22 72.72±0.26 74.62±0.21 λ1 = 1.000 1.00±0.00 37.60±0.82 2.59±0.52 68.62±0.03 λ1 = 2.000 1.00±0.00 1.57±0.35 0.97±0.06 62.33±0.52 Evidential CE loss λ1 = 0.000 66.37±3.47 73.73±0.38 75.91±0.20 76.19±0.22 λ1 = 0.001 68.62±2.41 74.44±0.08 76.23±0.09 76.35±0.06 λ1 = 0.010 71.94±0.66 75.45±0.12 75.95±0.14 76.13±0.24 λ1 = 0.100 67.25±1.84 75.75±0.21 74.02±0.09 74.69±0.13 λ1 = 1.000 1.00±0.00 73.10±0.20 37.36±0.73 69.40±0.16 λ1 = 2.000 1.00±0.00 52.99±0.56 12.94±1.11 63.93±0.34 Evidential MSE loss λ1 = 0.000 35.76±2.81 20.45±1.41 75.70±0.47 75.55±0.24 λ1 = 0.001 31.49±0.31 15.74±0.47 42.95±0.76 75.73±0.27 λ1 = 0.010 13.60±2.44 1.00±0.00 1.00±0.00 75.35±0.16 λ1 = 0.100 1.00±0.00 1.00±0.00 1.00±0.00 74.00±0.13 λ1 = 1.000 1.00±0.00 1.00±0.00 1.00±0.00 66.61±0.46 λ1 = 2.000 1.00±0.00 1.00±0.00 1.00±0.00 63.01±0.83 27,
The Modern Mathematics of Deep Learning∗ Julius Berner† Philipp Grohs‡ Gitta Kutyniok§ Philipp Petersen‡ Abstract We describe the new ﬁeld of mathematical analysis of deep learning.,
This ﬁeld emerged around a list of research questions that were not answered within the classical framework of learning theory.,
"These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which ﬁne aspects of an architecture aﬀect the behavior of a learning task in which way.",
We present an overview of modern approaches that yield partial answers to these questions.,
"For selected approaches, we describe the main ideas in more detail.",
Contents 1 Introduction 2 1.1 Notation .,
4 1.2 Foundations of learning theory .,
4 1.3 Do we need a new theory?,
17 2 Generalization of large neural networks 22 2.1 Kernel regime .,
23 2.2 Norm-based bounds and margin theory .,
24 2.3 Optimization and implicit regularization .,
25 2.4 Limits of classical theory and double descent .,
27 3 The role of depth in the expressivity of neural networks 29 3.1 Approximation of radial functions .,
29 3.2 Deep ReLU networks .,
31 3.3 Alternative notions of expressivity .,
32 4 Deep neural networks overcome the curse of dimensionality 34 4.1 Manifold assumption .,
34 4.2 Random sampling .,
35 4.3 PDE assumption .,
36 5 Optimization of deep neural networks 39 5.1 Loss landscape analysis .,
39 5.2 Lazy training and provable convergence of stochastic gradient descent .,
41 ∗A version of this review paper appears as a chapter in the book “Mathematical Aspects of Deep Learning” by Cambridge University Press.,
"†Faculty of Mathematics, University of Vienna.",
"‡Faculty of Mathematics and Research Network DataScience@UniVienna, University of Vienna.",
"§Department of Mathematics, Ludwig Maximilian University of Munich, and Department of Physics and Technology, University of Tromsø.",
1 arXiv:2105.04026v2 [cs.LG] 8 Feb 2023 6 Tangible eﬀects of special architectures 44 6.1 Convolutional neural networks .,
45 6.2 Residual neural networks .,
46 6.3 Framelets and U-Nets .,
47 6.4 Batch normalization .,
49 6.5 Sparse neural networks and pruning .,
50 6.6 Recurrent neural networks .,
52 7 Describing the features a deep neural network learns 52 7.1 Invariances and the scattering transform .,
52 7.2 Hierarchical sparse representations .,
53 8 Eﬀectiveness in natural sciences 55 8.1 Deep neural networks meet inverse problems .,
55 8.2 PDE-based models .,
56 1 Introduction Deep learning has undoubtedly established itself as the outstanding machine learning technique of recent times.,
This dominant position was claimed through a series of overwhelming successes in widely diﬀerent application areas.,
"Perhaps the most famous application of deep learning and certainly one of the ﬁrst where these techniques became state-of-the-art is image classiﬁcation [LBBH98, KSH12, SLJ+15, HZRS16].",
"In this area, deep learning is nowadays the only method that is seriously considered.",
The prowess of deep learning classiﬁers goes so far that they often outperform humans in image labelling tasks [HZRS15].,
"A second famous application area is the training of deep-learning-based agents to play board games or computer games, such as Atari games [MKS+13].",
"In this context, probably the most prominent achievement yet is the development of an algorithm that beat the best human player in the game of Go [SHM+16, SSS+17]— a feat that was previously unthinkable owing to the extreme complexity of this game.",
"Besides, even in multiplayer, team-based games with incomplete information deep-learning-based agents nowadays outperform world-class human teams [BBC+19, VBC+19].",
"In addition to playing games, deep learning has also led to impressive breakthroughs in the natural sciences.",
"For example, it is used in the development of drugs [MSL+15], molecular dynamics [FHH+17], or in high-energy physics [BSW14].",
One of the most astounding recent breakthroughs in scientiﬁc applications is the development of a deep-learning-based predictor for the folding behavior of proteins [SEJ+20].,
This predictor is the ﬁrst method to match the accuracy of lab-based methods.,
"Finally, in the vast ﬁeld of natural language processing, which includes the subtasks of understanding, summarizing, or generating text, impressive advances were made based on deep learning.",
"Here, we refer to [YHPC18] for an overview.",
"One technique that recently stood out is based on a so-called transformer neural network [BCB15, VSP+17].",
"This network structure gave rise to the impressive GPT-3 model [BMR+20] which not only creates coherent and compelling texts but can also produce code, such as, for the layout of a webpage according to some instructions that a user inputs in plain English.",
"Transformer neural networks have also been successfully employed in the ﬁeld of symbolic mathematics [SGHK18, LC19].",
"In this article, we present and discuss the mathematical foundations of the success story outlined above.",
"More precisely, our goal is to outline the newly emerging ﬁeld of mathematical analysis of deep learning.",
"To accurately describe this ﬁeld, a necessary preparatory step is to sharpen our deﬁnition of the term deep learning.",
"For the purposes of this article, we will use the term in the following narrow sense: Deep learning refers to techniques where deep neural networks1 are trained with gradient-based methods.",
"This narrow 1We will deﬁne the term neural network later but, for this deﬁnition, one can view it as a parametrized family of functions with a diﬀerentiable parametrization.",
2 deﬁnition is helpful to make this article more concise.,
"We would like to stress, however, that we do not claim in any way that this is the best or the right deﬁnition of deep learning.",
"Having ﬁxed a deﬁnition of deep learning, three questions arise concerning the aforementioned emerging ﬁeld of mathematical analysis of deep learning: To what extent is a mathematical theory necessary?",
Is it truly a new ﬁeld?,
What are the questions studied in this area?,
Let us start by explaining the necessity of a theoretical analysis of the tools described above.,
"From a scientiﬁc perspective, the primary reason why deep learning should be studied mathematically is simple curiosity.",
"As we will see throughout this article, many practically observed phenomena in this context are not explained theoretically.",
"Moreover, theoretical insights and the development of a comprehensive theory are often the driving force underlying the development of new and improved methods.",
Prominent examples of mathematical theories with such an eﬀect are the theory of ﬂuid mechanics which is an invaluable asset to the design of aircraft or cars and the theory of information that aﬀects and shapes all modern digital communication.,
"In the words of Vladimir Vapnik2: “Nothing is more practical than a good theory”, [Vap13, Preface].",
"In addition to being interesting and practical, theoretical insight may also be necessary.",
"Indeed, in many applications of machine learning, such as medical diagnosis, self-driving cars, and robotics, a signiﬁcant level of control and predictability of deep learning methods is mandatory.",
"Also, in services, such as banking or insurance, the technology should be controllable to guarantee fair and explainable decisions.",
Let us next address the claim that the ﬁeld of mathematical analysis of deep learning is a newly emerging area.,
"In fact, under the aforementioned deﬁnition of deep learning, there are two main ingredients of the technology: deep neural networks and gradient-based optimization.",
The ﬁrst artiﬁcial neuron was already introduced in 1943 in [MP43].,
This neuron was not trained but instead used to explain a biological neuron.,
The ﬁrst multi-layered network of such artiﬁcial neurons that was also trained can be found in [Ros58].,
"Since then, various neural network architectures have been developed.",
We will discuss these architectures in detail in the following sections.,
"The second ingredient, gradient-based optimization, is made possible by the observation that due to the graph-based structure of neural networks the gradient of an objective function with respect to the parameters of the neural network can be computed eﬃciently.",
"This has been observed in various ways, see [Kel60, Dre62, Lin70, RHW86].",
"Again, these techniques will be discussed in the upcoming sections.",
"Since then, techniques have been improved and extended.",
"As the rest of the manuscript is spent reviewing these methods, we will keep the discussion of literature at this point brief.",
"Instead, we refer to some overviews of the history of deep learning from various perspectives: [LBH15, Sch15, GBC16, HH19].",
"Given the fact that the two main ingredients of deep neural networks have been around for a long time, one would expect that a comprehensive mathematical theory has been developed that describes why and when deep-learning-based methods will perform well or when they will fail.",
"Statistical learning theory [AB99, Vap99, CS02, BBL03, Vap13] describes multiple aspects of the performance of general learning methods and in particular deep learning.",
We will review this theory in the context of deep learning in Subsection 1.2 below.,
"Hereby, we focus on classical, deep learning-related results that we consider well-known in the machine learning community.",
"Nonetheless, the choice of these results is guaranteed to be subjective.",
"We will ﬁnd that the presented, classical theory is too general to explain the performance of deep learning adequately.",
"In this context, we will identify the following questions that appear to be diﬃcult to answer within the classical framework of learning theory: Why do trained deep neural networks not overﬁt on the training data despite the enormous power of the architecture?",
What is the advantage of deep compared to shallow architectures?,
Why do these methods seemingly not suﬀer from the curse of dimensionality?,
"Why does the optimization routine often succeed in ﬁnding good solutions despite the non-convexity, non-linearity, and often non-smoothness of the problem?",
Which aspects of an architecture aﬀect the performance of the associated models and how?,
Which features of data are learned by deep architectures?,
Why do these methods perform as well as or better than specialized numerical tools in natural sciences?,
The new ﬁeld of mathematical analysis of deep learning has emerged around questions like the ones listed above.,
"In the remainder of this article, we will collect some of the main recent advances to answer these questions.",
"Because this ﬁeld of mathematical analysis of deep learning is incredibly active and new material is added at breathtaking speeds, a brief survey on recent advances in this area is guaranteed to miss not only 2This claim can be found earlier in a non-mathematical context in the works of Kurt Lewin [Lew43].",
3 a couple of references but also many of the most essential ones.,
"Therefore, we do not strive for a complete overview, but instead, showcase several fundamental ideas on a mostly intuitive level.",
"In this way, we hope to allow the reader to familiarize themselves with some exciting concepts and provide a convenient entry-point for further studies.",
"1.1 Notation We denote by N the set of natural numbers, by Z the set of integers and by R the ﬁeld of real numbers.",
"For N ∈N, we denote by [N] the set {1, .",
"For two functions f, g: X →[0, ∞), we write f ≲g, if there exists a universal constant c such that f(x) ≤cg(x) for all x ∈X.",
"In a pseudometric space (X, dX ), we deﬁne the ball of radius r ∈(0, ∞) around a point x ∈X by BdX r (x) or Br(x) if the pseudometric dX is clear from the context.",
"By ∥· ∥p, p ∈[1, ∞], we denote the ℓp-norm, and by ⟨·, ·⟩the Euclidean inner product of given vectors.",
By ∥· ∥op we denote the operator norm induced by the Euclidean norm and by ∥· ∥F the Frobenius norm of given matrices.,
"For p ∈[1, ∞], s ∈[0, ∞), d ∈N, and X ⊂Rd, we denote by W s,p(X) the Sobolev-Slobodeckij space, which for s = 0 is just a Lebesgue space, i.e., W 0,p(X) = Lp(X).",
"For measurable spaces X and Y, we deﬁne M(X, Y) to be the set of measurable functions from X to Y.",
"We denote by ˆg the Fourier transform3 of a tempered distribution g. For probabilistic statements, we will assume a suitable underlying probability space with probability measure P. For an X-valued random variable X, we denote by E[X] and V[X] its expectation and variance and by PX the image measure of X on X, i.e., PX(A) = P(X ∈A) for every measurable set A ⊂X.",
"If possible, we use the corresponding lowercase letter to denote the realization x ∈X of the random variable X for a given outcome.",
"We write Id for the d-dimensional identity matrix and, for a set A, we write 1A for the indicator function of A, i.e., 1A(x) = 1 if x ∈A and 1A(x) = 0 else.",
"1.2 Foundations of learning theory Before we continue to describe recent developments in the mathematical analysis of deep learning methods, we start by providing a concise overview of the classical mathematical and statistical theory underlying machine learning tasks and algorithms which, in their most general form, can be formulated as follows.",
Deﬁnition 1.1 (Learning - informal).,
"Let X, Y, and Z be measurable spaces.",
"In a learning task, one is given data in Z and a loss function L: M(X, Y) × Z →R.",
"The goal is to choose a hypothesis set F ⊂M(X, Y) and construct a learning algorithm, i.e., a mapping A: [ m∈N Zm →F, that uses training data s = (z(i))m i=1 ∈Zm to ﬁnd a model fs = A(s) ∈F that performs well on the training data s and also generalizes to unseen data z ∈Z.",
"Here, performance is measured via the loss function L and the corresponding loss L(fs, z) and, informally speaking, generalization means that the out-of-sample performance of fs at z behaves similar to the in-sample performance on s. Deﬁnition 1.1 is deliberately vague on how to measure generalization performance.",
"Later, we will often study the expected out-of-sample performance.",
"To talk about expected performance, a data distribution needs to be speciﬁed.",
We will revisit this point in Assumption 1.10 and Deﬁnition 1.11.,
"For simplicity, we focus on one-dimensional, supervised prediction tasks with input features in Euclidean space as deﬁned in the following.",
Deﬁnition 1.2 (Prediction task).,
"In a prediction task, we have that Z := X × Y, i.e., we are given training data s = ((x(i), y(i)))m i=1 that consist of input features x(i) ∈X and corresponding labels y(i) ∈Y.",
"For one-dimensional regression tasks with Y ⊂R, we consider the quadratic loss L(f, (x, y)) = (f(x) −y)2 and, 3Respecting common notation, we will also use the hat symbol to denote the minimizer of the empirical risk bfs in Deﬁnition 1.8 but this clash of notation does not cause any ambiguity.",
"4 for binary classiﬁcation tasks with Y = {−1, 1}, we consider the 0-1 loss L(f, (x, y)) = 1(−∞,0)(yf(x)).",
"We assume that our input features are in Euclidean space, i.e., X ⊂Rd with input dimension d ∈N.",
"In a prediction task, we aim for a model fs : X →Y, such that, for unseen pairs (x, y) ∈X × Y, fs(x) is a good prediction of the true label y.",
"However, note that large parts of the presented theory can be applied to more general settings.",
Remark 1.3 (Learning tasks).,
"Apart from straightforward extensions to multi-dimensional prediction tasks and other loss functions, we want to mention that unsupervised and semi-supervised learning tasks are often treated as prediction tasks.",
"More precisely, one transforms unlabeled training data z(i) into features x(i) = T1(z(i)) ∈X and labels y(i) = T2(z(i)) ∈Y using suitable transformations T1 : Z →X, T2 : Z →Y.",
"In doing so, one asks for a model fs approximating the transformation T2 ◦T −1 1 : X →Y which is, e.g., done in order to learn feature representations or invariances.",
"Furthermore, one can consider density estimation tasks, where X = Z, Y := [0, ∞], and F consists of probability densities with respect to some σ-ﬁnite reference measure µ on Z.",
One then aims for a probability density fs that approximates the density of the unseen data z with respect to µ.,
"One can perform L2(µ)- approximation based on the discretization L(f, z) = −2f(z) + ∥f∥2 L2(µ) or maximum likelihood estimation based on the surprisal L(f, z) = −log(f(z)).",
"In deep learning the hypothesis set F consists of realizations of neural networks Φa(·, θ), θ ∈P, with a given architecture a and parameter set P. In practice, one uses the term neural network for a range of functions that can be represented by directed acyclic graphs, where the vertices correspond to elementary almost everywhere diﬀerentiable functions parametrizable by θ ∈P and the edges symbolize compositions of these functions.",
"In Section 6, we will review some frequently used architectures, in the other sections, however, we will mostly focus on fully connected feedforward (FC) neural networks as deﬁned below.",
Deﬁnition 1.4 (FC neural network).,
"A fully connected feedforward neural network is given by its architecture a = (N, ϱ), where L ∈N, N ∈NL+1, and ϱ: R →R.",
"We refer to ϱ as the activation function, to L as the number of layers, and to N0, NL, and Nℓ, ℓ∈[L −1], as the number of neurons in the input, output, and ℓ-th hidden layer, respectively.",
"We denote the number of parameters by P(N) := L X ℓ=1 NℓNℓ−1 + Nℓ and deﬁne the corresponding realization function Φa : RN0 × RP (N) →RNL which satisﬁes for every input x ∈RN0 and parameters θ = (θ(ℓ))L ℓ=1 = ((W (ℓ), b(ℓ)))L ℓ=1 ∈ L× ℓ=1 (RNℓ×Nℓ−1 × RNℓ) ∼= RP (N) that Φa(x, θ) = Φ(L)(x, θ), where Φ(1)(x, θ) = W (1)x + b(1), ¯Φ(ℓ)(x, θ) = ϱ  Φ(ℓ)(x, θ)  , ℓ∈[L −1], and Φ(ℓ+1)(x, θ) = W (ℓ+1) ¯Φ(ℓ)(x, θ) + b(ℓ+1), ℓ∈[L −1], (1.1) and ϱ is applied componentwise.",
"We refer to W (ℓ) ∈RNℓ×Nℓ−1 and b(ℓ) ∈RNℓas the weight matrices and bias vectors, and to ¯Φ(ℓ) and Φ(ℓ) as the activations and pre-activations of the Nℓneurons in the ℓ-th layer.",
The width and depth of the architecture are given by ∥N∥∞and L and we call the architecture deep if L > 2 and shallow if L = 2.,
"The underlying directed acyclic graph of FC networks is given by compositions of the aﬃne linear maps x 7→W (ℓ)x + b(ℓ), ℓ∈[L], with the activation function ϱ intertwined, see Figure 1.1.",
"Typical activation 5 x1 x2 x3 Φ(1) 1 Φ(1) 2 Φ(1) 3 Φ(1) 4 x 7→W (1)x + b(1) ¯Φ(1) 1 ¯Φ(1) 2 ¯Φ(1) 3 ¯Φ(1) 4 ϱ Φ(2) 1 Φ(2) 2 Φ(2) 3 Φ(2) 4 Φ(2) 5 Φ(2) 6 x 7→W (2)x + b(2) ¯Φ(2) 1 ¯Φ(2) 2 ¯Φ(2) 3 ¯Φ(2) 4 ¯Φ(2) 5 ¯Φ(2) 6 ϱ Φa x 7→W (3)x + b(3) Figure 1.1: Graph (grey) and (pre-)activations of the neurons (white) of a deep fully connected feedforward neural network Φa : R3 × R53 7→R with architecture a = ((3, 4, 6, 1), ϱ) and parameters θ = ((W (ℓ), b(ℓ))3 ℓ=1.",
"functions used in practice are variants of the rectiﬁed linear unit (ReLU) given by ϱR(x) := max{0, x} and sigmoidal functions ϱ ∈C(R) satisfying ϱ(x) →1 for x →∞and ϱ(x) →0 for x →−∞, such as the logistic function ϱσ(x) := 1/(1+e−x) (often referred to as the sigmoid function).",
See also Table 1 for a comprehensive list of widely used activation functions.,
Remark 1.5 (Neural networks).,
"If not further speciﬁed, we will use the term (neural) network, or the abbreviation NN, to refer to FC neural networks.",
"Note that many of the architectures used in practice (see Section 6) can be written as special cases of Deﬁnition 1.4 where, e.g., speciﬁc parameters are prescribed by constants or shared with other parameters.",
"Furthermore, note that aﬃne linear functions are NNs with depth L = 1.",
"We will also consider biasless NNs given by linear mappings without bias vector, i.e., b(ℓ) = 0, ℓ∈[L].",
"In particular, any NN can always be written without bias vectors by redeﬁning x → x 1  , (W (ℓ), b(ℓ)) →  W (ℓ) b(ℓ) 0 1  , ℓ∈[L −1], and (W (L), b(L)) →  W (L) b(L) .",
"To enhance readability we will often not specify the underlying architecture a = (N, ϱ) or the parameters θ ∈ RP (N) and use the term NN to refer to the architecture as well as the realization functions Φa(·, θ): RN0 →RNL or Φa : RN0 ×RP (N) →RNL.",
"However, we want to emphasize that one cannot infer the underlying architecture or properties like magnitude of parameters solely from these functions as the mapping (a, θ) 7→Φa(·, θ) is highly non-injective.",
"As an example, we can set W (L) = 0 which implies Φa(·, θ) = b(L) for all architectures a = (N, ϱ) and all values of (W (ℓ), b(ℓ))L−1 ℓ=1 .",
"In view of our considered prediction tasks in Deﬁnition 1.2, this naturally leads to the following hypothesis sets of neural networks.",
Deﬁnition 1.6 (Hypothesis sets of neural networks).,
"Let a = (N, ϱ) be a NN architecture with input dimension N0 = d, output dimension NL = 1, and measurable activation function ϱ.",
"For regression tasks the corresponding hypothesis set is given by Fa =  Φa(·, θ): θ ∈RP (N) and for classiﬁcation tasks by Fa,sgn =  sgn(Φa(·, θ)): θ ∈RP (N) , where sgn(x) := ( 1, if x ≥0, −1, if x < 0.",
"6 Name Given as a function of x ∈R by Plot linear x Heaviside / step function 1(0,∞)(x) logistic / sigmoid 1 1+e−x rectiﬁed linear unit (ReLU) max{0, x} power rectiﬁed linear unit max{0, x}k for k ∈N parametric ReLU (PReLU) max{ax, x} for a ≥0, a ̸= 1 exponential linear unit (ELU) x · 1[0,∞)(x) + (ex −1) · 1(−∞,0)(x) softsign x 1+|x| inverse square root linear unit x · 1[0,∞)(x) + x √ 1+ax2 · 1(−∞,0)(x) for a > 0 inverse square root unit x √ 1+ax2 for a > 0 tanh ex−e−x ex+e−x arctan arctan(x) softplus ln(1 + ex) Gaussian e−x2/2 Table 1: List of commonly used activation functions.",
"7 Note that we compose the output of the NN with the sign function in order to obtain functions mapping to Y = {−1, 1}.",
This can be generalized to multi-dimensional classiﬁcation tasks by replacing the sign by an argmax function.,
"Given a hypothesis set, a popular learning algorithm is empirical risk minimization (ERM), which minimizes the average loss on the given training data, as described in the next deﬁnitions.",
Deﬁnition 1.7 (Empirical risk).,
"For training data s = (z(i))m i=1 ∈Zm and a function f ∈M(X, Y), we deﬁne the empirical risk by bRs(f) := 1 m m X i=1 L(f, z(i)).",
Deﬁnition 1.8 (ERM learning algorithm).,
"Given a hypothesis set F, an empirical risk minimization algorithm Aerm chooses4 for training data s ∈Zm a minimizer bfs ∈F of the empirical risk in F, i.e., Aerm(s) ∈arg min f∈F bRs(f).",
(1.2) Remark 1.9 (Surrogate loss and regularization).,
"Note that, for classiﬁcation tasks, one needs to optimize over non-diﬀerentiable functions with discrete outputs in (1.2).",
"For NN hypothesis sets Fa,sgn one typically uses the corresponding hypothesis set for regression tasks Fa to ﬁnd an approximate minimizer bf surr s ∈Fa of 1 m m X i=1 Lsurr(f, z(i)), where Lsurr : M(X, R) × Z →R is a surrogate loss guaranteeing that sgn( bf surr s ) ∈arg minf∈Fa,sgn bRs(f).",
"A frequently used surrogate loss is the logistic loss5 given by Lsurr(f, z) = log  1 + e−yf(x) .",
"In various learning tasks one also adds regularization terms to the minimization problem in (1.2), such as penalties on the norm of the parameters of the NN, i.e., min θ∈RP (N) bRs(Φa(·, θ)) + α∥θ∥2 2, where α ∈(0, ∞) is a regularization parameter.",
"Note that in this case the minimizer depends on the chosen parameters θ and not only on the realization function Φa(·, θ), see also Remark 1.5.",
"Coming back to our initial, informal description of learning in Deﬁnition 1.1, we have now outlined potential learning tasks in Deﬁnition 1.2, NN hypothesis sets in Deﬁnition 1.6, a metric for the in-sample performance in Deﬁnition 1.7, and a corresponding learning algorithm in Deﬁnition 1.8.",
"However, we are still lacking a mathematical concept to describe the out-of-sample (generalization) performance of our learning algorithm.",
"This question has been intensively studied in the ﬁeld of statistical learning theory, see Section 1 for various references.",
"In this ﬁeld one usually establishes a connection between unseen data z and the training data s = (z(i))m i=1 by imposing that z and z(i), i ∈[m], are realizations of independent samples drawn from the same distribution.",
Assumption 1.10 (Independent and identically distributed data).,
"We assume that z(1), .",
", z(m), z are realizations of i.i.d.",
"random variables Z(1), .",
"4For simplicity, we assume that the minimum is attained which, for instance, is the case if F is a compact topological space on which b Rs is continuous.",
"Hypothesis sets of NNs F(N,ϱ) constitute a compact space if, e.g., one chooses a compact parameter set P ⊂RP (N) and a continuous activation function ϱ.",
"One could also work with approximate minimizers, see [AB99].",
5This can be viewed as cross-entropy between the label y and the output of f composed with a logistic function ϱσ.,
In a multi-dimensional setting one can replace the logistic function with a softmax function.,
"8 In this formal setting, we can compute the average out-of-sample performance of a model.",
"Recall from our notation in Section 1.1 that we denote by PZ the image measure of Z on Z, which is the underlying distribution of our training data S = (Z(i))m i=1 ∼Pm Z and unknown data Z ∼PZ.",
Deﬁnition 1.11 (Risk).,
"For a function f ∈M(X, Y), we deﬁne6 the risk by R(f) := E  L(f, Z)  = Z Z L(f, z) dPZ(z).",
"Deﬁning S := (Z(i))m i=1, the risk of a model fS = A(S) is thus given by R(fS) = E  L(fS, Z)|S  .",
"For prediction tasks, we can write Z = (X, Y ), such that the input features and labels are given by an X-valued random variable X and a Y-valued random variable Y , respectively.",
"Note that for classiﬁcation tasks the risk equals the probability of misclassiﬁcation R(f) = E[1(−∞,0)(Y f(X))] = P[f(X) ̸= Y ].",
"For noisy data, there might be a positive, lower bound on the risk, i.e., an irreducible error.",
"If the lower bound on the risk is attained, one can also deﬁne the notion of an optimal solution to a learning task.",
Deﬁnition 1.12 (Bayes-optimal function).,
"A function f ∗∈M(X, Y) achieving the smallest risk, the so-called Bayes risk R∗:= inf f∈M(X,Y) R(f), is called a Bayes-optimal function.",
"For the prediction tasks in Deﬁnition 1.2, we can represent the risk of a function with respect to the Bayes risk and compute the Bayes-optimal function, see, e.g., [CZ07, Propositions 1.8 and 9.3].",
Lemma 1.1 (Regression and classiﬁcation risk).,
"For a regression task with V[Y ] < ∞, the risk can be decomposed into R(f) = E  (f(X) −E[Y |X])2 + R∗, f ∈M(X, Y), (1.3) which is minimized by the regression function f ∗(x) = E[Y |X = x].",
"For a classiﬁcation task, the risk can be decomposed into R(f) = E  |E[Y |X]|1(−∞,0)(E[Y |X]f(X))  + R∗, f ∈M(X, Y), which is minimized by the Bayes classiﬁer f ∗(x) = sgn(E[Y |X = x]).",
"As our model fS is depending on the random training data S, the risk R(fS) is a random variable and we might aim7 for R(fS) small with high probability or in expectation over the training data.",
The challenge for the learning algorithm A is to minimize the risk by only using training data but without knowing the underlying distribution.,
"One can even show that for every learning algorithm there exists a distribution where convergence of the expected risk of fS to the Bayes risk is arbitrarily slow with respect to the number of samples m [DGL96, Theorem 7.2].",
Theorem 1.13 (No free lunch).,
"Let am ∈(0, ∞), m ∈N, be a monotonically decreasing sequence with a1 ≤1/16.",
Then for every learning algorithm A of a classiﬁcation task there exists a distribution PZ such that for every m ∈N and training data S ∼Pm Z it holds that E  R(A(S))  ≥R∗+ am.,
"6Note that this requires z 7→L(f, z) to be measurable for every f ∈M(X, Y), which is the case for our considered prediction tasks.",
"7In order to make probabilistic statements on R(fS) we assume that R(fS) is a random variable, i.e., measurable.",
"This is, e.g., the case if F constitutes a measurable space and s 7→A(s) and f →R|F are measurable.",
9 Figure 1.2: Illustration of the errors (A)–(C) in the decomposition of (1.4).,
"It shows an exemplary risk bR (blue) and empirical risk bRs (red) with respect to the projected space of measurable functions M(X, Y).",
Note that the empirical risk and thus εgen and εopt depend on the realization s = (z(i))m i=1 of the training data S ∼Pm Z .,
Theorem 1.13 shows the non-existence of a universal learning algorithm for every data distribution PZ and shows that useful bounds must necessarily be accompanied by a priori regularity conditions on the underlying distribution PZ.,
"Such prior knowledge can then be incorporated in the choice of the hypothesis set F. To illustrate this, let f ∗ F ∈arg minf∈F R(f) be a best approximation in F, such that we can bound the error R(fS) −R∗= R(fS) −bRS(fS) + bRS(fS) −bRS(f ∗ F) + bRS(f ∗ F) −R(f ∗ F) + R(f ∗ F) −R∗ ≤εopt + 2εgen + εapprox (1.4) by (A) an optimization error εopt := bRS(fS) −bRS( bfS) ≥bRS(fS) −bRS(f ∗ F), with bfS as in Deﬁnition 1.8, (B) a (uniform8) generalization error εgen := supf∈F |R(f) −bRS(f)| ≥max{R(fS) −bRS(fS), bRS(f ∗ F) − R(f ∗ F)}, and (C) an approximation error εapprox := R(f ∗ F) −R∗, see also Figure 1.2.",
"The approximation error is decreasing when enlarging the hypothesis set, but taking F = M(X, Y) prevents controlling the generalization error, see also Theorem 1.13.",
"This suggests a sweet-spot for the complexity of our hypothesis set F and is usually referred to as the bias-variance trade-oﬀ, see also Figure 1.4 below.",
"In the next sections, we will sketch mathematical ideas to tackle each of the errors in (A)–(C) in the context of deep learning.",
Observe that we bound the generalization and optimization error with respect to the empirical risk bRS and its minimizer bfS which is motivated by the fact that in deep-learning-based applications one typically tries to minimize variants of bRS.,
1.2.1 Optimization The ﬁrst error in the decomposition of (1.4) is the optimization error: εopt.,
This error is primarily inﬂuenced by the numerical algorithm A that is used to ﬁnd the model fs in a hypothesis set of NNs for given training data s ∈Zm.,
We will focus on the typical setting where such an algorithm tries to approximately minimize the empirical risk bRs.,
"While there are many conceivable methods to solve this minimization problem, by far the most common are gradient-based methods.",
The main reason for the popularity of gradient-based 8Although this uniform deviation can be a coarse estimate it is frequently considered to allow for the application of uniform laws of large numbers from the theory of empirical processes.,
"10 methods is that for FC networks as in Deﬁnition 1.4, the accurate and eﬃcient computation of pointwise derivatives ∇θΦa(x, θ) is possible by means of automatic diﬀerentiation, a speciﬁc form of which is often referred to as the backpropagation algorithm [Kel60, Dre62, Lin70, RHW86, GW08].",
"This numerical scheme is also applicable in general settings, such as, when the architecture of the NN is given by a general directed acyclic graph.",
"Using these pointwise derivatives, one usually attempts to minimize the empirical risk bRs by updating the parameters θ according to a variant of stochastic gradient descent (SGD), which we shall review below in a general formulation: Algorithm 1: Stochastic gradient descent Input : Diﬀerentiable function r: Rp →R, sequence of step-sizes ηk ∈(0, ∞), k ∈[K], Rp-valued random variable Θ(0).",
Output : Sequence of Rp-valued random variables (Θ(k))K k=1.,
"for k = 1, .",
", K do Let D(k) be a random variable such that E[D(k)|Θ(k−1)] = ∇r(Θ(k−1)); Set Θ(k) := Θ(k−1) −ηkD(k); end If D(k) is chosen deterministically in Algorithm 1, i.e., D(k) = ∇r(Θ(k−1)), then the algorithm is known as gradient descent.",
"To minimize the empirical loss, we apply SGD with r: RP (N) →R set to r(θ) = bRs(Φa(·, θ)).",
"More concretely, one might choose a batch-size m′ ∈N with m′ ≤m and consider the iteration Θ(k) := Θ(k−1) −ηk m′ X z∈S′ ∇θL(Φa(·, Θ(k−1)), z), (1.5) where S′ is a so-called mini-batch of size |S′| = m′ chosen uniformly9 at random from the training data s. The sequence of step-sizes (ηk)k∈N is often called learning rate in this context.",
"Stopping at step K, the output of a deep learning algorithm A is then given by fs = A(s) = Φa(·, ¯θ), where ¯θ can be chosen to be the realization of the last parameter Θ(K) of (1.5) or a convex combination of (Θ(k))K k=1 such as the mean.",
Algorithm 1 was originally introduced in [RM51] in the context of ﬁnding the root of a nondecreasing function from noisy measurements.,
Shortly afterwards this idea was applied to ﬁnd a unique minimum of a Lipschitz-regular function that has no ﬂat regions away from the global minimum [KW52].,
"In some regimes, we can guarantee convergence of SGD at least in expectation, see [NY83, NJLS09, SSSSS09], [SDR14, Section 5.9], [SSBD14, Chapter 14].",
One prototypical convergence guarantee that is found in the aforementioned references in various forms is stated below.,
Theorem 1.14 (Convergence of SGD).,
"Let p, K ∈N and let r: Rp ⊃B1(0) →R be diﬀerentiable and convex.",
"Further let (Θ(k))K k=1 be the output of Algorithm 1 with initialization Θ(0) = 0, step-sizes ηk = K−1/2, k ∈[K], and random variables (D(k))K k=1 satisfying that ∥D(k)∥2 ≤1 almost surely for all k ∈[K].",
"Then E[r(¯Θ)] −r(θ∗) ≤ 1 √ K , where ¯Θ := 1 K PK k=1 Θ(k) and θ∗∈arg minθ∈B1(0) r(θ).",
Theorem 1.14 can be strengthened to yield a faster convergence rate if the convexity is replaced by strict convexity.,
"If r is not convex, then convergence to a global minimum can in general not be guaranteed.",
"In fact, in that case, stochastic gradient descent may converge to a local, non-global minimum, see Figure 1.3 for an example.",
9We remark that in practice one typically picks S′ by selecting a subset of training data in a way to cover the full training data after one epoch of ⌈m/m′⌉many steps.,
"This, however, does not necessarily yield an unbiased estimator D(k) of ∇θr(Θ(k−1)) given Θ(k−1).",
11 Figure 1.3: Examples of the dynamics of gradient descent (left) and stochastic gradient descent (right) for an objective function with one non-global minimum next to the global minimum.,
We see that depending on the initial condition and also on ﬂuctuations in the stochastic part of SGD the algorithm can fail or succeed in ﬁnding the global minimum.,
"Moreover, gradient descent, i.e., the deterministic version of Algorithm 1, will stop progressing if at any point the gradient of r vanishes.",
"This is the case in every stationary point of r. A stationary point is either a local minimum, a local maximum, or a saddle point.",
"One would expect that if the direction of the step D(k) in Algorithm 1 is not deterministic, then the random ﬂuctuations may allow the iterates to escape saddle points.",
"Indeed, results guaranteeing convergence to local minima exist under various conditions on the type of saddle points that r admits, [NJLS09, GL13, GHJY15, LSJR16, JKNvW20].",
"In addition, many methods that improve the convergence by, for example, introducing more elaborate step-size rules or a momentum term have been established.",
"We shall not review these methods here, but instead refer to [GBC16, Chapter 8] for an overview.",
"1.2.2 Approximation Generally speaking, NNs, even FC NNs (see Deﬁnition 1.4) with only L = 2 layers, are universal approximators, meaning that under weak conditions on the activation function ϱ they can approximate any continuous function on a compact set up to arbitrary precision [Cyb89, Fun89, HSW89, LLPS93].",
Theorem 1.15 (Universal approximation theorem).,
"Let d ∈N, let K ⊂Rd be compact, and let ϱ ∈L∞ loc(R) be an activation function such that the closure of the points of discontinuity of ϱ is a Lebesgue null set.",
"Further let eF := [ n∈N F((d,n,1),ϱ) be the corresponding set of two-layer NN realizations.",
Then it holds that C(K) ⊂cl( eF) (where the closure is taken with respect to the topology induced by the L∞(K)-norm) if and only if there does not exist a polynomial p: R →R with p = ϱ almost everywhere.,
"The theorem can be proven by the theorem of Hahn–Banach, which implies that eF being dense in some 12 real normed vector space S is equivalent to the following condition: For all non-trivial functionals F ∈S′ \{0} from the topological dual space of S there exist parameters w ∈Rd and b ∈R such that F(ϱ(⟨w, ·⟩+ b)) ̸= 0.",
"In case of S = C(K) we have by the Riesz–Markov–Kakutani representation theorem that S′ is the space of signed Borel measures on K, see [Rud06].",
"Therefore, Theorem 1.15 holds, if ϱ is such that, for a signed Borel measure µ, Z K ϱ(⟨w, x⟩+ b) dµ(x) = 0 (1.6) for all w ∈Rd and b ∈R implies that µ = 0.",
An activation function ϱ satisfying this condition is called discriminatory.,
It is not hard to see that any sigmoidal ϱ is discriminatory.,
"Indeed, assume that ϱ satisﬁes (1.6) for all w ∈Rd and b ∈R.",
"Since for every x ∈Rd it holds that ϱ(ax+b) →1(0,∞)(x)+ϱ(b)1{0}(x) for a →∞, we conclude by superposition and passing to the limit that for all c1, c2 ∈R and w ∈Rd, b ∈R Z K 1[c1,c2](⟨w, x⟩+ b) dµ(x) = 0.",
"Representing the exponential function x 7→e−2πix as the limit of sums of elementary functions yields that R K e−2πi(⟨w,x⟩+b) dµ(x) = 0 for all w ∈Rd, b ∈R.",
"Hence, the Fourier transform of µ vanishes which implies that µ = 0.",
Theorem 1.15 addresses a uniform approximation problem on a general compact set.,
"If we are given a ﬁnite number of points and only care about good approximation at these points, then one can ask if this approximation problem is potentially simpler.",
"Below we see that, if the number of neurons is larger or equal to the number of data points, then one can always interpolate, i.e., exactly ﬁt the data on a given ﬁnite number of points.",
Proposition 1.1 (Interpolation).,
"Let d, m ∈N, let x(i) ∈Rd, i ∈[m], with x(i) ̸= x(j) for i ̸= j, let ϱ ∈C(R), and assume that ϱ is not a polynomial.",
"Then, there exist parameters θ(1) ∈Rm×d × Rm with the following property: For every k ∈N and every sequence of labels y(i) ∈Rk, i ∈[m], there exist parameters θ(2) = (W (2), 0) ∈Rk×m × Rk for the second layer of the NN architecture a = ((d, m, k), ϱ) such that Φa(x(i), (θ(1), θ(2))) = y(i), i ∈[m].",
Let us sketch the proof in the following.,
"First, note that Theorem 1.15 also holds for functions g ∈C(K, Rm) with multi-dimensional output by approximating each one-dimensional component x 7→(g(x))i and stacking the resulting networks.",
"Second, one can add an additional row containing only zeros to the weight matrix W (1) of the approximating neural network as well as an additional entry to the vector b(1).",
The eﬀect of this is that we obtain an additional neuron with constant output.,
"Since ϱ ̸= 0, we can choose b(1) such that the output of this neuron is not zero.",
"Therefore, we can include the bias vector b(2) of the second layer into the weight matrix W (2), see also Remark 1.5.",
"Now choose g ∈C(Rm, Rm) to be a function satisfying g(x(i)) = e(i), i ∈[m], where e(i) ∈Rm denotes the i-th standard basis vector.",
"By the discussion before there exists a neural network architecture ˜a = ((d, n, m), ϱ) and parameters ˜θ = ((f W (1),˜b(1)), (f W (2), 0)) such that ∥Φ˜a(·, ˜θ) −g∥L∞(K) < 1 m, (1.7) where K is a compact set with x(i) ∈K, i ∈[m].",
Let us abbreviate the output of the activations in the ﬁrst layer evaluated at the input features by eA := h ϱ(f W (1)(x(1)) + ˜b(1))) .,
ϱ(f W (1)(x(m)) + ˜b(1))) i ∈Rn×m.,
"The equivalence of the max and operator norm and (1.7) establish that ∥f W (2) eA −Im∥op ≤m max i,j∈[m] (f W (2) eA −Im)i,j = m max j∈[m] ∥Φ˜a(x(j), ˜θ) −g(x(j))∥∞< 1, 13 where Im denotes the m × m identity matrix.",
"Thus, the matrix f W (2) eA ∈Rm×m needs to have full rank and we can extract m linearly independent rows from eA resulting in an invertible matrix A ∈Rm×m.",
"Now, we deﬁne the desired parameters θ(1) for the ﬁrst layer by extracting the corresponding rows from f W (1) and ˜b(1) and the parameters θ(2) of the second layer by W (2) :=  y(1) .",
y(m) A−1 ∈Rk×m.,
"This proves that with any discriminatory activation function we can interpolate arbitrary training data (x(i), y(i)) ∈Rd × Rk, i ∈[m], using a two-layer NN with m hidden neurons, i.e., O(m(d + k)) parameters.",
One can also ﬁrst project the input features to a one-dimensional line where they are separated and then apply Proposition 1.1 with d = 1.,
"For nearly all activation functions, this can be represented by a three-layer NN using only O(d + mk) parameters10.",
"Beyond interpolation results, one can obtain a quantitative version of Theorem 1.15 if one knows additional regularity properties of the Bayes optimal function f ∗, such as smoothness, compositionality, and symmetries.",
"For surveys on such results, we refer the reader to [DHP20, GRK20].",
"For instructive purposes, we review one such result, which can be found in [Mha96, Theorem 2.1], below: Theorem 1.16 (Approximation of smooth functions).",
"Let d, k ∈N and p ∈[1, ∞].",
Further let ϱ ∈C∞(R) and assume that ϱ is not a polynomial.,
"Then there exists a constant c ∈(0, ∞) with the following property: For every n ∈N there exist parameters θ(1) ∈Rn×d × Rn for the ﬁrst layer of the NN architecture a = ((d, n, 1), ϱ) such that for every g ∈W k,p((0, 1)d) it holds that inf θ(2)∈R1×n×R ∥Φa(·, (θ(1), θ(2))) −g∥Lp((0,1)d) ≤cn−d k ∥g∥W k,p((0,1)d).",
"Theorem 1.16 shows that NNs achieve the same optimal approximation rates that, for example, spline- based approximation yields for smooth functions.",
The idea behind this theorem is based on a strategy that is employed repeatedly throughout the literature.,
This is the idea of re-approximating classical approximation methods by NNs and thereby transferring the approximation rates of these methods to NNs.,
"In the example of Theorem 1.16, approximation by polynomials is used.",
"The idea is that due to the non-vanishing derivatives of the activation function11, one can approximate every univariate polynomial via divided diﬀerences of the activation function.",
"Speciﬁcally, accepting unbounded parameter magnitudes, for any activation function ϱ: R →R which is p-times diﬀerentiable at some point λ ∈R with ϱ(p)(λ) ̸= 0, one can approximate the monomial x 7→xp on a compact set K ⊂R up to arbitrary precision by a ﬁxed-size NN via rescaled p-th order diﬀerence quotients as lim h→0 sup x∈K p X i=0 (−1)i p i  hpϱ(p)(λ)ϱ  (p/2 −i)hx + λ  −xp = 0.",
(1.8) Let us end this subsection by clarifying the connection of the approximation results above to the error decomposition of (1.4).,
"Consider, for simplicity, a regression task with quadratic loss.",
"Then, the approximation error εapprox equals a common L2-error εapprox = R(f ∗ F) −R∗(∗) = Z X (f ∗ F(x) −f ∗(x))2 dPX(x) (∗) = min f∈F ∥f −f ∗∥2 L2(PX) ≤min f∈F ∥f −f ∗∥2 L∞(X), where the identities marked by (∗) follow from Lemma 1.1.",
"Hence, Theorem 1.15 postulates that εapprox →0 for increasing NN sizes, whereas Theorem 1.16 additionally explains how fast εapprox converges to 0.",
"10To avoid the m × d weight matrix (without using shared parameters as in [ZBH+17]) one interjects an approximate one- dimensional identity [PV18, Deﬁnition 2.5], which can be arbitrarily well approximated by a NN with architecture a = ((1, 2, 1), ϱ) given that ϱ′(λ) ̸= 0 for some λ ∈R, see (1.8) below.",
"11The Baire category theorem ensures that for a non-polynomial ϱ ∈C∞(R) there exists λ ∈R with ϱ(p)(λ) ̸= 0 for all p ∈N, see, e.g., [Don69, Chapter 10].",
"14 1.2.3 Generalization Towards bounding the generalization error εgen = supf∈F |R(f) −bRS(f)|, one observes that, for every f ∈F, Assumption 1.10 ensures that L(f, Z(i)), i ∈[m], are i.i.d.",
random variables.,
"Thus, one can make use of concentration inequalities to bound the deviation of the empirical risk bRS(f) = 1 m Pm i=1 L(f, Z(i)) from its expectation R(f).",
"For instance, assuming boundedness12 of the loss, Hoeﬀding’s inequality [Hoe63] and a union bound directly imply the following generalization guarantee for countable, weighted hypothesis sets F, see, e.g., [BBL03].",
"Theorem 1.17 (Generalization bound for countable, weighted hypothesis sets).",
"Let m ∈N, δ ∈(0, 1) and assume that F is countable.",
"Further let p be a probability distribution on F and assume that L(f, Z) ∈[0, 1] almost surely for every f ∈F.",
Then with probability 1−δ (with respect to repeated sampling of Pm Z -distributed training data S) it holds for every f ∈F that |R(f) −bRS(f)| ≤ r ln(1/p(f)) + ln(2/δ) 2m .,
"While the weighting p needs to be chosen before seeing the training data, one could incorporate prior information on the learning algorithm A.",
"For ﬁnite hypothesis sets without prior information, setting p(f) = 1/|F| for every f ∈F, Theorem 1.17 implies that, with high probability, it holds that εgen ≲ r ln(|F|) m .",
"(1.9) Again, one notices that, in line with the bias-variance trade-oﬀ, the generalization bound is increasing with the size of the hypothesis set |F|.",
"Although in practice the parameters θ ∈RP (N) of a NN are discretized according to ﬂoating-point arithmetic, the corresponding quantities |Fa| or |Fa,sgn| would be huge and we need to ﬁnd a replacement for the ﬁniteness condition.",
We will focus on binary classiﬁcation tasks and present a main result of VC theory which is to a great extent derived from the work of Vladimir Vapnik and Alexey Chervonenkis [VC71].,
"While in (1.9) we counted the number of functions in F, we now reﬁne this analysis to the number of functions restricted to a ﬁnite subset of X, given by the growth function growth(m, F) := max (x(i))m i=1∈X m |{f|(x(i))m i=1 : f ∈F}|.",
"The growth function can be interpreted as the maximal number of classiﬁcation patterns in {−1, 1}m which functions in F can realize on m points and thus growth(m, F) ≤2m.",
"The asymptotic behavior of the growth function is determined by a single intrinsic dimension of our hypothesis set F, the so-called VC-dimension VCdim(F) := sup  m ∈N ∪{0}: growth(m, F) = 2m , which deﬁnes the largest number of points such that F can realize any classiﬁcation pattern, see, e.g., [AB99, BBL03].",
"There exist various results on VC-dimensions of NNs with diﬀerent activation functions, see, for instance, [BH89, KM97, BMM98, Sak99].",
We present the result of [BMM98] for piecewise polynomial activation functions ϱ.,
"It establishes a bound on the VC-dimension of hypothesis sets of NNs for classiﬁcation tasks F(N,ϱ),sgn that scales, up to logarithmic factors, linear in the number of parameters P(N) and quadratic in the number of layers L. Theorem 1.18 (VC-dimension of neural network hypothesis sets).",
Let ϱ be a piecewise polynomial activation function.,
"Then there exists a constant c ∈(0, ∞) such that for every L ∈N and N ∈NL+1 it holds that VCdim(F(N,ϱ),sgn) ≤c  P(N)L log(P(N)) + P(N)L2 .",
"12Note that for our classiﬁcation tasks in Deﬁnition 1.2 it holds that L(f, Z) ∈{0, 1} for every f ∈F.",
"For the regression tasks, one typically assumes boundedness conditions, such as |Y | ≤c and supf∈F |f(X)| ≤c almost surely for some c ∈(0, ∞), which yields that supf∈F |L(f, Z)| ≤4c2.",
"15 Given (x(i))m i=1 ∈X m, there exists a partition of RP (N) such that Φ(x(i), ·), i ∈[m], are polynomials on each region of the partition.",
The proof of Theorem 1.18 is based on bounding the number of such regions and the number of classiﬁcation patterns of a set of polynomials.,
"A ﬁnite VC-dimension ensures the following generalization bound [Tal94, AB99]: Theorem 1.19 (VC-dimension generalization bound).",
"There exists a constant c ∈(0, ∞) with the following property: For every classiﬁcation task as in Deﬁnition 1.2, every Z-valued random variable Z, and every m ∈N, δ ∈(0, 1) it holds with probability 1 −δ (with respect to repeated sampling of Pm Z -distributed training data S) that sup f∈F |R(f) −bRS(f)| ≤c r VCdim(F) + log(1/δ)) m .",
"In summary, using NN hypothesis sets F(N,ϱ),sgn with a ﬁxed depth and piecewise polynomial activation ϱ for a classiﬁcation task, with high probability it holds that εgen ≲ r P(N) log(P(N)) m .",
"(1.10) In the remainder of this section we will sketch a proof of Theorem 1.19 and, in doing so, present further concepts and complexity measures connected to generalization bounds.",
"We start by observing that McDiarmid’s inequality [McD89] ensures that εgen is sharply concentrated around its expectation, i.e., with probability 1 −δ it holds that13 εgen −E  εgen ≲ r log(1/δ) m .",
(1.11) To estimate the expectation of the uniform generalization error we employ a symmetrization argu- ment [GZ84].,
"Deﬁne G := L ◦F := {L(f, ·): f ∈F}, let eS = ( eZ(i))m i=1 ∼Pm Z be a test data set independent of S, and note that R(f) = E[ bReS(f)].",
"By properties of the conditional expectation and Jensen’s inequality it holds that E  εgen = E h sup f∈F |R(f) −bRS(f)| i = E h sup g∈G 1 m m X i=1 E  g( eZ(i)) −g(Z(i))|S  i ≤E h sup g∈G 1 m m X i=1 g( eZ(i)) −g(Z(i)) i = E h sup g∈G 1 m m X i=1 τi  g( eZ(i)) −g(Z(i))  i ≤2E h sup g∈G 1 m m X i=1 τig(Z(i)) i , where we used that multiplications with Rademacher variables (τ1, .",
", τm) ∼U({−1, 1}m) only amount to interchanging Z(i) with eZ(i) which has no eﬀect on the expectation, since Z(i) and eZ(i) have the same distribution.",
"The quantity Rm(G) := E h sup g∈G 1 m m X i=1 τig(Z(i)) i is called the Rademacher complexity14 of G. One can also prove a corresponding lower bound [vdVW97], i.e., Rm(G) − 1 √m ≲E  εgen ≲Rm(G).",
"(1.12) 13For precise conditions to ensure that the expectation of εgen is well-deﬁned, we refer the reader to [vdVW97, Dud14].",
"14Due to our decomposition in (1.4), we want to uniformly bound the absolute value of the diﬀerence between the risk and the empirical risk.",
It is also common to just bound supf∈F R(f) −b RS(f) leading to a deﬁnition of the Rademacher complexity without the absolute values which can be easier to deal with.,
16 Now we use a chaining method to bound the Rademacher complexity of F by covering numbers on diﬀerent scales.,
"Speciﬁcally, Dudley’s entropy integral [Dud67, LT91] implies that Rm(G) ≲E h Z ∞ 0 r log Nα(G, dS) m dα i , (1.13) where Nα(G, dS) := inf n |G|: G ⊂G, G ⊂ [ g∈G BdS α (g) o denotes the covering number with respect to the (random) pseudometric given by dS(f, g) = d(Z(i))m i=1(f, g) := v u u t 1 m m X i=1  f(Z(i)) −g(Z(i)) 2.",
"For the 0-1 loss L(f, z) = 1(−∞,0)(yf(x)) = (1 −f(x)y)/2, we can get rid of the loss function by the fact that Nα(G, dS) = N2α(F, d(X(i))m i=1).",
"(1.14) The proof is completed by combining the inequalities in (1.11), (1.12), (1.13) and (1.14) with a result of David Haussler [Hau95] which shows that for α ∈(0, 1) we have log(Nα(F, d(X(i))m i=1)) ≲VCdim(F) log(1/α).",
(1.15) We remark that this resembles a typical behavior of covering numbers.,
"For instance, the logarithm of the covering number log(Nα(M)) of a compact d-dimensional Riemannian manifold M essentially scales like d log(1/α).",
"Finally, note that there exists a similar bound to the one in (1.15) for bounded regression tasks making use of the so-called fat-shattering dimension [MV03, Theorem 1].",
1.3 Do we need a new theory?,
"Despite the already substantial insight that the classical theories provide, a lot of open questions remain.",
We will outline these questions below.,
The remainder of this article then collects modern approaches to explain the following issues: Why do large neural networks not overﬁt?,
"In Subsection 1.2.2, we have observed that three-layer NNs with commonly used activation functions and only O(d + m) parameters can interpolate any training data (x(i), y(i)) ∈Rd ×R, i ∈[m].",
"While this speciﬁc representation might not be found in practice, [ZBH+17] indeed trained convolutional15 NNs with ReLU activation function and about 1.6 million parameters to achieve zero empirical risk on m = 50000 training images of the CIFAR10 dataset [KH09] with 32 × 32 pixels per image, i.e., d = 1024.",
"For such large NNs, generalization bounds scaling with the number of parameters P(N) as the VC-dimension bound in (1.10) are vacuous.",
"However, they observed close to state-of-the-art generalization performance16.",
"Generally speaking, NNs in practice are observed to generalize well despite having more parameters than training samples (usually referred to as overparametrization) and approximately interpolating the training data (usually referred to as overﬁtting).",
"As we cannot perform any better on the training data, there is no trade-oﬀ between ﬁt to training data and complexity of the hypothesis set F happening, seemingly contradicting the classical bias-variance trade-oﬀof statistical learning theory.",
"This is quite surprising, especially given the following additional empirical observations in this regime, see [NTS14, ZBH+17, NBMS17, BHMM19, NKB+20]: 15The basic deﬁnition of a convolutional NN will be given in Section 6.",
In [ZBH+17] more elaborate versions such as an Inception architecture [SLJ+15] are employed.,
"16In practice one usually cannot measure the risk R(fs) and instead evaluates the performance of a trained model fs by b R˜s(fs) using test data ˜s, i.e., realizations of i.i.d.",
random variables distributed according to PZ and drawn independently of the training data.,
In this context one often calls Rs(fs) the training error and R˜s(fs) the test error.,
"17 0 6 12 18 d 0.00 0.05 0.10 linear regression ( = 0) test train 0 20 40 d 10 10 10 1 108 0 6 12 18 d 0.00 0.05 0.10 ridge regression ( = 0.001) test train 0 20 40 d 10 2 10 1 0.5 0.0 0.5 x 0 1 2 d = 40 = 0 = 0.001 f * training data Figure 1.4: The ﬁrst plot (and its semi-log inset) shows median and interquartile range of the test and training errors of ten independent linear regressions with m = 20 samples, polynomial input features X = (1, Z, .",
", Zd) of degree d ∈[40], and labels Y = f ∗(Z) + ν, where Z ∼U([−0.5, 0.5]), f ∗is a polynomial of degree three, and ν ∼N(0, 0.01).",
This clearly reﬂects the classical u-shaped bias-variance curve with a sweet-spot at d = 3 and drastic overﬁtting beyond the interpolation threshold at d = 20.,
"However, the second plot shows that we can control the complexity of our hypothesis set of linear models by restricting the Euclidean norm of their parameters using ridge regression with a small regularization parameter α = 10−3, i.e., minimizing the regularized empirical risk 1 m Pm i=1(Φ(X(i), θ) −Y (i))2 + α∥θ∥2 2, where Φ(·, θ) = ⟨θ, ·⟩.",
Corresponding examples of bfs are depicted in the last plot.,
"Zero training error on random labels: Zero empirical risk can also be achieved for random labels using the same architecture and training scheme with only slightly increased training time: This suggests that the considered hypothesis set of NNs F can ﬁt arbitrary binary labels, which would imply that VCdim(F) ≈m or Rm(F) ≈1 rendering our uniform generalization bounds in Theorem 1.19 and in (1.12) vacuous.",
"Lack of explicit regularization: The test error depends only mildly on explicit regularization like norm- based penalty terms or dropout (see [G´er17] for an explanation of diﬀerent regularization methods): As such regularization methods are typically used to decrease the complexity of F, one might ask if there is any implicit regularization (see Figure 1.4), constraining the range of our learning algorithm A to some smaller, potentially data-dependent subset, i.e., A(s) ∈eFs ⊊F.",
Dependence on the optimization: The same NN trained to zero empirical risk using diﬀerent variants of SGD or starting from diﬀerent initializations can exhibit diﬀerent test errors: This indicates that the dynamics of gradient descent and properties of the local neighborhood around the model fs = A(s) might be correlated with generalization performance.,
Interpolation of noisy training data: One still observes low test error when training up to approximately zero empirical risk using a regression (or surrogate) loss on noisy training data.,
"This is particularly interesting, as the noise is captured by the model but seems not to hurt generalization performance.",
"Further overparametrization improves generalization performance: Further increasing the NN size can lead to even lower test error: Together with the previous item, this might ask for a diﬀerent treatment of models complex enough to ﬁt the training data.",
"According to the traditional lore “The training error tends to decrease whenever we increase the model complexity, that is, whenever we ﬁt the data harder.",
"However with too much ﬁtting, the model adapts itself too closely to the training data, and will not generalize well (i.e., have large test error)”, [HTF01].",
"While this ﬂawlessly describes the situation for certain machine learning tasks (see Figure 1.4), it seems not to be directly applicable here.",
"In summary, this suggests that the generalization performance of NNs depends on an interplay of the data distribution PZ combined with properties of the learning algorithm A, such as the optimization procedure and its range.",
"In particular, classical uniform bounds as in Item (B) of our error decomposition might only 18 deliver insuﬃcient explanation, see also [NK19].",
The mismatch between predictions of classical theory and the practical generalization performance of deep NNs is often referred to as generalization puzzle.,
In Section 2 we will present possible explanations for this phenomenon.,
What is the role of depth?,
"We have seen in Subsection 1.2.2 that NNs can closely approximate every function if they are suﬃciently wide [Cyb89, Fun89, HSW89].",
"There are additional classical results that even provide a trade-oﬀbetween the width and the approximation accuracy [CLM94, Mha96, MP99].",
"In these results, the central concept is the width of a NN.",
"In modern applications, however at least as much focus if not more lies on the depth of the underlying architectures, which can have more than 1000 layers [HZRS16].",
"After all, the depth of NNs is responsible for the name of deep learning.",
This consideration begs the question of whether there is a concrete mathematically quantiﬁable beneﬁt of deep architectures over shallow NNs.,
"Indeed, we will see eﬀects of depth at many places throughout this manuscript.",
"However, one of the aspects of deep learning that is most clearly aﬀected by deep architectures is the approximation theoretical aspect.",
"In this framework, we will discuss in Section 3 multiple approaches that describe the eﬀect of depth.",
Why do neural networks perform well in very high-dimensional environments?,
We have seen in Subsection 1.2.2 and will see in Section 3 that from the perspective of approximation theory deep NNs match the performance of the best classical approximation tool in virtually every task.,
"In practice, we observe something that is even more astounding.",
"In fact, NNs seem to perform incredibly well on tasks that no classical, non-specialized approximation method can even remotely handle.",
The approximation problem that we are talking about here is that of approximation of high-dimensional functions.,
"Indeed, the classical curse of dimensionality [Bel52, NW09] postulates that essentially every approximation method deteriorates exponentially fast with increasing dimension.",
"For example, for the uniform approximation error of 1-Lipschitz continuous functions on a d-dimensional unit cube in the uniform norm, we have a lower bound of Ω(p−1/d), for p →∞, when approximating with a continuous scheme17 of p free parameters [DeV98].",
"On the other hand, in most applications, the input dimensions are massive.",
"For example, the following datasets are typically used as benchmarks in image classiﬁcation problems: MNIST [LBBH98] with 28 × 28 pixels per image, CIFAR-10/CIFAR-100 [KH09] with 32×32 pixels per image and ImageNet [DDS+09, KSH12] which contains high-resolution images that are typically down-sampled to 256 × 256 pixels.",
"Naturally, in real-world applications, the input dimensions may well exceed those of these test problems.",
"However, already for the simplest of the test cases above, the input dimension is d = 784.",
"If we use d = 784 in the aforementioned lower bound for the approximation of 1-Lipschitz functions, then we require O(ε−784) parameters to achieve a uniform error of ε ∈(0, 1).",
Already for moderate ε this value will quickly exceed the storage capacity of any conceivable machine in this universe.,
"Considering the aforementioned curse of dimensionality, it is puzzling to see that NNs perform adequately in this regime.",
"In Section 4, we describe three approaches that oﬀer explanations as to why deep NN-based approximation is not rendered meaningless in the context of high-dimensional input dimensions.",
Why does stochastic gradient descent converge to good local minima despite the non-convexity of the problem?,
"As mentioned in Subsection 1.2.1, a convergence guarantee of stochastic gradient descent to a global minimum is typically only given if the underlying objective function admits some form of convexity.",
"However, the empirical risk of a NN, i.e., bRs(Φ(·, θ)), is typically not a convex function with respect to the parameters θ.",
"For a simple intuitive reason why this function fails to be convex, it is instructive to consider the following example.",
17One can achieve better rates at the cost of discontinuous (with respect to the function to be approximated) parameter assignment.,
This can be motivated by the use of space-ﬁlling curves.,
"In the context of NNs with piecewise polynomial activation functions, a rate of p−2/d can be achieved by very deep architectures [Yar18a, YZ20].",
19 Figure 1.5: Two-dimensional projection of the loss landscape of a neural network with four layers and ReLU activation function on four diﬀerent scales.,
"From top-left to bottom-right, we zoom into the global minimum of the landscape.",
Example 1.20.,
"Consider the NN Φ(x, θ) = θ1ϱR(θ3x + θ5) + θ2ϱR(θ4x + θ6), θ ∈R6, x ∈R, with the ReLU activation function ϱR(x) = max{0, x}.",
"It is not hard to see that the two parameter values θ = (1, −1, 1, 1, 1, 0) and ¯θ = (−1, 1, 1, 1, 0, 1) produce the same realization function18, i.e., Φ(·, θ) = Φ(·, ¯θ).",
"However, since (θ + ¯θ)/2 = (0, 0, 1, 1, 1/2, 1/2), we conclude that Φ(·, (θ + ¯θ)/2) = 0.",
"Clearly, for the data s = ((−1, 0), (1, 1)), we now have that bRs(Φ(·, θ)) = bRs(Φ(·, ¯θ)) = 0 and bRs  Φ(·, (θ + ¯θ)/2)  = 1 2, showing the non-convexity of bRs.",
18This corresponds to interchanging the two neurons in the hidden layer.,
In general it holds that the realization function of a FC NN is invariant under permutations of the neurons in a given hidden layer.,
"20 Given this non-convexity, Algorithm 1 faces serious challenges.",
"Firstly, there may exist multiple suboptimal local minima.",
"Secondly, the objective may exhibit saddle points, some of which may be of higher order, i.e., the Hessian vanishes.",
"Finally, even if no suboptimal local minima exist, there may be extensive areas of the parameter space where the gradient is very small, so that escaping these regions can take a very long time.",
"These issues are not mere theoretical possibilities, but will almost certainly arise.",
"For example, [AHW96, SS18] show the existence of many suboptimal local minima in typical learning tasks.",
"Moreover, for ﬁxed-sized NNs, it has been shown in [BEG19, PRV20], that with respect to Lp-norms the set of NNs is generally a very non-convex and non-closed set.",
"Also, the map θ 7→Φa(·, θ) is not a quotient map, i.e., not continuously invertible when accounting for its non-injectivity.",
"In addition, in various situations ﬁnding the global optimum of the minimization problem is shown to be NP-hard in general [BR89, Jud90, ˇS´ım02].",
"In Figure 1.5 we show the two-dimensional projection of a loss landscape, i.e., the projection of the graph of the function θ 7→bRs(Φ(·, θ)).",
It is apparent from the visualization that the problem exhibits more than one minimum.,
"We also want to add that in practice one neglects that the loss is only almost everywhere diﬀerentiable in case of piecewise smooth activation functions, such as the ReLU, although one could resort to subgradient methods [KL18].",
"In view of these considerations, the classical framework presented in Subsection 1.2.1 oﬀers no explanation as to why deep learning works in practice.",
"Indeed, in the survey [OM98, Section 1.4] the state of the art in 1998 was summarized by the following assessment: “There is no formula to guarantee that (1) the NN will converge to a good solution, (2) convergence is swift, or (3) convergence even occurs at all.” Nonetheless, in applications, not only would an explanation of when and why SGD converges be extremely desirable, convergence is also quite often observed even though there is little theoretical explanation for it in the classical set-up.",
"In Section 5, we collect modern approaches explaining why and when convergence occurs and can be guaranteed.",
Which aspects of a neural network architecture aﬀect the performance of deep learning?,
"In the introduction to classical approaches to deep learning above, we have seen that in classical results, such as in Theorem 1.16, only the eﬀect of few aspects of the NN architectures are considered.",
In Theorem 1.16 only the impact of the width of the NN was studied.,
"In further approximation theorems below, e.g., in Theorems 2.1 and 3.2, we will additionally have a variable depth of NNs.",
"However, for deeper architectures, there are many additional aspects of the architecture that could potentially aﬀect the performance of the model for the associated learning task.",
"For example, even for a standard FC NN with L layers as in Deﬁnition 1.4, there is a lot of ﬂexibility in choosing the number of neurons (N1, .",
", NL−1) ∈NL−1 in the hidden layers.",
One would expect that certain choices aﬀect the capabilities of the NNs considerably and some choices are preferable over others.,
"Note that, one aspect of the neural network architecture that can have a profound eﬀect on the performance, especially regarding approximation theoretical aspects of the performance, is the choice of the activation function.",
"For example, in [MP99, Yar21] activation functions were found that allow uniform approximation of continuous functions to arbitrary accuracy with ﬁxed-size neural networks.",
"In the sequel we will, however, focus on architectural aspects other than the activation function.",
"In addition, practitioners have invented an immense variety of NN architectures for speciﬁc problems.",
"These include NNs with convolutional blocks [LBBH98], with skip connections [HZRS16], sparse connections [ZAP16, BBC17], batch normalization blocks [IS15], and many more.",
"In addition, for sequential data, recurrent connections are used [RHW86] and these often have forget mechanisms [HS97] or other gates [CvMG+14] included in their architectures.",
The choice of an appropriate NN architecture is essential to the success of many deep learning tasks.,
"This goes so far, that frequently an architecture search is applied to ﬁnd the most suitable one [ZL17, PGZ+18].",
"In most cases, though, the design and choice of the architecture is based on the intuition of the practitioner.",
"Naturally, from a theoretical point of view, this situation is not satisfactory.",
"Instead, it would be highly desirable to have a mathematical theory guiding the choice of NN architectures.",
"More concretely, one would wish for mathematical theorems that identify those architectures that work for a speciﬁc problem and those that will yield suboptimal results.",
"In Section 6, we discuss various results that explain theoretically quantiﬁable eﬀects of certain aspects or building blocks of NN architectures.",
21 Which features of data are learned by deep architectures?,
It is commonly believed that the neurons of NNs constitute feature extractors in diﬀerent levels of abstraction that correspond to the layers.,
"This belief is partially grounded in experimental evidence as well as in drawing connections to the human visual cortex, see [GBC16, Chapter 9.10].",
"Understanding the features that are learned can, in a way, be linked to understanding the reasoning with which a NN-based model ended up with its result.",
"Therefore, analyzing the features that a NN learns constitutes a data-aware approach to understanding deep learning.",
"Naturally, this falls outside of the scope of the classical theory, which is formulated in terms of optimization, generalization, and approximation errors.",
"One central obstacle towards understanding these features theoretically is that, at least for practical problems, the data distribution is unknown.",
"However, one often has partial knowledge.",
One example is that in image classiﬁcation it appears reasonable to assume that any classiﬁer is translation and rotation invariant as well as invariant under small deformations.,
"In this context, it is interesting to understand under which conditions trained NNs admit the same invariances.",
Biological NNs such as the visual cortex are believed to be evolved in a way that is based on sparse multiscale representations of visual information [OF96].,
"Again, a fascinating question is whether NNs trained in practice can be shown to favor such multiscale representations based on sparsity or if the architecture is theoretically linked to sparse representations.",
We will discuss various approaches studying the features learned by neural networks in Section 7.,
Are neural networks capable of replacing highly specialized numerical algorithms in natural sciences?,
"Shortly after their successes in various data-driven tasks in data science and AI applications, NNs have been used also as a numerical ansatz for solving highly complex models from the natural sciences which may be combined with data driven methods.",
This per se is not very surprising as many such models can be formulated as optimization problems where the common deep learning paradigm can be directly applied.,
What might be considered surprising is that this approach seems to be applicable to a wide range of problems which have previously been tackled by highly specialized numerical methods.,
"Particular successes include the data-driven solution of ill-posed inverse problems [AM¨OS19] which have, for example, led to a fourfold speedup in MRI scantimes [ZKS+18] igniting the research project fastmri.org.",
"Deep-learning-based approaches have also been very successful in solving a vast array of diﬀerent partial diﬀerential equation (PDE) models, especially in the high-dimensional regime [EY18, RPK19, HSN20, PSMF20] where most other methods would suﬀer from the curse of dimensionality.",
"Despite these encouraging applications, the foundational mechanisms governing their workings and limitations are still not well understood.",
In Subsection 4.3 and Section 8 we discuss some theoretical and practical aspects of deep learning methods applied to the solution of inverse problems and PDEs.,
"2 Generalization of large neural networks In the following, we will shed light on the generalization puzzle of NNs as described in Subsection 1.3.",
"We focus on four diﬀerent lines of research which, of course, do not cover the wide range of available results.",
"In fact, we had to omit a discussion of a multitude of important works, some of which we reference in the following paragraph.",
"First, let us mention extensions of the generalization bounds presented in Subsection 1.2.3 making use of local Rademacher complexities [BBM05] or dropping assumptions on boundedness or rapidly decaying tails [Men14].",
"Furthermore, there are approaches to generalization which do not focus on the hypothesis set F, i.e., the range of the learning algorithm A, but the way A chooses its model fs.",
"For instance, one can assume that fs does not depend too strongly on each individual sample (algorithmic stability [BE02, PRMN04]), only on a subset of the samples (compression bounds [AGNZ18]), or satisﬁes local properties (algorithmic robustness [XM12]).",
"Finally, we refer the reader to [JNM+20] and the references mentioned therein for an empirical study of various measures related to generalization.",
"Note that many results on generalization capabilities of NNs can still only be proven in simpliﬁed settings, e.g., for deep linear NNs, i.e., ϱ(x) = x, or basic linear models, i.e., one-layer NNs.",
"Thus, we start by 22 emphasizing the connection of deep, nonlinear NNs to linear models (operating on features given by a suitable kernel) in the inﬁnite width limit.",
"2.1 Kernel regime We consider a one-dimensional prediction setting where the loss L(f, (x, y)) depends on x ∈X only through f(x) ∈Y, i.e., there exists a function ℓ: Y × Y →R such that L(f, (x, y)) = ℓ(f(x), y).",
"For instance, in case of the quadratic loss we have that ℓ(ˆy, y) = (ˆy −y)2.",
"Further, let Φ be a NN with architecture (N, ϱ) = ((d, N1, .",
", NL−1, 1), ϱ) and let Θ0 be a RP (N)-valued random variable.",
"For simplicity, we evolve the parameters of Φ according to the continuous version of gradient descent, so-called gradient ﬂow, given by dΘ(t) dt = −∇θ bRs(Φ(·, Θ(t))) = −1 m m X i=1 ∇θΦ(x(i), Θ(t))Di(t), Θ(0) = Θ0, (2.1) where Di(t) := ∂ℓ(ˆy,y(i)) ∂ˆy |ˆy=Φ(x(i),Θ(t)) is the derivative of the loss with respect to the prediction at input feature x(i) at time t ∈[0, ∞).",
"The chain rule implies the following dynamics of the NN realization dΦ(·, Θ(t)) dt = −1 m m X i=1 KΘ(t)(·, x(i))Di(t) (2.2) and its empirical risk d bRs(Φ(·, Θ(t)) dt = −1 m2 m X i=1 m X j=1 Di(t)KΘ(t)(x(i), x(j))Dj(t), (2.3) where Kθ, θ ∈RP (N), is the so-called neural tangent kernel (NTK) Kθ : Rd × Rd →R, Kθ(x1, x2) =  ∇θΦ(x1, θ) T ∇θΦ(x2, θ).",
"(2.4) Now let σw, σb ∈(0, ∞) and assume that the initialization Θ0 consists of independent entries, where entries corresponding to the weight matrix and bias vector in the ℓ-th layer follow a normal distribution with zero mean and variances σ2 w/Nℓand σ2 b, respectively.",
"Under weak assumptions on the activation function, the central limit theorem implies that the pre-activations converge to i.i.d.",
"centered Gaussian processes in the inﬁnite width limit N1, .",
", NL−1 →∞, see [LBN+18, MHR+18].",
"Similarly, also KΘ0 converges to a deterministic kernel K∞which stays constant in time and only depends on the activation function ϱ, the depth L, and the initialization parameters σw and σb [JGH18, ADH+19, Yan19, LXS+20].",
"Thus, within the inﬁnite width limit, gradient ﬂow on the NN parameters as in (2.1) is equivalent to functional gradient ﬂow in the reproducing kernel Hilbert space (HK∞, ∥· ∥K∞) corresponding to K∞, see (2.2).",
"By (2.3), the empirical risk converges to a global minimum as long as the kernel evaluated at the input features, ¯K∞:= (K∞(x(i), x(j)))m i,j=1 ∈Rm×m, is positive deﬁnite (see, e.g., [JGH18, DLL+19] for suitable conditions) and the ℓ(·, y(i)) are convex and lower bounded.",
"For instance, in case of the quadratic loss the solution of (2.2) is then given by Φ(·, Θ(t)) = C(t)(y(i))m i=1 +  Φ(·, Θ0) −C(t)(Φ(x(i), Θ0))m i=1  , (2.5) where C(t) :=  (K∞(·, x(i)))m i=1 T ( ¯K∞)−1(Im −e−2 ¯ K∞t m ).",
"As the initial realization Φ(·, Θ0) constitutes a centered Gaussian process, the second term in (2.5) follows a normal distribution with zero mean at each input.",
"In the limit t →∞, its variance vanishes on the input features x(i), i ∈[m], and the ﬁrst term convergences to the minimum kernel-norm interpolator, i.e., to the solution of min f∈HK∞∥f∥K∞ s.t.",
f(x(i)) = y(i).,
"23 Therefore, within the inﬁnite width limit, the generalization properties of the NN could be described by the generalization properties of the minimizer in the reproducing kernel Hilbert space corresponding to the kernel K∞[BMM18, LR20, LRZ20, GMMM21, Li21].",
"This so-called lazy training, where a NN essentially behaves like a linear model with respect to the nonlinear features x 7→∇θΦ(x, θ), can already be observed in the non-asymptotic regime, see also Subsection 5.2.",
"For suﬃciently overparametrized (P(N) ≫m) and suitably initialized models, one can show that Kθ(0) is close to K∞at initialization and Kθ(t) stays close to Kθ(0) throughout training, see [DZPS18, ADH+19, COB19, DLL+19].",
"The dynamics of the NN under gradient ﬂow in (2.2) and (2.3) can thus be approximated by the dynamics of the linearization of Φ at initialization Θ0, given by Φlin(·, θ) := Φ(·, Θ0) + ⟨∇θΦ(·, Θ0), θ −Θ0⟩, which motivates to study the behavior of linear models in the overparametrized regime.",
"2.2 Norm-based bounds and margin theory For piecewise linear activation functions, one can improve upon the VC-dimension bounds in Theorem 1.18 and show that, up to logarithmic factors, the VC-dimension is asymptotically bounded both above and below by P(N)L, see [BHLM19].",
The lower bound shows that the generalization bound in Theorem 1.19 can only be non-vacuous if the number of samples m scales at least linearly with the number of NN parameters P(N).,
"However, heavily overparametrized NNs used in practice seem to generalize well outside of this regime.",
One solution is to bound other complexity measures of NNs taking into account various norms on the parameters and avoid the direct dependence on the number of parameters [Bar98].,
"For instance, we can compute bounds on the Rademacher complexity of NNs with positively homogeneous activation function, where the Frobenius norm of the weight matrices is bounded, see also [NTS15].",
"Note that, for instance, the ReLU activation is positively homogeneous, i.e., it satisﬁes that ϱR(λx) = λϱR(x) for all x ∈R and λ ∈(0, ∞).",
Theorem 2.1 (Rademacher complexity of neural networks).,
"Let d ∈N, assume that X = B1(0) ⊂Rd, and let ϱ be a positively homogeneous activation function with Lipschitz constant 1.",
"We deﬁne the set of all biasless NN realizations with depth L ∈N, output dimension 1, and Frobenius norm of the weight matrices bounded by C ∈(0, ∞) as eFL,C :=  Φ(N,ϱ)(·, θ): N ∈NL+1, N0 = d, NL = 1, θ = ((W (ℓ), 0))L ℓ=1 ∈RP (N), ∥W (ℓ)∥F ≤C .",
"Then for every m ∈N it holds that Rm( eFL,C) ≤C(2C)L−1 √m .",
The term 2L−1 depending exponentially on the depth can be reduced to √ L or completely omitted by invoking also the spectral norm of the weight matrices [GRS18].,
"Further, observe that for L = 1, i.e., linear classiﬁers with bounded Euclidean norm, this bound is independent of the input dimension d. Together with (1.12), this motivates why the regularized linear model in Figure 1.4 did perform well in the overparametrized regime.",
"The proof of Theorem 2.1 is based on the contraction property of the Rademacher complexity [LT91] which establishes that Rm(ϱ ◦eFℓ,C) ≤2Rm( eFℓ,C), ℓ∈N.",
"We can iterate this together with the fact that for every τ ∈{−1, 1}m, and x ∈RNℓ−1 it holds that sup ∥W (ℓ)∥F ≤C m X i=1 τiϱ(W (ℓ)x) 2 = C sup ∥w∥2≤1 m X i=1 τiϱ(⟨w, x⟩) .",
"24 In summary, one establishes that Rm( eFL,C) = C mE  sup f∈e FL−1,C m X i=1 τiϱ(f(X(i))) 2  ≤C(2C)L−1 m E  m X i=1 τiX(i) 2  , which by Jensen’s inequality yields the claim.",
"Recall that for classiﬁcation problems one typically minimizes a surrogate loss Lsurr, see Remark 1.9.",
"This suggests that there could be a trade-oﬀhappening between complexity of the hypothesis class Fa and the corresponding regression ﬁt underneath, i.e., the margin M(f, z) := yf(x) by which a training example z = (x, y) has been classiﬁed correctly by f ∈Fa, see [BFT17, NBS18, JKMB19].",
"For simplicity, let us focus on the ramp surrogate loss with conﬁdence γ > 0, i.e., Lsurr γ (f, z) := ℓγ(M(f, z)), where ℓγ(t) := 1(−∞,γ](t) −t γ 1[0,γ](t), t ∈R.",
Note that the ramp function ℓγ is 1/γ-Lipschitz continuous.,
"Using McDiarmid’s inequality and a sym- metrization argument similar to the proof of Theorem 1.19, combined with the contraction property of the Rademacher complexity, yields the following bound on the probability of misclassiﬁcation: With probability 1 −δ for every f ∈Fa it holds that P[sgn(f(X)) ̸= Y ] ≤E  Lsurr γ (f, Z)  ≲1 m m X i=1 Lsurr γ (f, Z(i)) + Rm(Lsurr γ ◦Fa) + r ln(1/δ) m ≲1 m m X i=1 1(−∞,γ)(Y (i)f(X(i))) + Rm(M ◦Fa) γ + r ln(1/δ) m = 1 m m X i=1 1(−∞,γ)(Y (i)f(X(i))) + Rm(Fa) γ + r ln(1/δ) m .",
This shows the trade-oﬀbetween the complexity of Fa measured by Rm(Fa) and the fraction of training data that has been classiﬁed correctly with a margin of at least γ.,
"In particular this suggests, that (even if we classify the training data correctly with respect to the 0-1 loss) it might be beneﬁcial to further increase the complexity of Fa to simultaneously increase the margins by which the training data has been classiﬁed correctly and thus obtain a better generalization bound.",
"2.3 Optimization and implicit regularization The optimization algorithm, which is usually a variant of SGD, seems to play an important role for the generalization performance.",
"Potential indicators for good generalization performance are high speed of convergence [HRS16] or ﬂatness of the local minimum to which SGD converged, which can be characterized by the magnitude of the eigenvalues of the Hessian (or approximately as the robustness of the minimizer to adversarial perturbations on the parameter space), see [KMN+17].",
"In [DR17, NBMS17] generalization bounds depending on a concept of ﬂatness are established by employing a PAC-Bayesian framework, which can be viewed as a generalization of Theorem 1.17, see [McA99].",
"Further, one can also unite ﬂatness and norm-based bounds by the Fisher–Rao metric of information geometry [LPRS19].",
"Let us motivate the link between generalization and ﬂatness in the case of simple linear models: We assume that our model takes the form ⟨θ, ·⟩, θ ∈Rd, and we will use the abbreviations r(θ) := bRs(⟨θ, ·⟩) and γ(θ) := min i∈[m] M(⟨θ, ·⟩, z(i)) = min i∈[m] y(i)⟨θ, x(i)⟩ throughout this subsection to denote the empirical risk and the margin for given training data s = ((x(i), y(i)))m i=1.",
We assume that we are solving a classiﬁcation task with the 0-1 loss and that our training 25 data is linearly separable.,
This means that there exists a minimizer ˆθ ∈Rd such that r(ˆθ) = 0.,
"We observe that δ-robustness in the sense that max θ∈Bδ(0) r(ˆθ + θ) = r(ˆθ) = 0 implies that 0 < min i∈[m] y(i)⟨ˆθ −δy(i) x(i) ∥x(i)∥2 , x(i)⟩≤γ(ˆθ) −δ min i∈[m] ∥x(i)∥2, see also [PKL+17].",
This lower bound on the margin γ(ˆθ) then ensures generalization guarantees as described in Subsection 2.2.,
"Even without explicit19 control on the complexity of Fa, there do exist results showing that SGD acts as implicit regularization [NTS14].",
This is motivated by linear models where SGD converges to the minimal Euclidean norm solution for the quadratic loss and in the direction of the hard margin support vector machine solution for the logistic loss on linearly separable data [SHN+18].,
"Note that convergence to minimum norm or maximum margin solutions in particular decreases the complexity of our hypothesis set and thus improves generalization bounds, see Subsection 2.2.",
"While we have seen this behavior of gradient descent for linear regression already in the more general context of kernel regression in Subsection 2.1, we want to motivate the corresponding result for classiﬁcation tasks in the following.",
"We focus on the exponential surrogate loss Lsurr(f, z) = ℓ(M(f, z)) = e−yf(x) with ℓ(z) = e−z, but similar observations can be made for the logistic loss deﬁned in Remark 1.9.",
"We assume that the training data is linearly separable, which guarantees the existence of ˆθ ̸= 0 with γ(ˆθ) > 0.",
"Then for every linear model ⟨θ, ·⟩, θ ∈Rd, it holds that ˆθ, ∇θr(θ)⟩= 1 m m X i=1 ℓ′(y(i)⟨θ, x(i)⟩) | {z } <0 y(i)⟨ˆθ, x(i)⟩ | {z } >0 .",
"A critical point ∇θr(θ) = 0 can therefore be approached if and only if for every i ∈[m] we have ℓ′(y(i)⟨θ, x(i)⟩) = −e−y(i)⟨θ,x(i)⟩→0, which is equivalent to ∥θ∥2 →∞and γ(θ) > 0.",
"Let us now deﬁne rβ(θ) := ℓ−1(r(βθ)) β , θ ∈Rd, β ∈(0, ∞), and observe that rβ(θ) = −log(r(βθ)) β →γ(θ), β →∞.",
"(2.6) Due to this property, rβ is often referred to as the smoothed margin [LL19, JT19b].",
"We evolve θ according to gradient ﬂow with respect to the smoothed margin r1, i.e., dθ(t) dt = ∇θr1(θ(t)) = − 1 r(θ(t))∇θr(θ(t)), which produces the same trajectory as gradient ﬂow with respect to the empirical risk r under a rescaling of the time t. Looking at the evolution of the normalized parameters ˜θ(t) = θ(t)/∥θ(t)∥2, the chain rule establishes that d˜θ(t) dt = P˜θ(t) ∇θrβ(t)(˜θ(t)) β(t) with β(t) := ∥θ(t)∥2 and Pθ := Id −θθT , θ ∈Rd.",
"19Note that also diﬀerent architectures can exhibit vastly diﬀerent inductive biases [ZBH+20] and also within the architecture diﬀerent parameters have diﬀerent importance, see [FC18, ZBS19] and Proposition 6.2.",
"26 This shows that the normalized parameters perform projected gradient ascent with respect to the function rβ(t), which converges to the margin due to (2.6) and the fact that β(t) = ∥θ(t)∥2 →∞when approaching a critical point.",
This motivates that during gradient ﬂow the normalized parameters implicitly maximize the margin.,
"See [GLSS18a, GLSS18b, LL19, NLG+19, CB20, JT20] for a precise analysis and various extensions, e.g., to homogeneous or two-layer NNs and other optimization geometries.",
"To illustrate one research direction, we present an exemplary result in the following.",
"Let Φ = Φ(N,ϱ) be a biasless NN with parameters θ = ((W (ℓ), 0))L ℓ=0 and output dimension NL = 1.",
"For given input features x ∈RN0, the gradient ∇W (ℓ)Φ = ∇W (ℓ)Φ(x, θ) ∈RNℓ−1×Nℓwith respect to the weight matrix in the ℓ-th layer satisﬁes that ∇W (ℓ)Φ = ϱ(Φ(ℓ−1)) ∂Φ ∂Φ(ℓ+1) ∂Φ(ℓ+1) ∂Φ(ℓ) = ϱ(Φ(ℓ−1)) ∂Φ ∂Φ(ℓ+1) W (ℓ+1) diag  ϱ′(Φ(ℓ))  , where the pre-activations (Φ(ℓ))L ℓ=1 are given as in (1.1).",
"Evolving the parameters according to gradient ﬂow as in (2.1) and using an activation function ϱ with ϱ(x) = ϱ′(x)x, such as the ReLU, this implies that diag  ϱ′(Φ(ℓ))  W (ℓ)(t) dW (ℓ)(t) dt T = dW (ℓ+1)(t) dt T W (ℓ+1)(t) diag  ϱ′(Φ(ℓ))  .",
"(2.7) Note that this ensures the conservation of balancedness between weight matrices of adjacent layers, i.e., d dt  ∥W (ℓ+1)(t)∥2 F −∥W (ℓ)(t)∥2 F  = 0, see [DHL18].",
"Furthermore, for deep linear NNs, i.e., ϱ(x) = x, the property in (2.7) implies conservation of alignment of left and right singular spaces of W (ℓ) and W (ℓ+1).",
"This can then be used to show implicit pre- conditioning and convergence of gradient descent [ACH18, ACGH19] and that, under additional assumptions, gradient descent converges to a linear predictor that is aligned with the maximum margin solution [JT19a].",
"2.4 Limits of classical theory and double descent There is ample evidence that classical tools from statistical learning theory alone, such as Rademacher averages, uniform convergence, or algorithmic stability may be unable to explain the full generalization capabilities of NNs [ZBH+17, NK19].",
It is especially hard to reconcile the classical bias-variance trade-oﬀwith the observation of good generalization performance when achieving zero empirical risk on noisy data using a regression loss.,
"On top of that, this behavior of overparametrized models in the interpolation regime turns out not to be unique to NNs.",
"Empirically, one observes for various methods (decision trees, random features, linear models) that the test error decreases even below the sweet-spot in the u-shaped bias-variance curve when further increasing the number of parameters [BHMM19, GJS+20, NKB+20].",
"This is often referred to as the double descent curve or benign overﬁtting, see Figure 2.1.",
"For special cases, e.g., linear regression or random feature regression, such behavior can even be proven, see [HMRT19, MM19, BLLT20, BHX20, MVSS20].",
In the following we analyze this phenomenon in the context of linear regression.,
"Speciﬁcally, we focus on a prediction task with quadratic loss, input features given by a centered Rd-valued random variable X, and labels given by Y = ⟨θ∗, X⟩+ ν, where θ∗∈Rd and ν is a centered random variable independent of X.",
"For training data S = ((X(i), Y (i)))m i=1, we consider the empirical risk minimizer bfS = ⟨ˆθ, ·⟩with minimum Euclidean norm of its parameters ˆθ or, equivalently, the limit of gradient ﬂow with zero initialization.",
"Using (1.3) and a bias-variance decomposition we can write E[R( bfS)|(X(i))m i=1] −R∗= E[∥bfS −f ∗∥L2(PX)|(X(i))m i=1] = (θ∗)T PE[XXT ]Pθ∗+ E[ν2]Tr  Σ+E[XXT ]  , where Σ := Pm i=1 X(i)(X(i))T , Σ+ denotes the Moore–Penrose inverse of Σ, and P := Id −Σ+Σ is the orthogonal projector onto the kernel of Σ.",
"For simplicity, we focus on the variance Tr  Σ+E[XXT ]  , which can 27 Figure 2.1: This illustration shows the classical, underparametrized regime in green, where the u-shaped curve depicts the bias-variance trade-oﬀas explained in Section 1.2.",
"Starting with complexity of our algorithm A larger than the interpolation threshold we can achieve zero empirical risk bRs(fs) (training error), where fs = A(s).",
"Within this modern interpolation regime, the risk R(fs) (test error) might be even lower than at the classical sweet spot.",
"Whereas complexity(A) traditionally refers to the complexity of the hypothesis set F, there is evidence that also the optimization scheme and the data is inﬂuencing the complexity leading to deﬁnitions like complexity(A) := max  m ∈N: E  bRS(A(S))  ≤ε with S ∼Pm Z , for suitable ε > 0 [NKB+20].",
This illustration is based on [BHMM19].,
be viewed as setting θ∗= 0 and E[ν2] = 1.,
Assuming that X has i.i.d.,
"entries with unit variance and bounded ﬁfth moment, the distribution of the eigenvalues of 1 mΣ+ in the limit d, m →∞with d m →κ ∈(0, ∞) can be described via the Marchenko–Pastur law.",
"Therefore, the asymptotic variance can be computed explicitly as Tr  Σ+E[XXT ]  →1 −max{1 −κ, 0} |1 −κ| for d, m →∞ with d m →κ, almost surely, see [HMRT19].",
This shows that despite interpolating the data we can decrease the risk in the overparametrized regime κ > 1.,
"In the limit d, m →∞, such benign overﬁtting can also be shown for more general settings (including lazy training of NNs), some of which even achieve their optimal risk in the overparametrized regime [MM19, MZ20, LD21].",
"For normally distributed input features X such that E[XXT ] has rank larger than m, one can also compute the behavior of the variance in the non-asymptomatic regime [BLLT20].",
"Deﬁne k∗:= min{k ≥0: P i>k λi λk+1 ≥cm}, where λ1 ≥λ2 ≥· · · ≥λd ≥0 are the eigenvalues of E[XXT ] in decreasing order and c ∈(0, ∞) is a universal constant.",
"Assuming that k∗/m is suﬃciently small, with high probability it holds that Tr  Σ+E[XXT ]  ≈k∗ m + m P i>k∗λ2 i (P i>k∗λi)2 .",
"28 0 50 100 150 d 0.0 0.5 1.0 variance Figure 2.2: The expected variance of linear regression in (2.8) with d ∈ [150] and Xi ∼U({−1, 1}), i ∈[150], where Xi = X1 for i ∈{10, .",
", 20} ∪ {30, .",
", 50} and all other coordinates are independent.",
This precisely characterizes the regimes for benign overﬁtting in terms of the eigenvalues of the covariance matrix E[XXT ].,
"Furthermore, it shows that adding new input feature coordinates and thus increasing the number of parameters d can lead to either an increase or decrease of the risk.",
"To motivate this phenomenon, which is considered in much more depth in [CMBK20], let us focus on a single sample m = 1 and features X that take values in X = {−1, 1}d. Then it holds that Σ+ = X(1)(X(1))T ∥X(1)∥4 = X(1)(X(1))T d2 and thus E  Tr  Σ+E[XXT ]  = 1 d2 E  XXT  2 F .",
"(2.8) In particular, this shows that incrementing the input feature dimen- sions d 7→d + 1 one can increase or decrease the risk depending on the correlation of the coordinate Xd+1 with respect to the previous coordinates (Xi)d i=1, see also Figure 2.2.",
"Generally speaking, overparametrization and perfectly ﬁtting noisy data does not exclude good generalization performance, see also [BRT19].",
"However, the risk crucially depends on the data distribution and the chosen algorithm.",
"3 The role of depth in the expressivity of neural networks The approximation theoretical aspect of a NN architecture, responsible for the approximation component εapprox := R(f ∗ F) −R∗of the error R(fS) −R∗in (1.4), is probably one of the most well-studied parts of the deep learning pipe-line.",
The achievable approximation error of an architecture most directly describes the power of the architecture.,
"As mentioned in Subsection 1.3, many classical approaches only study the approximation theory of NNs with few layers, whereas modern architectures are typically very deep.",
A ﬁrst observation into the eﬀect of depth is that it can often compensate for insuﬃcient width.,
"For example, in the context of the universal approximation theorem, it was shown that very narrow NNs are still universal if instead of increasing the width, the number of layers can be chosen arbitrarily [HS17, Han19, KL20].",
"However, if the width of a NN falls below a critical number, then the universality will not hold any longer.",
"Below, we discuss three additional observations that shed light on the eﬀect of depth on the approximation capacities or alternative notions of expressivity of NNs.",
"3.1 Approximation of radial functions One technique to study the impact of depth relies on the construction of speciﬁc functions which can be well approximated by NNs of a certain depth, but require signiﬁcantly more parameters when approximated to the same accuracy by NNs of smaller depth.",
"In the following we present one example for this type of approach, which can be found in [ES16].",
Theorem 3.1 (Power of depth).,
"Let ϱ ∈{ϱR, ϱσ, 1(0,∞)} be the ReLU, the logistic, or the Heaviside function.",
"Then there exist constants c, C ∈(0, ∞) with the following property: For every d ∈N with d ≥C there exist a probability measure µ on Rd, a three-layer NN architecture a = (N, ϱ) = ((d, N1, N2, 1), ϱ) with ∥N∥∞≤Cd5, and corresponding parameters θ∗∈RP (N) with ∥θ∗∥∞≤CdC and ∥Φa(·, θ∗)∥L∞(Rd) ≤2 such that for every n ≤cecd it holds that inf θ∈RP ((d,n,1)) ∥Φ((d,n,1),ϱ)(·, θ) −Φa(·, θ∗)∥L2(µ) ≥c.",
"29 In fact, the activation function in Theorem 3.1 is only required to satisfy mild conditions and the result holds, for instance, also for more general sigmoidal functions.",
"The proof of Theorem 3.1 is based on the construction of a suitable radial function g: Rd →R, i.e., g(x) = ˜g(∥x∥2 2) for some ˜g: [0, ∞) →R, which can be eﬃciently approximated by three-layer NNs but approximation by only a two-layer NN requires exponentially large complexity, i.e., the width being exponential in d. The ﬁrst observation of [ES16] is that g can typically be well approximated on a bounded domain by a three-layer NN, if ˜g is Lipschitz continuous.",
"Indeed, for the ReLU activation function it is not diﬃcult to show that, emulating a linear interpolation, one can approximate a univariate C-Lipschitz function uniformly on [0, 1] up to precision ε by a two-layer architecture of width O(C/ε).",
"The same holds for smooth, non-polynomial activation functions due to Theorem 1.16.",
"This implies that the squared Euclidean norm, as a sum of d univariate functions, i.e., [0, 1]d ∋x 7→Pd i=1 x2 i , can be approximated up to precision ε by a two-layer architecture of width O(d2/ε).",
"Moreover, this shows that the third layer can eﬃciently approximate ˜g, establishing approximation of g on a bounded domain up to precision ε using a three-layer architecture with number of parameters polynomial in d/ε.",
"The second step in [ES16] is to choose g in such a way that the realization of any two-layer neural network Φ = Φ((d,n,1),ϱ)(·, θ) with width n not being exponential in d is on average (with respect to the probability measure µ) a constant distance away from g. Their argument is heavily based on ideas from Fourier analysis and will be outlined below.",
"In this context, let us recall that we denote by ˆf the Fourier transform of a suitable function or, more generally, tempered distribution f. Assuming that the square-root ϕ of the density function associated with the probability measure µ as well as Φ and g are well-behaved, the Plancherel theorem yields that ∥Φ −g∥2 L2(µ) = ∥Φϕ −gϕ∥2 L2(Rd) = c Φϕ −c gϕ 2 L2(Rd).",
"(3.1) Next, the speciﬁc structure of two-layer NNs is used, which implies that for every j ∈[n] there exists wj ∈Rd with ∥wj∥2 = 1 and ϱj : R →R (subsuming the activation function ϱ, the norm of wj, and the remaining parameters corresponding to the j-th neuron in the hidden layer) such that Φ is of the form Figure 3.1: This illustration shows the largest possible support (blue) of c Φϕ, where ˆϕ = 1Br(0) and Φ is a shallow neural network with architec- ture N = (2, 4, 1) and weight matrix W (1) = [w1 .",
w4]T in the ﬁrst layer.,
Any radial function with enough of its L2-mass located at high frequencies (indicated by the red area) cannot be well approximated by Φϕ.,
"Φ = n X j=1 ϱj(⟨wj, ·⟩) = n X j=1 (ϱj ⊗1Rd−1) ◦Rwj.",
"The second equality follows by viewing the action of the j-th neuron as a tensor product of ϱj and the indicator function 1Rd−1(x) = 1, x ∈Rd−1, composed with a d-dimensional rotation Rwj ∈SO(d) which maps wj to the ﬁrst standard basis vector e(1) ∈Rd.",
"Noting that the Fourier transform respects linearity, rotations, and tensor products, we can compute ˆΦ = n X j=1 (ˆϱj ⊗δRd−1) ◦Rwj, where δRd−1 denotes the Dirac distribution on Rd−1.",
"In particular, the support of ˆΦ has a particular star-like shape, namely Sn j=1 span{wj}, which are in fact lines passing through the origin.",
"Now we choose ϕ to be the inverse Fourier transform of the indicator function of a ball Br(0) ⊂Rd with vol(Br(0)) = 1, ensuring that ϕ2 is a valid probability density for µ as µ(Rd) = ∥ϕ2∥L1(Rd) = ∥ϕ∥2 L2(Rd) = ∥ˆϕ∥2 L2(Rd) = ∥1Br(0)∥2 L2(Rd) = 1.",
"Using the convolution theorem, this choice of ϕ yields that supp( c Φϕ) = supp(ˆΦ ∗ˆϕ) ⊂ n [ j=1 (span{wj} + Br(0)) .",
30 Thus the lines passing through the origin are enlarged to tubes.,
"It is this particular shape which allows the construction of some g so that ∥c Φϕ−c gϕ∥2 L2(Rd) can be suitably lower bounded, see also Figure 3.1.",
"Intriguingly, the peculiar behavior of high-dimensional sets now comes into play.",
"Due to the well known concentration of measure principle, the variable n needs to be exponentially large for the set Sn j=1 (span{wj} + Br(0)) to be not sparse.",
"If it is smaller, one can construct a function g so that the main energy content of c gϕ has a certain distance from the origin, yielding a lower bound for ∥c Φϕ −c gϕ∥2 and hence ∥Φ −g∥2 L2(µ), see (3.1).",
"One key technical problem is the fact that such a behavior for ˆg does not immediately imply a similar behavior of c gϕ, requiring a quite delicate construction of g. 3.2 Deep ReLU networks 0 1 8 2 8 3 8 4 8 5 8 6 8 7 8 1 0 1 32 2 32 3 32 4 32 5 32 6 32 7 32 8 32 g I1 g I1 I2 I1 g I2 I3 I2 Figure 3.2: Interpolation In of [0, 1] ∋ x 7→g(x) := x −x2 on 2n + 1 equidis- tant points, which can be represented as a sum In = Pn k=1 Ik −Ik−1 = Pn k=1 hk 22k of n sawtooth functions.",
Each sawtooth function hk = hk−1◦h in turn can be written as a k-fold composition of a hat function h. This illustration is based on [EPGB19].,
"Maybe for no activation function is the eﬀect of depth clearer than for the ReLU activation function ϱR(x) = max{0, x}.",
"We refer to corresponding NN architectures (N, ϱR) as ReLU (neural) networks (ReLU NNs).",
"A two-layer ReLU NN with one-dimensional input and output is a function of the form Φ(x) = n X i=1 w(2) i ϱR(w(1) i x + b(1) i ) + b(2), x ∈R, where w(1) i , w(2) i , b(1) i , b(2) ∈R for i ∈[n].",
It is not hard to see that Φ is a continuous piecewise aﬃne linear function.,
"Moreover, Φ has at most n + 1 aﬃne linear pieces.",
"On the other hand, notice that the hat function h: [0, 1] →[0, 1], x 7→2ϱR(x) −4ϱR(x −1 2) = ( 2x, if 0 ≤x < 1 2, 2(1 −x), if 1 2 ≤x ≤1, is a NN with two layers and two neurons.",
Telgarsky observed that the n-fold convolution hn(x) := h ◦· · · ◦h produces a sawtooth function with 2n spikes [Tel15].,
"In particular, hn admits 2n aﬃne linear pieces with only 2n many neurons.",
"In this case, we see that deep ReLU NNs are in some sense exponentially more eﬃcient in generating aﬃne linear pieces.",
"Moreover, it was noted in [Yar17] that the diﬀerence of interpolations of [0, 1] ∋x 7→x −x2 at 2n + 1 and 2n−1 + 1 equidistant points equals the scaled sawtooth function hn 22n , see Figure 3.2.",
"This allows to eﬃciently implement approximative squaring and, by polarization, also approximative multiplication using ReLU NNs.",
"Composing these simple functions one can approximate localized Taylor polynomials and thus smooth functions, see [Yar17].",
"We state below a generalization [GKP20] of the result of [Yar17] which includes more general norms, but for p = ∞and s = 0 coincides with the original result of Dmitry Yarotsky.",
Theorem 3.2 (Approximation of Sobolev-regular functions).,
"Let d, k ∈N with k ≥2, let p ∈[1, ∞], s ∈[0, 1], B ∈(0, ∞), and let ϱ be a piecewise linear activation function with at least one break-point.",
"Then there exists a constant c ∈(0, ∞) with the following property: For every ε ∈(0, 1/2) there exists a NN architecture a = (N, ϱ) with P(N) ≤cε−d/(k−s) log(1/ε) such that for every function g ∈W k,p((0, 1)d) with ∥g∥W k,p((0,1)d) ≤B it holds that inf θ∈RP (N) ∥Φa(θ, ·) −g∥W s,p((0,1)d) ≤ε.",
31 The ability of deep ReLU neural networks to emulate multiplication has also been employed to reap- proximate wide ranges of high-order ﬁnite element spaces.,
In [OPS20] and [MOPS20] it was shown that deep ReLU neural networks are capable of achieving the approximation rates of hp-ﬁnite element methods.,
"Concretely, this means that for piecewise analytic functions, which appear, for example, as solutions of elliptic boundary and eigenvalue problems with analytic data, exponential approximation rates can be achieved.",
"In other words, the number of parameters of neural networks to approximate such a function in the W 1,2-norm up to an error of ε is logarithmic in ε. Theorem 3.2 requires the depth of the NN to grow.",
"In fact, it can be shown that the same approximation rate cannot be achieved with shallow NNs.",
"Indeed, there exists a certain optimal number of layers and, if the architecture has fewer layers than optimal, then the NNs need to have signiﬁcantly more parameters, to achieve the same approximation ﬁdelity.",
"This has been observed in many diﬀerent settings in [LS17, SS17, Yar17, PV18, EPGB19].",
We state here the result of [Yar17]: Theorem 3.3 (Depth-width approximation trade-oﬀ).,
"Let d, L ∈N with L ≥2 and let g ∈C2([0, 1]d) be a function which is not aﬃne linear.",
"Then there exists a constant c ∈(0, ∞) with the following property: For every ε ∈(0, 1) and every ReLU NN architecture a = (N, ϱR) = ((d, N1, .",
", NL−1, 1), ϱR) with L layers and ∥N∥1 ≤cε−1/(2(L−1)) neurons it holds that inf θ∈RP (N) ∥Φa(·, θ) −g∥L∞([0,1]d) ≥ε.",
depth width Figure 3.3: Standard feed-forward neural network.,
"For certain approximation results, depth and width need to be in a ﬁxed relationship to achieve optimal results.",
This results is based on the observation that ReLU NNs are piecewise aﬃne linear.,
The number of pieces they admit is linked to their capacity of approximating functions that have non-vanishing curvature.,
"Using a construction similar to the ex- ample at the beginning of this subsection, it can be shown that the number of pieces that can be gener- ated using an architecture ((1, N1, .",
", NL−1, 1), ϱR) scales roughly like QL−1 ℓ=1 Nℓ.",
"In the framework of the aforementioned results, we can speak of a depth-width trade-oﬀ, see also Fig- ure 3.3.",
A ﬁne-grained estimate of achievable rates for freely varying depths has also been established in [She20].,
3.3 Alternative notions of expressivity Conceptual approaches to study the approximation power of deep NNs besides the classical approximation framework usually aim to relate structural properties of the NN to the “richness” of the set of possibly expressed functions.,
"One early result in this direction is [MPCB14] which describes bounds on the number of aﬃne linear regions of a ReLU NN Φ(N,ϱR)(·, θ).",
"In a simpliﬁed setting, we have seen estimates on the number of aﬃne linear pieces already at the beginning of Subsection 3.2.",
"Aﬃne linear regions can be deﬁned as the connected components of RN0 \ H, where H is the set of non-diﬀerentiability of the realization20 Φ(N,ϱR)(·, θ).",
"A reﬁned analysis on the number of such regions was, for example, conducted by [HvdG19].",
It is found that deep ReLU neural networks can exhibit signiﬁcantly more regions than their shallow counterparts.,
"20One can also study the potentially larger set of activation regions given by the connected components of RN0 \  ∪L−1 ℓ=1 ∪Nℓ i=1Hi,ℓ  , where Hi,ℓ:= {x ∈RN0 : Φ(ℓ) i (x, θ) = 0}, with Φ(ℓ) i as in (1.1), is the set of non-diﬀerentiability of the activation of the i-th neuron in the ℓ-th layer.",
"In contrast to the linear regions, the activation regions are necessarily convex [RPK+17, HR19].",
"32 Figure 3.4: Shape of the trajectory t 7→Φ((2,n,...,n,2),ϱR)(γ(t), θ) of the output of a randomly initialized network with 0, 3, 10 hidden layers.",
The input curve γ is the circle given in the leftmost image.,
The hidden layers have n = 20 neurons and the variance of the initialization is taken as 4/n.,
"The reason for this eﬀectiveness of depth is described by the following analogy: Through the ReLU each neuron Rd ∋x 7→ϱR(⟨x, w⟩+ b), w ∈Rd, b ∈R, splits the space into two aﬃne linear regions separated by the hyperplane {x ∈Rd : ⟨x, w⟩+ b = 0}.",
"A shallow ReLU NN Φ((d,n,1),ϱR)(·, θ) with n neurons in the hidden layer therefore produces a number of regions deﬁned through n hyperplanes.",
"Using classical bounds on the number of regions deﬁned through hyperplane arrangements [Zas75], one can bound the number of aﬃne linear regions by Pd j=0  n j  .",
Deepening neural networks then corresponds to a certain folding of the input space.,
Through this interpretation it can be seen that composing NNs can lead to a multiplication of the number of regions of the individual NNs resulting in an exponential eﬃciency of deep neural networks in generating aﬃne linear regions21.,
This approach was further developed in [RPK+17] to a framework to study expressivity that to some extent allows to include the training phase.,
One central object studied in [RPK+17] are so-called trajectory lengths.,
"In this context, one analyzes how the length of a non-constant curve in the input space changes in expectation through the layers of a NN.",
The authors ﬁnd an exponential dependence of the expected curve length on the depth.,
"Let us motivate this in the special case of a ReLU NN with architecture a = ((N0, n, .",
", n, NL), ϱR) and depth L ∈N.",
"Given a non-constant continuous curve γ : [0, 1] →RN0 in the input space, the length of the trajectory in the ℓ-th layer of the NN Φa(·, θ) is then given by Length(¯Φ(ℓ)(γ(·), θ)), ℓ∈[L −1], where ¯Φ(ℓ)(·, θ) is the activation in the ℓ-th layer, see (1.1).",
"Here the length of the curve is well-deﬁned since ¯Φ(ℓ)(·, θ)) is continuous and therefore ¯Φ(ℓ)(γ(·), θ) is continuous.",
"Now, let the parameters Θ1 of the NN Φa be initialized independently so that the entries corresponding to the weight matrices and bias vectors follow a normal distribution with zero mean and variances 1/n and 1, respectively.",
"It is not hard to see, e.g., by Proposition 1.1, that the probability that ¯Φ(ℓ)(·, Θ1) will map γ to a non-constant curve is positive and hence, for ﬁxed ℓ∈[L −1], E  Length(¯Φ(ℓ)(γ(·), Θ1))  = c > 0.",
"Let σ ∈(0, ∞) and consider a second initialization Θσ, where we change the variances of the entries corresponding to the weight matrices and bias vectors to σ2/n and σ2, respectively.",
"Recall that the ReLU is positively homogeneous, i.e., we have that ϱR(λx) = λϱR(x) for all λ ∈(0, ∞).",
"Then it is clear that ¯Φ(ℓ)(·, Θσ) ∼σℓ¯Φ(ℓ)(·, Θ1), 21However, to exploit this eﬃciency with respect to the depth, one requires highly oscillating pre-activations which in turn can only be achieved with a delicate selection of parameters.",
"In fact, it can be shown that through random initialization the expected number of activation regions per unit cube depends mainly on the number of neurons in the NN, rather than its depth [HR19].",
"33 i.e., the activations corresponding to the two initialization strategies are identically distributed up to the factor σℓ.",
"Therefore, we immediately conclude that E  Length(¯Φ(ℓ)(γ(·), Θσ))  = σℓc.",
"This shows that the expected trajectory length depends exponentially on the depth of the NN, which is in line with the behavior of other notions of expressivity [PLR+16].",
In [RPK+17] this result is also extended to the tanh activation function and the constant c is more carefully resolved.,
"Empirically one also ﬁnds that the shapes of the trajectories become more complex in addition to becoming longer on average, see Figure 3.4.",
4 Deep neural networks overcome the curse of dimensionality M Figure 4.1: Illustration of a one- dimensional manifold M embedded in R3.,
For every point x ∈M there exists a neighborhood in which the manifold can be linearly projected onto its tangent space at x such that the corresponding inverse function is diﬀerentiable.,
"In Subsection 1.3, one of the main puzzles of deep learning that we identiﬁed was the surprising performance of deep architectures on problems where the input dimensions are very high.",
"This perfor- mance cannot be explained in the framework of classical approx- imation theory, since such results always suﬀer from the curse of dimensionality [Bel52, DeV98, NW09].",
"In this section, we present three approaches that oﬀer explana- tions of this phenomenon.",
"As before, we had to omit certain ideas which have been very inﬂuential in the literature to keep the length of this section under control.",
"In particular, an important line of reasoning is that functions to be approximated often have composi- tional structures which NNs may approximate very well as reviewed in [PMR+17].",
"Note that also a suitable feature descriptor, factor- ing out invariances, might lead to a signiﬁcantly reduced eﬀective dimension, see Subsection 7.1.",
4.1 Manifold assumption A ﬁrst remedy to the high-dimensional curse of dimensionality is what we call the manifold assumption.,
"Here it is assumed that we are trying to approximate a function g: Rd ⊃X →R, where d is very large.",
"However, we are not seeking to optimize with respect to the uniform norm or a regular Lp space, but instead consider a measure µ which is supported on a d′-dimensional manifold M ⊂X.",
Then the error is measured in the Lp(µ)-norm.,
Here we consider the case where d′ ≪d.,
"This setting is appropriate if the data z = (x, y) of a prediction task is generated from a measure supported on M × R. This set-up or generalizations thereof have been fundamental in [CM18, SCC18, CJLZ19, SH19, CK20, NI20].",
"Let us describe an exemplary approach, where we consider locally Ck-regular functions and NNs with ReLU activation functions below: 1.",
"Describe the regularity of g on the manifold: Naturally, we need to quantify the regularity of the function g restricted to M in an adequate way.",
The typical approach would be to make a deﬁnition via local coordinate charts.,
"If we assume that M is an embedded submanifold of X, then locally, i.e., in a neighborhood of a point x ∈M, the orthogonal projection of M onto the d′-dimensional tangent space TxM is a diﬀeomorphism.",
The situation is depicted in Figure 4.1.,
"Assuming M to be compact, we can choose a ﬁnite set of open balls (Ui)p i=1 that cover M and on which the local projections γi onto the respective tangent spaces as described above exists and are diﬀeomorphisms.",
Now we can deﬁne the regularity of g via classical regularity.,
"In this example, we say that g ∈Ck(M) if g ◦γ−1 i ∈Ck(γi(M ∩Ui)) for all i ∈[p].",
"Construct localization and charts via neural networks: According to the construction of local coordinate charts in Step 1, we can write g as follows: g(x) = p X i=1 φi(x)  g ◦γ−1 i (γi(x))  =: p X i=1 ˜gi(γi(x), φi(x)), x ∈M, (4.1) where φi is a partition of unity such that supp(φi) ⊂Ui.",
"Note that γi is a linear map, hence representable by a one-layer NN.",
"Since multiplication is a smooth operation, we have that if g ∈Ck(M) then ˜gi ∈Ck(γi(M ∩Ui) × [0, 1]).",
The partition of unity φi needs to be emulated by NNs.,
"For example, if the activation function is the ReLU, then such a partition can be eﬃciently constructed.",
"Indeed, in [HLXZ20] it was shown that such NNs can represent linear ﬁnite elements exactly with ﬁxed-size NNs and hence a partition of unity subordinate to any given covering of M can be constructed.",
"Use a classical approximation result on the localized functions: By some form of Whitney’s extension theorem [Whi34], we can extend each ˜gi to a function ¯gi ∈Ck(X ×[0, 1]) which by classical results can be approximated up to an error of ε > 0 by NNs of size O(ε−(d′+1)/k) for ε →0, see [Mha96, Yar17, SCC18].",
"Use the compositionality of neural networks to build the ﬁnal network: We have seen that every component in the representation (4.1), i.e., ˜gi, γi, and φi can be eﬃciently represented by NNs.",
"In addition, composition and summation are operations which can directly be implemented by NNs through increasing their depth and widening their layers.",
"Hence (4.1) is eﬃciently—i.e., with a rate depending only on d′ instead of the potentially much larger d—approximated by a NN.",
"Overall, we see that NNs are capable of learning local coordinate transformations and therefore reduce the complexity of a high-dimensional problem to the underlying low-dimensional problem given by the data distribution.",
"4.2 Random sampling Already in 1992, Andrew Barron showed that under certain seemingly very natural assumptions on the function to approximate, a dimension-independent approximation rate by NNs can be achieved [Bar92, Bar93].",
"Speciﬁcally, the assumption is formulated as a condition on the Fourier transform of a function and the result is as follows.",
Theorem 4.1 (Approximation of Barron-regular functions).,
Let ϱ: R →R be the ReLU or a sigmoidal function.,
"Then there exists a constant c ∈(0, ∞) with the following property: For every d, n ∈N, every probability measure µ supported on B1(0) ⊂Rd, and every g ∈L1(Rd) with Cg := R Rd ∥ξ∥2|ˆg(ξ)| dξ < ∞it holds that inf θ∈RP ((d,n,1)) ∥Φ((d,n,1),ϱ)(·, θ) −g∥L2(µ) ≤ c √nCg, Note that the L2-approximation error can be replaced by an L∞-estimate over the unit ball at the expense of a factor of the order of √ d on the right-hand side.",
"The key idea behind Theorem 4.1 is the following application of the law of large numbers: First, we observe that, per assumption, g can be represented via the inverse Fourier transform, as g −g(0) = Z Rd ˆg(ξ)(e2πi⟨·,ξ⟩−1) dξ = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨·,ξ⟩−1) 1 Cg ∥ξ∥2ˆg(ξ) dξ = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨·,ξ⟩−1) dµg(ξ), (4.2) 35 where µg is a probability measure.",
"Then it is further shown in [Bar92] that there exist (Rd × R)-valued random variables (Ξ, eΞ) such that (4.2) can be written as g(x) −g(0) = Cg Z Rd 1 ∥ξ∥2 (e2πi⟨x,ξ⟩−1) dµg(ξ) = CgE  Γ(Ξ, eΞ)(x)  , x ∈Rd, (4.3) where for every ξ ∈Rd, ˜ξ ∈R the function Γ(ξ, ˜ξ): Rd →R is given by Γ(ξ, ˜ξ) := s(ξ, ˜ξ)(1(0,∞)(−⟨ξ/∥ξ∥2, ·⟩−˜ξ) −1(0,∞)(⟨ξ/∥ξ∥2, ·⟩−˜ξ)) with s(ξ, ˜ξ) ∈{−1, 1}.",
"Now, let ((Ξ(i), eΞ(i)))i∈N be i.i.d.",
"random variables with (Ξ(1), eΞ(1)) ∼(Ξ, eΞ).",
"Then, Bienaym´e’s identity and Fubini’s theorem establish that E "" g −g(0) −Cg n n X i=1 Γ(Ξ(i), eΞ(i)) 2 L2(µ) # = Z B1(0) V "" Cg n n X i=1 Γ(Ξ(i), eΞ(i))(x) # dµ(x) = C2 g R B1(0) V  Γ(Ξ, eΞ)(x)  dµ(x) n ≤(2πCg)2 n , (4.4) where the last inequality follows from combining (4.3) with the fact that |e2πi⟨x,ξ⟩−1|/∥ξ∥2 ≤2π, x ∈B1(0).",
"This implies that there exists a realization ((ξ(i), ˜ξ(i)))i∈N of the random variables ((Ξ(i), eΞ(i)))i∈N that achieves L2-approximation error of n−1/2.",
"Therefore, it remains to show that NNs can well approximate the functions ((Γ(ξ(i), ˜ξ(i)))i∈N.",
"Now it is not hard to see that the function 1(0,∞) and hence functions of the form Γ(ξ, ˜ξ), ξ ∈Rd, ˜ξ ∈R, can be arbitrarily well approximated with a ﬁxed-size, two-layer NN with a sigmoidal or ReLU activation function.",
"Thus, we obtain an approximation rate of n−1/2 when approximating functions with one ﬁnite Fourier moment by two-layer NNs with n hidden neurons.",
"It was pointed out already in the dissertation of Emmanuel Cand`es [Can98] that the approximation rate of NNs for Barron-regular functions is also achievable by n-term approximation with complex exponentials, as is apparent by considering (4.2).",
"However, for deeper NNs, the results also extend to high-dimensional non- smooth functions, where Fourier-based methods are certain to suﬀer from the curse of dimensionality [CPV20].",
"In addition, the random sampling idea above was extended in [EMW19b, EMWW20, EW20b, EW20c] to facilitate dimension-independent approximation of vastly more general function spaces.",
"Basically, the idea is to use (4.3) as an inspiration and deﬁne the generalized Barron space as all functions that may be represented as E  1(0,∞)(⟨Ξ, ·⟩−eΞ)  for any random variable (Ξ, eΞ).",
"In this context, deep and compositional versions of Barron spaces were introduced and studied in [BK18, EMW19a, EW20a], which considerably extend the original theory.",
4.3 PDE assumption Another structural assumption that leads to the absence of the curse of dimensionality in some cases is that the function we are trying to approximate is given as the solution to a partial diﬀerential equation.,
"It is by no means clear that this assumption leads to approximation without the curse of dimensionality, since most standard methods, such as ﬁnite elements, sparse grids, or spectral methods typically suﬀer from the curse of dimensionality.",
"This is not merely an abstract theoretical problem: Very recently, in [AHNB+20] it was shown that two diﬀerent gold standard methods for solving the multi-electron Schr¨odinger equation produce completely diﬀerent interaction energy predictions when applied to large delocalized molecules.",
Classical numerical representations are simply not expressive enough to accurately represent complicated high-dimensional structures such as wave functions with long-range interactions.,
"Interestingly, there exists an emerging body of work that shows that NNs do not suﬀer from these shortcomings and enjoy superior expressivity properties as compared to standard numerical representations.",
"36 Such results include, for example, [GHJVW20, GS20, HJKN20] for (linear and semilinear) parabolic evolution equations, [GH22] for stationary elliptic PDEs, [GH21] for nonlinear Hamilton–Jacobi–Bellman equations, or [KPRS19] for parametric PDEs.",
"In all these cases, the absence of the curse of dimensionality in terms of the theoretical approximation power of NNs could be rigorously established.",
"One way to prove such results is via stochastic representations of the PDE solutions, as well as associated sampling methods.",
"We illustrate the idea for the simple case of linear Kolmogorov PDEs, that is the problem of representing the function g: Rd × [0, ∞) →R satisfying22 ∂g ∂t (x, t) = 1 2Tr  σ(x, t)[σ(x, t)]∗∇2 xg(x, t)  + ⟨µ(x, t), ∇xg(x, t)⟩, g(x, 0) = ϕ(x), (4.5) where the functions ϕ: Rd →R (initial condition) and σ: Rd →Rd×d, µ: Rd →Rd (coeﬃcient functions) are continuous and satisfy suitable growth conditions.",
"A stochastic representation of g is given via the Ito processes (Sx,t)t≥0 satisfying dSx,t = µ(Sx,t)dt + σ(Sx,t)dBt, Sx,0 = x, (4.6) where (Bt)t≥0 is a d-dimensional Brownian motion.",
"Then g is described via the Feynman–Kac formula which states that g(x, t) = E[ϕ(Sx,t)], x ∈Rd, t ∈[0, ∞).",
"(4.7) Roughly speaking, a NN approximation result can be proven by ﬁrst approximating, via the law of large numbers, g(x, t) = E[ϕ(Sx,t)] ≈1 n n X i=1 ϕ(S(i) x,t), (4.8) where (S(i) x,t)n i=1 are i.i.d.",
"random variables with S(1) x,t ∼Sx,t.",
"Care has to be taken to establish such an approximation uniformly in the computational domain, for example, for every (x, t) in the unit cube [0, 1]d × [0, 1], see (4.4) for a similar estimate and [GHJVW20, GS20] for two general approaches to ensure this property.",
"Aside from this issue, (4.8) represents a standard Monte Carlo estimator which can be shown to be free of the curse of dimensionality.",
"As a next step, one needs to establish that realizations of the processes (x, t) 7→Sx,t can be eﬃciently approximated by NNs.",
"This can be achieved by emulating a suitable time-stepping scheme for the SDE (4.6) by NNs which, roughly speaking, can be done without incurring the curse of dimensionality whenever the coeﬃcient functions µ, σ can be approximated by NNs without incurring the curse of dimensionality and some growth conditions hold true.",
"In a last step one assumes that the initial condition ϕ can be approximated by NNs without incurring the curse of dimensionality which, by the compositionality of NNs and the previous step, directly implies that realizations of the processes (x, t) 7→ϕ(Sx,t) can be approximated by NNs without incurring the curse of dimensionality.",
By (4.8) this implies a corresponding approximation result for the solution of the Kolmogorov PDE g in (4.5).,
"Informally, we have discovered a regularity result for linear Kolmogorov equations, namely that (modulo some technical conditions on µ, σ), the solution g of (4.5) can be approximated by NNs without incurring the curse of dimensionality whenever the same holds true for the initial condition ϕ, as well as the coeﬃcient functions µ, σ.",
"In other words, the property of being approximable by NNs without curse of dimensionality is preserved under the ﬂow induced by the PDE (4.5).",
"Some comments are in order: 22The natural solution concept to this type of PDEs is the viscosity solution concept, a thorough study of which can be found in [HHJ15].",
37 Assumption on the initial condition: One may wonder if the assumption that the initial condition ϕ can be approximated by NNs without incurring the curse of dimensionality is justiﬁed.,
This is at least the case in many applications in computational ﬁnance where the function ϕ typically represents an option pricing formula and (4.5) represents the famous Black–Scholes model.,
"It turns out that nearly all common option pricing formulas are constructed from iterative applications of linear maps and maximum/minimum functions—in other words, in many applications in computational ﬁnance, the initial condition ϕ can be exactly represented by a small ReLU NN.",
101 102 input dimension 107 108 109 1010 1011 x cx2.36 #parameters avg.,
#steps ± 2 std.,
Figure 4.2: Computational complexity as number of neural network parameters times number of SGD steps to solve heat equations of varying dimensions up to a speciﬁed precision.,
"According to the ﬁt above, the scaling is polynomial in the dimension [BDG20].",
"Generalization and optimization error: The Feynman–Kac representation (4.7) directly implies that g(·, t) can be computed as the Bayes opti- mal function of a regression task with input fea- tures X ∼U([0, 1]d) and labels Y = ϕ(SX,t), which allows for an analysis of the generalization error as well as implementations based on ERM algo- rithms [BGJ20, BBG+21].",
"While it is in principle possible to analyze the approximation and generalization error, the analysis of the computational cost and/or convergence of corresponding SGD algorithms is completely open.",
"Some promising numerical results exist, see, for instance, Figure 4.2, but the stable training of NNs approximating PDEs to very high accuracy (that is needed in several applications such as quantum chemistry) remains very challenging.",
The recent work [GV21] has even proven several impossibility results in that direction.,
"Extensions and abstract idea: Similar techniques may be used to prove expressivity results for nonlinear PDEs, for example, using nonlinear Feynman–Kac-type representations of [PP92] in place of (4.7) and multilevel Picard sampling algorithms of [EHJK19] in place of (4.8).",
We can also formulate the underlying idea in an abstract setting (a version of which has also been used in Subsection 4.2).,
"Assume that a high-dimensional function g: Rd →R admits a probabilistic representation of the form g(x) = E[Yx], x ∈Rd, (4.9) for some random variable Yx which can be approximated by an iterative scheme Y(L) x ≈Yx and Y(ℓ) x = Tℓ(Y(ℓ−1) x ), ℓ= 1, .",
", L, with dimension-independent convergence rate.",
"If we can approximate realizations of the initial mapping x 7→Y0 x and the maps Tℓ, ℓ∈[L], by NNs and the numerical scheme is stable enough, then we can also approximate Y(L) x using compositionality.",
Emulating a uniform Monte-Carlo approximator of (4.9) then leads to approximation results for g without curse of dimensionality.,
"In addition, one can choose a Rd-valued random variable X as input features and deﬁne the corresponding labels by YX to obtain a prediction task, which can be solved by means of ERM.",
"Other methods: There exist a number of additional works related to the approximation capacities of NNs for high-dimensional PDEs, for example, [EGJS18, LTY19, SZ19].",
"In most of these works, the proof technique consists of emulating an existing method that does not suﬀer from the curse of dimensionality.",
"For instance, in the case of ﬁrst-order transport equations, one can show in some cases that NNs are capable of emulating the method of characteristics, which then also yields approximation results that are free of the curse of dimensionality [LP21].",
38 5 Optimization of deep neural networks We recall from Subsections 1.3 and 1.2.1 that the standard algorithm to solve the empirical risk minimization problem over the hypothesis set of NNs is stochastic gradient descent.,
"This method would be guaranteed to converge to a global minimum of the objective if the empirical risk were convex, viewed as a function of the NN parameters.",
"However, this function is severely nonconvex, may exhibit (higher-order) saddle points, seriously suboptimal local minima, and wide ﬂat areas where the gradient is very small.",
"On the other hand, in applications, excellent performance of SGD is observed.",
This indicates that the trajectory of the optimization routine somehow misses suboptimal critical points and other areas that may lead to slow convergence.,
"Clearly, the classical theory does not explain this performance.",
Below we describe some exemplary novel approaches that give partial explanations of this success.,
"In the ﬂavor of this article, the aim of this section is to present some selected ideas rather than giving an overview of the literature.",
"To give at least some detail about the underlying ideas and to keep the length of this section reasonable, a selection of results had to be made and some ground-breaking results had to be omitted.",
"5.1 Loss landscape analysis Given a NN Φ(·, θ) and training data s ∈Zm the function θ 7→r(θ) := bRs(Φ(·, θ)) describes, in a natural way, through its graph, a high-dimensional surface.",
This surface may have regions associated with lower values of bRs which resemble valleys of a landscape if they are surrounded by regions of higher values.,
The analysis of the topography of this surface is called loss landscape analysis.,
Below we shall discuss a couple of approaches that yield deep insights into the shape of this landscape.,
Index Loss No negative curvature at globally minimal risk.,
Critical points with high risk are unstable.,
0 0.25 0.5 Figure 5.1: Sketch of the distribution of critical points of the Hamiltonian of a spin glass model.,
Spin glass interpretation: One of the ﬁrst discoveries about the shape of the loss landscape comes from deep results in statistical physics.,
The Hamiltonian of the spin glass model is a random function on the (n −1)-dimensional sphere of radius √n.,
"Making certain simplifying assumptions, it was shown in [CHM+15] that the loss of a NN with random inputs can be considered as the Hamiltonian of a spin glass model, where the inputs of the model are the parameters of the NN.",
"This connection has far-reaching implications for the loss land- scape of NNs because of the following surprising property of the Hamiltonian of spin glass models: Consider the set of critical points of this set, and associate to each point an index that denotes the percentage of the eigenvalues of the Hessian at that point which are negative.",
This index corresponds to the relative number of directions in which the loss landscape has negative curvature.,
"Then with high probability, a picture like we see in Figure 5.1 emerges [AAˇC13].",
"More precisely, the further away from the optimal loss we are, the more unstable the critical points become.",
"Conversely, if one ﬁnds oneself in a local minimum, it is reasonable to assume that the loss is close to the global minimum.",
"While some of the assumptions establishing the connection between the spin glass model and NNs are unrealistic in practice [CLA15], the theoretical distribution of critical points as in Figure 5.1 is visible in many practical applications [DPG+14].",
Paths and level sets: Another line of research is to understand the loss landscape by analyzing paths through the parameter space.,
"In particular, the existence of paths in parameter space, such that the associated empirical risks are monotone along the path.",
"Surely, should there exist a path of nonincreasing empirical risk from every point to the global minimum, then we can be certain that no non-global minima exist, since no 39 such path can escape a minimum.",
An even stronger result holds.,
"In fact, the existence of such paths shows that the loss landscape has connected level sets [FB17, VBB19].",
A crucial ingredient of the analysis of such paths are linear substructures.,
"Consider a biasless two-layer NN Φ of the form Rd ∋x 7→Φ(x, θ) := n X j=1 θ(2) j ϱ  ⟨θ(1) j , x 1  ⟩  , (5.1) where θ(1) j ∈Rd+1 for j ∈[n], θ(2) ∈Rn, ϱ is a Lipschitz continuous activation function, and we augment the vector x by a constant 1 in the last coordinate as outlined in Remark 1.5.",
"If we consider θ(1) to be ﬁxed, then it is clear that the space eFθ(1) := {Φ(·, θ): θ = (θ(1), θ(2)), θ(2) ∈Rn} is a linear space.",
"If the risk23 is convex, as is the case for the widely used quadratic or logistic loss, then this implies that θ(2) 7→r  (θ(1), θ(2))  is a convex map and hence, for every parameter set P ⊂Rn this map assumes its maximum on ∂P.",
"Therefore, within the vast parameter space, there are many paths traveling along which does not increase the risk above the risk of the start and end points.",
"This idea was, for example, used in [FB17] in a way similar to the following simple sketch: Assume that, for two parameters θ and θmin there exists a linear subspace of NNs eFˆθ(1) such that there are paths γ1 and γ2 connecting Φ(·, θ) and Φ(·, θmin) to eFˆθ(1) respectively.",
"Further assume that the paths are such that along γ1 and γ2 the risk does not signiﬁcantly exceed max{r(θ), r(θmin)}.",
Figure 5.2 shows a visualization of these paths.,
"In this case, a path from θ to θmin not signiﬁcantly exceeding r(θ) along the way is found by concatenating the paths γ1, a path along eFˆθ(1), and γ2.",
"By the previous discussion, we know that only γ1 and γ2 determine the extent to which the combined path exceeds r(θ) along its way.",
"Hence, we need to ask about the existence of eFˆθ(1) that facilitates the construction of appropriate γ1 and γ2.",
"Φ(·, θmin) Φ(·, θ) e Fˆθ(1) Φ(·, θ∗) γ1 Figure 5.2: Construction of a path from an initial point θ to the global minimum θmin that does not have sig- niﬁcantly higher risk than the initial point along the way.",
We depict here the landscape as a function of the neural network realizations instead of their parametrizations so that this landscape is convex.,
"To understand why a good choice of eFˆθ(1), so that the risk along γ1 and γ2 will not rise much higher than r(θ), is likely possible, we set24 ˆθ(1) j := ( θ(1) j for j ∈[n/2], (θ(1) min)j for j ∈[n] \ [n/2].",
"In other words, the ﬁrst half of ˆθ(1) is made from θ(1) and the second from θ(1) min.",
"If θ(1) j , j ∈[N], are realizations of random variables distributed uniformly on the d-dimensional unit sphere, then by invoking standard covering bounds of spheres (e.g., [Ver18, Corollary 4.2.13]), we expect that, for ε > 0 and a suﬃciently large number of neurons n, the vectors (θ(1) j )n/2 j=1 already ε-approximate all vectors (θ(1) j )n j=1.",
"Replacing all vectors (θ(1) j )n j=1 by their nearest neighbor in (θ(1) j )n/2 j=1 can be done with a linear path in the parameter space, and, given that r is locally Lipschitz continuous and ∥θ(2)∥1 is bounded, this operation will not increase the risk by more than O(ε).",
We denote the vector resulting from this replacement procedure by θ(1) ∗.,
"Since for all j ∈[n] \ [n/2] we now have that ϱ  ⟨(θ(1) ∗)j,  · 1  ⟩  ∈  ϱ  ⟨(θ(1) ∗)k,  · 1  ⟩  : k ∈[n/2]  , 23As most statements in this subsection are valid for the empirical risk r(θ) = b Rs(Φ(·, θ)) as well as the risk r(θ) = R(Φ(·, θ)), given a suitable distribution of Z, we will just call r the risk.",
24We assume w.l.o.g.,
that n is a multiple of 2.,
"40 there exists a vector θ(2) ∗ with (θ(2) ∗)j = 0, j ∈[n] \ [n/2], so that Φ(·, (θ(1) ∗, θ(2))) = Φ(·, (θ(1) ∗, λθ(2) ∗ + (1 −λ)θ(2))), λ ∈[0, 1].",
"In particular, this path does not change the risk between (θ(1) ∗, θ(2)) and (θ(1) ∗, θ(2) ∗).",
"Now, since (θ(2) ∗)j = 0 for j ∈[n] \ [n/2], the realization Φ(·, (θ(1) ∗, θ(2) ∗)) is computed by a sub-network consisting of the ﬁrst n/2 hidden neurons and we can replace the parameters corresponding to the other neurons without any eﬀect on the realization function.",
"Speciﬁcally, it holds that Φ(·, (θ(1) ∗, θ(2) ∗)) = Φ(·, (λˆθ(1) + (1 −λ)θ(1) ∗, θ(2) ∗)), λ ∈[0, 1], yielding a path of constant risk between (θ(1) ∗, θ(2) ∗) and (ˆθ(1), θ(2) ∗).",
Connecting these paths completes the construction of γ1 and shows that the risk along γ1 does not exceed that at θ by more than O(ε).,
"Of course, γ2 can be constructed in the same way.",
The entire construction is depicted in Figure 5.2.,
"Overall, this derivation shows that for suﬃciently wide NNs (appropriately randomly initialized) it is very likely possible to almost connect a random parameter value to the global minimum with a path which along the way does not need to climb much higher than the initial risk.",
"In [VBB19], a similar approach is taken and the convexity in the last layer is used.",
"However, the authors invoke the concept of intrinsic dimension to elegantly solve the non-linearity of r((θ(1), θ(2))) with respect to θ(1).",
"Additionally, [SS16] constructs a path of decreasing risk from random initializations.",
"The idea here is that if one starts at a point of suﬃciently high risk, one can always ﬁnd a path to the global optimum with strictly decreasing risk.",
"The intriguing insight behind this result is that if the initialization is suﬃciently bad, i.e., worse than that of a NN outputting only zero, then there exist two operations that inﬂuence the risk directly.",
"Multiplying the last layer with a number smaller than one will decrease the risk, whereas the opposite will increase it.",
"Using this tuning mechanism, any given potentially non-monotone path from the initialization to the global minimum can be modiﬁed so that it is strictly monotonically decreasing.",
"In a similar spirit, [NH17] shows that if a deep NN has a layer with more neurons than training data points, then under certain assumptions the training data will typically be mapped to linearly independent points in that layer.",
"Of course, this layer could then be composed with a linear map that maps the linearly independent points to any desirable output, in particular one that achieves vanishing empirical risk, see also Proposition 1.1.",
"As for two-layer NNs, the previous discussion on linear paths immediately shows that in this situation a monotone path to the global minimum exists.",
"5.2 Lazy training and provable convergence of stochastic gradient descent When training highly overparametrized NNs, one often observes that the parameters of the NNs barely change during training.",
"In Figure 5.3, we show the relative distance that the parameters travel through the parameter space during the training of NNs of varying numbers of neurons per layer.",
"The eﬀect described above has been observed repeatedly and theoretically explained, see, e.g., [DZPS18, LL18, AZLS19, DLL+19, ZCZG20].",
"In Subsection 2.1, we have already seen a high-level overview and, in particular, the function space perspective of this phenomenon in the inﬁnite width limit.",
Below we present a short and highly simpliﬁed derivation of this eﬀect and show how it leads to provable convergence of gradient descent for suﬃciently overparametrized deep NNs.,
A simple learning model: We consider again the simple NN model of (5.1) with a smooth activation function ϱ which is not aﬃne linear.,
"For the quadratic loss and training data s = ((x(i), y(i)))m i=1 ∈(Rd ×R)m, where xi ̸= xj for all i ̸= j, the empirical risk is given by r(θ) = bRs(θ) = 1 m m X i=1 (Φ(x(i), θ) −y(i))2.",
"Let us further assume that Θ(1) j ∼N(0, 1/n)d+1, j ∈[n], and Θ(2) j ∼N(0, 1/n), j ∈[n], are independent random variables.",
"41 Figure 5.3: Four networks with architecture ((1, n, n, 1), ϱR) and n ∈{20, 100, 500, 2500} neurons per hidden layer were trained by gradient descent to ﬁt four points that are shown in the middle ﬁgure as black dots.",
We depict on the left the relative Euclidean distance of the parameters from the initialization through the training process.,
"In the middle, we show the ﬁnal trained NNs.",
On the right we show the behavior of the training error.,
"A peculiar kernel: Next, we would like to understand how the gradient ∇θr(Θ) looks like with high probability over the initialization Θ = (Θ(1), Θ(2)).",
"Similar to (2.3), we have by restricting the gradient to θ(2) and applying the chain rule that ∥∇θr(Θ)∥2 2 ≥ 4 m2 m X i=1 ∇θ(2)Φ(x(i), Θ)(Φ(x(i), Θ) −y(i)) 2 2 = 4 m2  (Φ(x(i), Θ) −y(i))m i=1 T ¯KΘ(Φ(x(j), Θ) −y(j))m j=1, (5.2) where ¯KΘ is a random Rm×m-valued kernel given by ( ¯KΘ)i,j :=  ∇θ(2)Φ(x(i), Θ) T ∇θ(2)Φ(x(j), Θ), i, j ∈[m].",
This kernel is closely related to the neural tangent kernel in (2.4) evaluated at the features (x(i))m i=1 and the random initialization Θ.,
"It is a slightly simpliﬁed version thereof, as in (2.4) the gradient is taken with respect to the full vector θ.",
This can also be regarded as the kernel associated with a random features model [RR+07].,
"Note that for our two-layer NN we have that  ∇θ(2)Φ(x, Θ)  k = ϱ  Θ(1) k , x 1   , x ∈Rd, k ∈[n].",
"Thus, we can write ¯KΘ as the following sum of (random) rank one matrices: ¯KΘ = n X k=1 vkvT k with vk =  ϱ  Θ(1) k ,  x(i) 1  m i=1 ∈Rm, k ∈[n].",
(5.3) The kernel ¯KΘ are symmetric and positive semi-deﬁnite by construction.,
"It is positive deﬁnite if it is non-singular, i.e., if at least m of the n vectors vk, k ∈[n], are linearly independent.",
"Proposition 1.1 shows that for n = m the probability of that event is not zero, say δ, and is therefore at least 1 −(1 −δ)⌊n/m⌋for arbitrary n. In other words, the probability increases rapidly with n. It is also clear from (5.3) that E[ ¯KΘ] scales linearly with n. From this intuitive derivation, we conclude that for suﬃciently large n, with high probability ¯KΘ is a positive deﬁnite kernel with smallest eigenvalue λmin( ¯KΘ) scaling linearly with n. The properties of ¯KΘ, in particular its positive deﬁniteness, have been studied much more rigorously as already described in Subsection 2.1.",
"42 Control of the gradient: Applying the expected behavior of the smallest eigenvalue λmin( ¯KΘ) of ¯KΘ to (5.2), we conclude that with high probability ∥∇θr(Θ)∥2 2 ≥ 4 m2 λmin( ¯KΘ)∥(Φ(x(i), Θ) −y(i))m i=1∥2 2 ≳n mr(Θ).",
"(5.4) To understand what will happen when applying gradient descent, we ﬁrst need to understand how the situation changes in a neighborhood of Θ.",
"We ﬁx x ∈Rd and observe that by the mean value theorem for all ¯θ ∈B1(0) we have ∇θΦ(x, Θ) −∇θΦ(x, Θ + ¯θ) 2 2 ≲ sup ˆθ∈B1(0) ∇2 θΦ(x, Θ + ˆθ) 2 op, (5.5) where ∥∇2 θΦ(x, Θ + ˆθ)∥op denotes the operator norm of the Hessian of Φ(x, ·) at Θ + ˆθ.",
"From inspection of (5.1), it is not hard to see that for all i, j ∈[n] and k, ℓ∈[d + 1] E ""∂2Φ(x, Θ) ∂θ(2) i ∂θ(2) j 2 # = 0, E "" ∂2Φ(x, Θ) ∂θ(2) i ∂(θ(1) j )k 2 # ≲δi,j, and E "" ∂2Φ(x, Θ) ∂(θ(1) i )k∂(θ(1) j )ℓ 2 # ≲δi,j n , where δi,j = 0 if i ̸= j and δi,i = 1 for all i, j ∈[n].",
"For suﬃciently large n, we have that ∇2 θΦ(x, Θ) is in expectation approximately a block band matrix with band-width d + 1.",
"Therefore, we conclude that E  ∥∇2 θΦ(x, Θ)∥2 op  ≲1.",
"Hence, we obtain by concentration of Gaussian random variables that with high probability ∥∇2 θΦ(x, Θ)∥2 op ≲1.",
"By the block-banded form of ∇2 θΦ(x, Θ) we have that, even after perturbation of Θ by a vector ˆθ with norm bounded by 1, the term ∥∇2 θΦ(x, Θ + ˆθ)∥2 op is bounded, which yields that the right-hand side of (5.5) is bounded with high probability.",
"Using (5.5), we can extend (5.4), which holds with high probability, to a neighborhood of Θ by the following argument: Let ¯θ ∈B1(0), then ∥∇θr(Θ + ¯θ)∥2 2 ≥ 4 m2 m X i=1 ∇θ(2)Φ(x(i), Θ + ¯θ)(Φ(x(i), Θ + ¯θ) −y(i)) 2 2 = (5.5) 4 m2 m X i=1 (∇θ(2)Φ(x(i), Θ) + O(1))(Φ(x(i), Θ + ¯θ) −y(i)) 2 2 ≳ (∗) 1 m2 (λmin( ¯KΘ) + O(1))∥(Φ(x(i), Θ + ¯θ) −y(i))m i=1∥2 2 ≳n mr(Θ + ¯θ), (5.6) where the estimate marked by (∗) uses the positive deﬁniteness of ¯KΘ again and only holds for suﬃciently large n, so that the O(1) term is negligible.",
"We conclude that, with high probability over the initialization Θ, on a ball of ﬁxed radius around Θ the squared Euclidean norm of the gradient of the empirical risk is lower bounded by n m times the empirical risk.",
"Exponential convergence of gradient descent: For suﬃciently small step sizes η, the observation in the previous paragraph yields the following convergence rate for gradient descent as in Algorithm 1, speciﬁcally (1.5), with m′ = m and Θ(0) = Θ: If ∥Θ(k) −Θ∥≤1 for all k ∈[K + 1], then25 r(Θ(K+1)) ≈r(Θ(K)) −η∥∇θr(Θ(K))∥2 2 ≤  1 −cηn m  r(Θ(K)) ≲  1 −cηn m K , (5.7) for c ∈(0, ∞) so that ∥∇θr(Θ(k))∥2 2 ≥cn m r(Θ(k)) for all k ∈[K].",
25Note that the step-size η needs to be small enough to facilitate the approximation step in (5.7).,
"Hence, we cannot simply put η = m/(cn) in (5.7) and have convergence after one step.",
43 Let us assume without proof that the estimate (5.6) could be extended to an equivalence.,
"In other words, we assume that we additionally have that ∥∇θr(Θ + ¯θ)∥2 2 ≲n mr(Θ + ¯θ).",
"This, of course, could be shown with similar tools as were used for the lower bound.",
Then we have that ∥Θ(k) −Θ∥2 ≤1 for all k ≲ p m/(η2n).,
"Setting t = p m/(η2n) and using the limit deﬁnition of the exponential function, i.e., limt→∞(1−x/t)t = e−x, yields for suﬃciently small η that (5.7) is bounded by e−c√ n/m.",
"We conclude that, with high probability over the initialization, gradient descent converges with an exponential rate to an arbitrary small empirical risk if the width n is suﬃciently large.",
"In addition, the iterates of the descent algorithm even stay in a small ﬁxed neighborhood of the initialization during training.",
"Because the parameters only move very little, this type of training has also been coined lazy training [COB19].",
"Similar ideas as above, have led to groundbreaking convergence results of SGD for overparametrized NNs in much more complex and general settings, see, e.g., [DZPS18, LL18, AZLS19].",
"In the inﬁnite width limit, NN training is practically equivalent to kernel regression, see Subsection 2.1.",
If we look at Figure 5.3 we see that the most overparametrized NN interpolates the data like a kernel-based interpolator would.,
"In a sense, which was also highlighted in [COB19], this shows that, while overparametrized NNs in the lazy training regime have very nice properties, they essentially act like linear methods.",
"6 Tangible eﬀects of special architectures In this section, we describe results that isolate the eﬀects of certain aspects of NN architectures.",
"As we have discussed in Subsection 1.3, typically only either the depth or the number of parameters are used to study theoretical aspects of NNs.",
We have seen instances of this throughout Sections 3 and 4.,
"Moreover, also in Section 5, we saw that wider NNs enjoy certain very favorable properties from an optimization point of view.",
"Below, we introduce certain specialized NN architectures.",
"We start with one of the most widely used types of NNs, the convolutional neural network (CNN).",
In Subsection 6.2 we introduce skip connections and in Subsection 6.3 we discuss a speciﬁc class of CNNs equipped with an encoder-decoder structure that are frequently used in image processing techniques.,
We introduce the batch normalization block in Subsection 6.4.,
"Then, we discuss sparsely connected NNs that typically result as an extraction from fully connected NNs in Subsection 6.5.",
"Finally, we brieﬂy comment on recurrent neural networks in Subsection 6.6.",
"As we have noted repeatedly throughout this manuscript, it is impossible to give a full account of the literature in a short introductory article.",
"In this section, this issue is especially severe since the number of special architectures studied in practice is enormous.",
"Therefore, we had to omit many very inﬂuential and widely used neural network architectures.",
"Among those are graph neural networks, which handle data from non- Euclidean input spaces.",
"We refer to the survey articles [BBL+17, WPC+21] for a discussion.",
"Another highly successful type of architectures are (variational) autoencoders [AHS85, HZ94].",
These are neural networks with a bottleneck that enforce a more eﬃcient representation of the data.,
"Similarly, generative adversarial networks [GPAM+14] which are composed of two neural networks, one generator and one discriminator, could not be discussed here.",
Another widely used component of architectures used in practice is the so-called dropout layer.,
This layer functions through removing some neurons randomly during training.,
This procedure empirically prevents overﬁtting.,
An in-detail discussion of the mathematical analysis behind this eﬀect is beyond the scope of this manuscript.,
"We refer to [WZZ+13, SHK+14, HV17, MAV18] instead.",
"Finally, the very successful attention mechanism [BCB15, VSP+17], that is the basis of transformer neural networks, had to be omitted.",
"Before we start describing certain eﬀects of special NN architectures, a word of warning is required.",
"The special building blocks, which will be presented below, have been developed based on a speciﬁc need in applications and are used and combined in a very ﬂexible way.",
"To describe these tools theoretically without completely inﬂating the notational load, some simplifying assumptions need to be made.",
It is very likely that the simpliﬁed building blocks do not accurately reﬂect the practical applications of these tools in all use cases.,
"44 6.1 Convolutional neural networks Especially for very high-dimensional inputs where the input dimensions are spatially related, fully connected NNs seem to require unnecessarily many parameters.",
"For example, in image classiﬁcation problems, neigh- boring pixels very often share information and the spatial proximity should be reﬂected in the architecture.",
"Based on this observation, it appears reasonable to have NNs that have local receptive ﬁelds in the sense that they collect information jointly from spatially close inputs.",
"In addition, in image processing, we are not necessarily interested in a universal hypothesis set.",
"A good classiﬁer is invariant under many operations, such as translation or rotation of images.",
It seems reasonable to hard-code such invariances into the architecture.,
These two principles suggest that the receptive ﬁeld of a NN should be the same on diﬀerent translated patches of the input.,
"In this sense, parameters of the architecture can be reused.",
"Together, these arguments make up the three fundamental principles of convolutional NNs: local receptive ﬁelds, parameter sharing, and equivariant representations, as introduced in [LBD+89].",
We will provide a mathematical formulation of convolutional NNs below and then revisit these concepts.,
"A convolutional NN corresponds to multiple convolutional blocks, which are special types of layers.",
"For a group G, which typically is either [d] ∼= Z/(dZ) or [d]2 ∼= (Z/(dZ))2 for d ∈N, depending on whether we are performing one-dimensional or two-dimensional convolutions, the convolution of two vectors a, b ∈RG is deﬁned as (a ∗b)i = X j∈G ajbj−1i, i ∈G.",
"Now we can deﬁne a convolutional block as follows: Let eG be a subgroup of G, let p : G →eG be a so-called pooling-operator, and let C ∈N denote the number of channels.",
"Then, for a series of kernels κi ∈RG, i ∈[C], the output of a convolutional block is given by RG ∋x 7→x′ := (p(x ∗κi))C i=1 ∈(R e G)C. (6.1) A typical example of a pooling operator is for G = (Z/(2dZ))2 and eG = (Z/(dZ))2 the 2 × 2 subsampling operator p : RG →R e G, x 7→(x2i−1,2j−1)d i,j=1.",
Popular alternatives are average pooling or max pooling.,
These operations then either pass the average or the maximum over patches of similar size.,
The convolutional kernels correspond to the aforementioned receptive ﬁelds.,
"They can be thought of as local if they have small supports, i.e., few nonzero entries.",
"As explained earlier, a convolutional NN is built by stacking multiple convolutional blocks after another26.",
"At some point, the output can be ﬂattened, i.e., mapped to a vector and is then fed into a FC NN (see Deﬁnition 1.4).",
We depict this setup in Figure 6.1.,
"Owing to the fact that convolution is a linear operation, depending on the pooling operation, one may write a convolutional block (6.1) as a FC NN.",
"For example, if G = (Z/(2dZ))2 and the 2 × 2 subsampling pooling operator is used, then the convolutional block could be written as x 7→Wx for a block circulant matrix W ∈R(Cd2)×(2d)2.",
"Since we require W to have a special structure, we can interpret a convolutional block as a special, restricted feed-forward architecture.",
"After these considerations, it is natural to ask how the restriction of a NN to a pure convolutional structure, i.e., consisting only of convolutional blocks, will aﬀect the resulting hypothesis set.",
The ﬁrst natural question is whether the set of such NNs is still universal in the sense of Theorem 1.15.,
The answer to this question depends strongly on the type of pooling and convolution that is allowed.,
"If the convolution is performed with padding, then the answer is yes [OS19, Zho20b].",
"On the other hand, for circular convolutions and without pooling, universality does not hold, but the set of translation equivariant functions can be universally approximated [Yar18b, PV20].",
"Furthermore, [Yar18b] illuminates the eﬀect of subsample pooling by showing that, if no pooling is applied, then universality cannot be achieved, whereas if pooling is applied 26We assume that the deﬁnition of a convolutional block is suitably extended to input data in the Cartesian product (RG)C. For instance, one can take an aﬃne linear combination of C mappings as in (6.1) acting on each coordinate.",
"Moreover, one may also interject an activation function between the blocks.",
45 Convolution Pooling Convolution Pooling Fully connected NN Figure 6.1: Illustration of a convolutional neural network with two-dimensional convolutional blocks and 2 × 2 subsampling as pooling operation.,
then universality is possible.,
The eﬀect of subsampling in CNNs from the viewpoint of approximation theory is further discussed in [Zho20a].,
The role of other types of pooling in enhancing invariances of the hypothesis set will be discussed in Subsection 7.1 below.,
6.2 Residual neural networks Let us ﬁrst illustrate a potential obstacle when training deep NNs.,
Consider for L ∈N the product operation RL ∋x 7→π(x) = L Y ℓ=1 xℓ.,
"It is clear that ∂ ∂xk π(x) = L Y ℓ̸=k xℓ, x ∈RL.",
"Therefore, for suﬃciently large L, we expect that ∂π ∂xk will be exponentially small, if |xℓ| < λ < 1 for all ℓ∈[L] or exponentially large, if |xℓ| > λ > 1 for all ℓ∈[L].",
"The output of a general NN, considered as a directed graph, is found by repeatedly multiplying the input with parameters in every layer along the paths that lead from the input to the output neuron.",
"Due to the aforementioned phenomenon, it is often observed that training NNs suﬀers from either the exploding or the vanishing gradient problem, which may prevent lower layers from training at all.",
The presence of an activation function is likely to exacerbate this eﬀect.,
The exploding or vanishing gradient problem seems to be a serious obstacle towards eﬃcient training of deep NNs.,
"In addition to the vanishing and exploding gradient problems, there is an empirically observed degradation problem [HZRS16].",
This phrase describes the phenomenon that FC NNs seem to achieve lower accuracy on both the training and test data when increasing their depth.,
"From an approximation theoretic perspective, deep NNs should always be superior to shallow NNs.",
The reason for this is that NNs with two layers can either exactly represent the identity map or approximate it arbitrarily well.,
"Concretely, for the ReLU activation function ϱR we have that x = ϱR(x + b) −b for x ∈Rd with xi > −bi, where b ∈Rd.",
"In addition, for any activation function ϱ which is continuously diﬀerentiable on a neighborhood of some point λ ∈R with ϱ′(λ) ̸= 0 one can approximate the identity arbitrary well, see (1.8).",
"Because of this, extending a NN architecture by one layer can only enlarge the associated hypothesis set.",
"Therefore, one may expect that the degradation problem is more associated with the optimization aspect of learning.",
This problem is addressed by a small change to the architecture of a feed-forward NN in [HZRS16].,
46 idR3 idR3 idR3 idR3 Figure 6.2: Illustration of a neural network with residual blocks.,
"Instead of deﬁning a FC NN Φ as in (1.1), one can insert a residual block in the ℓ-th layer by redeﬁning27 ¯Φ(ℓ)(x, θ) = ϱ(Φ(ℓ)(x, θ)) + ¯Φ(ℓ−1)(x, θ), (6.2) where we assume that Nℓ= Nℓ−1.",
Such a block can be viewed as the sum of a regular FC NN and the identity which is referred to as skip connection or residual connection.,
A sketch of a NN with residual blocks is shown in Figure 6.2.,
Inserting a residual block in all layers leads to a so-called residual NN.,
"A prominent approach to analyze residual NNs is by establishing a connection with optimal control problems and dynamical systems [E17, TvG18, EHL19, LLS19, RH19, LML+20].",
"Concretely, if each layer of a NN Φ is of the form (6.2), then we have that ¯Φ(ℓ) −¯Φ(ℓ−1) = ϱ(Φ(ℓ)) =: h(ℓ, Φ(ℓ)), where we abbreviate ¯Φ(ℓ) = ¯Φ(ℓ)(x, θ) and set ¯Φ(0) = x.",
"Hence, (¯Φ(ℓ))L−1 ℓ=0 corresponds to an Euler discretization of the ODE ˙φ(t) = h(t, φ(t)), φ(0) = x, where t ∈[0, L −1] and h is an appropriate function.",
"Using this relationship, deep residual NNs can be studied in the framework of the well-established theory of dynamical systems, where strong mathematical guarantees can be derived.",
"6.3 Framelets and U-Nets One of the most prominent application areas of deep NNs are inverse problems, particularly those in the ﬁeld of imaging science, see also Subsection 8.1.",
"A speciﬁc architectural design of CNNs, namely so-called U-nets introduced in [RFB15], seems to perform best for this range of problems.",
We depict a sketch of a U-net in Figure 6.3.,
"However, a theoretical understanding of the success of this architecture was lacking.",
"Recently, an innovative approach called deep convolutional framelets was suggested in [YHC18], which we now brieﬂy explain.",
"The core idea is to take a frame-theoretic viewpoint, see, e.g., [CKP12], and regard the forward pass of a CNN as a decomposition in terms of a frame (in the sense of a generalized basis).",
A similar approach will be taken in Subsection 7.2 for understanding the learned kernels using sparse coding.,
"However, based on the analysis and synthesis operators of the corresponding frame, the usage of deep convolutional framelets naturally leads to a theoretical understanding of encoder-decoder architectures, such as U-nets.",
"Let us describe this approach for one-dimensional convolutions on the group G := Z/(dZ) with kernels deﬁned on the subgroup H := Z/(nZ), where d, n ∈N with n < d, see also Subsection 6.1.",
"We deﬁne the convolution between u ∈RG and v ∈RH by zero-padding v, i.e., g ∗◦v := g ∗¯v, where ¯v ∈RG is deﬁned by ¯vi = vi for i ∈H and ¯vi = 0 else.",
"As an important tool, we consider the Hankel matrix Hn(x) = (xi+j)i∈G,j∈H ∈Rd×n associated with x ∈RG.",
"As one key property, matrix-vector multiplications with Hankel matrices are translated to convolutions via28 ⟨e(i), Hn(x)v⟩= X j∈H xi+jvj = ⟨x, e(i) ∗◦v⟩, i ∈G, (6.3) 27One can also skip multiple layers, e.g., in [HZRS16] two or three layers skipped, use a simple transformation instead of the identity [SGS15], or randomly drop layers [HSL+16].",
28Here and in the following we naturally identify elements in RG and RH with the corresponding vectors in Rd and Rn.,
47 Figure 6.3: Illustration of a simpliﬁed U-net neural network.,
"Down-arrows stand for pooling, up arrows for deconvolution or upsampling, right arrows for convolution or fully connected steps.",
Dashed lines are skip connections.,
"where e(i) := 1{i} ∈RG and v ∈RH, see [YGLD17].",
"Further, we can recover the k-th coordinate of x by the Frobenius inner product between Hn(x) and the Hankel matrix associated with e(k), i.e., 1 nTr  Hn(e(k))T Hn(x)  = 1 n X j∈H X i∈G e(k) i+jxi+j = 1 n|H|xk = xk.",
"(6.4) This allows us to construct global and local bases as follows: Let p, q ∈N, let U = u1 · · · up  ∈Rd×p, V = v1 · · · vq  ∈Rn×q, eU = ˜u1 · · · ˜up  ∈Rd×p, and eV = ˜v1 · · · ˜vq  ∈Rn×q, and assume that Hn(x) = eUU T Hn(x)V eV T .",
"(6.5) For p ≥d and q ≥n, this is, for instance, satisﬁed if U and V constitute frames with eU and eV being their respective dual frames, i.e., eUU T = Id and V eV T = In.",
"As a special case, one can consider orthonormal bases U = eU and V = eV with p = d and q = n. In the case p = q = r ≤n, where r is the rank of Hn(x), one can establish (6.5) by choosing the left and right singular vectors of Hn(x) as U = eU and V = eV , respectively.",
"The identity in (6.5), in turn, ensures the following decomposition: x = 1 n p X i=1 q X j=1 ⟨x, ui ∗◦vj⟩˜ui ∗◦˜vj.",
"(6.6) Observing that the vector vj ∈RH interacts locally with x ∈RG due to the fact that H ⊂G, whereas ui ∈RG acts on the entire vector x, we refer to (vj)q j=1 as local and (ui)p i=1 as global bases.",
"In the context of CNNs, vi can be interpreted as local convolutional kernel and ui as pooling operation29.",
"The proof of (6.6) 29Note that ⟨x, ui ∗◦vj⟩can also be interpreted as ⟨ui, vj ⋆x⟩, where ⋆denotes the cross-correlation between the zero-padded vj and x.",
"This is in line with software implementations for deep learning applications, e.g., TensorFlow and PyTorch, where typically cross-correlations are used instead of convolutions.",
"48 follows directly from properties (6.3), (6.4), and (6.5) as xk = 1 nTr  Hn(e(k))T Hn(x)  = 1 nTr  Hn(e(k))T eUU T Hn(x)V eV T  = 1 n p X i=1 q X j=1 ⟨ui, Hn(x)vj⟩⟨˜ui, Hn(e(k))˜vj⟩.",
"The decomposition in (6.6) can now be interpreted as a composition of an encoder and a decoder, x 7→C = (⟨x, ui ∗◦vj⟩)i∈[p],j∈[q] and C 7→1 n p X i=1 q X j=1 Ci,j ˜ui ∗◦˜vj, (6.7) which relates it to CNNs equipped with an encoder-decoder structure such as U-nets, see Figure 6.3.",
"Generalizing this approach to multiple channels, it is possible to stack such encoders and decoders which leads to a layered version of (6.6).",
"In [YHC18] it is shown that one can make an informed decision on the number of layers based on the rank of Hn(x), i.e., the complexity of the input features x.",
"Moreover, also an activation function such as the ReLU or bias vectors can be included.",
"The key question one can then ask is how the kernels can be chosen to obtain sparse coeﬃcients C in (6.7) and a decomposition such as (6.6), i.e., perfect reconstruction.",
"If U and V are chosen as the left and right singular vectors of Hn(x), one obtains a very sparse, however input-dependent, representation in (6.6) due to the fact that Ci,j = ⟨x, ui ∗◦vj⟩= ⟨ui, Hn(x)vj⟩= 0, i ̸= j.",
"Finally, using the framework of deep convolutional framelets, theoretical reasons for including skip connections can be derived, since they aid to obtain a perfect reconstruction.",
6.4 Batch normalization Batch normalization is a building block of NNs that was invented in [IS15] with the goal to reduce so-called internal covariance shift.,
"In essence, this phrase describes the (undesirable) situation where during training each layer receives inputs with diﬀerent distribution.",
"A batch normalization block is deﬁned as follows: For points b = (y(i))m i=1 ∈(Rn)m and β, γ ∈R, we deﬁne BN(β,γ) b (y) := γ y −µb σb + β, y ∈Rn, with µb = 1 m m X i=1 y(i) and σ2 b = 1 m m X i=1 (y(i) −µb)2, (6.8) where all operations are to be understood componentwise, see Figure 6.4.",
Such a batch normalization block can be added into a NN architecture.,
Then b is the output of the previous layer over a batch or the whole training data30.,
"Furthermore, the parameters β, γ are variable and can be learned during training.",
"Note that, if one sets β = µb and γ = σb, then BN(β,γ) b (y) = y for all y ∈Rn.",
"Therefore, a batch normalization block does not negatively aﬀect the expressivity of the architecture.",
"On the other hand, batch normalization does have a tangible eﬀect on the optimization aspects of deep learning.",
"Indeed, in [STIM18, Theorem 4.1], the following observation was made: 30In practice, one typically uses a moving average to estimate the mean µ and the standard deviation σ of the output of the previous layer over the whole training data by only using batches.",
49 µb σb β γ by = y−µb σb z = γby + β Figure 6.4: A batch normalization block after a fully connected neural network.,
"The parameters µb, σb are the mean and the standard deviation of the output of the fully connected network computed over a batch s, i.e., a set of inputs.",
"The parameters β, γ are learnable parts of the batch normalization block.",
Proposition 6.1 (Smoothening eﬀect of batch normalization).,
"Let m ∈N with m ≥2 and for every β, γ ∈R deﬁne B(β,γ) : Rm →Rm by B(β,γ)(b) = (BN(β,γ) b (y(1)), .",
", BN(β,γ) b (y(m))), b = (y(i))m i=1 ∈Rm, where BN(β,γ) b is given as in (6.8).",
"Let β, γ ∈R and let r: Rm →R be a diﬀerentiable function.",
"Then it holds for every b ∈Rm that ∥∇(r ◦B(β,γ))(b)∥2 2 = γ2 σ2 b  ∥∇r(b)∥2 −1 m⟨1, ∇r(b)⟩2 −1 m⟨B(0,1)(b), ∇r(b)⟩2 , where 1 = (1, .",
", 1) ∈Rm and σ2 b is given as in (6.8).",
"For multi-dimensional y(i) ∈Rn, i ∈[m], the same statement holds for all components as, by deﬁnition, the batch normalization block acts componentwise.",
"Proposition 6.1 follows from a convenient representation of the Jacobian of the mapping B(β,γ), given by ∂B(β,γ)(b) ∂b = γ σb  Im −1 m11T −1 mB(0,1)(b)(B(0,1)(b))T  , b ∈Rm, and the fact that { 1 √m, 1 √mB(0,1)(b)} constitutes an orthonormal set.",
"Choosing r to mimic the empirical risk of a learning task, Proposition 6.1 shows that, in certain situations— for instance, if γ is smaller than σb or if m is not too large—a batch normalization block can considerably reduce the magnitude of the derivative of the empirical risk with respect to the input of the batch normalization block.",
"By the chain rule, this implies that also the derivative of the empirical risk with respect to NN parameters inﬂuencing the input of the batch normalization block is reduced.",
"Interestingly, a similar result holds for second derivatives [STIM18, Theorem 4.2] if r is twice diﬀerentiable.",
One can conclude that adding a batch normalization block increases the smoothness of the optimization problem.,
"Since the parameters β and γ were introduced, including a batch normalization block also increases the dimension of the optimization problem by two.",
"6.5 Sparse neural networks and pruning For deep FC NNs, the number of trainable parameters usually scales like the square of the number of neurons.",
"For reasons of computational complexity and memory eﬃciency, it appears sensible to seek for techniques to reduce the number of parameters or extract sparse subnetworks (see Figure 6.5) without aﬀecting the output 50 Figure 6.5: A neural network with sparse connections.",
of a NN much.,
"One way to do this is by pruning [LDS89, HMD16].",
"Here, certain parameters of a NN are removed after training.",
"This is done, for example, by setting these parameters to zero.",
"In this context, the lottery ticket hypothesis was formulated in [FC18].",
"It states: “A randomly-initialized, dense NN contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original NN after training for at most the same number of iterations”.",
In [RWK+20] a similar hy- pothesis was made and empirically studied.,
"There, it is claimed that, for a suﬃciently overparametrized NN, there exists a subnetwork that matches the performance of the large NN after training without being trained itself, i.e., already at initialization.",
"Under certain simplifying assumptions, the existence of favorable subnetworks is quite easy to prove.",
We can use a technique that was previously indirectly used in Subsection 4.2—the Carath´eodory Lemma.,
"This result states the following: Let n ∈N, C ∈(0, ∞), and let (H, ∥· ∥) be a Hilbert space.",
"Let F ⊂H with supf∈F ∥f∥≤C and let g ∈H be in the convex hull of F. Then there exist fi ∈F, i ∈[n], and c ∈[0, 1]n with ∥c∥1 = 1 such that g − n X i=1 cifi ≤C √n, see, e.g., [Ver18, Theorem 0.0.2].",
Proposition 6.2 (Carath´eodory pruning).,
"Let d, n ∈N, with n ≥100 and let µ be a probability measure on the unit ball B1(0) ⊂Rd.",
"Let a = ((d, n, 1), ϱR) be the architecture of a two-layer ReLU network and let θ ∈RP ((d,n,1)) be corresponding parameters such that Φa(·, θ) = n X i=1 w(2) i ϱR(⟨(w(1) i , ·⟩+ b(1) i )), where (w(1) i , b(1) i ) ∈Rd × R, i ∈[n], and w(2) ∈Rn.",
Assume that for every i ∈[n] it holds that ∥w(1) i ∥2 ≤1/2 and b(1) i ≤1/2.,
"Then there exists a parameter ˜θ ∈RP ((d,n,1)) with at least 99% of its entries being zero such that ∥Φa(·, θ) −Φa(·, ˜θ)∥L2(µ) ≤15∥w(2)∥1 √n .",
"Speciﬁcally, there exists an index set I ⊂[n] with |I| ≤n/100 such that ˜θ satisﬁes that ew(2) i = 0, if i /∈I, and ( ew(1) i ,˜b(1) i ) = ( (w(1) i , b(1) i ), if i ∈I, (0, 0), if i /∈I.",
The result is clear if w(2) = 0.,
"Otherwise, deﬁne fi := ∥w(2)∥1ϱR(⟨w(1) i , ·⟩+ b(1) i ), i ∈[n], and observe that Φa(·, θ) is in the convex hull of {fi}n i=1 ∪{−fi}n i=1.",
"Moreover, by the Cauchy–Schwarz inequality, it holds that ∥fi∥L2(µ) ≤∥w(2)∥1∥fi∥L∞(B1(0)) ≤∥w(2)∥1.",
"We conclude with the Carath´eodory Lemma that there exists I ⊂[n] with |I| = ⌊n/100⌋≥n/200 and ci ∈[−1, 1], i ∈I, such that Φa(·, θ) − X i∈I cifi L2(µ) ≤∥w(2)∥1 p |I| ≤ √ 200∥w(2)∥1 √n , 51 which yields the result.",
"Proposition 6.2 shows that certain, very wide NNs can be approximated very well by sparse subnetworks where only the output weight matrix needs to be changed.",
"The argument of Proposition 6.2 is inspired by [BK18], where a much more reﬁned result is shown for deep NNs.",
6.6 Recurrent neural networks Figure 6.6: Sketch of a recurrent neu- ral network.,
Cycles in the computa- tional graph incorporate the sequen- tial structure of the input and output.,
"Recurrent NNs are NNs where the underlying graph is allowed to exhibit cycles as in Figure 6.6, see [Hop82, RHW86, Elm90, Jor90].",
"Previously, we had excluded cyclic computational graphs.",
"For a feed- forward NN, the computation of internal states is naturally performed step by step through the layers.",
"Since the output of a layer does not aﬀect previous layers, the order in which the computations of the NN are performed corresponds to the order of the layers.",
"For recurrent NNs, the concept of layers does not exist, and the order of operations is much more delicate.",
"Therefore, one considers time steps.",
"In each time step, all possible computations of the graph are applied to the current state of the NN.",
This yields a new internal state.,
"Given that time steps arise naturally from the deﬁnition of recurrent NNs, this NN type is typically used for sequential data.",
"If the input to a recurrent NN is a sequence, then every input determines the internal state of the recurrent NN for the following inputs.",
"Therefore, one can claim that these NNs exhibit a memory.",
"This fact is extremely desirable in natural language processing, which is why recurrent NNs are widely used in this application.",
"Recurrent NNs can be trained similarly to regular feed-forward NNs by an algorithm called backpropagation through time [MP69, Wer88, WZ95].",
This procedure essentially unfolds the recurrent structure yielding a classical NN structure.,
"However, the algorithm may lead to very deep structures.",
"Due to the vanishing and exploding gradient problem discussed earlier, very deep NNs are often hard to train.",
"Because of this, special recurrent structures were introduced that include gates which prohibit too many recurrent steps; these include the widely used LSTMs [HS97].",
The application area of recurrent NNs is typically quite diﬀerent from that of regular NNs since they are specialized on sequential data.,
"Therefore, it is hard to quantify the eﬀect of a recurrent connection on a fully connected NN.",
"However, it is certainly true that with recurrent connections certain computations can be performed much more eﬃciently than with feed-forward NN structures.",
"A particularly interesting construction can be found in [BF19, Theorem 4.4], where it is shown that a ﬁxed size, recurrent NN with ReLU activation function, can approximate the function x 7→x2 to any desired accuracy.",
The reason for this eﬃcient representation can be seen when considering the self-referential deﬁnition of the approximant to x −x2 shown in Figure 3.2.,
"On the other hand, with feed-forward NNs, it transpires from Theorem 3.3 that the approximation error of ﬁxed-sized ReLU NNs for any non-aﬃne function is greater than a positive lower bound.",
7 Describing the features a deep neural network learns This section presents two viewpoints which help in understanding the nature of features that can be described by NNs.,
"Section 7.1 summarizes aspects of the so-called scattering transform which constitutes a speciﬁc NN architecture that can be shown to satisfy desirable properties, such as translation and deformation invariance.",
Section 7.2 relates NN features to the current paradigm of sparse coding.,
7.1 Invariances and the scattering transform One of the ﬁrst theoretical contributions to the understanding of the mathematical properties of CNNs is [Mal12].,
"The approach taken in that work is to consider speciﬁc CNN architectures with ﬁxed parameters 52 that result in a stand-alone feature descriptor whose output may be fed into a subsequent classiﬁer (for example, a kernel support vector machine or a trainable FC NN).",
"From an abstract point of view, a feature descriptor is a function Ψ mapping from a signal space, such as L2(Rd) or the space of piecewise smooth functions, to a feature space.",
"In an ideal world, such a classiﬁer should “factor” out invariances that are irrelevant to a subsequent classiﬁcation problem while preserving all other information of the signal.",
"A very simple example of a classiﬁer which is invariant under translations is the Fourier modulus Ψ: L2(Rd) →L2(Rd), u 7→|ˆu|.",
"This follows from the fact that a translation of a signal u results in a modulation of its Fourier transform, i.e., \ u(· −τ)(ω) = e−2πi⟨τ,ω⟩ˆu(ω), τ, ω ∈Rd.",
"Furthermore, in most cases (for example, if u is a generic compactly supported function [GKR20]), u can be reconstructed up to a translation from its Fourier modulus [GKR20] and an energy conservation property of the form ∥Ψ(u)∥L2 = ∥u∥L2 holds true.",
"Translation invariance is, for example, typically exhibited by image classiﬁers, where the label of an image does not change if it is translated.",
In practical problems many more invariances arise.,
Providing an analogous representation that factors out general invariances would lead to a signiﬁcant reduction in the problem dimensionality and constitutes an extremely promising route towards dealing with the very high dimensionality that is commonly encountered in practical problems [Mal16].,
"This program is carried out in [Mal12] for additional invariances with respect to deformations u 7→uτ := u(· −τ(·)), where τ : Rd →Rd is a smooth mapping.",
"Such transformations may occur in practice, for instance, as image warpings.",
"In particular, a feature descriptor Ψ is designed that, with a suitable norm ∥· ∥on the image of Ψ, (a) is Lipschitz continuous with respect to deformations in the sense that ∥Ψ(u) −Ψ(uτ)∥≲K(τ, ∇τ, ∇2τ) holds for some K that only mildly depends on τ and essentially grows linearly in ∇τ and ∇2τ, (b) is almost (i.e., up to a small and controllable error) invariant under translations of the input data, and (c) contains all relevant information on the input data in the sense that an energy conservation property ∥Ψ(u)∥≈∥u∥L2 holds true.",
"Observe that, while the action of translations only represents a d-parameter group, the action of deforma- tions/warpings represents an inﬁnite-dimensional group.",
"Hence, a deformation invariant feature descriptor represents a big potential for dimensionality reduction.",
"Roughly speaking, the feature descriptor Ψ of [Mal12] (also coined the scattering transform) is deﬁned by collecting features that are computed by iteratively applying a wavelet transform followed by a pointwise modulus non-linearity and a subsequent low-pass ﬁltering step, i.e., |||u ∗ψj1| ∗ψj2 ∗.",
"| ∗ψjℓ| ∗ϕJ, where ψj refers to a wavelet at scale j and ϕJ refers to a scaling function at scale J.",
The collection of all these so-called scattering coeﬃcients can then be shown to satisfy the properties in (a)–(c) above in a suitable (asymptotic) sense.,
The proof of this result relies on a subtle interplay between a “deformation covariance” property of the wavelet transform and a “regularizing” property of the operation of convolution with the modulus of a wavelet.,
"We remark that similar results can be shown also for diﬀerent systems, such as Gabor frames [WGB17, CL19].",
"7.2 Hierarchical sparse representations The previous approach modeled the learned features by a speciﬁc dictionary, namely wavelets.",
It is well known that one of the striking properties of wavelets is to provide sparse representations for functions belonging to certain function classes.,
"More generally, we speak of sparse representations with respect to a representation system.",
"For a vector x ∈Rd, a sparsifying representation system D ∈Rd×p—also called dictionary—is such that x = Dφ with the coeﬃcients φ ∈Rp being sparse in the sense that ∥φ∥0 := | supp(φ)| = |{i ∈[p]: φi ̸= 0}| is small compared to p. A similar deﬁnition can be made for signals in inﬁnite-dimensional spaces.",
"Taking 53 sparse representations into account, the theory of sparse coding provides an approach to a theoretical understanding of the features a deep NN learns.",
"One common method in image processing is the utilization of not the entire image but overlapping patches of it, coined patch-based image processing.",
"Thus of particular interest are local dictionaries which sparsify those patches, but presumably not the global image.",
"This led to the introduction of the convolutional sparse coding model (CSC model), which links such local and global behaviors.",
"Let us describe this model for one-dimensional convolutions on the group G := Z/(dZ) with kernels supported on the subgroup H := Z/(nZ), where d, n ∈N with n < d, see also Subsection 6.1.",
"The corresponding CSC model is based on a decomposition of a global signal x ∈(RG)c with c ∈N channels as xi = C X j=1 κi,j ∗φj, i ∈[c], (7.1) where φ ∈(RG)C is supposed to be a sparse representation with C ∈N channels and κi,j ∈RG, i ∈[c], j ∈[C], are local kernels with supp(κi,j) ⊂H.",
"Let us consider a patch ((xi)g+h)i∈[c],h∈H of n adjacent entries, starting at position g ∈G, in each channel of x.",
"The condition on the support of the kernels κi,j and the representation in (7.1) imply that this patch is only aﬀected by a stripe of at most (2n −1) entries in each channel of φ.",
"The local, patch-based sparsity of the representation φ can thus be appropriately measured via ∥φ∥(n) 0,∞:= max g∈G ∥((φj)g+k)j∈[C],k∈[2n−1]∥0, see [PSE17].",
"Furthermore, note that we can naturally identify x and φ with vectors in Rdc and RdC and write x = Dφ, where D ∈Rdc×dC is a matrix consisting of circulant blocks, typically referred to as a convolutional dictionary.",
"The relation between the CSC model and deep NNs is revealed by applying the CSC model in a layer-wise fashion [PRE17, SPRE18, PRSE18].",
"To see this, let C0 ∈N and for every ℓ∈[L] let Cℓ, kℓ∈N and let D(ℓ) ∈RdCℓ−1×dCℓbe a convolutional dictionary with kernels supported on Z/(nℓZ).",
"A signal x = φ(0) ∈RdC0 is said to belong to the corresponding multi-layered CSC model (ML-CSC model) if there exist coeﬃcients φ(ℓ) ∈RdCℓwith φ(ℓ−1) = D(ℓ)φ(ℓ) and ∥φ(ℓ)∥(nℓ) 0,∞≤kℓ, ℓ∈[L].",
"(7.2) We now consider the problem of reconstructing the sparse coeﬃcients (φ(ℓ))L ℓ=1 from a noisy signal ˜x := x + ν, where the noise ν ∈RdC0 is assumed to have small ℓ2-norm and x is assumed to follow the ML-CSC model in (7.2).",
"In general, this problem is NP-hard.",
"However, under suitable conditions on the ML-CSC model, it can be approximately solved, for instance, by a layered thresholding algorithm.",
"More precisely, for D ∈Rdc×dC and b ∈RdC, we deﬁne a soft-thresholding operator by TD,b(x) := ϱR(DT x −b) −ϱR(−DT x −b), x ∈Rdc, (7.3) where ϱR(x) = max{0, x} is applied componentwise.",
"If x = Dφ as in (7.1), we obtain φ ≈TD,b(x) roughly under the following conditions: The distance of φ and ψ := DT x = DT Dφ can be bounded using the local sparsity of φ and the mutual coherence and locality of the kernels of the convolutional dictionary D. For a suitable threshold b, the mapping ψ 7→ϱR(ψ −b) −ϱR(−ψ −b) further recovers the support of φ by nullifying entries of ψ with ψi ≤|bi|.",
"Utilizing the soft-thresholding operator (7.3) iteratively for corresponding vectors b(ℓ) ∈RdCℓ, ℓ∈[L], then suggests the following approximations: φ(ℓ) ≈(TD(ℓ),b(ℓ) ◦· · · ◦TD(1),b(1))(˜x), ℓ∈[L].",
The resemblance with the realization of a CNN with ReLU activation function is evident.,
"The transposed dictionary (D(ℓ))T can be regarded as modeling the learned convolutional kernels, the threshold b(ℓ) models the bias vector, and the soft-thresholding operator TD(ℓ),b(ℓ) mimics the application of a convolutional block with a ReLU non-linearity in the ℓ-th layer.",
"54 Using this model, a theoretical understanding of CNNs from the perspective of sparse coding is now at hand.",
This novel perspective gives a precise mathematical meaning of the kernels in a CNN as sparsifying dictionaries of an ML-CSC model.,
"Moreover, the forward pass of a CNN can be understood as a layered thresholding algorithm for decomposing a noisy signal ˜x.",
"The results derived are then of the following ﬂavor: Given a suitable reconstruction procedure such as thresholding or ℓ1-minimization, the sparse coeﬃcients (φ(ℓ))L ℓ=1 of a signal x following a ML-CSC model can be stably recovered from the noisy signal ˜x under certain hypotheses on the ingredients of the ML-CSC model.",
8 Eﬀectiveness in natural sciences The theoretical insights of the previous sections do not always accurately describe the performance of NNs in applications.,
"Indeed, there often exists a considerable gap between the predictions of approximation theory and the practical performance of NNs [AD20].",
"In this section, we consider concrete applications which have been very successfully solved with deep- learning-based methods.",
In Section 8.1 we present an overview of deep-learning-based algorithms applied to inverse problems.,
"Section 8.2 then continues by describing how NNs can be used as a numerical ansatz for solving PDEs, highlighting their use in the solution of the multi-electron Schr¨odinger equation.",
"8.1 Deep neural networks meet inverse problems The area of inverse problems, predominantly in imaging, was presumably the ﬁrst class of mathematical methods embracing deep learning with overwhelming success.",
"Let us consider a forward operator K : Y →X with X, Y being Hilbert spaces and the associated inverse problem of ﬁnding y ∈Y such that Ky = x for given features x ∈X.",
"The classical model-based approach to regularization aims to approximate K by invertible operators, and is hence strongly based on functional analytic principles.",
"Today, such approaches take well-posedness of the approximation, convergence properties, as well as the structure of regularized solutions into account.",
"The last item allows to incorporate prior information of the original solution such as regularity, sharpness of edges, or—in the case of sparse regularization [JMS17]—a sparse coeﬃcient sequence with respect to a prescribed representation system.",
"Such approaches are typically realized in a variational setting and hence aim to minimize functionals of the form ∥Ky −x∥2 + αR(y), where α ∈(0, ∞) is a regularization parameter, R: Y →[0, ∞) a regularization term, and ∥· ∥denotes the norm on Y.",
"As said, the regularization term aims to model structural information about the desired solution.",
"However, one main hurdle in this approach is the problem that typically solution classes such as images from computed tomography cannot be modeled accurately enough to, for instance, allow reconstruction under the constraint of a signiﬁcant amount of missing features.",
"This has opened the door to data-driven approaches, and recently, deep NNs.",
Solvers of inverse problems which are based on deep learning techniques can be roughly categorized into three classes: 1.,
"Supervised approaches: The most straightforward approach is to train a NN Φ(·, θ): X →Y end-to-end, i.e., to completely learn the map from data x to the solution y.",
"More advanced approaches in this direction incorporate information about the operator K into the NN such as in [A¨O17, GOW19, MLE21].",
Yet another type of approaches aims to combine deep NNs with classical model-based approaches.,
"The ﬁrst suggestion in this realm was to start by applying a standard solver, followed by a deep NN Φ(·, θ): Y →Y which serves as a denoiser for speciﬁc reconstruction artifacts, e.g., [JMFU17].",
This was followed by more sophisticated methods such as plug-and-play frameworks for coupling inversion and denoising [REM17].,
"Semi-supervised approaches: These type of approaches aim to encode the regularization by a deep NN Φ(·, θ): Y →[0, ∞).",
The underlying idea is often to require stronger regularization on solutions y(i) 55 that are more prone to artifacts or other eﬀects of the instability of the problem.,
On solutions where typically few artifacts are observed less regularization can be used.,
"Therefore, the learning algorithm only requires a set of labels (y(i))m i=1 as well as a method to assess how hard the inverse problem for this label would be.",
"In this sense, the algorithm can be considered semi-supervised.",
"This idea was followed, for example, in [L¨OS18, LSAH20].",
"Taking a Bayesian viewpoint, one can also learn prior distributions as deep NNs, which was done in [BZAJ20].",
"Unsupervised approaches: One highlight of what we might coin unsupervised approaches in our problem setting is the introduction of deep image priors in [DKMB20, UVL18].",
"The key idea is to parametrize the solutions y as the output of a NN Φ(ξ, ·): P →Y with parameters in a suitable space P, applied to a ﬁxed input ξ.",
"Then, for given features x, one tries to solve minθ∈P ∥KΦ(ξ, θ) −x∥2 in order to obtain parameters ˆθ ∈P that yield a solution candidate y = Φ(ξ, ˆθ).",
Here often early stopping is applied in the training of the network parameters.,
"As can be seen, one key conceptual question is how to “take the best out of both worlds”, in the sense of optimally combining classical (model-based) methods—in particular the forward operator K—with deep learning.",
"This is certainly sensitively linked to all characteristics of the particular application at hand, such as availability and accuracy of training data, properties of the forward operator, or requirements for the solution.",
And each of the three classes of hybrid solvers follows a diﬀerent strategy.,
Let us now discuss advantages and disadvantages of methods from the three categories with a particular focus on a mathematical foundation.,
"Supervised approaches suﬀer on the one hand from the problem that often ground-truth data is not available or only in a very distorted form, leading to the fact that synthetic data constitutes a signiﬁcant part of the training data.",
"Thus the learned NN will mainly perform as well as the algorithm which generated the data, but not signiﬁcantly improve it—only from an eﬃciency viewpoint.",
"On the other hand, the inversion is often highly ill-posed, i.e., the inversion map has a large Lipschitz constant, which negatively aﬀects the generalization ability of the NN.",
"Improved approaches incorporate knowledge about the forward operator K as discussed, which helps to circumvent this issue.",
One signiﬁcant advantage of semi-supervised approaches is that the underlying mathematical model of the inverse problem is merely augmented by the neural network-based regularization.,
"Assuming that the learned regularizer satisﬁes natural assumptions, convergence proofs or stability estimates for the resulting regularized methods are still available.",
"Finally, unsupervised approaches have the advantage that the regularization is then fully due to the speciﬁc architecture of the deep NN.",
"This makes these methods slightly easier to understand theoretically, although, for instance, the deep prior approach in its full generality is still lacking a profound mathematical analysis.",
"8.2 PDE-based models Besides applications in image processing and artiﬁcial intelligence, deep learning methods have recently strongly impacted the ﬁeld of numerical analysis.",
"In particular, regarding the numerical solution of high- dimensional PDEs.",
These PDEs are widely used as a model for complex processes and their numerical solution presents one of the biggest challenges in scientiﬁc computing.,
We mention three exemplary problem classes: 1.,
"Black–Scholes model: The Nobel award-winning theory of Fischer Black, Robert Merton, and Myron Scholes proposes a linear PDE model for the determination of a fair price of a (complex) ﬁnancial derivative.",
The dimensionality of the model corresponds to the number of ﬁnancial assets which is typically quite large.,
"The classical linear model, which can be solved eﬃciently via Monte Carlo methods is quite limited.",
"In order to take into account more realistic phenomena such as default risk, the PDE that models a fair price becomes nonlinear, and much more challenging to solve.",
In particular (with the notable exception of Multilevel Picard algorithms [EHJK19]) no general algorithm exists that provably scales well with the dimension.,
Schr¨odinger equation: The electronic Schr¨odinger equation describes the stationary nonrelativistic behavior of a quantum mechanical electron system in the electric ﬁeld generated by the nuclei of a molecule.,
"Its numerical solution is required to obtain stable molecular conﬁgurations, compute vibrational spectra, or obtain forces governing molecular dynamics.",
"If the number of electrons is large, this is again a high-dimensional problem and to date there exist no satisfactory algorithms for its solution: It is well known that diﬀerent gold standard methods may produce completely diﬀerent energy predictions, for example, when applied to large delocalized molecules, rendering these methods useless for those problems.",
Hamilton–Jacobi–Bellman equation: The Hamilton–Jacobi–Bellman (HJB) equation models the value function of (deterministic or stochastic) optimal control problems.,
The underlying dimensionality of the model corresponds to the dimension of the space of states to be controlled and tends to be rather high in realistic applications.,
"The high dimensionality, together with the fact that HJB equations typically tend to be fully nonlinear with non-smooth solutions, renders the numerical solution of HJB equations extremely challenging and no general algorithms exist for this problem.",
"Due to the favorable approximation results of NNs for high-dimensional functions (see especially Subsec- tion 4.3), it might not come as a surprise that a NN ansatz has proven to be quite successful in solving the aforementioned PDE models.",
A pioneering work in this direction is [HJE18] which uses the backwards SDE reformulation of semilinear parabolic PDEs to reformulate the evaluation of such a PDE at a speciﬁc point as an optimization problem that can be solved by the deep learning paradigm.,
"The resulting algorithm proves quite successful in the high-dimensional regime and, for instance, enables the eﬃcient modeling of complex ﬁnancial derivatives including nonlinear eﬀects such as default risk.",
Another approach speciﬁcally tailored to the numerical solution of HJB equations is [NZGK21].,
"In this work, one uses the Pontryagin principle to generate samples of the PDE solution along solutions of the corresponding boundary value problem.",
"Other numerical approaches include the Deep Ritz Method [EY18], where a Dirichlet energy is minimized over a set of NNs, or so-called Physics Informed Neural Networks [RPK19], where typically the PDE residual is minimized along with some natural constraints, for instance, to enforce boundary conditions.",
Deep-learning-based methods arguably work best if they are combined with domain knowledge to inspire NN architecture choices.,
"We would like to illustrate this interplay at the hand of a speciﬁc and extremely relevant example: the electronic Schr¨odinger equation (under the Born–Oppenheimer approximation) which amounts to ﬁnding the smallest nonzero eigenvalue of the eigenvalue problem HRψ = λψψ, (8.1) for ψ: R3×n →R, where the Hamiltonian (HRψ)(r) = − n X i=1 1 2(∆riψ)(r) −   n X i=1 p X j=1 Zj ∥ri −Rj∥2 − p−1 X i=1 p X j=i+1 ZiZj ∥Ri −Rj∥2 − n−1 X i=1 n X j=i+1 1 ∥ri −rj∥2  ψ(r) describes the kinetic energy (ﬁrst term) as well as Coulomb attraction force between electrons and nuclei (second and third term) and the Coulomb repulsion force between diﬀerent electrons (third term).",
"Here, the coordinates R = R1 .",
"Rp  ∈R3×p refer to the positions of the nuclei, (Zi)p i=1 ∈Np denote the atomic numbers of the nuclei, and the coordinates r = r1, .",
", rn  ∈R3×n refer to the positions of the electrons.",
"The associated eigenfunction ψ describes the so-called wavefunction which can be interpreted in the sense that |ψ(r)|2/∥ψ∥2 L2 describes the joint probability density of the n electrons to be located at r. The smallest solution λψ of (8.1) describes the ground state energy associated with the nuclear coordinates R. It is of particular interest to know the ground state energy for all nuclear coordinates, the so-called potential energy surface whose gradient determines the forces governing the dynamic motions of the nuclei.",
The numerical solution of (8.1) is complicated by the Pauli principle which states that the wave function ψ must be antisymmetric in all coordinates representing electrons of equal spin.,
"To state it, we need to clarify that every electron is not only deﬁned by its location but also by its spin which may be positive or negative.",
"57 Depending on whether two electrons have the same spin or not, their interaction changes massively.",
"This is reﬂected by the Pauli principle that we already mentioned: Suppose that electrons i and j have equal spin, then the wave function must satisfy Pi,jψ = −ψ, (8.2) where Pi,j denotes the operator that swaps ri and rj, i.e., (Pi,jψ)(r) = ψ(r1, .",
"In particular, no two electrons with the same spin can occupy the same location.",
"The challenges associated with solving the Schr¨odinger equation inspired the following famous quote by Paul Dirac [Dir29]: “The fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the diﬃculty lies only in the fact that application of these laws leads to equations that are too complex to be solved.” We now describe how deep learning methods might help to mitigate this claim to a certain extent.",
Let X be a random variable with density |ψ(r)|2/∥ψ∥2 L2.,
"Using the Rayleigh–Ritz principle, ﬁnding the minimal nonzero eigenvalue of (8.1) can be reformulated as minimizing the Rayleigh quotient R R3×n ψ(r)(HRψ)(r) dr ∥ψ∥2 L2 = E (HRψ)(X) ψ(X)  (8.3) over all ψ’s satisfying the Pauli principle, see [SO12].",
Since this represents a minimization problem it can in principle be solved via a NN ansatz by generating training data distributed according to X using MCMC sampling31.,
"Since the wave function ψ will be parametrized as a NN, the minimization of (8.3) will require the computation of the gradient of (8.3) with respect to the NN parameters (the method in [PSMF20] even requires second order derivatives) which, at ﬁrst sight, might seem to require the computation of third order derivatives.",
"However, due to the Hermitian structure of the Hamiltonian one does not need to compute the derivative of the Laplacian of ψ, see, for example, [HSN20, Equation (8)].",
"Compared to the other PDE problems we have discussed, an additional complication arises from the need to incorporate structural properties and invariances such as the Pauli principle.",
"Furthermore, empirical evidence shows that it is also necessary to hard code the so-called cusp conditions which describe the asymptotic behavior of nearby electrons and electrons close to a nucleus into the NN architecture.",
"A ﬁrst attempt in this direction has been made in [HZE19] and signiﬁcantly improved NN architectures have been developed in [HSN20, PSMF20, SRG+21] opening the possibility of accurate ab initio computations for previously intractable molecules.",
The mathematical properties of this exciting line of work remain largely unexplored.,
"We brieﬂy describe the main ideas behind the NN architecture of [HSN20, SRG+21].",
"Standard numerical approaches (notably the Multireference Hartree Fock Method, see [SO12]) use a low rank approach to minimize (8.3).",
Such a low rank approach would approximate ψ by sums of products of one electron orbitals Qn i=1 ϕi(ri) but clearly this does not satisfy the Pauli principle (8.2).,
"In order to ensure the Pauli principle, one constructs so-called Slater determinants from one electron orbitals with equal spin.",
"More precisely, suppose that the ﬁrst n+ electrons with coordinates r1, .",
", rn+ have positive spin and the last n −n+ electrons have negative spin.",
"Then any function of the form det  (ϕi(rj))n+ i,j=1  · det  (ϕi(rj))n i,j=n++1  (8.4) satisﬁes (8.2) and is typically called a Slater determinant.",
"While the Pauli principle establishes an (non- classical) interaction between electrons of equal spin, the so-called exchange correlation, electrons with opposite spins are uncorrelated in the representation (8.4).",
"In particular, (8.4) ignores interactions between electrons that arise through Coulomb forces, implying that no nontrivial wavefunction can be accurately represented by a single Slater determinant.",
"To capture physical interactions between diﬀerent electrons, one needs to use sums of Slater determinants as an ansatz.",
"However, it turns out that the number of such determinants that are needed to guarantee a given accuracy scales very badly with the system size n (to the 31Observe that for such sampling methods one can just use the unnormalized density |ψ(r)|2 and thus avoid the computation of the normalization ∥ψ∥2 L2.",
"58 best of our knowledge the best currently known approximation results are contained in [Yse10], where an n-independent error rate is shown, however the implicit constant in this rate depends at least exponentially on the system size n).",
We would like to highlight the approach of [HSN20] whose main idea is to use NNs to incorporate interactions into Slater determinants of the form (8.4) using what is called the backﬂow trick [RMD+06].,
"The basic building blocks would now consist of functions of the form det  (ϕi(rj)Ψj(r, θj))n+ i,j=1  · det  (ϕi(rj)Ψj(r, θj))n i,j=n++1  , (8.5) where Ψk(·, θk), k ∈[n], are NNs.",
"If these are arbitrary NNs, it is easy to see that the Pauli principle (8.2) will not be satisﬁed.",
"However, if we require the NNs to be symmetric, for example, in the sense that for i, j, s ∈[n+] it holds that Pi,jΨk(·, θk) =      Ψk(·, θk), if k /∈{i, j}, Ψi(·, θi), if k = j, Ψj(·, θj), if k = i, (8.6) and analogous conditions hold for i, j, k ∈[n] \ [n+], the expression (8.5) does actually satisfy (8.2).",
The construction of such symmetric NNs can be achieved by using a modiﬁcation of the so-called SchNet Architecture [SKS+17] which can be considered as a speciﬁc residual NN.,
We describe a simpliﬁed construction which is inspired by [HZE19] and used in a slightly more complex form in [SRG+21].,
"We restrict ourselves to the case of positive spin (e.g., the ﬁrst n+ coordinates), the case of negative spin being handled in the same way.",
"Let Υ(·, θ+ emb) be a univariate NN (with possibly multivariate output) and denote Embk(r, θ+ emb) := n+ X i=1 Υ(∥rk −ri∥2, θ+ emb), k ∈[n+], the k-th embedding layer.",
"For k ∈[n+], we can now deﬁne Ψk (r, θk) = Ψk  r, (θk,fc, θ+ emb)  = Γk   Embk(r, θ+ emb), (rn++1, .",
", rn)  , θk,fc  , where Γk(·, θk,fc) denotes a standard FC NN with input dimension equal to the output dimension of Ψ+ plus the dimension of negative spin electrons.",
"The networks Ψk, k ∈[n] \ [n+], are deﬁned analogously using diﬀerent parameters θ− emb for the embeddings.",
"It is straightforward to check that the NNs Ψk, k ∈[n], satisfy (8.6) so that the backﬂow determinants (8.5) satisfy the Pauli principle (8.2).",
"In [HSN20] the backﬂow determinants (8.5) are further augmented by a multiplicative correction term, the so-called Jastrow factor which is also represented by a speciﬁc symmetric NN, as well as a correction term that ensures the validity of the cusp conditions.",
The results of [HSN20] show that this ansatz (namely using linear combinations of backﬂow determinants (8.5) instead of plain Slater determinants (8.4)) is vastly more eﬃcient in terms of number of determinants needed to obtain chemical accuracy.,
The full architecture provides a general purpose NN architecture to represent complicated wave functions.,
"A distinct advantage of this approach is that some parameters (for example, embedding layers) may be shared across diﬀerent nuclear geometries R ∈R3×p which allows for the eﬃcient computation of potential energy surfaces [SRG+21], see Figure 8.1.",
"Finally, we would like to highlight the customized NN design that incorporates physical invariances, domain knowledge (for example, in the form of cusp conditions), and existing numerical methods, all of which are required for the method to reach its full potential.",
Acknowledgment The research of JB was supported by the Austrian Science Fund (FWF) under grant I3403-N32.,
"GK acknowledges support from DFG-SPP 1798 Grants KU 1446/21-2 and KU 1446/27-2, DFG-SFB/TR 109 Grant C09, BMBF Grant MaGriDo, and NSF-Simons Foundation Grant SIMONS 81420.",
The authors would 59 Figure 8.1: By sharing layers across diﬀerent nuclear geometries one can eﬃciently compute diﬀerent geometries in one single training step [SRG+21].,
Left: Potential energy surface of H10 chain computed by the deep-learning-based algorithm from [SRG+21].,
The lowest energy is achieved when pairs of H atoms enter into a covalent bond to form ﬁve H2 molecules.,
Right: The method of [SRG+21] is capable of accurately computing forces between nuclei which allows for molecular dynamics simulations from ﬁrst principles.,
"like to thank H´ector Andrade Loarca, Dennis Elbr¨achter, Adalbert Fono, Pavol Harar, Lukas Liehr, Duc Anh Nguyen, Mariia Seleznova, and Frieder Simon for their helpful feedback on an early version of this article.",
"In particular, Dennis Elbr¨achter was providing help for several theoretical results.",
"References [AAˇC13] Antonio Auﬃnger, G´erard Ben Arous, and Jiˇr´ı ˇCern`y, Random matrices and complexity of spin glasses, Communications on Pure and Applied Mathematics 66 (2013), no.",
"2, 165–201.",
"[AB99] Martin Anthony and Peter L Bartlett, Neural network learning: Theoretical foundations, Cambridge University Press, 1999.",
"[ACGH19] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu, A convergence analysis of gradient descent for deep linear neural networks, International Conference on Learning Representations, 2019.",
"[ACH18] Sanjeev Arora, Nadav Cohen, and Elad Hazan, On the optimization of deep networks: Implicit acceleration by overparameterization, International Conference on Machine Learning, 2018, pp.",
"[AD20] Ben Adcock and Nick Dexter, The gap between theory and practice in function approximation with deep neural networks, 2020, arXiv preprint arXiv:2001.07523.",
"[ADH+19] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang, On exact computation with an inﬁnitely wide neural net, Advances in Neural Information Processing Systems, 2019, pp.",
"[AGNZ18] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang, Stronger generalization bounds for deep nets via a compression approach, International Conference on Machine Learning, 2018, pp.",
"[AHNB+20] Yasmine S Al-Hamdani, P´eter R Nagy, Dennis Barton, Mih´aly K´allay, Jan Gerit Brandenburg, and Alexandre Tkatchenko, Interactions between large molecules: Puzzle for reference quantum- mechanical methods, 2020, arXiv preprint arXiv:2009.08927.",
"60 [AHS85] David H Ackley, Geoﬀrey E Hinton, and Terrence J Sejnowski, A learning algorithm for Boltzmann machines, Cognitive Science 9 (1985), no.",
"1, 147–169.",
"[AHW96] Peter Auer, Mark Herbster, and Manfred K Warmuth, Exponentially many local minima for single neurons, Advances in Neural Information Processing Systems, 1996, p. 316–322.",
"[AM¨OS19] Simon Arridge, Peter Maass, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Solving inverse problems using data-driven models, Acta Numerica 28 (2019), 1–174.",
"[A¨O17] Jonas Adler and Ozan ¨Oktem, Solving ill-posed inverse problems using iterative deep neural networks, Inverse Problems 33 (2017), no.",
"12, 124007.",
"[AZLS19] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, A convergence theory for deep learning via over-parameterization, International Conference on Machine Learning, 2019, pp.",
"[Bar92] Andrew R Barron, Neural net approximation, Yale Workshop on Adaptive and Learning Systems, vol.",
"1, 1992, pp.",
"[Bar93] , Universal approximation bounds for superpositions of a sigmoidal function, IEEE Transactions on Information Theory 39 (1993), no.",
"3, 930–945.",
"[Bar98] Peter L Bartlett, The sample complexity of pattern classiﬁcation with neural networks: the size of the weights is more important than the size of the network, IEEE Transactions on Information Theory 44 (1998), no.",
"2, 525–536.",
"[BBC17] Alfred Bourely, John Patrick Boueri, and Krzysztof Choromonski, Sparse neural networks topologies, 2017, arXiv preprint arXiv:1706.05683.",
"[BBC+19] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse, Dota 2 with large scale deep reinforcement learning, 2019, arXiv preprint arXiv:1912.06680.",
"[BBG+21] Christian Beck, Sebastian Becker, Philipp Grohs, Nor Jaafari, and Arnulf Jentzen, Solving the kolmogorov pde by means of deep learning, Journal of Scientiﬁc Computing 88 (2021), no.",
"[BBL03] Olivier Bousquet, St´ephane Boucheron, and G´abor Lugosi, Introduction to statistical learning theory, Summer School on Machine Learning, 2003, pp.",
"[BBL+17] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst, Geometric deep learning: going beyond euclidean data, IEEE Signal Processing Magazine 34 (2017), no.",
"[BBM05] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson, Local Rademacher complexities, The Annals of Statistics 33 (2005), no.",
"4, 1497–1537.",
"[BCB15] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, Neural machine translation by jointly learning to align and translate, International Conference on Learning Representations, 2015.",
"[BDG20] Julius Berner, Markus Dablander, and Philipp Grohs, Numerically solving parametric families of high-dimensional Kolmogorov partial diﬀerential equations via deep learning, Advances in Neural Information Processing Systems, 2020, pp.",
16615–16627.,
"[BE02] Olivier Bousquet and Andr´e Elisseeﬀ, Stability and generalization, Journal of Machine Learning Research 2 (2002), no.",
"Mar, 499–526.",
"61 [BEG19] Julius Berner, Dennis Elbr¨achter, and Philipp Grohs, How degenerate is the parametrization of neural networks with the ReLU activation function?, Advances in Neural Information Processing Systems, 2019, pp.",
"[Bel52] Richard Bellman, On the theory of dynamic programming, Proceedings of the National Academy of Sciences 38 (1952), no.",
"[BF19] Jan Bohn and Michael Feischl, Recurrent neural networks as optimal mesh reﬁnement strategies, 2019, arXiv preprint arXiv:1909.04275.",
"[BFT17] Peter L Bartlett, Dylan J Foster, and Matus Telgarsky, Spectrally-normalized margin bounds for neural networks, Advances in Neural Information Processing Systems, 2017, pp.",
"[BGJ20] Julius Berner, Philipp Grohs, and Arnulf Jentzen, Analysis of the generalization error: Empirical risk minimization over deep artiﬁcial neural networks overcomes the curse of dimensionality in the numerical approximation of black–scholes partial diﬀerential equations, SIAM Journal on Mathematics of Data Science 2 (2020), no.",
"3, 631–657.",
"[BH89] Eric B Baum and David Haussler, What size net gives valid generalization?, Neural Computation 1 (1989), no.",
"1, 151–160.",
"[BHLM19] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian, Nearly-tight VC- dimension and pseudodimension bounds for piecewise linear neural networks, Journal of Machine Learning Research 20 (2019), 63–1.",
"[BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal, Reconciling modern machine- learning practice and the classical bias–variance trade-oﬀ, Proceedings of the National Academy of Sciences 116 (2019), no.",
"32, 15849–15854.",
"[BHX20] Mikhail Belkin, Daniel Hsu, and Ji Xu, Two models of double descent for weak features, SIAM Journal on Mathematics of Data Science 2 (2020), no.",
"4, 1167–1180.",
"[BK18] Andrew R Barron and Jason M Klusowski, Approximation and estimation for high-dimensional deep learning networks, 2018, arXiv preprint arXiv:1809.03090.",
"[BLLT20] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler, Benign overﬁtting in linear regression, Proceedings of the National Academy of Sciences 117 (2020), no.",
"48, 30063–30070.",
"[BMM98] Peter L Bartlett, Vitaly Maiorov, and Ron Meir, Almost linear VC-dimension bounds for piecewise polynomial networks, Neural Computation 10 (1998), no.",
"8, 2159–2173.",
"[BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal, To understand deep learning we need to understand kernel learning, International Conference on Machine Learning, 2018, pp.",
"[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeﬀrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei, Language models are few-shot learners, Advances in Neural Information Processing Systems, 2020, pp.",
"[BR89] Avrim Blum and Ronald L Rivest, Training a 3-node neural network is NP-complete, Advances in Neural Information Processing Systems, 1989, pp.",
"62 [BRT19] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov, Does data interpolation con- tradict statistical optimality?, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[BSW14] Pierre Baldi, Peter Sadowski, and Daniel Whiteson, Searching for exotic particles in high-energy physics with deep learning, Nature Communications 5 (2014), no.",
"[BZAJ20] Riccardo Barbano, Chen Zhang, Simon Arridge, and Bangti Jin, Quantifying model uncertainty in inverse problems via bayesian deep gradient descent, 2020, arXiv preprint arXiv:2007.09971.",
"[Can98] Emmanuel J Cand`es, Ridgelets: Theory and applications, Ph.D. thesis, Stanford University, 1998.",
"[CB20] Lenaic Chizat and Francis Bach, Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss, Conference on Learning Theory, 2020, pp.",
"[CHM+15] Anna Choromanska, Mikael Henaﬀ, Michael Mathieu, G´erard Ben Arous, and Yann LeCun, The loss surfaces of multilayer networks, International Conference on Artiﬁcial Intelligence and Statistics, 2015, pp.",
"[CJLZ19] Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao, Eﬃcient approximation of deep ReLU networks for functions on low dimensional manifolds, Advances in Neural Information Processing Systems, 2019, pp.",
"[CK20] Alexander Cloninger and Timo Klock, ReLU nets adapt to intrinsic dimensionality beyond the target domain, 2020, arXiv preprint arXiv:2008.02545.",
"[CKP12] Peter G Casazza, Gitta Kutyniok, and Friedrich Philipp, Introduction to ﬁnite frame theory, Finite Frames: Theory and Applications, Birkh¨auser Boston, 2012, pp.",
"[CL19] Wojciech Czaja and Weilin Li, Analysis of time-frequency scattering transforms, Applied and Computational Harmonic Analysis 47 (2019), no.",
"1, 149–171.",
"[CLA15] Anna Choromanska, Yann LeCun, and G´erard Ben Arous, Open problem: The landscape of the loss surfaces of multilayer networks, Conference on Learning Theory, 2015, pp.",
"[CLM94] Charles K Chui, Xin Li, and Hrushikesh N Mhaskar, Neural networks for localized approximation, Mathematics of Computation 63 (1994), no.",
"208, 607–623.",
"[CM18] Charles K Chui and Hrushikesh N Mhaskar, Deep nets for local manifold learning, Frontiers in Applied Mathematics and Statistics 4 (2018), 12.",
"[CMBK20] Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi, Multiple descent: Design your own generalization curve, 2020, arXiv preprint arXiv:2008.01036.",
"[COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, On lazy training in diﬀerentiable program- ming, Advances in Neural Information Processing Systems, 2019, pp.",
"[CPV20] Andrei Caragea, Philipp Petersen, and Felix Voigtlaender, Neural network approximation and estimation of classiﬁers with classiﬁcation boundary in a Barron class, 2020, arXiv preprint arXiv:2011.09363.",
"[CS02] Felipe Cucker and Steve Smale, On the mathematical foundations of learning, Bulletin of the American Mathematical Society 39 (2002), no.",
"[CvMG+14] Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, Learning phrase representations using rnn encoder–decoder for statistical machine translation, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014, pp.",
"63 [Cyb89] George Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of Control, Signals and Systems 2 (1989), no.",
"4, 303–314.",
"[CZ07] Felipe Cucker and Ding-Xuan Zhou, Learning theory: an approximation theory viewpoint, vol.",
"24, Cambridge University Press, 2007.",
"[DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: A large-scale hierarchical image database, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp.",
"[DeV98] Ronald A DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51–150.",
"[DGL96] Luc Devroye, L´aszl´o Gy¨orﬁ, and G´abor Lugosi, A probabilistic theory of pattern recognition, Springer, 1996.",
"[DHL18] Simon S Du, Wei Hu, and Jason D Lee, Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced, Advances in Neural Information Processing Systems, 2018, pp.",
"[DHP20] Ronald DeVore, Boris Hanin, and Guergana Petrova, Neural network approximation, 2020, arXiv preprint arXiv:2012.14501.",
"[Dir29] Paul Adrien Maurice Dirac, Quantum mechanics of many-electron systems, Proceedings of the Royal Society of London.",
"Series A, Containing Papers of a Mathematical and Physical Character 123 (1929), no.",
"792, 714–733.",
"[DKMB20] S¨oren Dittmer, Tobias Kluth, Peter Maass, and Daniel Otero Baguer, Regularization by ar- chitecture: A deep prior approach for inverse problems, Journal of Mathematical Imaging and Vision 62 (2020), no.",
"3, 456–470.",
"[DLL+19] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, Gradient descent ﬁnds global minima of deep neural networks, International Conference on Machine Learning, 2019, pp.",
"[Don69] William F Donoghue, Distributions and fourier transforms, Pure and Applied Mathematics, Academic Press, 1969.",
"[DPG+14] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio, Identifying and attacking the saddle point problem in high-dimensional non- convex optimization, Advances in Neural Information Processing Systems, 2014, pp.",
"[DR17] Gintare Karolina Dziugaite and Daniel M Roy, Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data, Conference on Uncertainty in Artiﬁcial Intelligence, 2017.",
"[Dre62] Stuart Dreyfus, The numerical solution of variational problems, Journal of Mathematical Analysis and Applications 5 (1962), no.",
"[Dud67] Richard M Dudley, The sizes of compact subsets of hilbert space and continuity of Gaussian processes, Journal of Functional Analysis 1 (1967), no.",
"3, 290–330.",
"[Dud14] , Uniform central limit theorems, vol.",
"142, Cambridge University Press, 2014.",
"[DZPS18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, Gradient descent provably optimizes over-parameterized neural networks, International Conference on Learning Representations, 2018.",
"[E17] Weinan E, A proposal on machine learning via dynamical systems, Communications in Mathe- matics and Statistics 5 (2017), no.",
"64 [EGJS18] Dennis Elbr¨achter, Philipp Grohs, Arnulf Jentzen, and Christoph Schwab, DNN expression rate analysis of high-dimensional PDEs: Application to option pricing, 2018, arXiv preprint arXiv:1809.07669.",
"[EHJK19] Weinan E, Martin Hutzenthaler, Arnulf Jentzen, and Thomas Kruse, On multilevel picard numerical approximations for high-dimensional nonlinear parabolic partial diﬀerential equations and high-dimensional nonlinear backward stochastic diﬀerential equations, Journal of Scientiﬁc Computing 79 (2019), no.",
"3, 1534–1571.",
"[EHL19] Weinan E, Jiequn Han, and Qianxiao Li, A mean-ﬁeld optimal control formulation of deep learning, Research in the Mathematical Sciences 6 (2019), no.",
"[Elm90] Jeﬀrey L Elman, Finding structure in time, Cognitive Science 14 (1990), no.",
"2, 179–211.",
"[EMW19a] Weinan E, Chao Ma, and Lei Wu, Barron spaces and the compositional function spaces for neural network models, 2019, arXiv preprint arXiv:1906.08039.",
"[EMW19b] , A priori estimates of the population risk for two-layer neural networks, Communications in Mathematical Sciences 17 (2019), no.",
"5, 1407–1425.",
"[EMWW20] Weinan E, Chao Ma, Stephan Wojtowytsch, and Lei Wu, Towards a mathematical understanding of neural network-based machine learning: what we know and what we don’t, 2020, arXiv preprint arXiv:2009.10713.",
"[EPGB19] Dennis Elbr¨achter, Dmytro Perekrestenko, Philipp Grohs, and Helmut B¨olcskei, Deep neural network approximation theory, 2019, arXiv preprint arXiv:1901.02220.",
"[ES16] Ronen Eldan and Ohad Shamir, The power of depth for feedforward neural networks, Conference on Learning Theory, vol.",
"49, 2016, pp.",
"[EW20a] Weinan E and Stephan Wojtowytsch, On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics, 2020, arXiv preprint arXiv:2007.15623.",
"[EW20b] , A priori estimates for classiﬁcation problems using neural networks, 2020, arXiv preprint arXiv:2009.13500.",
"[EW20c] , Representation formulas and pointwise properties for Barron functions, 2020, arXiv preprint arXiv:2006.05982.",
"[EY18] Weinan E and Bing Yu, The deep ritz method: a deep learning-based numerical algorithm for solving variational problems, Communications in Mathematics and Statistics 6 (2018), no.",
"[FB17] Daniel C Freeman and Joan Bruna, Topology and geometry of half-rectiﬁed network optimization, International Conference on Learning Representations, 2017.",
"[FC18] Jonathan Frankle and Michael Carbin, The lottery ticket hypothesis: Finding sparse, trainable neural networks, International Conference on Learning Representations, 2018.",
"[FHH+17] Felix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E Dahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld, Prediction errors of molecular machine learning models lower than hybrid DFT error, Journal of Chemical Theory and Computation 13 (2017), no.",
"11, 5255–5264.",
"[Fun89] Ken-Ichi Funahashi, On the approximate realization of continuous mappings by neural networks, Neural Networks 2 (1989), no.",
"3, 183–192.",
"65 [GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep learning, MIT Press, 2016.",
"[G´er17] Aurelien G´eron, Hands-on machine learning with scikit-learn and tensorﬂow: Concepts, tools, and techniques to build intelligent systems, O’Reilly Media, 2017.",
"[GH21] Philipp Grohs and Lukas Herrmann, Deep neural network approximation for high-dimensional parabolic Hamilton-Jacobi-Bellman equations, 2021, arXiv preprint arXiv:2103.05744.",
"[GH22] , Deep neural network approximation for high-dimensional elliptic PDEs with boundary conditions, IMA Journal of Numerical Analysis 42 (2022), no.",
"3, 2055–2082.",
"[GHJVW20] Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger, A proof that artiﬁcial neural networks overcome the curse of dimensionality in the numerical approximation of Black-Scholes partial diﬀerential equations, Memoirs of the American Mathematical Society (2020).",
"[GHJY15] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan, Escaping from saddle points—online stochastic gradient for tensor decomposition, Conference on Learning Theory, 2015, pp.",
"[GJS+20] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St´ephane d’Ascoli, Giulio Biroli, Cl´ement Hongler, and Matthieu Wyart, Scaling description of generalization with number of parameters in deep learning, Journal of Statistical Mechanics: Theory and Experiment (2020), no.",
"[GKP20] Ingo G¨uhring, Gitta Kutyniok, and Philipp Petersen, Error bounds for approximations with deep ReLU neural networks in W s,p norms, Analysis and Applications 18 (2020), no.",
"05, 803–859.",
"[GKR20] Philipp Grohs, Sarah Koppensteiner, and Martin Rathmair, Phase retrieval: Uniqueness and stability, SIAM Review 62 (2020), no.",
"2, 301–350.",
"[GL13] Saeed Ghadimi and Guanghui Lan, Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming, SIAM Journal on Optimization 23 (2013), no.",
"4, 2341–2368.",
"[GLSS18a] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nathan Srebro, Characterizing implicit bias in terms of optimization geometry, International Conference on Machine Learning, 2018, pp.",
"[GLSS18b] , Implicit bias of gradient descent on linear convolutional networks, Advances in Neural Information Processing Systems, 2018, pp.",
"[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Linearized two- layers neural networks in high dimension, The Annals of Statistics 49 (2021), no.",
"2, 1029–1054.",
"[GOW19] Davis Gilton, Greg Ongie, and Rebecca Willett, Neumann networks for linear inverse problems in imaging, IEEE Transactions on Computational Imaging 6 (2019), 328–343.",
"[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, Generative adversarial nets, Advances in Neural Information Processing Systems, 2014, pp.",
"[GRK20] Ingo G¨uhring, Mones Raslan, and Gitta Kutyniok, Expressivity of deep neural networks, 2020, arXiv preprint arXiv:2007.04759.",
"[GRS18] Noah Golowich, Alexander Rakhlin, and Ohad Shamir, Size-independent sample complexity of neural networks, Conference On Learning Theory, 2018, pp.",
"[GS20] Lukas Gonon and Christoph Schwab, Deep ReLU network expression rates for option prices in high-dimensional, exponential L´evy models, 2020, ETH Zurich SAM Research Report.",
"66 [GV21] Philipp Grohs and Felix Voigtlaender, Proof of the theory-to-practice gap in deep learning via sampling complexity bounds for neural network approximation spaces, 2021, arXiv preprint arXiv:2104.02746.",
"[GW08] Andreas Griewank and Andrea Walther, Evaluating derivatives: principles and techniques of algorithmic diﬀerentiation, SIAM, 2008.",
"[GZ84] Evarist Gin´e and Joel Zinn, Some limit theorems for empirical processes, The Annals of Probability (1984), 929–989.",
"[Han19] Boris Hanin, Universal function approximation by deep neural nets with bounded width and ReLU activations, Mathematics 7 (2019), no.",
"[Hau95] David Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik- chervonenkis dimension, Journal of Combinatorial Theory, Series A 2 (1995), no.",
"69, 217–232.",
"[HH19] Catherine F Higham and Desmond J Higham, Deep learning: An introduction for applied mathematicians, SIAM Review 61 (2019), no.",
"4, 860–891.",
"[HHJ15] Martin Hairer, Martin Hutzenthaler, and Arnulf Jentzen, Loss of regularity for Kolmogorov equations, The Annals of Probability 43 (2015), no.",
"2, 468–527.",
"[HJE18] Jiequn Han, Arnulf Jentzen, and Weinan E, Solving high-dimensional partial diﬀerential equations using deep learning, Proceedings of the National Academy of Sciences 115 (2018), no.",
"34, 8505– 8510.",
"[HJKN20] Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen, A proof that recti- ﬁed deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations, SN Partial Diﬀerential Equations and Applications 1 (2020), no.",
"[HLXZ20] Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng, ReLU deep neural networks and linear ﬁnite elements, Journal of Computational Mathematics 38 (2020), no.",
"3, 502–527.",
"[HMD16] Song Han, Huizi Mao, and William J Dally, Deep compression: Compressing deep neural network with pruning, trained quantization and Huﬀman coding, International Conference on Learning Representations, 2016.",
"[HMRT19] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani, Surprises in high- dimensional ridgeless least squares interpolation, 2019, arXiv preprint arXiv:1903.08560.",
"[Hoe63] Wassily Hoeﬀding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (1963), no.",
"301, 13–30.",
"[Hop82] John J Hopﬁeld, Neural networks and physical systems with emergent collective computational abilities, Proceedings of the National Academy of Sciences 79 (1982), no.",
"8, 2554–2558.",
"[HR19] Boris Hanin and David Rolnick, Deep ReLU networks have surprisingly few activation patterns, Advances in Neural Information Processing Systems, 2019, pp.",
"[HRS16] Moritz Hardt, Ben Recht, and Yoram Singer, Train faster, generalize better: Stability of stochastic gradient descent, International Conference on Machine Learning, 2016, pp.",
"[HS97] Sepp Hochreiter and J¨urgen Schmidhuber, Long short-term memory, Neural Computation 9 (1997), no.",
"8, 1735–1780.",
"[HS17] Boris Hanin and Mark Sellke, Approximating continuous functions by ReLU nets of minimal width, 2017, arXiv preprint arXiv:1710.11278.",
"67 [HSL+16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger, Deep networks with stochastic depth, European Conference on Computer Vision, 2016, pp.",
"[HSN20] Jan Hermann, Zeno Sch¨atzle, and Frank No´e, Deep-neural-network solution of the electronic Schr¨odinger equation, Nature Chemistry 12 (2020), no.",
"10, 891–897.",
"[HSW89] Kurt Hornik, Maxwell Stinchcombe, and Halbert White, Multilayer feedforward networks are universal approximators, Neural Networks 2 (1989), no.",
"5, 359–366.",
"[HTF01] Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The elements of statistical learning: Data mining, inference, and prediction, Springer Series in Statistics, Springer, 2001.",
"[HV17] Benjamin D Haeﬀele and Ren´e Vidal, Global optimality in neural network training, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.",
"[HvdG19] Peter Hinz and Sara van de Geer, A framework for the construction of upper bounds on the number of aﬃne linear regions of ReLU feed-forward neural networks, IEEE Transactions on Information Theory 65 (2019), 7304–7324.",
"[HZ94] Geoﬀrey E Hinton and Richard S Zemel, Autoencoders, minimum description length, and helmholtz free energy, Advances in Neural Information Processing Systems 6 (1994), 3–10.",
"[HZE19] Jiequn Han, Linfeng Zhang, and Weinan E, Solving many-electron Schr¨odinger equation using deep neural networks, Journal of Computational Physics 399 (2019), 108929.",
"[HZRS15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Delving deep into rectiﬁers: Sur- passing human-level performance on imagenet classiﬁcation, Proceedings of IEEE International Conference on Computer Vision, 2015, pp.",
"[HZRS16] , Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.",
"[IS15] Sergey Ioﬀe and Christian Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, International Conference on Machine Learning, 2015, pp.",
"[JGH18] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler, Neural tangent kernel: Convergence and generalization in neural networks, Advances in Neural Information Processing Systems, 2018, pp.",
"[JKMB19] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio, Predicting the generaliza- tion gap in deep networks with margin distributions, International Conference on Learning Representations, 2019.",
"[JKNvW20] Arnulf Jentzen, Benno Kuckuck, Ariel Neufeld, and Philippe von Wurstemberger, Strong error analysis for stochastic gradient descent optimization algorithms, IMA Journal of Numerical Analysis 41 (2020), no.",
"1, 455–492.",
"[JMFU17] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser, Deep convolu- tional neural network for inverse problems in imaging, IEEE Transactions on Image Processing 26 (2017), no.",
"9, 4509–4522.",
"[JMS17] Bangti Jin, Peter Maaß, and Otmar Scherzer, Sparsity regularization in inverse problems, Inverse Problems 33 (2017), no.",
"[JNM+20] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio, Fan- tastic generalization measures and where to ﬁnd them, International Conference on Learning Representations, 2020.",
"68 [Jor90] Michael I Jordan, Attractor dynamics and parallelism in a connectionist sequential machine, Artiﬁcial neural networks: concept learning, IEEE Press, 1990, pp.",
"[JT19a] Ziwei Ji and Matus Telgarsky, Gradient descent aligns the layers of deep linear networks, International Conference on Learning Representations, 2019.",
"[JT19b] , A reﬁned primal-dual analysis of the implicit bias, 2019, arXiv preprint arXiv:1906.04540.",
"[JT20] , Directional convergence and alignment in deep learning, Advances in Neural Information Processing Systems, 2020, pp.",
17176–17186.,
"[Jud90] Stephen J Judd, Neural network design and the complexity of learning, MIT Press, 1990.",
"[Kel60] Henry J Kelley, Gradient theory of optimal ﬂight paths, Ars Journal 30 (1960), no.",
"10, 947–954.",
"[KH09] Alex Krizhevsky and Geoﬀrey Hinton, Learning multiple layers of features from tiny images, Tech.",
"report, University of Toronto, 2009.",
"[KL18] Sham M Kakade and Jason D Lee, Provably correct automatic subdiﬀerentiation for qualiﬁed programs, Advances in Neural Information Processing Systems, 2018, pp.",
"[KL20] Patrick Kidger and Terry Lyons, Universal approximation with deep narrow networks, Conference on Learning Theory, 2020, pp.",
"[KM97] Marek Karpinski and Angus Macintyre, Polynomial bounds for VC dimension of sigmoidal and general Pfaﬃan neural networks, Journal of Computer and System Sciences 54 (1997), no.",
"1, 169–176.",
"[KMN+17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang, On large-batch training for deep learning: Generalization gap and sharp minima, International Conference on Learning Representations, 2017.",
"[KPRS19] Gitta Kutyniok, Philipp Petersen, Mones Raslan, and Reinhold Schneider, A theoretical analysis of deep neural networks and parametric PDEs, 2019, arXiv preprint arXiv:1904.00377.",
"[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton, Imagenet classiﬁcation with deep convolutional neural networks, Advances in Neural Information Processing Systems, 2012, pp.",
"[KW52] Jack Kiefer and Jacob Wolfowitz, Stochastic estimation of the maximum of a regression function, The Annals of Mathematical Statistics 23 (1952), no.",
"3, 462–466.",
"[LBBH98] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner, Gradient-based learning applied to document recognition, Proceedings of the IEEE 86 (1998), no.",
"11, 2278–2324.",
"[LBD+89] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel, Backpropagation applied to handwritten zip code recognition, Neural Computation 1 (1989), no.",
"4, 541–551.",
"[LBH15] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton, Deep learning, Nature 521 (2015), no.",
"7553, 436–444.",
"[LBN+18] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington, and Jascha Sohl-Dickstein, Deep neural networks as Gaussian processes, International Conference on Learning Representations, 2018.",
"[LC19] Guillaume Lample and Fran¸cois Charton, Deep learning for symbolic mathematics, International Conference on Learning Representations, 2019.",
"69 [LD21] Licong Lin and Edgar Dobriban, What causes the test error?",
"Going beyond bias-variance via ANOVA, Journal of Machine Learning Research 22 (2021), no.",
"[LDS89] Yann LeCun, John S Denker, and Sara A Solla, Optimal brain damage, Advances in Neural Information Processing Systems, 1989, pp.",
"[Lew43] Kurt Lewin, Psychology and the process of group living, The Journal of Social Psychology 17 (1943), no.",
"1, 113–131.",
"[Li21] Weilin Li, Generalization error of minimum weighted norm and kernel interpolation, SIAM Journal on Mathematics of Data Science 3 (2021), no.",
"1, 414–438.",
"[Lin70] Seppo Linnainmaa, Alogritmin kumulatiivinen py¨oristysvirhe yksitt¨aisten py¨oristysvirheiden Taylor-kehitelm¨an¨a, Master’s thesis, University of Helsinki, 1970.",
"[LL18] Yuanzhi Li and Yingyu Liang, Learning overparameterized neural networks via stochastic gradient descent on structured data, Advances in Neural Information Processing Systems, 2018, pp.",
"[LL19] Kaifeng Lyu and Jian Li, Gradient descent maximizes the margin of homogeneous neural networks, International Conference on Learning Representations, 2019.",
"[LLPS93] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken, Multilayer feedforward networks with a nonpolynomial activation function can approximate any function, Neural Networks 6 (1993), no.",
"6, 861–867.",
"[LLS19] Qianxiao Li, Ting Lin, and Zuowei Shen, Deep learning via dynamical systems: An approximation perspective, 2019, arXiv preprint arXiv:1912.10382.",
"[LML+20] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying, A mean ﬁeld analysis of deep ResNet and beyond: Towards provably optimization via overparameterization from depth, International Conference on Machine Learning, 2020, pp.",
"[L¨OS18] Sebastian Lunz, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb, Adversarial regularizers in inverse problems, Advances in Neural Information Processing Systems, 2018, pp.",
"[LP21] Fabian Laakmann and Philipp Petersen, Eﬃcient approximation of solutions of parametric linear transport equations by ReLU DNNs, Advances in Computational Mathematics 47 (2021), no.",
"[LPRS19] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes, Fisher–Rao metric, geometry, and complexity of neural networks, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[LR20] Tengyuan Liang and Alexander Rakhlin, Just interpolate: Kernel “ridgeless” regression can generalize, The Annals of Statistics 48 (2020), no.",
"3, 1329–1347.",
"[LRZ20] Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai, On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels, Conference on Learning Theory, 2020, pp.",
"[LS17] Shiyu Liang and R Srikant, Why deep neural networks for function approximation?, International Conference on Learning Representations, 2017.",
"[LSAH20] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier, NETT: Solving inverse problems with deep neural networks, Inverse Problems 36 (2020), no.",
"70 [LSJR16] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht, Gradient descent only converges to minimizers, Conference on Learning Theory, 2016, pp.",
"[LT91] Michel Ledoux and Michel Talagrand, Probability in Banach spaces: Isoperimetry and processes, vol.",
"23, Springer Science & Business Media, 1991.",
"[LTY19] Bo Li, Shanshan Tang, and Haijun Yu, Better approximations of high dimensional smooth functions by deep neural networks with rectiﬁed power units, Communications in Computational Physics 27 (2019), no.",
"2, 379–411.",
"[LXS+20] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jeﬀrey Pennington, Wide neural networks of any depth evolve as linear models under gradient descent, Journal of Statistical Mechanics: Theory and Experiment 2020 (2020), no.",
"12, 124002.",
"[Mal12] St´ephane Mallat, Group invariant scattering, Communications on Pure and Applied Mathematics 65 (2012), no.",
"10, 1331–1398.",
"[Mal16] , Understanding deep convolutional networks, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 374 (2016), no.",
"2065, 20150203.",
"[MAV18] Poorya Mianjy, Raman Arora, and Rene Vidal, On the implicit bias of dropout, International Conference on Machine Learning, 2018, pp.",
"[McA99] David A McAllester, Pac-bayesian model averaging, Conference on Learning Theory, 1999, pp.",
"[McD89] Colin McDiarmid, On the method of bounded diﬀerences, Surveys in Combinatorics 141 (1989), no.",
"1, 148–188.",
"[Men14] Shahar Mendelson, Learning without concentration, Conference on Learning Theory, 2014, pp.",
"[Mha96] Hrushikesh N Mhaskar, Neural networks for optimal approximation of smooth and analytic functions, Neural Computation 8 (1996), no.",
"1, 164–177.",
"[MHR+18] Alexander G de G Matthews, Jiri Hron, Mark Rowland, Richard E Turner, and Zoubin Ghahramani, Gaussian process behaviour in wide deep neural networks, International Conference on Learning Representations, 2018.",
"[MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller, Playing atari with deep reinforcement learning, 2013, arXiv preprint arXiv:1312.5602.",
"[MLE21] Vishal Monga, Yuelong Li, and Yonina C Eldar, Algorithm unrolling: Interpretable, eﬃcient deep learning for signal and image processing, IEEE Signal Processing Magazine 38 (2021), no.",
"[MM19] Song Mei and Andrea Montanari, The generalization error of random features regression: Precise asymptotics and double descent curve, 2019, arXiv preprint arXiv:1908.05355.",
"[MOPS20] Carlo Marcati, Joost Opschoor, Philipp Petersen, and Christoph Schwab, Exponential ReLU neural network approximation rates for point and edge singularities, 2020, ETH Zurich SAM Research Report.",
"[MP43] Warren S McCulloch and Walter Pitts, A logical calculus of the ideas immanent in nervous activity, The Bulletin of Mathematical Biophysics 5 (1943), no.",
"4, 115–133.",
"71 [MP69] Marvin Minsky and Seymour A Papert, Perceptrons, MIT Press, 1969.",
"[MP99] Vitaly Maiorov and Allan Pinkus, Lower bounds for approximation by MLP neural networks, Neurocomputing 25 (1999), no.",
"1-3, 81–91.",
"[MPCB14] Guido Mont´ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio, On the number of linear regions of deep neural networks, Advances in Neural Information Processing Systems, 2014, pp.",
"[MSL+15] Junshui Ma, Robert P Sheridan, Andy Liaw, George E Dahl, and Vladimir Svetnik, Deep neural nets as a method for quantitative structure–activity relationships, Journal of chemical information and modeling 55 (2015), no.",
"2, 263–274.",
"[MV03] Shahar Mendelson and Roman Vershynin, Entropy and the combinatorial dimension, Inventiones mathematicae 152 (2003), no.",
"[MVSS20] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai, Harmless interpolation of noisy data in regression, IEEE Journal on Selected Areas in Information Theory 1 (2020), no.",
"[MZ20] Andrea Montanari and Yiqiao Zhong, The interpolation phase transition in neural networks: Memorization and generalization under lazy training, 2020, arXiv preprint arXiv:2007.12826.",
"[NBMS17] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro, Exploring generalization in deep learning, Advances in Neural Information Processing Systems, 2017, pp.",
"[NBS18] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro, A PAC-Bayesian approach to spectrally-normalized margin bounds for neural networks, International Conference on Learning Representations, 2018.",
"[NH17] Quynh Nguyen and Matthias Hein, The loss surface of deep and wide neural networks, Interna- tional Conference on Machine Learning, 2017, pp.",
"[NI20] Ryumei Nakada and Masaaki Imaizumi, Adaptive approximation and generalization of deep neural network with intrinsic dimensionality, Journal of Machine Learning Research 21 (2020), no.",
"[NJLS09] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro, Robust stochastic approximation approach to stochastic programming, SIAM Journal on Optimization 19 (2009), no.",
"4, 1574–1609.",
"[NK19] Vaishnavh Nagarajan and J Zico Kolter, Uniform convergence may be unable to explain general- ization in deep learning, Advances in Neural Information Processing Systems, 2019, pp.",
11615– 11626.,
"[NKB+20] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever, Deep double descent: Where bigger models and more data hurt, International Conference on Learning Representations, 2020.",
"[NLG+19] Mor Shpigel Nacson, Jason D Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry, Convergence of gradient descent on separable data, International Conference on Artiﬁcial Intelligence and Statistics, 2019, pp.",
"[NTS14] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro, In search of the real inductive bias: On the role of implicit regularization in deep learning, 2014, arXiv preprint arXiv:1412.6614.",
"72 [NTS15] , Norm-based capacity control in neural networks, Conference on Learning Theory, 2015, pp.",
"[NW09] Erich Novak and Henryk Wo´zniakowski, Approximation of inﬁnitely diﬀerentiable multivariate functions is intractable, Journal of Complexity 25 (2009), no.",
"4, 398–404.",
"[NY83] Arkadi Semenovich Nemirovsky and David Borisovich Yudin, Problem complexity and method eﬃciency in optimization, Wiley-Interscience Series in Discrete Mathematics, Wiley, 1983.",
"[NZGK21] Tenavi Nakamura-Zimmerer, Qi Gong, and Wei Kang, Adaptive deep learning for high- dimensional Hamilton–Jacobi–Bellman Equations, SIAM Journal on Scientiﬁc Computing 43 (2021), no.",
"2, A1221–A1247.",
"[OF96] Bruno A Olshausen and David J Field, Sparse coding of natural images produces localized, oriented, bandpass receptive ﬁelds, Nature 381 (1996), no.",
"[OM98] Genevieve B Orr and Klaus-Robert M¨uller, Neural networks: tricks of the trade, Springer, 1998.",
"[OPS20] Joost Opschoor, Philipp Petersen, and Christoph Schwab, Deep ReLU networks and high-order ﬁnite element methods, Analysis and Applications (2020), no.",
"[OS19] Kenta Oono and Taiji Suzuki, Approximation and non-parametric estimation of ResNet-type convolutional neural networks, International Conference on Machine Learning, 2019, pp.",
4922– 4931.,
"[PGZ+18] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and JeﬀDean, Eﬃcient neural architecture search via parameters sharing, International Conference on Machine Learning, 2018, pp.",
4095– 4104.,
"[PKL+17] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix, Jack Hidary, and Hrushikesh N Mhaskar, Theory of deep learning III: explaining the non-overﬁtting puzzle, 2017, arXiv preprint arXiv:1801.00173.",
"[PLR+16] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli, Exponential expressivity in deep neural networks through transient chaos, Advances in Neural Information Processing Systems, 2016, pp.",
"[PMR+17] Tomaso Poggio, Hrushikesh N Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao, Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review, International Journal of Automation and Computing 14 (2017), no.",
"5, 503–519.",
"[PP92] Etienne Pardoux and Shige Peng, Backward stochastic diﬀerential equations and quasilin- ear parabolic partial diﬀerential equations, Stochastic partial diﬀerential equations and their applications, Springer, 1992, pp.",
"[PRE17] Vardan Papyan, Yaniv Romano, and Michael Elad, Convolutional neural networks analyzed via convolutional sparse coding, Journal of Machine Learning Research 18 (2017), no.",
"1, 2887–2938.",
"[PRMN04] Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi, General conditions for predictivity in learning theory, Nature 428 (2004), no.",
"6981, 419–422.",
"[PRSE18] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad, Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks, IEEE Signal Processing Magazine 35 (2018), no.",
"[PRV20] Philipp Petersen, Mones Raslan, and Felix Voigtlaender, Topological properties of the set of functions generated by neural networks of ﬁxed size, Foundations of Computational Mathematics (2020), 1–70.",
"73 [PSE17] Vardan Papyan, Jeremias Sulam, and Michael Elad, Working locally thinking globally: Theoretical guarantees for convolutional sparse coding, IEEE Transactions on Signal Processing 65 (2017), no.",
"21, 5687–5701.",
"[PSMF20] David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes, Ab initio solution of the many-electron schr¨odinger equation with deep neural networks, Physical Review Research 2 (2020), no.",
"[PV18] Philipp Petersen and Felix Voigtlaender, Optimal approximation of piecewise smooth functions using deep ReLU neural networks, Neural Networks 108 (2018), 296–330.",
"[PV20] , Equivalence of approximation by convolutional neural networks and fully-connected networks, Proceedings of the American Mathematical Society 148 (2020), no.",
"4, 1567–1581.",
"[REM17] Yaniv Romano, Michael Elad, and Peyman Milanfar, The little engine that could: Regularization by denoising (red), SIAM Journal on Imaging Sciences 10 (2017), no.",
"4, 1804–1844.",
"[RFB15] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, U-net: Convolutional networks for biomedical image segmentation, International Conference on Medical image computing and computer-assisted intervention, 2015, pp.",
"[RH19] Lars Ruthotto and Eldad Haber, Deep neural networks motivated by partial diﬀerential equations, Journal of Mathematical Imaging and Vision (2019), 1–13.",
"[RHW86] David E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams, Learning representations by back-propagating errors, Nature 323 (1986), no.",
"6088, 533–536.",
"[RM51] Herbert Robbins and Sutton Monro, A stochastic approximation method, The Annals of Mathe- matical Statistics (1951), 400–407.",
"[RMD+06] P L´opez R´ıos, Ao Ma, Neil D Drummond, Michael D Towler, and Richard J Needs, Inhomoge- neous backﬂow transformations in quantum Monte Carlo calculations, Physical Review E 74 (2006), no.",
"[Ros58] Frank Rosenblatt, The perceptron: a probabilistic model for information storage and organization in the brain, Psychological review 65 (1958), no.",
"[RPK+17] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein, On the expressive power of deep neural networks, International Conference on Machine Learning, 2017, pp.",
"[RPK19] Maziar Raissi, Paris Perdikaris, and George E Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations, Journal of Computational Physics 378 (2019), 686–707.",
"[RR+07] Ali Rahimi, Benjamin Recht, et al., Random features for large-scale kernel machines, Advances in Neural Information Processing Systems, 2007, pp.",
"[Rud06] Walter Rudin, Real and complex analysis, McGraw-Hill Series in Higher Mathematics, Tata McGraw-Hill, 2006.",
"[RWK+20] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari, What’s hidden in a randomly weighted neural network?, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp.",
11893–11902.,
"[Sak99] Akito Sakurai, Tight bounds for the VC-dimension of piecewise polynomial networks, Advances in Neural Information Processing Systems, 1999, pp.",
"74 [SCC18] Uri Shaham, Alexander Cloninger, and Ronald R Coifman, Provable approximation properties for deep neural networks, Applied and Computational Harmonic Analysis 44 (2018), no.",
"3, 537–557.",
"[Sch15] J¨urgen Schmidhuber, Deep learning in neural networks: An overview, Neural Networks 61 (2015), 85–117.",
"[SDR14] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski, Lectures on stochastic pro- gramming: modeling and theory, SIAM, 2014.",
"[SEJ+20] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin ˇZ´ıdek, Alexander WR Nelson, and Alex Bridgland, Improved protein structure prediction using potentials from deep learning, Nature 577 (2020), no.",
"7792, 706–710.",
"[SGHK18] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli, Analysing mathematical reasoning abilities of neural models, International Conference on Learning Representations, 2018.",
"[SGS15] Rupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber, Training very deep networks, Advances in Neural Information Processing Systems, 2015, pp.",
"[SH19] Johannes Schmidt-Hieber, Deep ReLU network approximation of functions on a manifold, 2019, arXiv preprint arXiv:1908.00695.",
"[She20] Zuowei Shen, Deep network approximation characterized by number of neurons, Communications in Computational Physics 28 (2020), no.",
"5, 1768–1811.",
"[SHK+14] Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, Dropout: a simple way to prevent neural networks from overﬁtting, Journal of Machine Learning Research 15 (2014), no.",
"1, 1929–1958.",
"[SHM+16] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot, Mas- tering the game of go with deep neural networks and tree search, Nature 529 (2016), no.",
"7587, 484–489.",
"[SHN+18] Daniel Soudry, Elad Hoﬀer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro, The implicit bias of gradient descent on separable data, 2018.",
"[ˇS´ım02] Jiˇr´ı ˇS´ıma, Training a single sigmoidal neuron is hard, Neural Computation 14 (2002), no.",
"11, 2709–2728.",
"[SKS+17] Kristof T Sch¨utt, Pieter-Jan Kindermans, Huziel E Sauceda, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M¨uller, Schnet: A continuous-ﬁlter convolutional neural network for modeling quantum interactions, Advances in Neural Information Processing Systems, 2017, pp.",
"[SLJ+15] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich, Going deeper with convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp.",
"[SO12] Attila Szabo and Neil S Ostlund, Modern quantum chemistry: introduction to advanced electronic structure theory, Courier Corporation, 2012.",
"[SPRE18] Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad, Multilayer convolutional sparse modeling: Pursuit and dictionary learning, IEEE Transactions on Signal Processing 66 (2018), no.",
"15, 4090–4104.",
"75 [SRG+21] Michael Scherbela, Rafael Reisenhofer, Leon Gerard, Philipp Marquetand, and Philipp Grohs, Solving the electronic Schr¨odinger equation for multiple nuclear geometries with weight-sharing deep neural networks, 2021, arXiv preprint arXiv:2105.08351.",
"[SS16] Itay Safran and Ohad Shamir, On the quality of the initial basin in overspeciﬁed neural networks, International Conference on Machine Learning, 2016, pp.",
"[SS17] , Depth-width tradeoﬀs in approximating natural functions with neural networks, Interna- tional Conference on Machine Learning, 2017, pp.",
"[SS18] , Spurious local minima are common in two-layer ReLU neural networks, International Conference on Machine Learning, 2018, pp.",
"[SSBD14] Shai Shalev-Shwartz and Shai Ben-David, Understanding machine learning: From theory to algorithms, Cambridge University Press, 2014.",
"[SSS+17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton, Mastering the game of go without human knowledge, Nature 550 (2017), no.",
"7676, 354–359.",
"[SSSSS09] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan, Stochastic convex optimization, Conference on Learning Theory, 2009.",
"[STIM18] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, How does batch normalization help optimization?, Advances in Neural Information Processing Systems, 2018, pp.",
"[SZ19] Christoph Schwab and Jakob Zech, Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in uq, Analysis and Applications 17 (2019), no.",
"[Tal94] Michel Talagrand, Sharper bounds for Gaussian and empirical processes, The Annals of Proba- bility (1994), 28–76.",
"[Tel15] Matus Telgarsky, Representation beneﬁts of deep feedforward networks, 2015, arXiv preprint arXiv:1509.08101.",
"[TvG18] Matthew Thorpe and Yves van Gennip, Deep limits of residual neural networks, 2018, arXiv preprint arXiv:1810.11741.",
"[UVL18] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky, Deep image prior, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.",
"[Vap99] Vladimir Vapnik, An overview of statistical learning theory, IEEE Transactions on Neural Networks 10 (1999), no.",
"5, 988–999.",
"[Vap13] , The nature of statistical learning theory, Springer science & business media, 2013.",
"[VBB19] Luca Venturi, Afonso S Bandeira, and Joan Bruna, Spurious valleys in one-hidden-layer neural network optimization landscapes, Journal of Machine Learning Research 20 (2019), no.",
"[VBC+19] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, and Petko Georgiev, Grandmaster level in StarCraft II using multi-agent reinforcement learning, Nature 575 (2019), no.",
"7782, 350–354.",
"76 [VC71] Vladimir Vapnik and Alexey Chervonenkis, On the uniform convergence of relative frequencies of events to their probabilities, Theory of Probability & Its Applications 16 (1971), no.",
"2, 264–280.",
"[vdVW97] Aad W van der Vaart and Jon A Wellner, Weak convergence and empirical processes with applications to statistics, Journal of the Royal Statistical Society-Series A Statistics in Society 160 (1997), no.",
"3, 596–608.",
"[Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications in data science, vol.",
"47, Cambridge University Press, 2018.",
"[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in Neural Information Processing Systems, 2017, pp.",
"[Wer88] Paul J Werbos, Generalization of backpropagation with application to a recurrent gas market model, Neural Networks 1 (1988), no.",
"4, 339–356.",
"[WGB17] Thomas Wiatowski, Philipp Grohs, and Helmut B¨olcskei, Energy propagation in deep convolu- tional neural networks, IEEE Transactions on Information Theory 64 (2017), no.",
"7, 4819–4842.",
"[Whi34] Hassler Whitney, Analytic extensions of diﬀerentiable functions deﬁned in closed sets, Transac- tions of the American Mathematical Society 36 (1934), no.",
"[WPC+21] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip, A comprehensive survey on graph neural networks, IEEE Transactions on Neural Networks and Learning Systems 32 (2021), no.",
"[WZ95] Ronald J Williams and David Zipser, Gradient-based learning algorithms for recurrent, Back- propagation: Theory, Architectures, and Applications 433 (1995), 17.",
"[WZZ+13] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus, Regularization of neural networks using dropconnect, International Conference on Machine Learning, 2013, pp.",
"[XM12] Huan Xu and Shie Mannor, Robustness and generalization, Machine learning 86 (2012), no.",
"3, 391–423.",
"[Yan19] Greg Yang, Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation, 2019, arXiv preprint arXiv:1902.04760.",
"[Yar17] Dmitry Yarotsky, Error bounds for approximations with deep ReLU networks, Neural Networks 94 (2017), 103–114.",
"[Yar18a] , Optimal approximation of continuous functions by very deep ReLU networks, Conference on Learning Theory, 2018, pp.",
"[Yar18b] , Universal approximations of invariant maps by neural networks, 2018, arXiv preprint arXiv:1804.10306.",
"[Yar21] , Elementary superexpressive activations, 2021, arXiv preprint arXiv:2102.10911.",
"[YGLD17] Rujie Yin, Tingran Gao, Yue M Lu, and Ingrid Daubechies, A tale of two bases: Local-nonlocal regularization on image patches with convolution framelets, SIAM Journal on Imaging Sciences 10 (2017), no.",
"2, 711–750.",
"[YHC18] Jong Chul Ye, Yoseob Han, and Eunju Cha, Deep convolutional framelets: A general deep learning framework for inverse problems, SIAM Journal on Imaging Sciences 11 (2018), no.",
"2, 991–1048.",
"77 [YHPC18] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria, Recent trends in deep learning based natural language processing, IEEE Computational Intelligence Magazine 13 (2018), no.",
"[Yse10] Harry Yserentant, Regularity and approximability of electronic wave functions, Springer, 2010.",
"[YZ20] Dmitry Yarotsky and Anton Zhevnerchuk, The phase diagram of approximation rates for deep neural networks, Advances in Neural Information Processing Systems, vol.",
"[ZAP16] Hao Zhou, Jose M Alvarez, and Fatih Porikli, Less is more: Towards compact CNNs, European Conference on Computer Vision, 2016, pp.",
"[Zas75] Thomas Zaslavsky, Facing up to arrangements: Face-count formulas for partitions of space by hyperplanes: Face-count formulas for partitions of space by hyperplanes, Memoirs of the American Mathematical Society, American Mathematical Society, 1975.",
"[ZBH+17] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals, Under- standing deep learning requires rethinking generalization, International Conference on Learning Representations, 2017.",
"[ZBH+20] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C Mozer, and Yoram Singer, Identity crisis: Memorization and generalization under extreme overparameterization, International Conference on Learning Representations, 2020.",
"[ZBS19] Chiyuan Zhang, Samy Bengio, and Yoram Singer, Are all layers created equal?, 2019, arXiv preprint arXiv:1902.01996.",
"[ZCZG20] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, Gradient descent optimizes over- parameterized deep ReLU networks, Machine Learning 109 (2020), no.",
"3, 467–492.",
"[Zho20a] Ding-Xuan Zhou, Theory of deep convolutional neural networks: Downsampling, Neural Networks 124 (2020), 319–327.",
"[Zho20b] , Universality of deep convolutional neural networks, Applied and Computational Har- monic Analysis 48 (2020), no.",
"2, 787–794.",
"[ZKS+18] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia Johnson, Mary Bruno, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael Rabbat, Pascal Vincent, Naﬁssa Yakubova, James Pinkerton, Duo Wang, Erich Owens, C Lawrence Zitnick, Michael P Recht, Daniel K Sodickson, and Yvonne W Lui, fastMRI: An open dataset and benchmarks for accelerated MRI, 2018, arXiv preprint arXiv:1811.08839.",
"[ZL17] Barret Zoph and Quoc V Le, Neural architecture search with reinforcement learning, International Conference on Learning Representations, 2017.",
"CHANGING DATA SOURCES IN THE AGE OF MACHINE LEARNING FOR OFFICIAL STATISTICS PRESENTED AT UNECE MACHINE LEARNING FOR OFFICIAL STATISTICS WORKSHOP 2023 Cedric De Boom Statistics Flanders Belgium cedric.deboom@vlaanderen.be Michael Reusens Statistics Flanders Belgium michael.reusens@vlaanderen.be June 6, 2023 ABSTRACT Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data.",
"With such data science practices in place, it enables more timely, more insightful and more flexible reporting.",
"However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them.",
"In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.",
"This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics.",
"We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception.",
"Next, we highlight the repercussions of changing data sources on statistical reporting.",
"These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering.",
"We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring.",
"In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.",
"1 Introduction The field of statistics has long played a critical role in informing policy decisions, driving innovation, and advancing scientific knowledge.",
"Traditional statistical methods such as surveys and censuses have provided valuable insights into a wide range of topics, from population demographics to economic trends and public opinion.",
"However, in recent years, the increasing availability of open and large data sources has opened up new opportunities for statistical analysis.",
"In particular, the rise of machine learning has transformed the field of statistics, enabling the analysis of massive datasets, the identification of complex patterns and relationships, non-linear forecasting, etc.",
"Machine learning algorithms can be used to analyze data from a wide range of sources, providing insights that traditional survey methods may not capture.",
"The use of machine learning for official statistics has the potential to provide more timely, accurate and comprehensive insights into a wide range of societal topics [3].",
"By leveraging the vast amounts of data that are generated by individuals and entities on a daily basis, statistical agencies can gain a more nuanced understanding of trends and patterns, and respond more quickly to emerging issues.",
"However, this shift towards machine learning also presents a number of challenges.",
"In particular, there are concerns about data quality, privacy, and security, as well as the need for appropriate technical skills and infrastructure [4, 5], as arXiv:2306.04338v1 [stat.ML] 7 Jun 2023 well as challenges related to explainability, accuracy, reproducibility, timeliness, and cost effectiveness [6].",
"As statistical agencies grapple with these challenges, it is essential to ensure that the benefits of machine learning are balanced against the risks and that the resulting insights are both accurate and representative.",
"In this paper, we explore the changing data sources in the age of machine learning for official statistics, as we believe that this pervasive issue largely remains underexposed, as we will explain in Section 2.2.",
"In that respect, we highlight some of the key considerations for statistical agencies looking to incorporate machine learning into their workflows in Section 3, by zooming in on the causes and risks associated with using external data sources, the consequences on using such sources for statistical production, and, finally, a set of mitigations that should ideally be incorporated in any genuine deployment of machine learning for official statistics.",
"2 Machine learning for official statistics The data abundance in governmental, corporate, social and personal contexts, both online and offline, becomes a tantalizing source and opportunity for the improvement and expansion of official statistics.",
"For example, to inquire about the overall satisfaction with life of its citizens, a nation could organize periodic surveys.",
"But when this nation has access to its citizens’ social media posts, likes, reader’s letters, media consumption, ticket sales, (online) shopping carts, etc.",
it could use all of these data as a proxy to extract novel and innovative statistical insights [7].,
"Typically, the end result is either a new statistic, a statistic that complements an existing one, or an ersatz statistic that aims to replace one or more existing statistics.",
We will briefly zoom in on the different components of which such a novel statistic is generally comprised.,
"2.1 Machine learning To derive novel insights and innovative statistics from data, data scientists and statistical researchers often use a wide variety of powerful tools that are in the realm of machine learning1.",
In this paper we will not go into too much detail about machine learning.,
"However, the use of data sources together with machine learning models can cause unwanted effects regarding statistical production, see further in Section 3.",
"So, we deem it useful to provide a high-level overview of the typical process that involves machine learning for official statistics.",
Machine learning is a subdiscipline of artificial intelligence that enables machines to learn from data and improve their performance over time without being explicitly programmed.,
"This approach involves building algorithms that automatically learn from data to identify patterns, relationships, and structures that may be difficult or impossible for humans to discern.",
The typical process of designing a machine learning algorithm consists of two main phases: training and inference.,
"During the training phase, the parameters of a machine learning model are tuned to solve a specific task.",
"For this, a wide variety of data sources can be used, or even outputs from existing or pre-trained machine learning models.",
"After the model is trained, its parameters are kept fixed so that the model can be used to predict outcomes or identify patterns in new or previously unseen data.",
This process is called inference.,
It is important to keep in mind the distinction between training and inference.,
"After training, the model remains unchanged, and it remains unchanged until it is retrained again.",
"Later, in Section 3, we will focus on the disparities that this can cause w.r.t.",
"the inference phase and, in consequence, official statistics production.",
Supervised learning is one of the most common types of machine learning used for official statistics.,
"This method involves training a model on a labeled dataset, where each data point has a known outcome or target variable.",
"The model learns to associate features in the data with the target variable, enabling it to make predictions on new data with similar features.",
"In the context of official statistics, supervised learning can for example be used to predict the happiness of an individual based on their Twitter profile [8].",
"Unsupervised learning, on the other hand, is used when the target variable is unknown or the goal is to identify patterns or relationships within the data.",
"In this approach, the machine learning model learns to recognize similarities and differences among input data without explicit guidance from labeled data.",
"In the context of official statistics, unsupervised learning can for example be used to identify citizens, companies or events that are similar to each other on one or more aspects that could be hidden from plain sight [9].",
"Machine learning can be used to complement or even replace official statistics, and its ability to nowcast and forecast is an extremely valuable addition.",
"Modern machine learning models, tools and hardware can analyze vast amounts of data in real-time or near-real-time, providing more up-to-date and precise estimates of e.g.",
economic and social trends.,
"By 1Although originally (and technically) they imply different methods and techniques, the terms machine learning, data science, artificial intelligence, deep learning... are nowadays considered interchangeable.",
In this paper we consistently use the term machine learning to denote the scientific discipline concerned with learning the most optimal model parameters based on data.,
"It is a subdiscipline of artificial intelligence, while deep learning is a subdiscipline of machine learning.",
"Data science encompasses both machine learning as well as data preparation, analytics and visualization.",
"2 incorporating machine learning into official statistical production, one can benefit from the strengths of both approaches and make more informed decisions based on the most current and accurate data [10].",
2.2 External data sources Let’s focus on the data sources that will power such machine learning models.,
"Their nature, size, structure, frequency... can be vastly different, they must typically be gathered ‘in the wild’ and should often be combined with each other to extract meaningful insights.",
"Compared to more traditional data sources for official statistics, they may present unique and appealing characteristics such as: Broad-spectrum – Covers a wide variety of topics.",
Diversity – A large variety of sources to cover different perspectives.,
Availability – Lots of data is freely and easily accessible.,
"Size – Some datasets can be enormous, sometimes even complete.",
"Structure – Not only tabular data, but also images, video, text, audio, etc.",
Timeliness – (Near) up-to-date and real-time information.,
"Frequency – Raw data on various, even very fine-grained time scales.",
"Granularity – Raw data on various, even fine-grained levels of detail.",
Coverage – Various locations and regions can be filtered and covered.,
"On the other hand, before all this data is ready to be exerted for machine learning and official statistical production, a few challenges need to be overcome, such as: Data quality – Data may contain errors, biases, or missing values that need to be addressed to ensure accuracy and reliability.",
"Data interpretation – Understanding the context and meaning of data can be difficult, especially when dealing with unstructured data such as text or images.",
Data integration – Combining data from different sources with varying structures and formats can be challenging and time-consuming.,
"Selection bias – Proper randomization or compiling representative population samples can be challenging, and it greatly depends on the underlying data origins.",
"Operationalization bias – Reproducibility can be difficult as it depends on many implicit, hidden, and/or production- specific design choices [11, 12].",
Computational resources – Processing and analyzing large amounts of data may require significant computational resources.,
Privacy and security – Sensitive data may need to be protected and anonymized to ensure privacy and security.,
Data ethics – Data collection and use should adhere to ethical principles.,
Fairness and justness – The end solution should ideally be as neutral as possible and should not discriminate [13].,
"Cost – All of the above requires resources, budgets and a talented workforce.",
"In addition, the data source itself might need to be purchased.",
"In 2016, McKinsey reported that many companies have started to specialize in acquiring and selling data [14].",
"3 With the right tools, workforce, technological advances, mindset, and legislative support, these challenges can and should be manageable.",
"The most challenging piece of the puzzle, however – and one that is more than often ignored – is the lack of control you can exert over the data sources that are externally gathered.",
"As a national statistics agency, traditionally, survey data and administrative records that power official statistics are completely under your own control.",
"But once you start exploiting external data sources to power novel, innovative, complimentary or ersatz statistics, this lack of control of your data should never be ignored, and if possible, should be front and center on your agenda early on in the process.",
"As the popular saying goes: “With great power comes great responsibility” (from Spider-Man, 2002).",
Having control and power over your data is essential to fulfilling your responsibilities as a statistics agency.,
"However, in the world of data, the opposite is often true: with great amounts of external data comes great powerlessness.",
"Therefore, it is crucial to prioritize the issue of data control when incorporating external sources into official statistics.",
Taking the time to establish proper protocols and procedures for external data management can prevent a multitude of issues down the line and ensure that the data you rely on remain accurate and trustworthy.,
"This paper delves into the pervasive problem of powerlessness and lack of control, unraveling the multifaceted aspects, risks, and pitfalls that arise from utilizing external data sources for machine learning in official statistics.",
We will explore the concepts of ‘change’ and ‘consequence’ in their most expansive interpretations to comprehensively tackle this question.,
3 The challenge of changing data sources Relying heavily on external data sources for machine learning in official statistics comes with significant risks.,
Such a dependence can leave statistical agencies vulnerable since they have limited control over these sources.,
"This situation is similar to how our global economy, mobility, and prosperity were once highly dependent on the availability of oil.",
"Since the prices and availability of these precious resources are often beyond our control, countries can do nothing but endure price fluctuations and shortages.",
"Clive Humby proclaimed in 2006 that “data is the new oil”, given its powerful intrinsic value.",
"However, his statement keeps holding true in terms of vulnerability, powerlessness, and lack of control over external providers.",
"In the following paragraphs, we will delve into the various types and causes of data changes.",
We will then discuss the ramifications of changing data sources for machine learning in official statistics.,
"Finally, we will provide a list of best practices and tips, although it is important to remember that there is no free lunch: whenever we incorporate external data, we expose ourselves to the risk of future changes in these data sources.",
3.1 Types and Causes of Changing Data Sources 3.1.1 Data types and schemas A change in data types or schemas refers to modifications made to the data formats or the structure in which the data are stored and offered.,
"These types of changes may arise due to a need to accommodate future use cases or business requirements, to eliminate technical debt, or to improve data storage and retrieval efficiency.",
Even the most innocent changes – e.g.,
"integers becoming floats, data columns that are added or removed... – can break entire pipelines.",
In the most fortunate of cases the runtime environment will throw errors that reveal the cause of these data changes.,
"In other cases, however, the data changes remain undetected and secretly wreak havoc in the pipeline.",
"If the pipeline contains machine learning components, data type changes can e.g.",
induce feature mismatches – discrepancies between the feature distributions at train and inference time – that lead to unreliable predictions.,
"It is important to be vigilant about changes in data types or schemas, as even seemingly minor adjustments can have significant impacts further down the data pipeline.",
"To mitigate these risks, it is advisable to stay informed about data change announcements from providers and implement robust data checks during data ingestion, ranging from simple data (type) validation to full-blown automated feature analysis, outlier detection, etc.",
"Additionally, the deployment of effective monitoring systems can help catch machine learning failures quickly and prevent potentially costly errors.",
"3.1.2 Sharing and collection technology Data can be shared and collected using many different technologies, such as APIs, queues, network drives, external drives, e-mail... but also web scraping, online analytics tools, sensor networks... Changes in these technologies inevitably occur from time to time.",
"For example, API endpoints often need to be updated to improve functionality and performance.",
"Changes may be made to the API’s data structures or methods, to provide more efficient or comprehensive data access.",
"In addition, changes may be made to the API to address security vulnerabilities or to ensure compliance with new 4 regulations or standards.",
"Furthermore, changes in business requirements or strategy may also lead to changes in API endpoints.",
"For instance, a company may introduce new products or services, modify their existing offerings, or change their pricing.",
"A recent, telling example is the Twitter API.",
"In 2021, Twitter launched version 2 of its popular API that introduced many changes in endpoints, data fields, pricing... compared to version 1.1.",
"Twitter encouraged developers to migrate to this new API offering, but for many use cases such a migration would introduce breaking changes that, in their turn, would impact entire data processing pipelines, statistics production, etc.",
"For the time being, Twitter offered both version 1.1 and version 2 of their API in parallel, which caused many to bury their heads in the sand.",
The situation got even worse when Elon Musk acquired Twitter in 2022 and decided to suspend all existing API offerings.,
"Instead, in 2023 a new enterprise tier was introduced that put a price of more than 40 thousand USD per month on any reasonably effective use of the API.",
This caused great dissatisfaction in the development and research community and many initiatives were abandoned.,
"3.1.3 Concept drift Concept drift is related to changes in the data distribution between train and test time, which can have multiple causes [15, 16].",
"Changes in business logic can induce information shifts, for example, when categorical variables are expanded with additional categories or when the meaning of certain data fields is altered.",
"A particular pervasive issue is the calculation of derived data fields, especially when those calculations are not transparent or proprietary.",
"In the age of machine learning, you should always assume that derived data fields can be the result of a model prediction; when this model is updated without your knowing, the derived data fields will have a (slightly) different data distribution, which will cause issues in dependent machine learning models.",
"But even when data fields are not the result of a model’s prediction, it is important to periodically reevaluate and retrain models, since many sociological and economic processes are naturally prone to concept drift themselves.",
"3.1.4 Frequency and interruptions A change in data frequency refers to modifications made to the rate at which data is collected or updated, which can happen deliberately or randomly.",
Deliberate changes may arise due to shifts in business requirements or technological choices.,
"Random shifts are most often attributed to noisy factors such as network issues, component failures, down- time... or (human) errors.",
Such changes in frequency can impact machine learning components dramatically.,
"when periodical data is sampled every minute instead of every second, the data distribution changes on which the model was trained.",
"To mitigate these risks, data pipelines should be designed to monitor changes in incoming data frequency.",
"3.1.5 Ownership and discontinuation Worse than interruptions is downright discontinuation of the data source, which has immediate consequences on the future existence of the statistic.",
"Also, a change of ownership of the data source – e.g.",
"when acquired by another company – is not a fictional scenario, and it can trigger any of the risks that are discussed in this section.",
Building redundancy by diversifying data sources is a useful mitigating strategy to avoid single points of failures.,
"3.1.6 Legal properties Legal changes refer to modifications made to the legal landscape that governs the collection, storage, and use of data.",
"This type of change may arise due to new privacy laws, contractual obligations, or changes in the cost of data access or storage.",
"One cause of this type of change is the adoption of new regulations, such as GDPR, which require companies to comply with stricter rules for collecting and processing data.",
"Additionally, changes in the cost of data access or storage may require companies to modify their data sources or methods to reduce costs, which can have contractual consequences.",
"If possible, negotiate airtight SLAs with the data provider and make sure to attribute enough attention to future data changes.",
3.1.7 Ethics and public perception Ethical considerations and public perceptions can affect data collection methods and sources.,
"If certain data sources or variables are considered controversial or intrusive, there may be a shift towards alternative sources, which may require a refresh of the used machine learning models.",
It can also impact the way machine learning models are designed and trained.,
"If certain variables or factors are considered discriminatory or unethical, there may be a push towards eliminating or adjusting them to reduce algorithmic bias.",
Changes in ethics or public perceptions can also result in greater accountability and the need for transparency.,
"Stakeholders may demand more openness and clarity around the use of algorithms, data sources, and decision-making processes.",
"This can lead to greater scrutiny and oversight of 5 machine learning models, which may impact their performance if not adequately addressed, especially when black-box models need to be replaced by more interpretative variants [17, 18, 19].",
"Finally, public trust can be significantly affected.",
"If stakeholders perceive that machine learning is being used inappropriately or unethically, they may lose faith in the integrity and reliability of official statistics.",
This can have significant consequences for public policy and decision making.,
"3.2 Consequences of Changing Data Sources When data sources change, there will be consequences for official statistics production, especially if there are machine learning components involved.",
"We will broadly but briefly cover a variety of areas that can be impacted, some of which have already been mentioned above.",
"Concept drift – Concept drift means that the underlying patterns and relationships in the data may change over time, which can lead to model deterioration or loss of accuracy.",
"This issue can be particularly relevant when dealing with long-term trends, as changes in societal norms, technology, or other external factors can influence data over time [16].",
"Model staleness – When a model becomes outdated, it no longer reflects current trends or patterns in the data.",
This can occur if the machine learning model is not updated frequently enough to keep pace with changing data sources.,
"As a result, the model may not perform as well as it once did, leading to less accurate official statistics.",
"Bias and neutrality – Changing data sources can also introduce bias or incorrect data, which can impact the neutrality of the statistics produced, or which can lead to the phenomenon “garbage in, garbage out” [20, 21].",
"Since it is essential that official statistics remain neutral and objective, this will negatively impact the accuracy and validity of these statistics.",
"Availability – If data become unavailable (for a certain period in time or indefinitely) or are limited in scope, this may impact the ability to produce accurate and timely official statistics.",
Integration – A change in data sources can cause a domino effect when multiple statistics or models rely on this data source.,
"Especially be mindful when the output of machine learning models is used as input for other machine learning models, either directly or indirectly as part of a larger data pipeline.",
"Since the predictions of a machine learning model can become unreliable when the input data change, this prediction shift itself is a changing data source for other models.",
"Extra labor – The risk of changing data sources requires additional resources and labor to mitigate the effects of such changes, monitor the occurrence of changes, and ensure that the new data are properly integrated into existing machine learning models.",
"This has tremendous impacts on the costs and timeline of the produced statistics, and it may also require a significant team expansion.",
"Breaking changes or discontinuation – In some cases, changing data sources may cause the impossibility of producing a statistic any further.",
"If this is the case, it may be necessary to stop offering the statistic altogether or find alternative data sources that can produce accurate and reliable official statistics.",
"When alternative data sources are found, there will almost always be a mismatch with the original data source that has an impact on the resulting statistic, resulting in a breaking change.",
"In that case, it is important to overcome the mismatches as best as possible – e.g.",
"in terms of statistical properties – and, certainly, to be transparent about the breaking change, e.g.",
by indicating on a graph when exactly the data source was changed.,
"Quality metrics – Finally, changing data sources imply changes in timeliness, validity, accuracy, completeness, consistency and other quality metrics w.r.t.",
the produced official statistics [6].,
Ensuring that resulting statistic continues to meet these quality metrics remains critical.,
"3.3 Mitigating Changing Data Sources As has been illustrated above, changes in data sources can significantly impact the performance of a machine learning model.",
"The effects can be diverse, ranging from introducing biases in the data to producing incorrect results.",
"This can have serious implications, especially when the model is being used for official statistics, where accuracy and reliability are of paramount importance.",
"Therefore, it is essential to take measures to prevent and mitigate such changes.",
"This is not an easy task, as the consequences can be diverse, and the required efforts to mitigate them are often time-consuming and not straightforward.",
We do not claim to have definite answers.,
"However, we will propose several recommendations and best practices, including performing a risk analysis, monitoring, diversifying data sources, building technical robustness, using data normalization techniques, and incorporating data validation processes.",
6 Risk analysis – Performing a risk analysis before incorporating a new data source is an essential step in mitigating the impact of changes in data sources.,
"This analysis involves identifying the potential risks associated with the data source, which we have covered in Section 3.1.",
"The analysis should be comprehensive, considering both technical and non-technical aspects of the data source, and should ideally include potential solutions for the identified risks.",
This will often force you to face the hard truth and will lead you to decide that the candidate data sources are not adequate or reliable enough.,
"Trade-offs will nevertheless need to be considered, depending on the use case at hand.",
Monitoring – Monitoring everything that is relevant is another crucial step in mitigating the impact of changing data sources.,
"It involves tracking various aspects of the data sources, the machine learning models, and their outputs to detect and respond to changes promptly.",
Draft a list of variables and quantities that must be continuously tracked to ensure that the models remain reliable and accurate over time.,
"For this, inspiration can be drawn from the discussed topics in Section 3.1, but it will vary from use case to use case, as well as the nature of the models that have been used.",
"Supervised models, for example, can be tested against a reference test set or a historical reference model; if the accuracy, precision or recall starts to deviate significantly from this reference set, it should be flagged.",
"On the other hand, monitoring the performance of unsupervised models can be more challenging, because there is no clear performance measure that can be directly computed.",
One approach is to monitor the model’s ability to detect patterns and clusters in the data.,
"It is possible to use a reference test set or reference model for this, but the informative metrics – e.g.",
"cluster similarity, homogeneity, separation... – are more abstract and somewhat harder to interpret.",
"Another approach is to visualize projections of certain interesting data points in the learned latent spaces or preferably a reduction thereof, which greatly benefits interpretability but makes it harder to convert it into hard numbers.",
"As a suggestion, a good balance between interpretability and hard performance metrics is found when clusters are tested against pre-existing domain knowledge, e.g.",
by listing similar data points for given queries.,
Simply monitoring whether expected similarities emerge or not can provide powerful signals about model and data performance.,
Another effective approach is to create proxy supervised tasks that rely on the output of the unsupervised model.,
Monitoring the model’s performance on such proxy tasks can provide insights into the quality and usefulness of the unsupervised model’s output.,
"Diversification – Diversifying data sources is another important measure, but is easier said than done.",
One challenge of using multiple data sources is the potential for conflicts or inconsistencies between the sources.,
"Different data sources may have different formats, schemas, and levels of quality, which can create discrepancies and inconsistencies that must be resolved before the data can be used in the model.",
"Therefore, data normalization is key.",
"Additionally, integrating multiple data sources can be a complex and time-consuming process.",
"It can also create additional computational overhead, which may impact the model’s scalability and portability.",
"Finally, finding relevant and reliable data sources can be a challenging task, particularly for specialized or niche domains.",
It may require extensive research and communication with data providers to retrieve relevant data.,
"Again, this story is about economical, technical and practical trade-offs, and is of course highly use-case-dependent.",
Technical robustness – Building technical robustness is paramount and requires significant engineering efforts.,
"Building an automated, data-driven statistic that is resistant to changing data sources such as errors, outliers, outages, time-dependent variability, etc.",
ensures consistency in the statistical offering.,
"Using data normalization techniques and incorporating data validation into the pipeline are essential measures, but robust technical implementations also require thorough unit and integration testing, failover and deduplication, scalability solutions, security measures, etc.",
"Of course, this is an entire field of study on its own.",
"Legal robustness – Finally, we believe that agreeing on clear legal guidelines is the best mitigation strategy to counter the risk of changing data sources, for example, by closing formal data sharing agreements or SLAa with data providers.",
"Such agreements should specify the terms and conditions under which the data can be shared, as well as the legal responsibilities of each party.",
"In particular, the agreements should specify the legal consequences of non-compliance.",
4 Conclusion In this paper we have investigated the risks and consequences of changing data sources when using machine learning for official statistics.,
"The list is long and covers many different aspects, ranging from statistical issues and model inconsistencies to technical problems and ethical considerations.",
We have also looked at a few potential mitigation strategies.,
"However, we admit that these strategies do not provide all the adequate answers and might leave the reader unsatisfied or, worse still, beguiled, as the solutions require many additional resources and efforts.",
"As we have stressed a couple of times in this paper, this is a story of trade-offs.",
"Depending on the use case at hand, some trade-offs might be easier to handle than other ones.",
"However, in the context of official statistics, our advice is to not tread lightly on these matters and to minimize the risk of losing control over your data sources as much as possible.",
"This takes time, effort 7 and careful planning with a horizon of multiple years.",
"To end on a positive note, despite the challenges associated with changing data sources, machine learning offers many opportunities for official statistics.",
"By being aware of the risks and taking necessary precautions, statistical agencies can leverage these opportunities while maintaining the integrity and reliability of their data-driven products.",
We hope that our checklist of risks and mitigation strategies provides a useful starting point for statistical agencies and practitioners to ensure the robustness of their machine learning-based statistical reporting.,
References [1] Stuart J. Russell and Peter Norvig.,
Artificial Intelligence.,
"Pearson Education, 2009.",
"[2] Trevor Hastie, Jerome Friedman, and Robert Tisbshirani.",
The elements of Statistical Learning.,
"Springer, 2017.",
Machine learning for official statistics.,
"Technical report, UNECE, 2022.",
"[4] Hossein Hassani, Gilbert Saporta, and Emmanuel Sirimal Silva.",
"Data mining and official statistics: The past, the present and the future.",
"Big Data, 2(1):34–43, March 2014.",
[5] Marco Puts and Piet Daas.,
Machine learning from the perspective of official statistic.,
"The Survey Statistician, 84:12–17, July 2021.",
"[6] Wesley Yung, Siu-Ming Tam, Bart Buelens, Hugh Chipman, Florian Dumpert, Gabriele Ascari, Fabiana Rocci, Joep Burger, and InKyung Choi.",
A quality framework for statistical algorithms.,
"Statistical Journal of the IAOS, 38(1):291–308, March 2022.",
[7] Wesley Yung.,
The Evolution of Official Statistics in a Changing World.,
"Harvard Data Science Review, 3(4), oct 28 2021.",
"[8] Manon Reusens, Michael Reusens, Marc Callens, Bart Baesens, et al.",
Benchmark study for flemish twitter sentiment analysis.,
"Social Science Research Network, 2022.",
"[9] Annelien Crijns, Victor Vanhullebusch, Manon Reusens, Michael Reusens, and Bart Baesens.",
Topic modelling applied on innovation studies of flemish companies.,
"Journal of Business Analytics, pages 1–12, 2023.",
"[10] Sevgui Erman, Eric Rancourt, Yanick Beaucage, and Andre Loranger.",
The Use of Data Science in a National Statistical Office.,
"Harvard Data Science Review, 4(4), oct 27 2022.",
"[11] Matthias Haucke, Rink Hoekstra, and Don van Ravenzwaaij.",
When numbers fail: do researchers agree on operationalization of published research?,
"Royal Society Open Science, 8(9):191354, September 2021.",
[12] Nagireddy Neelakanteswar Reddy.,
Operationalization bias: A suboptimal research practice in psychology.,
December 2022.,
"[13] Matthias Kuppler, Christoph Kern, Ruben L. Bach, and Frauke Kreuter.",
From fair predictions to just decisions?,
conceptualizing algorithmic fairness and distributive justice in the context of data-driven decision-making.,
"Frontiers in Sociology, 2022.",
"[14] Nicolaus Henke, Jacques Bughin, Michael Chui, James Manyika, Tamim Saleh, Bill Wiseman, and Guru Sethupathy.",
The age of analytics: competing in a data-driven world.,
"Technical report, McKinsey & Company, 2016.",
"[15] Hanqing Hu, Mehmed Kantardzic, and Tegjyot S Sethi.",
No free lunch theorem for concept drift detection in streaming data classification: A review.,
"Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 10(2):e1327, 2020.",
"[16] Firas Bayram, Bestoun S. Ahmed, and Andreas Kassler.",
From concept drift to model degradation: An overview on performance-aware drift detectors.,
"Knowledge-Based Systems, 245:108632, June 2022.",
"[17] Alicja Gosiewska, Anna Kozak, and Przemysław Biecek.",
Simpler is better: Lifting interpretability-performance trade-off via automated feature engineering.,
"Decision Support Systems, 150:113556, November 2021.",
[18] Cynthia Rudin.,
Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.,
"Nature Machine Intelligence, 1(5):206–215, May 2019.",
[19] Alex John London.,
Artificial intelligence and black-box medical decisions: Accuracy versus explainability.,
"Hastings Center Report, 49(1):15–21, January 2019.",
[20] Bertie Vidgen and Leon Derczynski.,
"Directions in abusive language training data, a systematic review: Garbage in, garbage out.",
"PLOS ONE, 15(12):e0243300, December 2020.",
"[21] R. Stuart Geiger, Dominique Cope, Jamie Ip, Marsha Lotosh, Aayush Shah, Jenny Weng, and Rebekah Tang.",
"“garbage in, garbage out” revisited: What do machine learning application papers report about human-labeled training data?",
"Quantitative Science Studies, 2(3):795–827, 2021.",
"Screening Mammography Breast Cancer Detection Debajyoti Chakraborty Northeastern University 360 Huntington Ave, Boston, MA 02115 chakraborty.de@northeastern.edu Abstract Breast cancer is a leading cause of cancer-related deaths, but current programs are expensive and prone to false positives, leading to unnecessary follow-up and pa- tient anxiety.",
"This paper proposes a solution to automated breast cancer detection, to improve the efficiency and accu- racy of screening programs.",
"Different methodologies were tested against the RSNA dataset of radiographic breast im- ages of roughly 20,000 female patients and yielded an av- erage validation case pF1 score of 0.56 across methods.",
"Introduction Breast cancer is one of the most commonly occurring cancer in the world with 2.3 million new breasts cancer di- agnoses and 685,000 deaths in 2020 alone [6].",
"Although the mortality rate in developed nations have dropped by 40% over the last 40 years due to regular mammography screen- ing programs, such in not the case in many other countries due a looming shortage of radiologists.",
"As with any cancer or disease in general, early detec- tion and treatment is critical to reducing complications and fatalities.",
"However, currently such procedures require the expertise of highly-trained human observers, primarily ra- diologists, making the overall process expensive to conduct and prone to human error, worsening the problem.",
Problem Statement A major problem in mammography screening is that it often leads to a high incidence of false positive results.,
"This is usually followed by further screening tests, inconvenient follow-up, and sometimes, unneeded tissue sampling (nee- dle biopsy) which may lead to further unrelated complica- tions, causing unnecessary anxiety.",
"This paper aims to improve the automatic detection of breast cancer in screening mammograms obtained from reg- ular screening programs, with the goal being to reduce the occurrences of false positives in a clinical setting.",
Dataset [This dataset contains radiographic breast images of female subjects.],
The dataset [2] has been generously provided by the Ra- diological Society of North America (RSNA).,
RSNA is a non-profit organization that represents 31 radio-logic sub- specialties from 145 countries around the world.,
"It contains radiographic breast images of roughly 20,000 female patients with usually four images per patient with two lateral [left, right] images per view [mediolateral- oblique (MLO), crainal-caudal (CC)].",
"Metadata for each patient and image site id ID code for the source hospital machine id ID code for the imaging device patient id ID code for the patient image id ID code for the respective image laterality whether the image is of the left or right breast view orientation of the image age patient’s age in years implant whether the patient had breast im- plants at the patient level density rating for how dense the breast tis- sue is, with A being the least dense and D being the most dense biopsy whether a follow-up biopsy was performed on the breast invasive whether or not the cancer (if true) proved to be invasive BIRADS 0 if the breast required follow-up, 1 if the breast was rated as nega- tive for cancer, and 2 if the breast was rated as normal difficult negative case true if the case was unusually dif- ficult to diagnose cancer whether or not the breast was pos- itive for malignant cancer arXiv:2307.11274v1 [eess.IV] 21 Jul 2023 Figure 1.",
Example MLO and CC view of left(L) and right(R) breasts for patient id 32254.,
"The raw dataset contains around 54,700 mammogra- phy images in the Digital Imaging and Communications in Medicine (DICOM) [4] format.",
"There is a significant class imbalance in the target variable cancer, with 1,158 data-points for the positive class and 53,548 data-points for the negative class.",
"For the purpose of this paper, image id serves as the input data, cancer as the binary target, along with only age and implant information as addi- tional metadata.",
The rest of the metadata will be included in a future implementation for better inference.,
Approach 3.1.,
Image encoding Majority of the work was involved in pre-processing the DICOM images and converting them into png files for eas- ier post-processing and training.,
Most of the dicoms in the dataset contained JPEG2000 encoded images.,
"The bit- stream was extracted and decoded on the GPU, saved as png files.",
"Although this process was not lossless, it saved a lot of overhead for processing them individually, as they were quite high-dimensional.",
Pre-processing Different pre-processing techniques were tried to im- prove the chances of accurate classification.,
Two of the pri- mary techniques were Region of Interest ROI cropping and normalization.,
"However, upon further research, it was only decided to use normalization and drop cropping, due to the variability in image dimensions and loss of information by resizing.",
"Photometric interpretation, an attribute that speci- fies the intended interpretation of the raw pixel data were ei- ther MONOCHROME1 or MONOCHROME2.",
MONOCHROME1 is usually used when the mammogram is intended to be viewed in a white background and MONOCHROME2 is usu- ally used when the mammogram is intended to be viewed in a black background.,
"The author chose to invert all MONOCHROME1 images and keep all MONOCHROME2 im- ages intact, as they display more lesion-based information when viewed by a machine.",
"All original image arrays were normalized to between 0 and 1 for uniformity, and resized into 512 × 512 gray-scale images for consistency.",
(left) Random sample of a MONOCHROME1 image show- ing the minimum pixel value and (right) the same image after in- verting and normalizing all pixel values to between 0 and 1.,
"Feature extraction An EfficientNetV2 [8] with pre-trained weights was used to extract essential features from the images, en- coding them to a 1000-dimensional feature vector.",
"It was developed by researchers at Google [3] that achieved im- proved accuracy and efficiency than conventional Convolu- tional Neural Networks (CNNs), lowering parameter size and execution time by an order of magnitude.",
An modification of the dataset was later prepared by combining the exported image feature vectors with two meta features: normalized age and implant information.,
"For the purpose of improving predictions, a combination of synthetic under-sampling and over-sampling techniques were also tested for performance.",
Classification methods These popular machine learning techniques for classifi- cation were trained on this data to derive inference.,
3.4.1 Logistic Regression (LR) A simple LR model with L2 regularization was trained on the unbalanced dataset with individually assigned class weights and has been described below.,
"Given Xi as the input feature vector and yi ∈{0, 1} as the target variable for data point i, the probability of the positive class yi = 1 was predicted by the fitted model as: ˆp(Xi) = P(yi = 1|x) = 1 1 + exp(−Xiw −w0) (1) The cost function that was minimized using a solver was: min w C Nn X i=1 (−yilog(ˆp(Xi))− (1 −yi) log(1 −ˆp(Xi))) + 1 2∥x∥2 2 (2) 3.4.2 Support Vector Machine (SVM) For implementing a SVM model, a scalable, input data independent polynomial kernel approximation method [7] was used.",
"Given x, y as input features and d as the polyno- mial kernel degree, a simple kernel function used was: k(x, z) = (γx⊤z + c0)d (3) Next, given that Xi ∈Rd and yi ∈{−1, 1} as the target variable for data point i, the following problem was solved:      minw,b,ξ 1 2∥w∥+ C Pn i=1 ξi such that yi(w⊤k(x, z) + b) ≥1 −ξi ξi ≥0, ∀ i = 1, · · · , n (4) Equivalently formulated, it was written as: min w,b 1 2∥w∥+ C n X i=1 max(0, 1 −yi(w⊤k(x, z) + b)) (5) 3.4.3 Breast-level single-view-single-laterality model A simple deep-neural network was designed to perform in- ference on the ensemble of features described above.",
• Input: Each 1000×1 vector was fed into an image en- coder part of the network and after two hidden dense layers was concatenated with the feature outputs from a metadata encoder.,
"• Activation: Each intermediate layer had a ReLU ac- tivation function, except for the output layer, where a Sigmoid activation was used.",
"ReLU: f(z) = max(0, z) ∈[0, z] (6) Sigmoid: f(z) = 1 1 + exp(−z) ∈[0, 1] (7) • Loss: The loss with respect to the target variable was calculated using Binary Cross-entropy (BCE) loss, once while using categorical class weights and another without.",
"−1 N N X i=0 yi log( ˆyi) + (1 −yi) log(1 −ˆyi) (8) • Class imbalance: In the latter case, a Straified Batch Sampling (SBS) methodology was used when ran- domly sampling from the dataset during training.",
SBS = Total sample size Dataset population × Class population (9) • Optimizer: Adam [5] was chosen as the optimizer for our minimization problem.,
"Given η = 0.0003 as ini- tial learning rate, gt as gradient at time t along wj, νt as exponential average of gradients along wj, st as ex- ponential average of squares of gradients along wj and β1, β2 as hyper-parameters: νt = β1νt−1 −(1 −β1)gt (10) st = β2st−1 −(1 −βs)g2 t (11) ∂wt = −η νt √st + ϵgt (12) wt+1 = wt + ∂wt (13) • Metrics: A range of different metrics were tried to un- derstand the maximum efficiency of the model.",
"As for a problem with a class imbalance, accuracy was unreliable because of the major bias towards negative class.",
"In this regard, performances of binary area under the receiver operating characteristic curve (AUROC), binary precision, binary recall, binary F1 score were compared.",
A comprehensive comparison of these met- rics have been provided in Figure 3.,
The model was finally evaluated using the probabilis- tic F1 score (pF1) [9] as this extension accepts proba- bilities instead of binary classifications.,
With px as the probabilistic version of X: pF1 = 2 PprecisionPrecall Pprecision + Precall (14) Pprecision = Ptrue positive Ptrue positive + Pfalse positive (15) Precall = Ptrue positive true positive + false negative (16) Table 2.,
"Simple Single-view-single-laterality model architecture Layer Input Output Parameters SimpleFCN [,1002] [,1] – Sequential: 1-1 [,1000] [,10] – Linear: 2-1 [,1000] [,100] 100, 100 ReLU: 2-2 [,100] [,100] – Linear: 2-3 [,100] [,10] 1, 010 ReLU: 2-4 [,10] [,10] – Linear: 2-3 [,10] [,1] 11 Sigmoid: 2-4 [,1] [,1] – Sequential: 1-2 [,2] [,1] – Linear: 2-5 [,2] [,2] 6 ReLU: 2-6 [,2] [,2] – Linear: 2-7 [,2] [,1] 3 Sigmoid: 2-8 [,1] [,1] – Concatenate: 1-3 [,2] [,1] – Sigmoid: 1-4 [,1] [,1] – The total parameters in the model was estimated at 101, 130, all of which were trainable.",
"The total model size excluding the feature extractor was estimated at 20.56 MB, with input size at 16.42 MB, forward/backward pass size at 3.74 MB and all parameter size at 0.40 MB.",
Comparison of different evaluation metrics.,
Results An extensive array of experiments were carried out to estimate the best machine learning model suited for this dataset.,
"This not only included varying hyper-parameters and trying out different models, but also making efforts in transforming, augmenting and generating synthetic data points for the imbalanced classes.",
Validation loss graph trained for 1000 iterations with breast-level single-view-single-laterality DNN model.,
"The results for pF1 score were compared as well in or- der to have a more holistic view at all the approaches, and served well to gauge different model performances com- pared to each other and state of the art at present.",
"This was due to lack to good test data, lack of sufficient positive classes, and personally, lack of time and resources.",
Probabilistic F-1 scores Model pF1 score State-of-the-art 0.630 Logistic Regression 0.627 Support Vector Machine 0.572 Random Forest Classifier 0.626 Complement Naive Bayes 0.579 Deep Neural Network 0.481 5.,
"Summary As it can be inferred from the above table, the current state of any of the models is not better than 0.50 probability, that is a 50% chance of providing the correct class predic- tion, which is not any better than random chance.",
Compar- ing the results to the current state-of-the-art shows that the topic needs more correct predictions to be relevant.,
"Other investigations that did not produce significant results include Region-of-interest (ROI) cropping, down- sampling, up-sampling or synthetically generating new samples for the dataset, and almost certainly leading to over-fitting.",
"The rest of the metadata was not part of future test samples, so they were not included as well.",
Literature survey shows that the use of intense data aug- mentation pipelines and training models externally on sim- ilar datasets work well.,
This is mainly done to lessen the possibility of detection of false positives due to exposure to more data points in the positive class.,
"There is also strong evidence that networks that capture both spatial and tempo- ral information for a single patient, in a multi-view-multi- lateral model show drastically improved performance.",
"The recent use of transformers for vision tasks has shown signif- icant promise as well, but that calls for further investigation and is also outside the scope of this paper.",
"References [1] Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski.",
A systematic study of the class imbalance problem in convolu- tional neural networks.,
"Neural Networks, 106:249–259, oct 2018.",
"2 [2] George Partridge inversion Jayashree Kalpathy-Cramer John Mongan Katherine Andriole Lavender Maryam Vazirabad Michelle Riopel Robyn Ball Sohier Dane Yan Chen Chris Carr, FelipeKitamura.",
"Rsna screening mammography breast cancer detection, 2022.",
"1 [3] Mingxing Tan [Staff Software Engineer and Google AI] Quoc V. Le, Principal Scientist.",
"EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling, 5 2019.",
2 [4] Llc Innolitics.,
Digital Mammography X-Ray Image CIOD.,
2 [5] Diederik P. Kingma and Jimmy Ba.,
"Adam: A method for stochastic optimization, 2017.",
3 [6] World Health Organization.,
World health organization: Who.,
"breast cancer., 2021. https://www.who.int/news- room/fact-sheets/detail/breast-cancer.",
1 [7] Ninh Pham and Rasmus Pagh.,
Fast and scalable polynomial kernels via explicit feature maps.,
"In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Dis- covery and Data Mining, KDD ’13, page 239–247, New York, NY, USA, 2013.",
Association for Computing Machinery.,
3 [8] Mingxing Tan and Quoc V. Le.,
"Efficientnetv2: Smaller mod- els and faster training, 2021.",
2 [9] Reda Yacouby and Dustin Axman.,
"Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification models.",
"In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 79– 91, Online, Nov. 2020.",
Association for Computational Lin- guistics.,
"International Journal of Intelligent Systems and Applications in Engineering Advanced Technology and Science ISSN:2147-67992147-6799 www.atscience.org/IJISAE Original Research Paper International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 185 Novel approach to locate region of interest in mammograms for Breast cancer BV Divyashree1, Amarnath R1, Naveen M1, G Hemantha Kumar*1 Accepted : 12/07/2018 Published: 29/09/2018 DOI: 10.1039/b000000x Abstract: Locating region of interest for breast cancer masses in the mammographic image is a challenging problem in medical image processing.",
"In this research work, the keen idea is to efficiently extract suspected mass region for further examination.",
In particular to this fact breast boundary segmentation on sliced rgb image using modified intensity based approach followed by quad tree based division to spot out suspicious area are proposed in the paper.,
To evaluate the performance DDSM standard dataset are experimented and achieved acceptable accuracy.,
"Keywords: ROI, breast cancer, mammographic images, segmentation, entropy, quad tree 1.",
Introduction Potentially fatal disease among women who has crossed 40 years is none other than breast cancer.,
The cells in the breast rampant their growth and form abnormal shapes of breast tissues leading to cancer .The cause for the disease is not known exactly.,
The disease has killed many of the women’s life when they are not diagnosed in the early stage.,
"Hence, early detection is the key to reduce the death rates from breast cancer and to increase the life span of a patient.",
"Clinically advised successful tool for the early detection that discloses the cancer tissues in the breast up to two years before a patient or a physician can feel or see some symptoms in the breast is mammography [1, 2].",
"American cancer society estimates that approximately 252,710 women and 2,470 men are diagnosed as new cases of breast cancer.",
"Also tells that among new cases the mortality rate is about 40,610 women and 460 men [3].",
"Masses, calcifications, architectural distortion and bilateral asymmetry observed are considered as abnormalities in the mammographic images.",
"Also masses occur in different shapes like round, oval, speculated, nodular lobulated and stellate.",
"However in the present work we are concentrating on finding the suspicious region for obscured masses which is very important action to root out breast cancer in the early stage [4,5].",
"Though mammograms are the efficient tool for early detection, there are many challenges in detecting mammographic lesions because all masses may not be cancer.",
"The closely compacted tissues can hide some of the cancerous masses also, both looks white and in contrast, fatty tissue looks almost black on black background.",
"Thus, pinpointing the region for the detection of cancer in the early stage to aid the radiologists survived as still hot task in automation.",
In the proposed method we have developed an efficient model to overcome the difficulties discussed.,
"In the proposed work firstly, RGB color bands are segregated separately, and layered each of them as multiple segments based on threshold.",
"Secondly, each segment is analyzed for understanding the foreground (breast region) and background (background) region using masking technique.",
"Thirdly, we performed intersection operation on layers of all channel.",
Then entropy is calculated on the segmented region using quad tree division to find the region of interest.,
Background Many researchers had proposed several techniques for the automation of breast cancer detection.,
"Though it emerged as a challenging issue because the cancerous mass are subtle, infiltrative, looks almost like a normal tissue inside the dense scattered breast tissues and which is a place where radiologist’s attention is needed.",
"In the year 2006, Kolahdoorzan et al [8] proposed a breast pectoral muscle segmentation on digital mammograms using dyadic wavelet transform and by approximate bordering the breast.",
An approach was done by Houjinchen et al [9] to remove background from foreground by minimum cross entropy threshold method and to segment breast pectoral muscle using spectral clustering method.,
"Another approach by Sreedevi and Sherly [10] proposed to remove breast pectoral muscle using global threshold, gray level threshold and canny edge detection along with a DCT based non local mean filter technique for preprocessing.",
"In 2014, an attempt was made by Pereira et al [11] for segmentation and detection wherein they have used morphological operation followed by Ostu’s thresholding method for segmentation and genetic algorithm for detection.",
"Anuradha et al [12] proposed for segmentation, ROI extraction, watershed methods were proposed, which is compared with graph based saliency map with optimal threshold and regional maxima of saliency for accuracy.",
The saliency thresholding method attempted in ROI segmentation by not removing the pectoral muscle but the accuracy varies for chosen threshold on different images and also the morphological method depends on the structuring element.,
"In 2015, Chun-Chu Jen and Shyr-Shen Yu [1] introduced a method for the detection of abnormal parts in mammograms where in segmentation is done using gray level quantization.",
"Then, in the year 2016, Khalid El Fahssi et al [5] proposed a novel approach to classify abnormalities in the mammogram images.",
"They have used shrink wrap function for preprocessing and for segmentation they _______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 1 University of Mysore, Department of studies in Computer Science , Manasagangotri, Mysore - 570006, India * Corresponding Author Email: hemanthakumar@uni-mysore.ac.in International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 186 have used combined approaches like level set theory based method and active contours.",
"Combined top-hat and region growing methods for segmentation, classification by artificial neural networks were used by Karthikeyan et al [7].",
"Then algorithm for segmentation, based on intensity value, which are discontinuous, are also proposed by Jasmeen Kaur and MandeepKaur [13].",
"In 2017 by Luis Antonio Salazar-Licea et al [15] proposed a method for locating ROI using combined Shi-Tomasi corner detection, image thresholding and SIFT descriptors to improve the accuracy rates.",
"However the literature presented provides some solution for the detection of breast cancer, yet it is still a challenging problem because the detection completely dependent on locating doubtful regions without missing any information.",
"Segmenting foreground accurately with a single threshold value for different images is a challenging task, as it demands different threshold value for different images.",
Our motivation is that to provide an absolute technique for both (segmentation and ROI).,
"Proposed Algorithm It is important not to lose any information from mammographic images, which may lead to misclassification of the malignant cases.",
"In the proposed section, we introduced a new method for segmentation as input image.",
In the first stage we perform breast segmentation which are discussed in detail in coming section.,
"Segmentation For segmenting, the mammographic images are taken as input.",
"The image is split separately into red channel, green channel and blue channel.",
Then the separated channels are again sliced into five layers by fixing a threshold (Fig 1).,
This can be computed by the formula given Eq.1.,
(1) First and the fifth layers of each channels does not provide required information (Fig.,
"So, rest of the layers of all the channels are used for segmentation purpose.",
The considered layers of the channels are again chopped up into number of blocks.,
The pixels in the blocks are separated into foreground and background regions based on the threshold value using masking operation.,
"Block diagram of segmentation (a) Original image (b) Red channel and layers 1, 2, 3, 4, 5 redchannel layer 1 layer 2 layer 3 layer 4 layer 5 International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 187 (c) Green channel and layers 1, 2, 3, 4, 5 (d) Blue channel and layers 1, 2, 3, 4, 5 Fig.",
Channel and Layers Fig.,
3. shows the segmented images of each channel layers wherein thresholding method made all pixels black and white considering threshold value.,
(a) Segmented red channel layers (b) Segmented green channel layers (c) Segmented blue channel layers Fig.,
Segmented channel and layers Algorithm for separating foreground from background 1.,
Input image.,
"Take out red channel, green channel and blue channel.",
Take red channel.,
Initialize the size of the channel to height and width of the channel.,
"For j=1 and for i=1, find If channel(j,i)>=0 and channel(j,i)<=50, Then layer1=channel(j,i) else layer1=0.",
Repeat step5 till j=height and i=width.,
Endof the loop.,
"For j=1 and for i=1, find If channel(j,i)>50, Then layer2=channel(j,i) else layer2=0.",
Repeat step8 till j=height and i=width.,
End of the loop.,
"For j=1and for i=1, find If channel(j,i)>100, Then layer3=channel(j,i) else layer3=0.",
Repeat step11 till j=height and i=width.,
End of the loop.,
"For j=1and for i=1, find If channel(j,i)>150, Then layer4=channel(j,i) else layer4=0.",
Repeat step14 till j=height and i=width.,
End of the loop.,
"For j=1and for i=1, find If channel(j,i)>200, Then layer5=channel(j,i) else layer5=0.",
End of the loop.,
Repeat the same for green channel and blue channel.,
Initialize block size.,
Then set the threshold.,
Initialize the size of the layer to height and width.,
Assign floor width of a block to number of widths of a block and floor height of a block to number of heights of a block.,
"For j=1 to number of height and for i=1 to number of width, initialize count to zero.",
"For k=start height of a block and for l=start of width of a block, if layer(k,l)==0 then count=count+1.",
Repeat the step25 till k=end of the start height of a block and l=end of start width of a block.,
End of the loop.,
"If count>thresh then layer(k,l)=0 else layer(k,l)=255.",
End of if statement.,
Initialize starting width =end of the start width +1 and end of start width=end of start width+blocks.,
"Then, if end of the start width >width then end of the start width=width 31.",
Initialize starting height= end of the start height+1 and end of the start height=end of height + blocks.,
"Then, if end of the start height >height then end of the start height=height.",
Repeat from step17 to 35 till j=height and i=width.,
End of the loop.,
"greenchannel layer 1 layer 2 layer 3 layer 4 layer 5 bluechannel layer 1 layer 2 layer 3 layer 4 layer 5 segmented red layer2 segmented red layer3 segmented red layer4 segmented green layer2 segmented green layer3 segmented green layer4 segmented blue layer2 segmented blue layer3 segmented blue layer4 International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 188 3.2.",
Intersection The three segmented layers of each channel has to be merged to a single channel.,
"To perform such, we used intersection method.",
This is done by using formula given below Eq.2.,
A∩B = {x|x∈ A and x ∈ B} (2) The intersection of the segmented layers is as shown in the Fig.,
(a) (b) (c) Fig 4.,
Intersection of the segmented layers ( (a) Red segmented channel (b) Green segmented channel (c) Blue segmented channel) 3.3.,
Region of Interest Fig.,
Block diagram of ROI 3.3.1.,
"Quad tree Quad tree is a tree which has four children for each nodes and this method is often used in image compression, also used in calculating image complexity.",
Quad tree decomposition adopted shows the boundary region of the mass for radiologists to analyze.,
Divide and conquer rule repeatedly applied divides the image until it meets the criteria.,
"Here, quad tree applied based on entropy divides the area repeatedly which are having high entropy.",
It ignores the regions having low entropy from division (Eq.,
"Entropy = -∑PiLog2Pi (3) The image subjected to quad tree is initially divided into quad blocks, then again subdivided the blocks successively into quadrant until it meets the criteria ie.",
a thick cluster of edges Fig.,
The edges arranged closely looks very bright to form thick cluster of bands.,
The masses size during early stage is about less than 1cm.,
Our algorithm highlighted those small regions by closer decomposition telling that it needs attention.,
(a) (b) (c) Fig 6.,
Thick cluster of edges ((a) ROI of red channel (b) ROI of green channel (c) ROI of blue channel) The quad tree outputs of the channels are merged using intersection operations to get the resultant region of interest shown in Fig 7.,
Final ROI output 4.,
Results The automated pinpointing ROI using quad tree decomposition is tested on the Digital Database for Screening Mammography (DDSM) datasets which is a benchmark datasets.,
These datasets are publicly available which contains both positive and negative sample of images.,
The informations about suspicious region locations and ground truth for the presence of abnormality in the image are also mentioned in the datasets.,
Experimentation on Benchmark Datasets For the experimentation we took 100 cancer positive images which are already marked by the radiologist and 100 cancer negative images.,
"Since this is a medical image, information loss from the image may lead to misclassification.",
Hence RGB images are not converted to gray images.,
Segmentation is performed on each channel separately.,
We considered different thresholds for layers of each channel which are shown in the table 1.,
Different thresholds for different layers Sl.,
"Red, green, blue layers Threshold values 1 Layer1 0-50 2 Layer2 50-100 3 Layer3 100-150 4 Layer4 150-200 5 Layer5 200 and above As Fig 2 ((b), (c), (d)) depicts layer1 and layer 5 has not provide any information.",
"We performed segmentation on layers 2, 3 and 4 by fixing block size=10 and threshold value=50.",
All the images persue good segmentation results compared to previous work.,
"The red intersection green intersection blue intersection ROI, red channel ROI, green channel ROI, blue channel ROI,image International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 189 segmented parts are tested by applying quad tree on images containing cancer, which shows the ROI after decomposition.",
Number of iterations performed in quad tree is 10.,
"It starts from 512, 256, 128, 64, 32, 16, 8, 4, 2, 1.",
Fig 7(a) shows the area with more division and having more edges which means a group of edges closely arranged to form an abnormal shape are the regions where some abnormality can be predicted.,
The detection rate is shown about 55 percent on the images.,
Comparison Of Normal Images With Cancer Images Here we compare the cancer positive images with the normal (cancer negative) images to find the differences between them in the results.,
The most important thing in the cancer image is that the abnormal masses possess high intensity at the center and intensity decreases at the outer side of the boundary.,
Quad tree supports division when there is a change in the intensities.,
Hence the quad tree decomposes more at the boundary level of the mass rather than at the center of the mass in the cancer image.,
Quad tree applied on the normal image shown in Fig.,
(c) also shows decomposition but here we could not find such divisions since it does not possess masses.,
This clearly shows that results obtained after applying quad trees for normal and cancer images possessing different way of divisions leading to region of interest in only the cancer images.,
The region of interest is the place where the radiologists have already marked as cancer and perhaps this again is a cross conformation to give attention on that region.,
"Fig 8 shows the original RGB image, segmented image and ROI image for normal image.",
"Fig 9 shows the original RGB, segmented image and ROI image of cancer image.",
(a) (b) (c) Fig.,
(a) Original image(normal) (b) Segmented image (c) ROI image (a) (b) (c) Fig.,
"(a) original image(cancer), (b) Segmented image (c) ROI image.",
PERFORMANCES Table 2.,
Segmentation and location of ROI results Image type Number of images Properly Segmented images Percentage of proper segmentation ROI Percentage of ROIs Cancer images 200 180 90% 120 60% Normal images 100 90 90% 80 80% 5.,
Discussion Our result shown tells that RGB images considered for processing has produced proper segmentation.,
"Sometimes while imaging the breast, pectoral muscle which is a homogeneous triangular part on the top left region will be overlapped with the mass.",
As per radiologists such overlapped masses on the pectoral muscle are undoubtedly cancer.,
Hence we did not remove pectoral muscle from the mammograms.,
Segmentation results produced best accuracy rate.,
The algorithm designed has given good segmentation for all images but accuracy dropped in locating ROI in images.,
"For images having larger masses, quad tree division gives cluster which is hard to distinguish between normal tissue and abnormal tissue.",
"In future, the thick clusters can be analyzed based on their texture and boundaries for the automated classification of the breast cancer.",
Extremely dense and heterogeneously dense breasts can also be tested for classification in future.,
Conclusion The proposed method contributed to improve the segmentation results to the best accuracy for all kinds of breasts without loss of any information.,
Also suggested that pectoral muscle remove is not necessary as in some images masses might overlap on the pectoral muscle.,
Quad tree division is a novel approach as it forms a cluster of edges which demands attention for the classification and to find the aggressiveness of that region.,
"References [1] Cheng HD, Cai X, Chen XW, Hu L, Lou X (2003) Computer-Aided Detection and Classification of Microcalcifications in Mammograms: A Survey, Pattern Recognition, 36: 2967–2991, 2003.",
[2] Lochanambal (2013) Mammogram image analysis - a soft computing approach.,
"Ph D thesis, Dept of computer Science, Mother Teresa Womens University, India.",
http://hdl.handle.net/ 10603/16541.,
"[3] American Cancer Society, USA.",
"https://www.cancer.org/cancer/breast-cancer/about/how-common- is-breast-cancer.html [4] Kanadam KP, Chereddy SR (2016) Mammogram classification using sparse ROI: A novel representation to arbitrary shaped masses, Expert Systems with Applications, 57: 204-213.",
"[5] Fahssi KE, Elmouﬁdi A, Abenaou A, Jai-Andaloussi S, Sekkaki A (2016) Novel approach to classification of Abnormalities in the mammogram image.",
International Journal of Biology and Biomedical Engineering 10.,
"[6] Bozek J, Mustra M, Delac K, Grgic M (2009).",
A survey of image processing algorithms in digital mammography.,
"Recent Advances in Multimedia Signal Processing and Communications, SCI, 231, 631– 657.",
"[7] Ganesan K, Acharya UR, Chua CK, Min LC, Abraham KT, Ng K merged intersection ROI,image merged intersection ROI,image International Journal of Intelligent Systems and Applications in Engineering IJISAE, 2018, 6(3), 185–190 | 190 (2013) Computer-aided breast cancer detection using mammograms: A review, Biomedical Engineering, IEEE Reviews, 6:77-98.",
"[8] Kolahdoozan F, Ahmadzadeh MR, Hekmatnia A, Mirzaalian H (2006) Pectoral Muscle Segmentation on Digital Mammogram.",
Proceedings of the Int.,
"on Computer and Communication Engineering, ICCCE’06.",
"1: 9-11 [9] Li Y, Chen H, Yang Y, Yang N (2013) Pectoral muscle segmentation in mammograms based on homogenous texture and intensity deviation.",
Pattern Recognition.,
(46): 681 – 691.,
"[10] Sreedevi S, Sherly E (2015) A novel approach for removal of pectoral muscles in digital mammogram.",
"Procedia Computer Science, 46: 724-1731.",
"[11] Pereira DC, Ramos R.P, Nascimento MZ (2014) Segmentation and detection of breast cancer in mammograms combining wavelet analysis and genetic algorithm Computer Methods and Programs Biomedicine 114 (1): 88-101.",
"[12] Anuradha.PV, Jose BR, Mathew J (2015) Improved Segmentation of Suspicious Regions of Masses in Mammograms by Watershed Transform.",
Procedia Computer Science 46:1483-1490.,
"[13] Kaur J, Kaur M (2016) Automatic cancer detection in mammographic images.",
International Journal of advanced Research in Computer Communications in Engineering (5) 7:473-476.,
[14] Pam Stephan (2017) The basics on benign and cancerous breast lumps.,
"[15] Salazar-Licea LA, Pedraza-Ortega JC, Pastrana-Palma A, Marco A, Aceves-Fernandez (2017) Location of mammograms ROI’s and reduction of false-positive.",
Computer methods and Programs in Biomedicine 143:97-111.,
"A CNN-BASED METHODOLOGY FOR BREAST CANCER DIAGNOSIS USING THERMAL IMAGES A PREPRINT J. Zuluaga-Gomez a, d, e, ∗, Z. Al Masry a , K. Benaggoune b , S. Meraghni c , N. Zerhouni a aFEMTO-ST institute, Univ.",
"Bourgogne Franche-Comté, CNRS, ENSMM, Besançon, France bLaboratory of Automation and Production engineering, Batna University, Batna, Algeria cLINFI Laboratory, University of Biskra, Biskra, Algeria dElectrical Engineering Department, University of Oviedo, Gijon, Spain eUniversidad Autonoma del Caribe, Barranquilla, Colombia October 31, 2019 Highlights: • Efﬁciency and reliability for breast cancer diagnosis through thermography • CNNs performance enhancement with data augmentation techniques • Smaller and simpler CNNs architectures perform better than state-of-the-art CNNs • Trade-off measurement between data augmentation and the database size ABSTRACT MicroAbstract: A recent study from GLOBOCAN disclosed that during 2018 two million women worldwide had been diagnosed from breast cancer.",
This study presents a computer- aided diagnosis system based on convolutional neural networks as an alternative diagnosis methodology for breast cancer diagnosis with thermal images.,
Experimental results showed that lower false-positives and false-negatives classiﬁcation rates are obtained when data pre- processing and data augmentation techniques are implemented in these thermal images.,
"Background: There are many types of breast cancer screening techniques such as, mammography, magnetic resonance imaging, ultrasound and blood sample tests, which require either, expensive devices or personal qualiﬁed.",
"Currently, some countries still lack access to these main screening techniques due to economic, social or cultural issues.",
"The objective of this study is to demonstrate that computer-aided diagnosis(CAD) systems based on convolutional neural networks (CNN) are faster, reliable and robust than other techniques.",
"Methods: Despite the disadvantages of the traditional machine learning techniques with spatial information, CNNs stand as main techniques for pattern recognition on images -or thermal images-.",
"We performed a study of the inﬂuence of data pre- processing, data augmentation and database size versus a proposed set of CNN models.",
"Furthermore, we developed a CNN hyper-parameters ﬁne-tuning optimization algorithm using a tree parzen estima- tor.",
"Results: Among the 57 patients database, our CNN models obtained a higher accuracy (92%) and F1-score (92%) that outperforms several state-of-the-art architectures such as ResNet50, SeResNet50 and Inception.",
"Also, we demonstrated that a CNN model that implements data-augmentation tech- niques reach identical performance metrics in comparison with a CNN that uses a database up to 50% bigger.",
Conclusion: This study highlights the beneﬁts of data augmentation and CNNs in thermal breast images.,
"Also, it measures the inﬂuence of the database size in the performance of CNNs.",
"MSC 2010: 62P10 & 68T10 ∗Corresponding author: Juan Pablo Zuluaga, ORCiD: 0000-0002-6947-2706.",
"Prognostics & Health Management Team, Femto-ST Sciences & Technologies, Besançon Cedex, France, 25000.",
"Email: juan.zuluaga@eu4m.eu arXiv:1910.13757v1 [cs.CV] 30 Oct 2019 A PREPRINT - OCTOBER 31, 2019 Keywords Breast Cancer · Breast Thermography · Hyper-parameters Optimization · Convolutional Neural Network · Computer Aided Diagnosis Systems 1 Introduction Breast cancer is the most commonly diagnosed cancer in women worldwide; then, it has become signiﬁcant public health.",
"It was the ﬁrst leading cause of cancer-linked death among women in 2018, reaching approximately 15% of the total number of registered cases [1].",
"The early detection of breast cancer is imperative to reduce the mortality and morbidity index [2, 3, 4].",
"Some studies suggest that emerging economies have almost a double risk of cancer, where the mortality-to incidence ratio in developed countries is 0.20, but in less developed countries is almost twice, thus 0.37 [5, 6].",
"Other factors like socioeconomic [5, 7], aging, unhealthy lifestyle [7, 8, 6, 9], environmental issues, and growth of the population may perhaps lead to higher risks.",
"In perspective, Li et al.",
"[10] prove the correlation between body weight, parity, number of births, and menopausal status concerning breast cancer.",
"On the other hand, some countries keep multiple barriers for developing an effective breast cancer screening system, e.g., organizational, psychological, structural, socio-cultural, and religious [11].",
"Physicians, self-examination, and imaging techniques can perform detection of abnormalities in the breast, but a biopsy is the only way to conﬁrm whether there is cancer [12].",
"Imaging techniques like mammography, ultrasound, and magnetic resonance imaging currently stand as the main techniques for early breast cancer screening.",
"However, limitations such as x-rays, expensiveness, dense tissue during young age, false positives (FP), and false-negative (FN) rates encouraged researchers and institutions to research alternative techniques like thermography deeply.",
"Contrary to other modalities, thermography is a non-invasive, non- inclusive, radiation-free, and low-cost technique [13].",
"Thus, thermography could be used to diagnose early-stage breast cancer e.g., young women and in dense breast patients.",
"Frequently, these novel techniques, such as thermography, are coupled with computer-aided diagnosis (CAD) systems.",
"A CAD system is a computational tool or algorithm capable of identifying patterns in many types of data e.g., clinical 2D and 3D clinical databases; consequently, several research teams are measuring the impact of CAD systems in the diagnosis of breast cancer patients [14].",
Thermography measure the temperature based on infrared radiation.,
"In medicine, the skin’s surface temperature gives health insights because, the radiance from human skin is an exponential function of the surface temperature, in other words, it is inﬂuenced by the level of blood perfusion in the tumor [15], e.g.",
Krawczykm et al.,
"[16] summarize that thermography is well suited to detect changes in blood perfusion that are led by inﬂammation, angiogenesis, benign and malignant tumor.",
"In 1956 the M. D. Lawson [17] recorded for the ﬁrst time the skin’s heat energy using a “thermocouple” [17], then authors presented similar devices e.g.",
the Pyroscan [18].,
"On the one hand, thermography has advantages over other techniques, in particular when the tumor is in an early-stage or in dense tissue [19, 20].",
"On the other hand, the thermography stands as a technique capable of overcoming the limitations of mammography such as x-rays, painfulness during the test, and not-permissible cost in some underdevelopment countries.",
"Consequently, during the last decades, there is an increasing research focused on machine learning techniques (MLT) for breast cancer diagnosis using thermal images; some researchers focus their works on the localization and size of tumors in phantoms and simulated models; but others scientists have been focused on characteristics like breast quadrants, menstrual cycle and acquisition protocols.",
"During the last years, promising results have been achieved in various medical imaging applications for breast cancer diagnosis [21, 22] with CNNs.",
"As mentioned in Section 2, we concluded that CNNs have not been used widely in the past for breast cancer diagnosis with thermography, maybe because the CNNs were not efﬁcient as texture or statistical features, or perhaps because the computing load was too high.",
"Nonetheless, during the last years, CNNs techniques stand as one of the main techniques for pattern recognition in images -or thermal images-.",
"In this work, we develop a novel CNN-CAD methodology that targets the public breast thermography database called “DMR-IR” proposed by Marquez [23] and Silva et al.",
This CNN-based study has ﬁve main contributions listed as follows: • We created baseline CNN models to replicate the results obtained by most of the recent studies regarding the DMR-IR database.,
This allowed us to ﬁnd a weak spot during the training process that other studies have not to tackle previously.,
"Therefore, we propose a new unbiased methodology to reduce further the likely training overﬁtting.",
"• In order to compare our CNN performance, here, we present a benchmark comparison between state-of-the-art architectures like ResNet, SeResNet, VGG16, Inception, InceptionResNetV2, and Xception.",
We demonstrated that smaller and less-complex CNN architectures are much better for the DMR-IR database.,
"• Following some survey articles [13, 25], we concluded that just a couple of authors had employed CNN techniques instead of texture and statistical features for this database.",
"Thus, we propose a better CNN architecture than the state-of-the-art ones, but also limiting the overﬁtting during the training process.",
"2 A PREPRINT - OCTOBER 31, 2019 • Besides the comparison between state-of-the-art and our proposed CNN architectures, we also developed a hyper-parameters optimization algorithm based on a tree parzen estimator to increase the performance of the models further.",
"• Finally, knowing that normally the databases in the biomedical environment are limited, expensive, hard to acquire and changes depending on the acquisition protocol, we measured the inﬂuence of data augmentation and database size during the training, with the intention to suggest a minimum number of patients to obtain an effective CAD system.",
"Although the CAD system has been trained with the DMR-IR database, this approach is useful for other databases of thermal breast images.",
The outline of this article is as follows.,
Section 2 explains the related work and the main ideas behind thermography and the inﬂuence of breast tumors in temperature changes.,
Section 3 describes the acquisition protocol and the methodology for data pre-processing and data augmentation.,
"In order to illustrate the novelty and advantage of our methodology regarding other studies, we compared the results of four sets of experiments in Section 4.",
"Lastly, discussion and conclusions are presented in Section 5 and 6.",
"2 Current techniques for breast cancer diagnosis from thermal images The rapid growth of virtual collaboration, programming tools and computing performance have raised the interest of many researchers over CAD systems in the biomedical area.",
Arena et al.,
[26] summarize the beneﬁts of thermography over the classical methods for breast cancer diagnosis.,
"They tested a weighted algorithm in 109 tissue proven cases of breast cancer, that generates positive or negative result based on six features (threshold, nipple, areola, global, asymmetry and hot spot).",
Krawczyk et al.,
"[16] propose an ensemble method for clustering and classiﬁcation in breast cancer thermal images; additionally, a 5x2 K-fold cross-validation was made to reduce the bias and obtain a more robust model.",
"Later, in 2009 Schaefer et al.",
"[27] performed a fuzzy logic classiﬁcation algorithm on 150 cases having an accuracy of 80%; they explain that statistical feature analysis is a key source of information for achieving high accuracy, i.e.",
"symmetry between left and right breast, standard temperature deviation, max-min temperatures, among others.",
"In addition, some researchers centered their studies on the tumor’s characteristics and behavior such as Partridge and Wrobel [28], whose designed a method using dual reciprocity joined with genetic algorithms to localize tumors, where they found that smaller and deeply located tumors produce only a limited thermal perturbation making harder their detection.",
"Contrary, it is possible to determine the tumor’s characteristics when the thermal surface behavior is known, Das and Mishra [29] afﬁrmed this.",
Kennedy et al.,
"[30] make a comparison between breast cancer screening techniques such as thermography, mammography and ultrasound.",
"The majority of studies related to CAD systems and infrared imaging techniques for breast cancer diagnosis employ the public and web-available database from [23, 24].",
"The database is composed of 1140 images from 57 patients, where 38 carried anomalies and 19 were healthy women from Brazil; additionally, each patient has a sequence-set of 20 images.",
"The interpretation of a breast thermography test could be either, temperature matrices or heat map images, as it is the proposed database (also called DMR-IR database).",
"Thermal images share similarities with a standard gray-scale or colored image; thus, mostly of the studies over the DMR-IR database try to identify texture and statistical features from those thermal images and matrices.",
Rajendra et al.,
[31] built an algorithm using support vector machines for automatic classiﬁcation of normal and malignant breast cancer.,
"They extracted from the DMR-IR database texture features from the co-occurrence matrix and statistical features from the temperature matrix, achieving an accuracy, sensitivity and speciﬁcity of 88.1%, 85.71% and 90.48%, respectively.",
"Araujo [32] presented a symbolic data analysis on 50 patients’ thermograms obtaining four variables from the thermal matrices; also, he applied leave-one-out cross validation framework during the training process obtaining 85.7% of sensitivity for the malignant class and accuracy of 84%.",
Mambou et al.,
"[3] describe a method to use deep neural networks and support vector machines for breast cancer diagnoses, but also, they call attention to camera sensitivity and physical model of the breast.",
"Nevertheless, in these above-mentioned studies are not stated how it is split the 1140-image database, neither if during the training process the whole twenty-image sequences from each patient belongs to either, train or test set or both datasets simultaneously.",
Table 1 shows a summary of the last studies using the DMR-IR database.,
"Their approaches have two big branches: on the one hand, a big portion of these studies have used texture and statistical features, where they have achieved 95% accuracy [33, 34].",
"On the other hand, other authors rather have chosen CNNs, achieving more than 90% accuracy [35, 36].",
"A main concerning with Table 1 studies is that each one presents a variable number of patients, which allows us to infer that the database has suffered changes over the last years such as, the inclusion of new patients.",
"Besides that, it is important to recall that most of these works do not mention sufﬁcient information regarding the database split methodology during the training framework.",
"Then, there are two possible approaches.",
"On the one hand, it is possible to stack all twenty-image sequences from each patient in one database and then split it in train/test datasets.",
"On the other hand, each patient’s image-sequence is assigned to either, train or test set as presented in Figure 1.",
"Indeed, 3 A PREPRINT - OCTOBER 31, 2019 Feature extraction & model selection Convolutional neural networks (CNN) Texture features and statistical analysis Machine learning classifiers SVM RF KNN ANN DMR-IR Database Preprocessing Phase ROI segmentation Database cleaning Proposed Methodology Breast Thermography Other databases Benchmark Fine-tuned models Database split (methodology) Data augmentation Train Valid.",
"Test … Patient 1 Patient 2 Patient N Image: 1,2,3,…,20 … … Approach 1 Approach 2 … Full Database Patient 1 Patient 2 Patient N Image: 1,2,3,…,20 … Train Valid.",
Test Figure 1: Current approaches for feature extraction and database splitting in DMR-IR database.,
"our main contributions are done under Approach 2 in the red-delimited are of Figure 1; also, this ﬁgure deﬁnes the pre-processing, training and database split frameworks of Table 1 studies.",
"During the last six years, several reviews concerning infrared technologies have emerged, well delimiting the status, main protocols and mew directions of breast cancer imaging diagnosis techniques [43, 25, 44, 13].",
One signiﬁcant fact mentioned in those reviews is that CAD thermography systems need to reduce the utmost non-relevant information in the thermal images.,
"A thermal image typically has unnecessary areas such as chess, background and other parts of the body, but this data is not useful and acts as noise during the training in CNN models or during the features identiﬁcation process.",
"Hence, the process provides a clean breast image without a non-necessary area to a CAD system is executed by a region of interest (ROI) algorithm.",
"Regarding the DMR-IR database [23, 24], several authors have based their research on ROI algorithms rather than identifying patterns in thermal images e.g.",
"[37] use extended hidden Markov models (EHMM), BayesNet and random forest in a 160-individuals for optimization of breast segmentation algorithms.",
Sathish et al.,
[39] extracted the breast’s ROI and uses asymmetry and local energy features of wavelet sub-bands to determine whether the patient has cancer.,
They also concluded that the normalization of each thermal image could improve the general efﬁciency of the segmentation algorithm.,
"In addition, extreme learning machines [42] and efﬁcient coding [41] have been used for ROI segmentation.",
"To summarize, it is essential to recall that several aspects inﬂuence the overall performance and complexity of a given system such as pre-processing techniques, features extraction, statistical analysis, a region of interest selection, CAD technique (machine learning approach), training framework, database splitting and post-processing.",
"Nevertheless, algorithm’s complexity is not directly proportional to the algorithm’s performance.",
"This paper differs from previously published studies (see Table 1) using the DMR-IR database [23, 24], since our main goal is to demonstrate that a CNN-based CAD system outperforms the texture features based algorithms from Table 1, but at the same time, it 4 A PREPRINT - OCTOBER 31, 2019 Table 1: Summary of algorithms based on machine learning techniques, statistical and texture features.",
These studies are based on the DMR-IR database for breast cancer diagnosis Ref.,
Year Machine learning technique / Extracted Features Acquisition protocol Numb.,
of pa- tients (Malig./Heal.),
"[31] 2012 Support vector machines (SVM) for texture features and statistical analysis Static 50 (25/25) [24] 2014 K-nearest neighbors (KNN) algorithm to classify Afﬁne Scale-Invariant Feature Transform (database owners) Static & Dynamic 149 [32] 2014 They obtained the nterval data in the symbolic data analy- sis & statistical analysis Static 50 (31/19) [37] 2015 Extended hidden Markov models for breast segmentation Static 160 [38] 2015 K-means and clustering from silhouette, Davies-Bouldin and Calinski-Harabasz indexes Dynamic 22 (11/11) [33] 2016 BayesNet, KNN & Radom Forest (RF) models for pixel intensity and time series analysis & Static Dynamic 80 (40/40) [39] 2017 SVM & Genetic Algorithm (GA) for classiﬁcation of nor- malized breast thermograms using local energy features Static 100 (47/53) [3] 2018 SVM, Artiﬁcial Neural Networks (ANN), Deep ANN, Recurrent ANN Static 64 (32/32) [40] 2018 SVM, KNN & ANN for texture features and statistical analysis Static 80 (30/50) [41] 2018 Bilateral asymmetry and statistical analysis for annotation of thermograms Static 100 (49/51) [42] 2018 Multi-Layer Perceptron (MLP), DT & RF using Zernike and Haralick moments as features Static 100 (30/70) [35] 2018 CNN models for static & dynamical analysis Static & Dynamic 137 (42/95) [36] 2019 State-of-the-art benchmark of several CNN architectures Static 216 (41/175) [34] 2019 Learning-to-rank (LTR) and texture analysis methods like histogram of oriented gradients Dynamic 56 (37/19) is less complex, easier to train and capable of generalizing more when new patients come.",
"This paper (i) presents a new methodology (see Figure 1) which has a greater performance outperforming some studies from Table 1; (ii) compares the performance of several state-of-the-art CNN architectures (benchmark); (iii) proposes a methodology for increase the CNN performance when hyper-parameters optimization is used and; (iv) determines the impact of data augmentation, data pre-processing and database size during and after the training process.",
"3 Database description and proposed methodology In this research, we propose several CNN-based experiments for the diagnosis of breast cancer using thermal images using the popular, free, and public available DMR-IR database, which is accessible through a user-friendly online interface (http://visual.ic.uff.br/dmi).",
"In the ﬁrst step, we applied data pre-processing and data augmentation for each thermal image e.g.",
"crop, resizing, and breast normalization.",
"In the second step, we deﬁned several sets of interconnected experiments that tested different CNNs architectures under different training frameworks based on the database split methodologies from Figure 1 (Section 2) and following the Figure 2 workﬂow.",
"As our study is based on CNN, the overview of a training process is as follows: ﬁrstly, each thermal breast image is forwarded through a given number of hidden layers until a loss function is computed; secondly, the loss function is back-propagated into these layers, modifying the weights in accordance with an optimizer e.g.",
"Finally, this procedure is looped for given N numbers of epochs until it reaches the desired performance metric value.",
The pipeline is delimited over three phases.,
"Firstly, it is uploaded all the 1140 thermal matrices and images into Python.",
"Then, the algorithm divides and matches the information for each patient with their respective diagnose (healthy or breast cancer).",
We used OpenCV python’s library for ROI extraction.,
Phase 1 supports the data pre-processing and augmentation for each of our proposed CAD systems.,
"Phase 2 conveys the core of our scientiﬁc contribution, which is four sets of experiments further explained in the three following sections.",
"This phase behaves depending on two auxiliary input functions: (i) a conventional training strategy and, (ii) a Bayesian optimization + conventional training.",
"5 A PREPRINT - OCTOBER 31, 2019 Lastly, Phase 3 evaluates the performance of our model using several types of metrics.",
Figure 2 summarizes the pipeline of our methodology.,
"3.1 Workﬂow description As mentioned before, our methodology is governed by Figure 2 workﬂow.",
"As the proposed methodology is intercon- nected, some experimental outputs become experimental inputs for other phases.",
"Therefore, our experimental results are conducted by the following steps.",
"Step 1: Database acquisition protocol The DMR-IR database has a population of 57 patients, with an age between 21 and 80 years old; 19 patients are healthy and 37 present a malignant breast.",
"The diagnostic has been prior conﬁrmed via mammography, ultrasound and biopsies.",
"The thermal images are captured with a FLIR thermal camera model SC620, which has a sensitivity of less than 0.04◦C and captures standard -40◦C to 500◦C.",
Each infrared image has a dimension of 640x480 pixels; the software creates two types of ﬁles: (i) a heat-map ﬁle; (ii) a matrix with 640x480 points e.g.,
307200 thermal points.,
"Firstly, each patient undergoes thermal stress for decreasing the breast surface temperature and then twenty-image sequences are captured per 5 minutes.",
"As a thermography test may be considerably affected when guidelines are not followed, the DMR-IR database followed the Ng [45] and Satish [43] acquisition protocol, which has been gathered jointly with physicians to ensure the database’s quality.",
"Here, it is mentioned several standards that lead to high quality and unbiased thermal images.",
"Firstly, each patient should avoid tea, coffee, large meals, alcohol and smoking before the test.",
"Secondly, the camera needs to run at least 15 min prior to the evaluation, having a resolution of 100mK at 30◦C; the camera at least should have 120x120 thermal points.",
"Third, the recommended room’s temperature is between 18 and 25◦C, humidity between 40% and 75%, carpeted ﬂoor, avoiding any source of heat such as, personal computers, devices that generate heat and strong lights.",
"Cancer (38 cases) Healthy (19 cases) Input Database Data pre-processing Data engineering Benchmark models ResNet Xception InceptionResnet50 Inception V3 VGG16 Performance metrics Accuracy Sensitivity Precision F1 Score ROC-AUC Phase 1 Phase 2 Phase 3 CNN models Auxiliary input functions Conventional approach: Train, validation and test split Hyper-parameters optimization (Bayesian) Benchmark CNN models SeResNet Proposed CNNs Fine-tuned CNNs Figure 2: The detailed workﬂow of ﬁne-tuned and benchmark models.",
"There are three phases: (1) describes the data acquisition, pre-processing and data augmentation; (2) shows the three core activities (baseline, benchmark and ﬁne-tuned CNN models) and; (3) displays the performance metrics used for evaluating all the CNN architectures.",
"6 A PREPRINT - OCTOBER 31, 2019 Step 2: Pre-processing and data augmentation In the proposed algorithm at ﬁrst, we have uploaded all the temperature matrices and mask to Python 3.7.",
"After the data acquisition step, each breast ROI is segmented from the original gray-scale mask image, but depending on the patient’s health status, one (sick) or both (healthy) breasts are taken into consideration.",
"In the pre-processing phase, we followed referenced methodologies [39, 42, 41, 46], such as cropping, resizing and normalization of each thermal breast image.",
"The product of this process is a thermal image with a size of 250x300 temperature points; consequently, we reduced by a quarter the computational cost.",
"The data augmentation step conveys four types of image data generation: (i) horizontal and vertical ﬂip; (ii) rotation between 0-45 degrees; (iii) 20% zoom and; (iv) normalized noises, e.g.",
"Algorithm 1 presents a pseudo-code of the above-mentioned techniques, where the pre-processing and data augmentation methodologies are the same for all the performed experiments (excluding the fourth experiment).",
The fourth experiment measures the inﬂuence of data augmentation and database (DMR-IR) size on the CNN’s performance.,
"It is important to mention that we assumed that the database’s acquisition protocol has been done rigorously [43, 45], thus, minimizing the bias and obtaining a high-quality dataset.",
Algorithm 1 Data pre-processing & data augmentation procedure DATA ENGINEERING DBase ←Input pre-processed database if Augmentation = True then select one or more: DBase ←DBase + horizontal or vertical ﬂip DBase ←DBase + 0-45◦image rotation DBase ←DBase + 20% zoom DBase ←DBase + normalized noises YIELD (DBase) end if end procedure procedure MAIN ALGORITHM main: (select one or more enhancement techniques) if Scale Database →True then Dbase ←Scale (Dbase) end if if Crop Database →True then Dbase ←Crop (Dbase) end if if Resize Database →True then Dbase ←Resize (Dbase)to250x300 end if DBase augmented ←YIELD (Database pre-processed) goto Training Algorithm.,
"END end procedure Step 3: Baseline and benchmark CNN models During the second phase, the CAD system has two types of the auxiliary input function, as depicted in Figure 2.",
"Firstly, we propose a database train, validation, and test split of 50/20/30, respectively.",
"In order to match the methodologies done by other authors, we propose a CAD system that tests several baseline CNN architectures (proposed in Figure 3) under this training framework.",
"In fact, some authors have obtained promising results using different methodologies and pre-processing techniques; nonetheless, other authors do not mention explicitly how is the database split [3, 31, 32] during the training process; thus, there are doubts about the algorithms’ reliability and robustness when new cases will come.",
"Contrary, we provide a detailed methodology starting from data preparation until the train/test phase, which guarantees the bias and overﬁtting minimization, even when new cases will come.",
"Under that proposed training framework, we have tested several state-of-the-art CNN architectures: ResNet [47], SeResNet [48], VGG16, Inception, InceptionResNetV2 [49]and Xception [50].",
"Afterward, we made a comparison between the baseline and the state-of- the-art models, ﬁnding that simpler CNN models performed better than big CNN architectures; therefore, we applied optimization techniques to design an optimal CNN architecture that should perform better than experiment 1 and benchmark ones.",
"7 A PREPRINT - OCTOBER 31, 2019 Step 4: CNN ﬁne-tuning and hyper-parameters optimization After comparing the baseline models with the two proposed approaches and the benchmark models, we decided to explore methodologies further to raise the performance of our CNN models; therefore, we propose a hyper-parameters Bayesian optimization based on a tree parzen estimator.",
Bayesian optimization is a probabilistic model-based approach for ﬁnding the global minimum of any function that returns a real-value metric (in our case F1-score).,
This methodology is also called a sequential model-based optimization because it builds a probabilistic model of an objective function that is based on past results.,
"Each time the model receives new evidence, it updates the probability model (also called ""surrogate model""), creating a new one with the last examples.",
"The longer the algorithm runs, the closer the surrogate function comes to resembling the actual objective function.",
We implemented on Python 3.7 the tree parzen estimator (TPE) using the HyperOPT library [51].,
"In fact, there are four key phases in order to build a TPE pipeline: • Firstly, it is necessary to deﬁne a domain space that will change depending on the model’s evolution.",
"The domain space may vary depending on the past results or the type of optimizer, • The optimization algorithm.",
"In this case, a TPE bayesian model, • The objective function receives a set of hyper-parameters then, create a machine-learning model (here, a CNN), • An evaluation metrics function receives a set of predicted values and real labels (cancer or healthy).",
"Next, it returns metrics like accuracy, precision, sensitivity, F1-score and ROC-AUC.",
Original Image Cropped (250 x 300) Conv2D & Batch N. Standard Block 1 .,
Standard Block N Global Avg.,
Pool Flatten Classification N x Conv2D Batch N. Dropout Max Pool Dense Softmax (i) (ii) (iv) Data augmentation Fit generator Cancer Healthy (iii) Batch N. Dense Figure 3: Convolutional neural network architecture for the baseline and hyper-parameters optimization experiments.,
The (i) global overview of the model is composed of (ii) standard blocks (iii) coupled with two possible top layers (ﬂatten or global average pooling).,
Then a classiﬁcation block (iv) provides the breast cancer diagnostic.,
The CAD system’s performance tells how close is the system to correctly diagnose whether a patient is having the disease and the ones who do not.,
"In our experimentation, the people carrying a malignant breast are the true class and the ones healthy are the false class.",
"Therefore, the easiest way to summarize a CAD system’s performance is with evaluation metrics.",
"We demonstrate the performance of our CNN models with several metrics such as accuracy, precision, sensitivity, F1-score and ROC-AUC.",
"However, we have chosen F1-score rather than accuracy.",
"On the one hand, the F1-score takes both false positives and false negatives into account; on the other hand, accuracy takes true positives and true negatives.",
The false-negative (FN) is a result that indicates a person does not have breast cancer when the person actually does have it.,
The false positive (FP) is a result that indicates a person does have breast cancer when the person actually does not have it.,
The below equations depict the proposed metrics.,
precision = TP TP + FP (1) Sensitivity = TP TP + FN (2) F1 −score = 2.,
"Precision ∗Sensitivity Precision + Sensitivity (3) 8 A PREPRINT - OCTOBER 31, 2019 Remark 1: Thence, knowing that the early diagnosis of breast cancer is crucial for the patient’s survival, the FN and FP are much more crucial parameters for a CAD system, in order to diagnose the disease and reduce the mortality index.",
"Additionally, F1-score deals with the imbalanced class distribution problem where accuracy does not.",
"Thus, in the biomedical area and speciﬁcally in breast cancer diagnosis, it is much more convenient than the F1-score.",
This metric is also recognized as the harmonic mean between precision (Equation 1) and recall (Equation 2) as depicted in Equation 3.,
"Finally, in the case of an equal F1-score in two CNN models, we have chosen the one with greater sensitivity, as it takes into account the FN.",
4 Experimental setup up and results This section delimits our study but also conveys the main ﬁnding of our studies and the top CNN models obtained from empirically experimentation and hyper-parameters optimization.,
"Additionally, we study the inﬂuence of data augmentation and database size on the CNNs performance.",
"4.1 Experimental setup The experimental set up is composed of four consecutive experiments, explained as follows: ﬁrstly, we have developed a baseline CNN models following Figure 3 architecture.",
"Our algorithm was trained with a Tesla K80 GPU unit, free and available in Google Colab.",
"The training framework splits the database as follows, 50% train, 20% validation and 30% test sets, following the Approach methodology from Figure 2.",
"All the experiments had batch normalization layers, ReLU activation function and we tested several optimizers such as Adam, RMSprop and SGD.",
"Throughout the experiments, it has been tested several architectures varying the number of Conv2D layers, the dropout rate and the number of units in the last dense connected layers.",
The input image size is 250x300 temperature “pixels”.,
"The training process has been done under the mini-augmented training batches (32 augmented images per step), with 50 steps per epoch (50 evaluation of 32 instances, per epoch), and 40 epochs in total.",
"Finally, we summarized the results and we selected the best model based on performance metrics and execution time.",
"As a second part of the ﬁrst set of experiments, we have done a crucial change in the database splitting from here and now on (Approach 2 from Figure 1).",
"Instead of splitting the whole database immediately, we have done a balanced splitting by patient thus, 39 patients for the train (780 images) and 17 for the test set (340 images).",
"Again, we summarized the results and we selected the best model.",
The signiﬁcant change in the performance metrics found during the ﬁrst set of experiments motivated us to search alternative CNNs architectures to further improve our proposed CAD system.,
"Consequently, the second set of experiments compares state-of-the-art CNN architectures with our previous results from the ﬁrst experimentation.",
We prearranged several novel and up-to-date CNNs architectures.,
"We keep each original CNN architecture, but we changed the top layer by a Flatten or global average pooling (GAP) layer, followed by two dense layers of 1024 units and a two-unit dense layer with Softmax activation function.",
"It is important to recall that all the performance metrics obtained from here and now on are based on blind test samples, i.e.",
samples that have not been seen by the models during the training.,
"Similarly, we have applied three callback functions in Keras, those are: (i) model checkpoint to save the weights of our top model; (ii) learning rate scheduler to apply a decay learning rate after each epoch and; (iii) early stopping monitor to reduce the overﬁtting and stop the training process when the model has stopped to learn.",
"The second set of experiments brought a main conclusion: the simpler the CNN model, the higher the performance metrics.",
"Thus, as discussed in Section 3, we decided to apply optimization techniques to ﬁnd an optimal CNN architecture.",
The third set of experiments aims to ﬁnd the optimal CNN architecture; particularly we have developed a Bayesian optimization.,
"As explained in Section 3, the optimization algorithm needs a search space where to examine and chose the optimal hyper-parameters.",
"Consequently, we deﬁned as hyper-parameters (based on Figure 3): (i) minimum and maximum number of blocks; (ii) number of Conv2D layers per each block; (iii) number of ﬁlters per Conv2D layer; (iv); (v) type of optimizer; (vi) kernel size; (vii) pooling layer size; (viii) batch normalization; (ix) dropout rate; (x) number of dense units in the last two layers; (xi) top layer type.",
"In the ﬁnal step, the optimization algorithm chooses from ﬁfty CNN models, the one with highest F1-score.",
The fourth set of experiments takes as input the top model from the Bayesian optimization phase and applies different training frameworks.,
Our main goal is to measure the CNN’s performance under different training scenarios.,
"On the one hand, we compared the results with and without data augmentation; on the other hand, we varied the database split ratio between train, validation and test datasets.",
"As a result of this experimentation, we are able to advise an optimized population size needed to replicate each result obtained throughout this study.",
"The training framework is as follows, we (1) selected randomly ten patients, ﬁve healthy and ﬁve with the disease; (2) trained ﬁve CNN models with 10, 20, 30, 40 and 47 patients with an 80/20 train and validation sets split; (3) evaluated the performance of each of the ﬁve models.",
"9 A PREPRINT - OCTOBER 31, 2019 DMR-IR Database EXPERIMENTS Approach 1 Benchmark CNNs Bayesian Optimization High performance High bias High overfitting Medium performance Low bias Low overfitting Approach 2 Proposed CNNs Proposed CNNs High performance Low bias Low overfitting Search space Objective function Performance metrics Tree parzen estimator TOP CNN Architecture Set #3 of Experiments Set #4 of Experiments Set #1 of Experiments Set #2 of Experiments TOP CNN Architecture Comparison Augmentation Non-Augmentation Database Size Figure 4: Experiment proposed workﬂows.",
Experiment 1 compares the CNN performance with Approach 1 and Approach 2 from Figure 1.,
Experiment 2 presents a CNN state-of-the-art benchmark.,
Experiment 3 applies a Bayesian optimization to determine the optimal CNN architecture.,
Experiment 4 deﬁnes the inﬂuence of data pre-processing and data augmentation on the DMR-IR database.,
"Afterward, we repeated the process from (1) to (3) four times, such as a “k-fold” cross-validation and we obtained the mean of each performance metric.",
A general overview of these proposed set of experiments could be found in Figure 4.,
"4.2 Experimental results The purpose of the ﬁrst set of experiments was to establish baseline CNN models, which tells the advantages of MLT over texture and statistical features in breast thermography.",
"As mentioned in Section 3, a CNN has various hyper-parameters that inﬂuence the learning during the training framework, resulting in satisfactory or unacceptable results; thus, it is imperative to ﬁnd the best combination of these parameters to ensure the CAD system reliability and robustness.",
"In the ﬁrst set of experiments, the leading parameters were: (i) number of CNN layers and ﬁlters; (ii) batch normalization and dropout rate; (iii) optimizer.",
"Despite the training methodology is not a cataloged as hyper-parameter, it was important to separate the results depending on it.",
"As discussed in the experimental setup, during the ﬁrst set of experiments, we tested two different database split approaches (see Figure 1) in four CNN architectures changing the set of hyper-parameters randomly.",
"In total, the algorithm runs a forty epochs simulation per model, which summed 200 epochs per database split methodology.",
"Table 2 summarizes all the CNN performance metrics with each proposed splitting approach, where CNNi, but i = 1, 2, 3, 4.",
We implemented a dropout rate to increase the model’s robustness by dropping out inputs from one layer to the next one.,
"Additionally, it is set a ten-epoch early stopping callback to reduce the overﬁtting during the training process.",
"The model CNN 1 yielded the best performance in both cases, approaches 1 and 2.",
It is important to recall that we selected the best model based on Remark 1 from Section 3.,
"In the ﬁrst instance, CNN1 yielded 99% accuracy, 99% precision, 98% sensitivity, 99% F1 Score and 99% ROU-AUC.",
"Nevertheless, the second instance showed a lower performance with an 88% accuracy, 88% precision, 91% sensitivity, 89% F1-score and 88% ROU-AUC.",
"Indeed, each CNN model has better result when using Approach 1, because there is a high probability that the CNN models under this database split methodology had images from the same patient in both datasets, train and test.",
"In other words, images from the twenty-image sequences pertaining to a given patient could be belonging to both, the train and test set (or validation set) simultaneously.",
"The baseline results obtained from the ﬁrst set of experiments suggested that more experimentation was needed in order to reach a CAD system with high performance, low bias and low overﬁtting.",
"Therefore, the idea of searching for better CNN architectures concluded in a new set of experiments based 10 A PREPRINT - OCTOBER 31, 2019 Table 2: Comparison of ﬁve performance metrics on four CNN architectures.",
The hyper-parameters were given empirically and it has been tested Approach 1 (biased) and Approach 2 (unbiased) database split methodology from Figure 1 for each CNN.,
Model Class Architecture (Num.,
"of blocks, num of layers) Optimizer Top Layer Accuracy F1 score Precision Sensitivity ROC-AUC Time per epoch (s) CNN 1 Biased (5,3) SGD Flatten 0.99 0.99 0.99 0.98 0.99 26 Unbiased (5,3) SGD GAP 0.86 0.87 0.84 0.90 0.85 30 CNN 2 biased (6,4) SGD Flatten 0.99 0.99 0.99 0.97 0.99 26 unbiased (6,4) Adam GAP 0.83 0.82 0.92 0.75 0.84 29 CNN 3 biased (7,4) Adam GAP 0.92 0.92 0.94 0.90 0.92 23 unbiased (7,4) Adam GAP 0.85 0.86 0.83 0.89 0.84 21 CNN 4 biased (4,3) Adam Flatten 0.89 0.89 0.92 0.87 0.90 21 unbiased (4,3) Adam GAP 0.86 0.87 0.90 0.84 0.87 25 on state-of-the-art CNN architectures, which might be capable of overcoming the weaknesses encountered during the ﬁrst set of experiments.",
"The second set of experiments involves the benchmark of state-of-the-art CNN architectures such as ResNet, SeResNet, Inception version 3, VGG16, InceptionResNet V2 and Xception.",
Table 3 exhibits the performance metrics for all the proposed models.,
"Generally, these cutting-edge CNN models are well optimized in architecture, but come at a cost of high number of parameters; indeed, higher than the models from experiment 1.",
"We kept the database split methodology (Approach 2), datasets proportion, the number of training epochs and the early stopping callback all along the second set of experiments.",
Table 3: Summary of performance metrics of each CNN model from the second set of experiments (benchmark) and the top model from the ﬁrst set of experiment (Approach 1).,
Model Accuracy F1-score Precision Sensitivity ROC AUC Time per epoch (s) SeResNet18 0.90 0.91 0.91 0.90 0.90 30 SeResNet34 0.86 0.86 0.91 0.81 0.86 35 SeResNet50 0.82 0.81 0.85 0.78 0.82 42 ResNet50 0.79 0.77 0.90 0.68 0.80 30 VGG16 0.90 0.89 0.85 0.94 0.90 22 InceptionV3 0.80 0.80 0.82 0.78 0.80 21 InceptionResNetV2 0.65 0.72 0.93 0.59 0.72 44 Xception 0.90 0.89 0.89 0.90 0.90 30 Top CNN (App 2.),
0.86 0.87 0.84 0.90 0.85 30 Table 3 shows the performance metrics for each individual model during the forty-epoch training.,
"We tested for each CNN model both top layers, ﬂatten and GAP layers.",
"In all the cases, the GAP predominated with higher performance e.g.",
the Inception V3 CNN model had a 30% improvement of F1-score when using GAP (not shown in Table 3) rather than ﬂatten layer.,
"During the experimentation, GAP layers and Adam (rather than RMSProp or SGD) optimizer yielded much better results; thus, we implemented this set of hyper-parameters for all the proposed state-of-the-art CNN models.",
"The preeminent CNN model was the SeResNet18 with a 90% accuracy, 91% precision, 90% sensitivity, 91% F1-score and 90% ROU-AUC.",
"We reach a breakpoint during this experiment, which suggests that simpler CNN models are better for the DMR-IR database.",
"As the complex the CNN architecture, the worse the model’s performance, the Figure 5 plots the accuracy and loss results over the training process of three SeResNet CNN models (presented in Table 3).",
Each plot represents one CNN architecture with a GAP layer followed by two fully connected layers of 1024 units each.,
"Likewise, some CNN did not reach the forty-epoch goal due to the early stopping callback, which allows us to stop training when the model has stopped to learn.",
We applied L2 regularization after detecting overﬁtting in some CNN models.,
"From a general 11 A PREPRINT - OCTOBER 31, 2019 Early Stopping Early Stopping Faster convergence Figure 5: Validation datasets performance of SeResNet18, SeResNet34 and SeResNet50 during training for 40 epochs.",
Model’s (a) accuracy and (b) losses in the validation dataset.,
"point of view, most of the state-of-the-art CNN architectures were not as regular as the ones presented in experiment 1 (suggested CNN architecture by the authors); we believe that these models are better for small datasets such as DMR-IR database, which it is a binary classiﬁcation problem (healthy or malignant breast).",
"Contrary, Table 3 CNN benchmark models are for multi-class classiﬁcation on huge databases like ImageNet.",
"In conclusion, these benchmark results motivated us to pursuit optimization techniques to obtain the best CNN architecture.",
A hyper-parameter is a parameter that controls the learning process of a given algorithm.,
"The acquisition process of biomedical databases in most of the cases is expensive and should follow strict guidelines in order to obtain high- quality data; thus, these databases usually are both, small and unbalanced.",
We implemented a Bayesian optimization (explained in Section 3) of CNN hyper-parameters to deal with these problems.,
"Firstly, our algorithm chooses a set of hyper-parameters from the proposed search space of Table 4.",
"Secondly, the objective function creates a CNN model based on these learning and architectural parameters.",
The TPE algorithm provides a result based on the current and past performance metrics of the previous models.,
Table 4: Search space for Bayesian optimization of CNN hyper-parameters with a tree parzen estimator.,
Figure 3 delimits the CNN architecture.,
Hyper-parameter Min Max Quantitative Number of blocks 2 4 2D Conv.,
layers per block 2 5 Number of ﬁlters per layer 64 512 Kernel size (n x n) 2 4 Pooling layer size (n x n) 2 3 Dense Layers (Num.,
"units) 256 1024 L2 regularizer 0 0.2 Qualitative Optimizer type Adam, SGD, RMSProp Droupout Yes, No Batch Normalization Yes, No Type of activation function Elu, ReLU Type of top layer Flatten or GAP The best result was obtained when we associated GAP or ﬂatten layers with a dropout rate between 0 and 0.3, but also with 6 or 7 CNN blocks.",
"To demonstrate the success of the hyper-parameters optimization, our CNN top model yielded a 92% accuracy, 98% precision, 87% sensitivity, 92% F1-score and 92% ROU-AUC as classiﬁcation metrics in the DMR-IR database.",
"Non-conventional techniques such as our optimization algorithm are able to increase the CAD system’s performance; speciﬁcally, the mean accuracy score raised by a 6% and 8% compared with experiments 1 and 12 A PREPRINT - OCTOBER 31, 2019 2, respectively.",
"The hyper-parameters optimization problem was targeted as a minimization problem; despite an overall of 207.360 possible combinations of hyper-parameters, we tested ﬁfty different sets.",
Table 5 presents a summary of the top models obtained during the Bayesian optimization and during experiments 1 and 2.,
"We proposed three CNN models, where CNN-Hypi, but i = 1, 2, 3.",
Table 5: Performance metrics comparison between the top CNN models from the third set of experiments (Bayesian optimization) and experiment 1 and 2.,
The database splitting follows the Approach 2 from Figure Figure 1.,
Model Architecture (Num.,
"of blocks, num of layers) Optimizer Top Layer Accuracy F1 score Precision Sensitivity ROC-AUC Time per epoch CNN-Hyp 1 (6,3) RMSProp Flatten 0.94 0.91 0.92 0.92 0.92 40 CNN-Hyp 2 (6,4) SGD GAP 0.99 0.83 0.90 0.90 0.91 64 CNN-Hyp 3 (7,2) Adam ﬂatten 0.98 0.87 0.92 0.92 0.92 24 SeResNet18 - Adam 30 0.91 0.90 0.91 0.90 0.90 30 Top CNN (App 2.)",
"(5,3) SGD GAP 0.84 0.90 0.87 0.86 0.85 30 To get a general overview of the results of our proposed experiments, Figure 6 summarizes the averaged performance metrics from experiments 1 to 3.",
"We decided to show in this ﬁgure the experiments in ascending order: ﬁrstly, despite experiment 1 successful performance, we concluded that it was biased and over-ﬁtted due to the training framework (database split Approach 1), weakening the models’ robustness.",
"Secondly, the benchmark experimentation had higher dispersion in comparison with the other set of experiments, diminishing the model’s reliability.",
"Finally, we measured the evolution in the performance metrics from the empirically given hyper-parameters (CNN with Approach 2) and the optimized set of hyper-parameters (Bayesian Optimization) for obtaining the topmost CNN architecture.",
It is important to note that the Bayesian optimization experiment displays an average increase of 7% in F1-score compared with experiment 1 (App.,
2) and the benchmark experiments.,
4.3 Inﬂuence of data augmentation and database size The reliability and availability of databases for breast cancer diagnosis using thermography are major challenges nowadays.,
"Consequently, this section brings guidance for new researchers in breast thermography, dealing with databases’ size issues and the role of data augmentation.",
"In the fourth and ﬁnal set of experiments, we measured the inﬂuence of data augmentation techniques and database size in the models’ performance for the DMR-IR database.",
"In addition, each performed experiment has been tested with and without data augmentation techniques.",
"Figure 7 plots the averaged performance metrics varying the train/validation dataset size from 10 to 47 patients, with and without data augmentation techniques.",
"We decided to choose an averaged performance (4 fold of metrics) rather than one set metrics because, the averaged performance decreases the excessively high bias and variance of CNNs in unseen data.",
"We followed the k-fold cross-validation methodology, but instead changing the train/validation set, we tested four different “test sets” i.e.",
four test folds.,
The main objective during this set of experiments is to prove the advantage of data augmentation rather than no data augmentation.,
Section 5 discuss the main insights about the results presented throughout Section 4 and some recommendations towards future works.,
5 Discussion and conclusion The results presented throughout Tables 2 to 5 provide a general overview of our contribution in (i) comparing the CNNs’ performance over different database split methodologies in the DMR-IR database; (ii) providing a new methodology that highly decrease the overﬁtting and biasing during the training process of CNNs for this database; (iii) a benchmark comparison of state-of-the-art CNN models for the DMR-IR database.,
"In addition, we (iv) demonstrated the beneﬁts of hyper-parameters optimization for ﬁne-tuning CNN architecture and; (v) measured the inﬂuence of data augmentation techniques and datasets sizes in the DMR-IR database.",
"Hence, it is becoming as baseline information for future works in either breast thermography databases or conception of new ones.",
"During the last years, there have been a demand for high quality, cheap, and reliable CAD systems for a breast cancer diagnosis; but speciﬁcally, early detection.",
CNN-based CAD systems for thermography stands as one methodology that could satisfy those requirements.,
"However, the lack of public databases has limited the studies towards thermography.",
"In fact, the only public and free database is the DMR-IR [23, 24].",
"We assumed that DMR-IR is one of the main 13 A PREPRINT - OCTOBER 31, 2019 CNN (Approach 1) (Table 2) Benchmark (Table 3) CNN (Approach 2) (Table 2) Bayesian Optimization (Table 5) 0.5 0.6 0.7 0.8 0.9 1.0 Performance Metrics Metrics Accuracy Sensitivity Precision F1 score Figure 6: Summarized box-plot of performance metrics for all sets of experiments.",
"Experiment 1 uses the database split approach 1 and 2 from Figure 1, respectively.",
Experiment 2 corresponds to CNN benchmarking models.,
Experiment 3 shows the top results obtained throughout the Bayesian optimized CNN models.,
"10 20 30 40 47 0.4 0.5 0.6 0.7 0.8 0.9 Accuracy Data Augmentetion Non Augmentation 10 20 30 40 47 0.4 0.6 0.8 1.0 Precision Data Augmentetion Non Augmentation 10 20 30 40 47 Number of Patients 0.4 0.6 0.8 1.0 (c) Sensitivity Data Augmentetion Non Augmentation 10 20 30 40 47 Number of Patients 0.4 0.5 0.6 0.7 0.8 (d) F1 score Data Augmentetion Non Augmentation Figure 7: CNN averaged performance metrics over number of patients taken from the DMR-IR database, with and without data augmentation.",
The test set has randomly chosen ten patients for all cases (4 folds).,
"(a) Accuracy, (b) Precision, (c) Sensitivity and (d) F1-score.",
"14 A PREPRINT - OCTOBER 31, 2019 databases in thermography due to both, his high quality (fulﬁlling the standard acquisition protocols [43, 45]) and his acceptance in the research community (see Table 1).",
"Nonetheless, when referring to past studies, it has been becoming almost impossible to compare the results impartially from study to study due to difference in the training framework, database size, datasets split ratio (between train/validation/test sets), normalization techniques and types of CAD system (texture and statistical features or CNNs).",
"Despite past studies that have diverged in the database sizes, we have seen two experimental methodologies.",
"On the one hand, some authors have used texture and silhouette features coupled with machine learning techniques [31, 24, 32, 40, 41, 42, 34] to detect whether a patient does have cancer.",
"On the other hand, a pair of studies use machine-learning techniques straightforward with the DMR-IR thermal images [35, 36].",
"As far as it is known, gathering texture, silhouette and statistical features demand much more time than applying MLT e.g.",
"In addition, algorithms based on these features need more time and resources to reach the required reliability and robustness, this due to the large intra-class appearance variations triggered by changes in illumination, rotation, scale, blur, noise, occlusion, etc.",
"Likewise, the main idea of CNN-based CAD system is to minimize the rate of pre-processing and data management needed prior to conceiving a robust machine-learning system, focusing further on the CNN architecture itself.",
"In other words, the developing time of a fully operational CAD system based on CNNs is fewer compared to one based on texture and statistical features.",
"As this study is the employing of parallel MLT rather than algorithms based on texture and statistical features from thermal images, we have chosen CNN due to their performance in spatial-databases.",
High-level APIs such as Keras from Tensorﬂow [52] allows the rapid development of robust CNN architectures.,
We focused our ﬁrst set of experiments on demonstrating the impact and consequences of the database splitting methodology over the training.,
"On the one hand, the ﬁrst set of Table 2 CNN models follows the Approach 1 for database split methodology, where some authors have presented results using a small [32, 39], medium [41] and full [42, 35] part of the DMR-IR database.",
The main concern with this methodology is the high performance achieved during training e.g.,
"our top model has an accuracy, F1-score and precision of more than 98%.",
"On the other hand, the second set of Table 2 CNN models use a more robust training framework, which all images/sequences are pertaining to a given patient either, all belong to the training or the testing set (or validation set); thence, minimizing the bias and over-ﬁtting.",
"Although these models yielded an average accuracy and F1-score of 84% and 85%, a thermography-based CAD system requires higher performance to overcome techniques like mammography.",
"Generally, in the ﬁrst set of experiments, CNN models with ﬂatten layer and SGD optimizer had better results when training under Approach 1; contrary, mixing GAP layer and Adam optimizer yielded higher performance under Approach 2.",
No one before Fernández-Ovies et al.,
"[36] has made a benchmark of state-of-the-art CNN architectures such as ResNet or VGG, employing the DMR-IR database.",
"Likewise, as a second general contribution, Table 3 and Figure 5 depict a benchmark study of several state-of-the-art CNN architectures.",
"In previous studies [35, 36], the essential contribution was not a CNN benchmark study but rather the employment CNNs as core MLT for their CAD systems.",
We noticed that CNNs models with Inception modules (e.g.,
"Inception V3 and InceptionResnet) had a lower performance because these architectures have many weights and parameters to tune, so we arrive to a breakthrough conclusion: the patterns in the DMR-IR thermal images are not too complex to be generalized by a CNN.",
"In consequence, the complex the CNN (width, depth and number of ﬁlters), the hard to generalize the DMR-IR thermal images.",
"In order to verify these conclusions, we developed speciﬁc experimentation using several SeResNet [48] but changing the number of residual layers.",
"In the ﬁrst case, we obtained an 81% accuracy, 85% precision, 78% sensitivity and 81% F1-score with a SeResNet50, but following our assumption that the simpler the model, the better; we tested a SeResNet34 and SeResNet18.",
"Consequently, we obtained a 9% accuracy, sensitivity and F1-score improvement when using the simpler model –SeResNet18-.",
"To further prove our hypothesis, Figure 5 shows the validation accuracy and losses versus epoch during the training period, where the simpler the model, the faster the model converged.",
The idea of simpler a CNN model yields better results that motivated to seek non-conventional techniques to improve our CNN models.,
"Speciﬁcally, we implemented a Bayesian Optimization based on a TPE to obtain the optimal CNN architecture (see Figure 3) from the search space suggested in Table 4.",
"As mentioned before, the top models obtained throughout the optimization performed much better than the experiment 1 and 2.",
"In general, the ﬂatten layer achieved better results than GAP, the SGD needed more processing time than RMSprop and Adam optimizer, but in all the cases, the results were comparable.",
"To summarize, we plotted in Figure 6 the averaged results per experiment and per metric, from experiments 1 to 3.",
We deduced from Figure 6 that experiment 1 App.,
"1 obtained the best performance metrics but at the cost of high bias and over-ﬁtting during the training; contrary, the App.",
"2 yielded high performance, but the CNN architecture was given empirically.",
"The average of experiment 2 produced a high variance in the box-plots, because some benchmark CNN models achieved high performance, but other who does not.",
"Finally, experiment 3 collects all the positive things such as low variance, low bias and low overﬁtting on the averaged performance metrics on three CNN models; moreover, rather than gives an empirical architectures to this models, we opted to apply a Bayesian optimization that yielded the optimal architecture, which overcomes all the previous CNN models.",
"15 A PREPRINT - OCTOBER 31, 2019 Despite the main advantages of CNNs, one of the main known drawbacks in MLT-based CAD systems is the quantity of available data, speciﬁcally in our case, breast thermal images.",
"In most of the circumstances, gather more data demands expensive and rigorous protocols, which should ensure the databases’ high quality and reliability.",
"Consequently, we targeted this problem inversely following the performance evolution of several CNN models when was applied data augmentation and when was altered the database size i.e.",
the number of patients.,
"The Figure 7 summarizes the accuracy, sensitivity, precision and F1-score of the proposed before-mentioned comparison and also explained in Section 4.",
"Therefore, the fourth and last set of experiments suggested –as expected- that the larger the database size (i.e.",
"from 10 to 47 patients), the more the CNNs generalize the data and the more the performance increase.",
"When the performance increased, the CNN models were more regular, having therefore less variance, as can be seen in Figure 7 (d).",
"Overall, the data augmentation techniques during all the simulations performed much better than no data augmentation; for instance, the mean F1-score in all cases was at least 10% higher.",
"If we compare the F1-score (Figure 7 (d)) of the experiments with databases sizes of 10, 20 and 30 patients, we conclude that a CNN model which uses data augmentation techniques requires 50% less number of patients to reach the same performance that a model which does not use it.",
"Speciﬁcally, the performance of an experiment with 20-patients database and data augmentation is comparable with a one with 30-patients database and no data augmentation.",
"In addition, we saw an incremental evolution of the performance metrics when the database increased as well, but between 40 and 47 patients, it was seen stabilization and decreasing in the variance.",
"To put in context, the variance between data augmentation and no data augmentation when 10 patients were 7% and 16%, respectively; similarly, for 20 (10,1% in both), 30 (11% and 13%), 40 (9% and 1%) and 47 patients (5% and 4%) there was a constant decrease in variance, therefore showing the models’ robustness improving.",
"In conclusion, the CNNs performance is a trade-off between data augmentation versus database size, the higher the database’ volume, the higher the performance.",
"Likewise, the more data augmentation the better.",
"This is a far-reaching conclusion, which gives helpful insights for further experimentation with the DMR-IR database or for researchers, which seek to conceive new breast thermography databases.",
"Therefore, this pioneering study could clarify upcoming experimentation with breast thermograms, where there is no information on how big the database should be in order to obtain acceptable performances.",
"Finally, we note that the application of this work is centered on demonstrating that CNN-based CAD systems are more viable than the ones based on texture and statistical features because of both robustness and easy implementation.",
"We have reviewed several studies, their techniques and methodologies towards databases of thermal image for a breast cancer diagnosis; nevertheless, it is important to mention some limitations.",
"Firstly, the lack of information (thermal images) limits the generalization that an MLT could reach (the more data the better).",
"Secondly, the physicians and researchers expect to know what the algorithm is computing, but normally the CNN models are recognized as black box MLT; thus, innovative techniques are measuring the CNN’s inside behavior throughout the training process.",
Further research in this area could clarify some still unanswered questions.,
"Thirdly, the physicians prefer systems that give an image and a probability rather than a merely probability of having cancer; therefore, future work should develop CAD systems that deal with these issues.",
"To ﬁnish up, this article proposes a novel CNN-based method for breast cancer diagnosis using thermal images.",
We showed that a well-delimited database split technique is needed in order to reduce the bias and overﬁtting during the training process.,
The paper presents the last studies on the DMR-IR database.,
Experimental results conﬁrm that our database split methodology minimizes the overﬁtting and bias during training.,
"In addition, this paper conveys the ﬁrst state-of-the-art benchmark of CNN architectures such as ResNet, SeResNet, VGG16, Inception, InceptionResNetV2 and Xception for the DMR-IR database.",
"Likewise, this study establishes the ﬁrst CNN hyper-parameters optimization in a thermography database for breast cancer, where the top CNN model achieved a 92% accuracy, 94% precision, 91% sensitivity and 92% F1-score.",
We demonstrated that the trade-off between database size and data augmentation techniques are crucial in classiﬁcation tasks lacking sufﬁcient data such as the one presented in the present study.,
"We have demonstrated that CAD systems for breast cancer diagnosis with thermal images can be valuable and reliable additional tools for physicians, but further research is needed on bigger databases and in multi-class classiﬁcation problems.",
Disclosure statement The authors have stated that they have no conﬂicts of interest.,
Funding This work has been supported by the INTERREG (France - Switzerland) under the SBRA project.,
"16 A PREPRINT - OCTOBER 31, 2019 Acknowledgement JZ, ZA, KB, SM and NZ contributed to conception and design.",
JZ led the data pre-processing and algorithm conception.,
"JZ, ZA, KB and SM contributed to analysis and discussion of the results.",
JZ contributed with the preparation of the manuscript.,
"JZ, ZA, KB, SM and NZ contributed with to the reviews of the manuscript.",
All authors read and approved the ﬁnal manuscript.,
"This work has been supported by the EIPHI Graduate school (contract ""ANR-17-EURE-0002"").",
References [1] All Cancer Globocan 2018 - International Agency for Research on Cancer WHO.,
"http://gco.iarc.fr/today/ data/factsheets/cancers/39-All-cancers-fact-sheet.pdf, 2019.",
[Online; accessed 03-March-2019].,
[2] What is Cancer?,
National Cancer Institute.,
"https://www.cancer.gov/about-cancer/understanding/ what-is-cancer, 2015.",
[Online; accessed 03-March-2019].,
"[3] Sebastien Mambou, Petra Maresova, Ondrej Krejcar, Ali Selamat, and Kamil Kuca.",
Breast cancer detection using infrared thermal imaging and a deep learning model.,
"Sensors, 18(9):2799, 2018.",
"[4] Xu Li, Essex J Bond, Barry D Van Veen, and Susan C Hagness.",
An overview of ultra-wideband microwave imaging via space-time beamforming for early-stage breast-cancer detection.,
"IEEE Antennas and Propagation Magazine, 47(1):19–34, 2005.",
"[5] Freddie Bray, Jacques Ferlay, Isabelle Soerjomataram, Rebecca L Siegel, Lindsey A Torre, and Ahmedin Jemal.",
Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries.,
"CA: a cancer journal for clinicians, 68(6):394–424, 2018.",
"[6] Freddie Bray, Ahmedin Jemal, Nathan Grey, Jacques Ferlay, and David Forman.",
Global cancer transitions according to the human development index (2008–2030): a population-based study.,
"The lancet oncology, 13(8):790–801, 2012.",
[7] Omer Gersten and John R Wilmoth.,
The cancer transition in japan since 1951.,
"Demographic Research, 7:271–306, 2002.",
[8] AR Omran.,
The epidemiologic transition: a theory of the epidemiology of population change-milbank mem.,
"Quart.-49-1971, pages 509–538, 2005.",
[9] Milena Maule and Franco Merletti.,
Cancer transition and priorities for cancer control.,
"The lancet oncology, 13(8):745–746, 2012.",
"[10] Tong Li, Limei Sun, Naomi Miller, Trudey Nicklee, Jennifer Woo, Lee Hulse-Smith, Ming-Sound Tsao, Rama Khokha, Lisa Martin, and Norman Boyd.",
The association of measured breast tissue characteristics with mam- mographic density and other risk factors for breast cancer.,
"Cancer Epidemiology and Prevention Biomarkers, 14(2):343–349, 2005.",
[11] Larissa Remennick.,
The challenge of early breast cancer detection among immigrant and minority women in multicultural societies.,
"The breast journal, 12:S103–S110, 2006.",
"[12] Alisson Augusto Azevedo Figueiredo, Jefferson Gomes do Nascimento, Fernando Costa Malheiros, Luis Henrique da Silva Ignacio, Henrique Coelho Fernandes, and Gilmar Guimaraes.",
Breast tumor localization using skin surface temperatures from a 2d anatomic model without knowledge of the thermophysical properties.,
"Computer methods and programs in biomedicine, 172:65–77, 2019.",
"[13] J Zuluaga-Gomez, N Zerhouni, Z Al Masry, C Devalland, and C Varnier.",
A survey of breast cancer screening techniques: thermography and electrical impedance tomography.,
"Journal of medical engineering & technology, pages 1–18, 2019.",
"[14] Miguel Patrício, José Pereira, Joana Crisóstomo, Paulo Matafome, Manuel Gomes, Raquel Seiça, and Francisco Caramelo.",
"Using resistin, glucose, age and bmi to predict the presence of breast cancer.",
"BMC cancer, 18(1):29, 2018.",
[15] Bryan F Jones.,
A reappraisal of the use of infrared thermal image analysis in medicine.,
"IEEE transactions on medical imaging, 17(6):1019–1027, 1998.",
"[16] Bartosz Krawczyk, Gerald Schaefer, and Shao Ying Zhu.",
Breast cancer identiﬁcation based on thermal analysis and a clustering and selection classiﬁcation ensemble.,
"In International Conference on Brain and Health Informatics, pages 256–265.",
"Springer, 2013.",
[17] RN Lawson.,
A new infrared imaging device.,
"Canadian Medical Association Journal, 79(5):402, 1958.",
"17 A PREPRINT - OCTOBER 31, 2019 [18] William R Vogler and Ralph Waldo Powell.",
A clinical evaluation of thermography and heptyl aldehyde in breast cancer detection.,
"Cancer research, 19(2):207–209, 1959.",
[19] Valerie A McCormack and Isabel dos Santos Silva.,
Breast density and parenchymal patterns as markers of breast cancer risk: a meta-analysis.,
"Cancer Epidemiology and Prevention Biomarkers, 15(6):1159–1169, 2006.",
"[20] Giske Ursin, Linda Hovanessian-Larsen, Yuri R Parisky, Malcolm C Pike, and Anna H Wu.",
Greatly increased occurrence of breast cancers in areas of mammographically dense tissue.,
"Breast Cancer Research, 7(5):R605, 2005.",
"[21] Mugahed A Al-antari, Mohammed A Al-masni, Mun-Taek Choi, Seung-Moo Han, and Tae-Seong Kim.",
"A fully integrated computer-aided diagnosis system for digital x-ray mammograms via deep learning detection, segmentation, and classiﬁcation.",
"International journal of medical informatics, 117:44–54, 2018.",
"[22] Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani, Michal J Wesolowski, Kevin A Schneider, and Ralph Deters.",
Breast cancer diagnosis with transfer learning and global pooling.,
"arXiv preprint arXiv:1909.11839, 2019.",
[23] R de S Marques.,
"[automatic segmentation of thermal mammogram images, dissertation].",
In Instituto de Computação Universidade Federal Fluminense.,
"Instituto de Computação Universidade Federal Fluminense, 2012.",
Portuguese.,
"[24] LF Silva, DCM Saade, GO Sequeiros, AC Silva, AC Paiva, RS Bravo, and A Conci.",
A new database for breast research with infrared image.,
"Journal of Medical Imaging and Health Informatics, 4(1):92–100, 2014.",
"[25] Nisreen IR Yassin, Shaimaa Omran, Enas MF El Houby, and Hemat Allam.",
Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: A systematic review.,
"Computer methods and programs in biomedicine, 156:25–45, 2018.",
"[26] Francis Arena, Clement Barone, and Thomas DiCicco.",
Use of digital infrared imaging in enhanced breast cancer detection and monitoring of the clinical response to treatment.,
In Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat.,
"03CH37439), volume 2, pages 1129–1132.",
"IEEE, 2003.",
"[27] Gerald Schaefer, Michal Závišek, and Tomoharu Nakashima.",
Thermography based breast cancer analysis using statistical features and fuzzy classiﬁcation.,
"Pattern Recognition, 42(6):1133–1137, 2009.",
[28] PW Partridge and LC Wrobel.,
An inverse geometry problem for the localisation of skin tumours by thermal analysis.,
"Engineering Analysis with Boundary Elements, 31(10):803–811, 2007.",
[29] Koushik Das and Subhash C Mishra.,
Estimation of tumor characteristics in a breast tissue with known skin surface temperature.,
"Journal of Thermal Biology, 38(6):311–317, 2013.",
"[30] Deborah A Kennedy, Tanya Lee, and Dugald Seely.",
A comparative review of thermography as a breast cancer screening technique.,
"Integrative cancer therapies, 8(1):9–16, 2009.",
"[31] U Rajendra Acharya, Eddie Yin-Kwee Ng, Jen-Hong Tan, and S Vinitha Sree.",
Thermography based breast cancer detection using texture features and support vector machine.,
"Journal of medical systems, 36(3):1503–1510, 2012.",
"[32] Marcus C Araújo, Rita CF Lima, and Renata MCR De Souza.",
Interval symbolic feature extraction for thermography breast cancer detection.,
"Expert Systems with Applications, 41(15):6728–6737, 2014.",
"[33] Lincoln F Silva, Alair Augusto SMD Santos, Renato S Bravo, Aristófanes C Silva, Débora C Muchaluat-Saade, and Aura Conci.",
Hybrid analysis for indicating patients with breast cancer using temperature time series.,
"Computer methods and programs in biomedicine, 130:142–153, 2016.",
"[34] Mohamed Abdel-Nasser, Antonio Moreno, and Domenec Puig.",
Breast cancer detection in thermal infrared images using representation learning and texture analysis methods.,
"Electronics, 8(1):100, 2019.",
[35] Matheus de Freitas Oliveira Baffa and Lucas Grassano Lattari.,
Convolutional neural networks for static and dynamic breast infrared imaging classiﬁcation.,
"In 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), pages 174–181.",
"IEEE, 2018.",
"[36] Francisco Javier Fernández-Ovies, Edwin Santiago Alférez-Baquero, Enrique Juan de Andrés-Galiana, Ana Cernea, Zulima Fernández-Muñiz, and Juan Luis Fernández-Martínez.",
Detection of breast cancer using infrared thermography and deep neural networks.,
"In International Work-Conference on Bioinformatics and Biomedical Engineering, pages 514–523.",
"Springer, 2019.",
"[37] E Mahmoudzadeh, MA Montazeri, M Zekri, and S Sadri.",
Extended hidden markov model for optimized segmentation of breast thermography images.,
"Infrared Physics & Technology, 72:19–28, 2015.",
"[38] Lincoln F Silva, Giomar O Sequeiros, Maria Lúcia O Santos, Cristina AP Fontes, Débora C Muchaluat-Saade, and Aura Conci.",
Thermal signal analysis for breast cancer risk veriﬁcation.,
"In MedInfo, pages 746–750, 2015.",
"18 A PREPRINT - OCTOBER 31, 2019 [39] Dayakshini Sathish, Surekha Kamath, Keerthana Prasad, and Rajagopal Kadavigere.",
Role of normalization of breast thermogram images and automatic classiﬁcation of breast cancer.,
"The Visual Computer, pages 1–14, 2017.",
"[40] Chebbah Nabil Karim, Ouslim Mohamed, and Temmar Ryad.",
A new approach for breast abnormality detection based on thermography.,
"Medical Technologies Journal, 2(3):245–254, 2018.",
"[41] Mrinal Kanti Bhowmik, Usha Rani Gogoi, Gautam Majumdar, Debotosh Bhattacharjee, Dhritiman Datta, and Anjan Kumar Ghosh.",
Designing of ground-truth-annotated dbt-tu-ju breast thermogram database toward early abnormality prediction.,
"IEEE journal of biomedical and health informatics, 22(4):1238–1249, 2017.",
"[42] Maíra Araújo de Santana, Jessiane Mônica Silva Pereira, Fabrício Lucimar da Silva, Nigel Mendes de Lima, Felipe Nunes de Sousa, Guilherme Max Silva de Arruda, Rita de Cássia Fernandes de Lima, Washington Wagner Azevedo da Silva, and Wellington Pinheiro dos Santos.",
Breast cancer diagnosis based on mammary thermography and extreme learning machines.,
"Research on Biomedical Engineering, 1(AHEAD):0–0, 2018.",
"[43] Satish G Kandlikar, Isaac Perez-Raya, Pruthvik A Raghupathi, Jose-Luis Gonzalez-Hernandez, Donnette Daby- deen, Lori Medeiros, and Pradyumna Phatak.",
"Infrared imaging technology for breast cancer detection–current status, protocols and new directions.",
"International Journal of Heat and Mass Transfer, 108:2303–2320, 2017.",
"[44] Tiago B Borchartt, Aura Conci, Rita CF Lima, Roger Resmini, and Angel Sanchez.",
Breast thermography from an image processing viewpoint: A survey.,
"Signal Processing, 93(10):2785–2803, 2013.",
[45] EY-K Ng.,
A review of thermography as promising non-invasive detection modality for breast tumor.,
"International Journal of Thermal Sciences, 48(5):849–859, 2009.",
"[46] Mona AS Ali, Gehad Ismail Sayed, Tarek Gaber, Aboul Ella Hassanien, Vaclav Snasel, and Lincoln F Silva.",
Detection of breast abnormalities of thermograms based on a new segmentation method.,
"In 2015 Federated Conference on Computer Science and Information Systems (FedCSIS), pages 255–261.",
"IEEE, 2015.",
"[47] Xin Yu, Zhiding Yu, and Srikumar Ramalingam.",
Learning strict identity mappings in deep residual networks.,
"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018.",
"[48] Jie Hu, Li Shen, and Gang Sun.",
Squeeze-and-excitation networks.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018.",
"[49] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.",
"Inception-v4, inception-resnet and the impact of residual connections on learning.",
"In Thirty-First AAAI Conference on Artiﬁcial Intelligence, 2017.",
[50] François Chollet.,
Xception: Deep learning with depthwise separable convolutions.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.",
"[51] James Bergstra, Daniel Yamins, and David Daniel Cox.",
Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures.,
"Jmlr, 2013.",
"[52] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, and Craig Citro and.",
"TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.",
Software available from tensorﬂow.org.,
"Breast Cancer Data Analytics With Missing Values: A study on Ethnic, Age and Income Groups Santosh Tirunagari∗, Norman Poh†, Hajara Abdulrahman‡, Nawal Nemmour§ and David Windridge¶ University of Surrey, Guildford, Surrey GU2 7XH ∗s.tirunagari@surrey.ac.uk, †n.poh@surrey.ac.uk, ‡ha00162@surrey.ac.uk, §nn00070@surrey.ac.uk, ¶d.windridge@surrey.ac.uk Abstract—An analysis of breast cancer incidences in women and the relationship between ethnicity and survival rate has been an ongoing study with recorded incidences of missing values in the secondary data.",
"In this paper, we study and report the results of breast cancer survival rate by ethnicity, age and income groups from the dataset collected for 53593 patients in South East England between the years 1998 and 2003.",
"In addition to this, we also predict the missing values for the ethnic groups in the dataset.",
"The principle ﬁndings in our study suggest that: 1) women of white ethnicity in South East England have a highest percentage of survival rate when compared to the black ethnicity, 2) High income groups have higher survival rates to that of lower income groups and 3) Age groups between 80-95 and 20-35 have lower percentage of survival rate.",
INTRODUCTION A.,
Motivation for imputation Surveys often contain missing values and the process of replacing these missing values with substituted values is called as imputation [7] [8].,
These missing values might induce more useful information in predicting the trends and statistics.,
"For conveniences of analysis, statisticians normally discard observations containing the missing values.",
This results in the reduction of the sample size for the analysis and interesting information might get lost.,
This can be a serious hindrance not only by misleading the results [9] but also by producing overly simpliﬁed conclusions.,
"Hence, there is a need for imputation when dealing with the data analysis especially in the health care domain.",
From the literature the missing value problem can be categorised into two main types: 1) missing values at random and 2) missing values not at random [10].,
Missing values not at random can again be of two types 1) for discrete values (group membership) [11] and 2) for continues values.,
"Methods such as: 1) mean substitution, 2) median substitu- tion could be utilises for computing the missing values at random [12] [10].",
"Other method including, maximum votes and nearest neighbours techniques can also be employed for this purpose.",
Regression based substitution can be employed for continuous missing values that are not at random.,
The aforementioned methods fail when imputing the values for group membership (refer to [13] for computing missing values for group memberships).,
"When dealing with missing values for group memberships, methods such as: 1) analysis of variance (AnoVa) [14], 2) analysis of mean and 3) classiﬁcation based methods [15] could be employed.",
"In the earlier methods, the variance and mean for each group is calculated and mean square error is computed between the observations of group and the new group that contains missing values.",
The group member with minimum mean square error is assigned to the missing value.,
"In the classiﬁcation based approach, the missing values are considered as the unknown class labels and a classiﬁer model is built on the existing observations.",
Using the classiﬁer model the missing values are then computed.,
"In this process, the classiﬁer must be cross validated for the performance.",
"Using traditional statistical methods sometimes might give us overly simpliﬁed solutions, hence there is a need for statis- tical machine learning algorithms which are a combination of both probability and statistics [15].",
"Therefore, we in this study use classiﬁcation based method to impute the missing values for group membership, i.e ethnicity group.",
Motivation underlying the current study Breast cancer is a malignant tumour that originates from the cells of the breast and grows into surrounding and distant tissues [1].,
It is the second most common cancer in women which makes this study important.,
"The relationship between ethnicity, survival percentage, and breast cancer is complex.",
Studies carried out in the past have shown that women of different ethnicity have different rates of survival from breast cancer after diagnosis [2].,
"Many comparisons and links have been made between ethnicity, income and survival rates in women diagnosed with breast cancer [3] [4] [5].",
"According to a study carried out by the Cancer Research UK 1, after grouping women into ethnicity groups aged between 15 and 64 years, the percentage of survival from breast cancer of those of white ethnicity is relatively higher at 91.4% than women of black ethnicity with survival percentage of 85.0%.",
The National Cancer Intelligence Network 2 have produced a report [6] on ‘Cancer Incidence and Survival by Major Ethnic Groups in England between 2002 and 2006’.,
"This report shows that the survival rates of women with breast cancer categories into four major ethnicity groups namely: White, Asian, Black and Unknown, white women had a higher rate of survival compared to those of black ethnicity.",
Bradley et al.,
"in [5], showed that low socioeconomic status was associated with late-stage breast cancer at diagnosis and mostly in death.",
1http://www.cancerresearchuk.org/ 2http://www.ncin.org.uk/ arXiv:1503.03680v1 [q-bio.QM] 12 Mar 2015 C. Objectives and contribution The main objectives of this study are two-fold.,
"On one hand, from the computational perspective, we would like to examine the feasibility of using machine learning based classiﬁer (e.g, Naive Bayes) in ﬁlling up the missing values in the data.",
"On the other hand, from the scientiﬁc perspective, we would like understand whether the insights from breast cancer analytics correspond to what clinicians would expect.",
For example to answer the following questions that are exploratory in nature.,
• Does survival rate get affected by the age and ethnic group of the patient?,
"for instance, are black and older women more likely to die if they have breast cancer.",
• Does ﬁnancial status of the patient have any effect on the survival rate?,
"i.e, do wealthier have lower possibility of dying from the breast cancer.",
Our contribution in this paper is to show how a machine learning based classiﬁer can be utilised to impute the missing values in the health care data and obtain insights.,
"When there are many classiﬁcation methods available in the literature, it is difﬁcult to choose which one to use.",
"In such a case simplicity, reputation of the method and experience of its usage can inﬂuence the selection process.",
"Therefore, in this study, we have chosen Naive Bayes classiﬁer to compute the missing values because of its simplicity and inexpensiveness.",
"D. Organisation The organisation of the paper is as follows: In section II, we present and analyse the breast cancer dataset.",
"In section III, we discuss our methodology.",
Results are discussed in section IV.,
"Finally, in section V, we draw conclusions and summarise with the discussions.",
DATA PREPROCESSING The dataset is collected for 53593 breast cancer incidences in women taken in South East England between the years 1998 and 2003.,
The initial dataset consists of 13 features however some of these features are simply an alternative way of representing the existing ones.,
Therefore these features are removed from the dataset.,
Features such as ’Year of Diagnosis’ and ’Year of Death or Censored’ are removed as this data was available to us within the ’Survival’ feature.,
We also have removed the single year ’Age at diagnosis’ feature as we already have this information within the ’Age’ feature.,
This left us with a ﬁnal dataset of 9 features as shown in Table I along with its Data format.,
TABLE SHOWING THE FEATURES AND THEIR FORMAT.,
Feature Data Format Income Quintile 1 = (Most Afﬂuent) to 5 = (Most Deprived) Age at Diagnosis Group 0 = (0-4); 5 = (5-9) to 100 = (100+) Ethnic Group Ethnic Groups (Table 2) Radiotherapy 0=No; 1=Yes Chemo Therapy 0=No; 1=Yes Hormone Therapy 0=No; 1=Yes Cancer Surgery 0=No; 1=Yes Survival days Total no.,
of days Death of Breast Cancer 0=No; 1=Yes The second stage of our data preprocessing is to convert the ethnic group data from nominal to indices (Table II) and remove header labels from the dataset so that it can be used for further data analysis.,
NOMINAL VALUES OF ETHNIC GROUPS AND THEIR CORRESPONDING NUMERICAL VALUE.,
Ethnicity Group Nominal Values Indices White W 1 Not Known NK 2 Any Other Oth 3 Black Caribbean BC 4 Chinese C 5 Indian In 6 Black African BA 7 Pakistani P 8 Black Other BO 9 Asian Other AO 10 Mixed M 11 Bangladeshi Ba 12 A.,
Demographics The age group distribution in the dataset as seen in Figure 1 shows an expected normal distribution of age within the pa- tients and indicates the highest frequency of them are between 50 and 65 years of age.,
10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 0 1000 2000 3000 4000 5000 6000 7000 8000 Age group distribution in dataset Number of Participants Age Groups Fig.,
Distribution of age within the patients.,
The most common cancer treatment taken by women of white ethnicity is radiotherapy as seen in Figure 2.,
However this seems to be the least common treatment for women of black African ethnicity.,
Figure 3 shows that the women from the white ethnic group are quite evenly distributed in terms of their socioeconomic deprivation.,
"Looking at the average of survival days across the ethnic groups shows that women of Chinese ethnicity have the highest days of survival from breast cancer (see, Figure 4).",
"It also highlights that although the proportion of women of white ethnicity is signiﬁcantly higher than any other groups, an average count gives a better indication of the caner survival rate across the ethnicity.",
Figure 7 shows that the proportion of breast cancer survival compared to death is much higher within the white ethnic group and similar in black Caribbean and Indian ethnicity.,
W NK Oth BC C In BA P BO AO M Ba 0 2000 4000 6000 8000 10000 12000 Treatments taken by Ethnicity Number of Participants Ethnic Groups radiotherapy chemotherapy hormonetherapy cancersurgery Fig.,
Different treatments for breast cancer.,
W NK Oth BC C In BA P BO AO M Ba 0 0.5 1 1.5 2 2.5 3 3.5 x 10 4 Income Quintile for the Ethnic Groups Income Quintile (days) Ethnic Groups 1 (Most Affluent) 2 3 4 5 (Most Deprived) Fig.,
Distribution of income within the patients.,
METHODOLOGY A.,
"Naive Bayes for imputation In order to ﬁll up the ethnicity of the records with unknown ethnicity group, we decided to take a supervised-learning approach.",
The supervised-learning (machine learning) uses known set of input data and response to the data to build a predictor model that will generate predictions for the response to the new set of data.,
Since our dataset mainly consisted of binary and nominal data values we determined that this was a classiﬁcation problem and chose Naive Bayes classiﬁcation algorithm to model our predictor.,
The Naive Bayes algorithm seemed to be the optimal choice despite it having a low predictive accuracy because it handles categorical predictors very well and its speed and memory usage are good for simple distributions.,
More importantly this algorithm is easy to interpret because it is based on ﬁnding the posterior probability W NK Oth BC C In BA P BO AO M Ba 0 500 1000 1500 2000 2500 Average of survival days across for the ethnic groups Average Survival (days) Ethnic Groups Fig.,
Average survival days across the ethnic groups.,
for the new data belonging to the classes given that the features are independent of one another in each class.,
The Naive Bayes classiﬁer involves two stages.,
"The ﬁrst stage is training, where the probabilities of every features’ parameter given each class as well as the probability of each class are estimated.",
These are known as the Likelihood P(X|C) and Class Prior probability P(C) respectively.,
"The second stage is prediction, where the posterior probability algorithm (Eq.",
1) calculates the probability of each class given the parameters of each feature in the new data.,
Finally it predicts the class with the highest posterior probability as the result.,
As the features of our dataset are also assumed to be independent of each other and the class we believe that using the Naive Bayes algorithm will give us the best output.,
P(C|X) = P(X|C)P(C) P(X) .,
(1) B. Parameter tuning The Naive Bayes Classiﬁer supports a number of proba- bility distribution estimates.,
Based on theory the Multivariate Multinomial Distribution is the ideal distribution for us to choose as our dataset consists of categorical features; however we decided to conduct a set of parameter tuning experiments with the different distribution options available in Matlab to observe the legitimacy of the theory.,
"We chose: 1)Normal (Gaussian), 2) Kernel, 3) Multinomial and 4) Multivariate Multinomial and set the class prior probability as uniform for all cases so that the probabilities are equal for all classes.",
"sub- sequently, we decided to choose the best distribution depending on the highest value for accuracy after cross validations.",
C. Cross validation In order to run the cross validation we ﬁrst extracted the records of unknown ethnic group from the original dataset and created the training data with the remaining records.,
We decided to use the K Fold Cross validation process in order to enhance the accuracy of the results and a value of 10 for K seemed ideal for such a large dataset.,
"The 10 fold cross validation involved dividing our training data into 10 sets, then setting aside one set for validation we used the remaining 9 sets to train the Bayesian classiﬁer.",
Then we cross validated the results with the validation set and calculated its accuracy using unbiased F-measure [16].,
This cross validation was computed 10 times where every time a different set was used for validation and then an average of the F-measure percentages are calculated.,
After running this for each of the distribution parameters we chose to use the distribution with the highest accuracy.,
We ran the prediction model for the four distributions we considered.,
D. Performance evaluation The F-measure is a good way to calculate the performance of a prediction model by checking the predicted results against the actual results.,
"The process involves ﬁnding the total number of True Positives (tp), True Negatives (tn), False Positives (fp) and False Negatives (fn) from the result comparison.",
Then ﬁnding the Precision and Recall using the equations in Table III where Precision is the ratio of number of correct results to the number of all returned results and Recall is the ratio of the number of correct results to the number of results that should have been returned [11].,
Finally the unbiased F-measure is then calculated by ﬁnding the harmonic mean of the Precision and Recall rates.,
PERFORMANCE MEASUREMENT METHODS Method Formula Recall Re = T P T P +F N Precision P r = T P T P +F P F-measure F = 2 ∗P r×Re P r+Re E. Imputation Once the dataset was ﬁnally ready to be classiﬁed by the Bayesian model we assigned the previously extracted records of the unknown ethnic group as our testing dataset and keeping all of the remaining data records for training and the Ethnic group feature was assigned as the class label.,
The testing dataset consisted of 18595 records which is around 35% of the original records.,
This actually gives a close 30:70 ratio between the testing and training which is optimal as previ- ously mentioned.,
Once the predicted results were obtained we integrated the unknown records back into the original dataset and replaced the unknown values with the predicted ethnic groups.,
"RESULTS In this section, we present the results for the following: • Cross validation results for considered distributions.",
• Show the effect of ethnicity on the survival rate.,
• Show the impact of age on the survival rate.,
• Show the implication between ﬁnancial status and the survival rate.,
Cross validation Table IV indicates that ﬁtting the Bayesian model with a ’Kernel’ distribution with uniform prior and ’Gaussian’ distribution with no prior as parameters give the most accurate 94.10% and 94.01% results respectively when compared to the other distributions.,
"Therefore, we consider predictions based on kernel distribution for the imputation.",
F-MEASURE % ACCORDING TO EACH DISTRIBUTION.,
Distribution type F-measure % Gaussian with no prior 94.01% Gaussian with uniform prior 48.7% Kernel with uniform prior 94.10% Multinomial with uniform prior 41.98% Multinomial Multivariate with uniform prior 50.80% B.,
Ethnic groups vs survival rates 1) Before imputation: Figure 5 shows the distribution of ethnicity within our original dataset.,
"According to that, the majority of our samples are white women (58%).",
A high percentage (35%) of the samples are from unknown ethnic groups.,
Other ethnic groups exist in small percentages with Bangladesh being the lowest.,
The high number of white women is due to socio-demographic reasons.,
The data was col- lected in Southwest of England where most of the population is of white ethnic group.,
"This is justiﬁed by the data produced by the Ofﬁce for National Statistics census data, UK [17].",
The population in South East England by ethnic group in 2009 contains 90.7% of white ethnicity.,
W NK Oth BC C In BA P BO AO M Ba 0 0.5 1 1.5 2 2.5 3 3.5 x 10 4 Ethnic Group Dataset Population Number of Participants Ethnic Groups Fig.,
Distribution of ethnic groups with the majority of participants being white.,
The distribution of data affects our results due to the unequal number of samples between the ethnic groups.,
"In order to avoid that, we convert the existing numbers to percentages so as to make results more reliable.",
The results for the mortality rate according to ethnicity show that the highest number of people that died of breast cancer is in white ethnic group (Figure 5).,
This does not necessarily mean that this group is more probable to die from breast cancer.,
In order to get the possibility of each ethnic group facing cancer we translate our results in percentages within each group.,
Distribution of ethnic groups with the majority of participants being white.,
"Figure 6 shows that white women have lower mortality rate than black African, mixed and black other.",
W NK Oth BC C In BA P BO AO M Ba 0 0.5 1 1.5 2 2.5 3 3.5 x 10 4 Death from Breast Cancer grouped by Ethnicity Died of breast cancer or not Ethnic Groups Yes No Fig.,
Death and Survival from breast cancer across ethnic groups.,
2) After imputation: Figure 8 shows the predicted values for the unknown ethnicity records and indicates that the majority of them belong to the white ethnic group.,
"Figure 9 depicts, ‘For women aged between 15-64, the percentage of survival from Breast Cancer of those of white ethnicity is likely to be higher than those of black ethnicity’ as the white women are shown to have a 84% survival rate compared to a 77% survival rate for the women belonging to the Black ethnic group.",
"Figure 7 and Figure 10 shows the comparison between the ethnic group distribution before and after prediction, respec- W Oth BC C In BA P BO AO M Ba 0 10 20 30 40 50 60 70 80 90 Percentage of predicted ethnicity on the test data Percentage (%) Ethnic groups Fig.",
Percentage of predicted data for each ethnicity.,
White Asian Black 0 10 20 30 40 50 60 70 80 90 Percentage of Survival by Major Ethnic Groups between 15−64 Years Percentage of Survival Major Ethnic Groups Fig.,
Percentage of survival rate in white is higher than black ethnicity.,
"tively where all unknown records are classiﬁed into the existing ethnic groups based on the predicted percentages obtained for each ethnicity, as shown in Figure 8.",
"Similarly the comparison of numerals before and after imputation is shown in Table V. C. Age vs survival rates Similarly, we produce the results for the relationship be- tween ages and mortality.",
Figure 11 depicts that ages from 50 to 60 had the lowest possibility of dying from breast cancer.,
The highest death possibilities are detected in the ages between 80 and 95.,
Another interesting information is that high death possibilities are detected in the earlier ages of 20-35.,
This might be because younger people are not properly informed or do not visit their doctors in a frequent basis in comparison to older women.,
W NK Oth BC C In BA P BO AO M Ba 0 0.5 1 1.5 2 2.5 3 3.5 4 x 10 4Death from Breast Cancer grouped by Ethnicity after Prediction Died of breast cancer Ethnic Groups Yes No Fig.,
Death from Breast Cancer grouped by Ethnicity after prediction.,
0 5 10 15 20 25 30 35 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Mortality Rate in % Age Group Fig.,
Mortality rate according to age groups.,
D. Income vs survival rate The ﬁnal objective that we are interested is know whether the ﬁnancial status effects the death rate.,
Income feature indi- cates the ﬁnancial status of patients and values from 1(richest) to 5(poorer) are used.,
"In Figure 12, we see that wealthier patients have lower death rates.",
This is probably because they can afford better treatment facilities.,
"V. CONCLUSIONS AND DISCUSSIONS After obtaining and appraising our results, we afﬁrm that the type of dataset to be classiﬁed plays a role in selecting the appropriate distribution type for the Bayesian classiﬁer.",
"Based on our results, kernel distribution has the best F-measure percentage amongst all other distributions.",
"Comparing our results with previous statistical research in [18] and [5], we can conﬁrm that our scientiﬁc objectives are consistent with their ﬁndings.",
"In fact, referring to [6]and [18] 1 2 3 4 5 0 5 10 15 20 25 Number of Cases in % Income Quintile Fig.",
Distribution of income within the patients.,
TABLE V. COMPARISON BETWEEN THE ETHNIC GROUP DISTRIBUTION BEFORE AND AFTER IMPUTATION.,
"With Missing data After Prediction Ethnicity Death Survival Death Survival White 6072.00 25037.00 7184.00 31578.00 Not Known 3385.00 15210.00 0 0 Any Other 230.00 1249.00 986.00 4358.00 Black Caribbean 145.00 507.00 324.00 1156.00 Chinese 14.00 103.00 140.00 1770.00 Indian 113.00 526.00 260.00 1576.00 Black African 95.00 249.00 274.00 595.00 Pakistani 23.00 98.00 92.00 376.00 Black Other 54.00 158.00 147.00 463.00 Asian Other 22.00 148.00 68.00 878.00 Mixed 30.00 83.00 470.00 234.00 Bangladeshi 9.00 33.00 223.00 345.00 approves that white women have higher survival percent- age than black women with 91.4% and 85%, respectively.",
"Similarly, older woman and lower income groups have high mortality rates.",
There always exists some limitations with the data col- lection.,
"In fact, by examining the breast cancer dataset, we can notice a clear imbalanced number of participants between the different ethnic groups where white females were rep- resenting more than half of the population versus very few numbers amongst all other ethnicity.",
This limitation might have certainly misled our prediction results which may explain the low F-measure percentages obtained on other distributions excluding kernel and Gaussian.,
"ACKNOWLEDGEMENT The funding for this work has been provided by Depart- ment of Computing and Centre for Vision, Speech and Signal Processing (CVSSP) - University of Surrey.",
"REFERENCES [1] C. L. Grifﬁths and J. L. Olin, “Triple negative breast cancer: a brief review of its characteristics and treatment options,” Journal of pharmacy practice, vol.",
"319–323, 2012.",
"[2] J. W. Eley, H. A. Hill, V. W. Chen, D. F. Austin, M. N. Wesley, H. B. Muss, R. S. Greenberg, R. J. Coates, P. Correa, C. K. Redmond et al., “Racial differences in survival from breast cancer: results of the national cancer institute black/white cancer survival study,” Jama, vol.",
"947–954, 1994.",
"[3] V. Grann, A.",
"B. Troxel, N. Zojwalla, D. Hershman, S. A. Glied, and J. S. Jacobson, “Regional and racial disparities in breast cancer-speciﬁc mortality,” Social science & medicine, vol.",
"337–347, 2006.",
"[4] S. L. Parker, K. J. Davis, P. A. Wingo, L. A. Ries, and C. W. Heath, “Cancer statistics by race and ethnicity,” CA: a cancer journal for clinicians, vol.",
"31–48, 1998.",
"[5] C. J. Bradley, C. W. Given, and C. Roberts, “Race, socioeconomic status, and breast cancer treatment and survival,” Journal of the National Cancer Institute, vol.",
"490–496, 2002.",
"[6] R. J, “Cancer incidence and survival by major ethnic group, england, 2002 - 2006,” Cancer Research UK, 2009, [Online; accessed 19-May- 2014].",
"[7] D. B. Rubin, Multiple imputation for nonresponse in surveys.",
"John Wiley & Sons, 2004, vol.",
"[8] J. M. Brick and G. Kalton, “Handling missing data in survey research,” Statistical methods in medical research, vol.",
"215–238, 1996.",
"[9] J. L. Schafer, “Multiple imputation: a primer,” Statistical methods in medical research, vol.",
"3–15, 1999.",
"Little and D. B. Rubin, “Statistical analysis with missing data,” 2002.",
"[11] B. G. Tabachnick, L. S. Fidell et al., “Using multivariate statistics,” 2001.",
"[12] P. D. Allison, “Missing data: Quantitative applications in the social sciences,” British Journal of Mathematical and Statistical Psychology, vol.",
"193–196, 2002.",
"[13] S. Sethi and M. E. Seligman, “Optimism and fundamentalism,” Psy- chological Science, vol.",
"256–259, 1993.",
"[14] D. C. Hoaglin and R. E. Welsch, “The hat matrix in regression and anova,” The American Statistician, vol.",
"17–22, 1978.",
"[15] D. Lowd and P. Domingos, “Naive bayes models for probability estimation,” in Proceedings of the 22nd international conference on Machine learning.",
"ACM, 2005, pp.",
"[16] G. Forman and M. Scholz, “Apples-to-apples in cross-validation studies: pitfalls in classiﬁer performance measurement,” ACM SIGKDD Explo- rations Newsletter, vol.",
"49–57, 2010.",
"[17] CODE, “The center for dynamics of ethnicity,” ethnicity.ac.uk, 2009, [Online; accessed 10-May-2014].",
"[18] R. Jack, E. Davies, and H. Møller, “Breast cancer incidence, stage, treatment and survival in ethnic groups in south east england,” British journal of cancer, vol.",
"545–550, 2009.",
"MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion Raktim Kumar Mondol1, Ewan K.A.",
"Millar2, Arcot Sowmya1, and Erik Meijering1,* 1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia 2Department of Anatomical Pathology, NSW Health Pathology, St. George Hospital *Correspondence: erik.meijering@unsw.edu.au Abstract Survival risk stratification is an important step in clinical decision making for breast cancer manage- ment.",
"We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data.",
"It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level.",
"A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy.",
"Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods.",
"This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.",
"Keywords Multimodal Fusion · Breast Cancer · Whole Slide Images · Deep Neural Network · Survival Prediction 1 Introduction The inherent heterogeneity of breast cancer poses challenges for prediction of prognosis and treatment decisions particularly in post-menopausal estrogen receptor positive (ER+) breast cancer, with previous studies reporting conflicting results on the survival difference between Luminal A and B metastatic breast cancer patients [1, 2].",
A common critical clinical dilemma is the se- lection of those early ER+ breast cancer patients at high risk of recurrence who may benefit from the addition of chemotherapy to endocrine therapy.,
"Traditional risk prediction methods, based mainly on clinicopathological factors, may not entirely account for the complex biology of cancer [3].",
"To address this issue, molecular markers and gene expression profiles have emerged as a potential addition to current decision making tools [4].",
"Therefore, combining histopathology image, molecular and clin- icopathological factors (see Figure 1 adapted from [5]) into a single risk prediction model could potentially enhance risk prog- nostication in the clinic [6].",
Here we propose a multimodal risk prediction model for ER+ breast cancer and compare its perfor- mance to existing methods.,
"The significance of our work lies in potentially refining ER+ breast cancer survival risk prediction, facilitating informed therapeutic decision making [7].",
"2 Related Works Breast cancer accounts for about 30% of all cancer cases in women, with varying incidence rates across regions and notably higher rates in developed countries [8].",
"The ER status, specif- ically ER+ breast cancer, which encompasses Luminal A and Luminal B subtypes, is crucial in shaping treatment strategies and predicting prognosis [3].",
"While Luminal A tumours are usually low-grade with a favorable prognosis (Ki-67 < 14%), Luminal B tumours are usually higher grade and pose a higher Patient Organ Tissue Genomics Clinical Information Mammogram RNA Sequence Histopathological Image Figure 1: Multimodal Data for Breast Cancer Characterization: This figure illustrates the diverse data types at multiple biological levels to comprehensively characterize breast cancer.",
"It encom- passes clinical data at the patient level, mammogram images at the organ level, histopathology images at the tissue level, and gene expression data (e.g., RNA Sequencing) at the molecular level, thereby providing a holistic view of the cancer’s nature and behavior.",
"recurrence risk and worse outcome (Ki-67 ≥14%) [3, 9].",
"Tra- ditional prediction models primarily centred on clinical and pathological factors fall short, especially in capturing the intri- cacies and heterogeneity of ER+ breast cancer subtypes [10].",
"Transitioning from traditional methods, early research in sur- vival risk prediction using histopathology images shifted from reliance on hand-crafted features like texture and shape to lever- aging deep learning for automatic extraction of complex, high- level features, resulting in enhanced accuracy [11].",
"In recent developments, multimodal approaches that combine imaging, clinical and molecular data have been shown to enhance predic- tion accuracy [12–17].",
"Comprehensive multimodal approaches, arXiv:2402.11788v1 [cs.CV] 19 Feb 2024 2 Patch Generation Annotation Colour Transformation Tumour Expert Pathologist QuPath Annotation Tool Whole Slide (ER+ Sample) Figure 2: Preprocessing Methodology for Histopathology Images: Whole Slide Images (WSIs) are first annotated using the QuPath annotation tool by expert pathologists.",
"From these annotated regions, non-overlapping patches are systematically extracted.",
"Subsequently, a color transformation is applied to ensure uniformity in color representation across datasets.",
"especially those that fuse whole slide images, clinical data and genetic information, could pave the way for superior predictive models.",
"3 Data Preparation Our study uses the hematoxylin and eosin (H&E)-stained formalin-fixed paraffin-embedded (FFPE) digital slides of The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset [18], sourcing 249 whole-slide images (WSIs) from the Ge- nomic Data Commons (GDC) Portal with a focus on Luminal A (149 samples) and Luminal B (100 samples) molecular sub- types.",
"Slide annotations were obtained manually by an expert breast pathologist using QuPath [19], prioritising tumour local- isation including stroma and tumour-infiltrating lymphocytes (TILs) and excluding necrotic areas.",
"H&E-stained tissues un- derwent downsampling to a resolution of 0.25 µm/pixel, with tissue masks created to exclude artifacts and non-tissue sections, and tumour regions split into 224 × 224 pixel patches.",
"To coun- teract staining inconsistencies, singular value decomposition- based normalisation was applied on the images (See Figure 2) using Macenko method [20].",
"Furthermore, we processed RNA- sequencing data with RSEM and selected PAM50 genes for anal- ysis, given their significance in breast cancer subtyping [21, 22].",
"Pertinent clinical variables, encompassing tumour grade, size, patient age and lymph node status, were meticulously prepared to enhance the predictive accuracy of the subsequent deep learn- ing model evaluations.",
"For experimental purposes, we adopted a five-fold cross-validation strategy, allocating 200 samples for training and 50 for validation while maintaining the distribution of Luminal subtypes and survival statuses.",
4 Proposed Method 4.1 Histopathological Feature Extraction We utilised vision transformers (ViTs) and convolutional neu- ral networks (CNNs) to extract features from histopathological images.,
"Specifically, the MaxViT model, a prominent vision transformer, was employed for its effectiveness in capturing global image information and leveraging self-attention for com- prehensive tumour analysis [23].",
"For the purpose of comparison, ResNet50, a CNN-based model, was selected to extract local patterns within the images [24].",
"4.2 Patch-to-Patient Aggregation Using Self-Attention In our framework, the self-attention mechanism crucially ag- gregates patch-level features into a holistic patient-level repre- sentation.",
"Notably, the same image patches act as the Key (K), Query (Q), and Value (V) within the attention paradigm, aiming to discern intricate interdependencies among patches from a singular histopathological image.",
"Mathematically, self-attention processes a sequence of image patches K1, Q1, V1, .",
", Kn, Qn, Vn to yield a patient-level fea- ture vector y.",
"The mechanism computes a weighted sum of the Value vectors V j based on attention scores derived from the scoring function s(Qi, K j): yi = n X j=1 softmax  s(Qi, K j)  Vj.",
"(1) In this formulation, the softmax function normalises scores, facilitating the model’s emphasis on specific patches.",
"The re- sultant yi is a weighted blend of patch-level Value vectors V j, representing the patient-level feature.",
Two primary advantages emerge: 1.,
"Inherent Feature Importance: Using image patches for Key, Query and Value enables the model to natu- rally highlight significant regions in histopathological images, capturing inherent sample heterogeneity.",
"Inter-Patch Contextualisation: The mechanism grasps the context among distinct patches, contextual- ising each patch with respect to others for a compre- hensive patient-level representation.",
"The enriched patient-level vector, emphasising both context and focus, is pivotal for subsequent survival risk prediction tasks.",
of Patches (N) × No.,
of Features (F)] [ Each Patient × No.,
of Embedding (E)] .,
500 × 3 × 224 ×224 500 × 512 Preprocessed Histopathology Image Skip Connection Pretrained Feature Extractor: MaxViT Key Query Value Softmax = Matrix Multiplication Aggregated Output Self-Attention Module Output Image Features (Patient Level) Image Patches Average Pooling ...,
Figure 3: The proposed MM-SurvNet architecture.,
"It utilises a pretrained MaxViT to extract features from histopathology images, which are then processed through a self-attention network to aggregate patch-level features into a patient-level representation.",
Table 1: Performance Comparison using C-index on TCGA Data.,
CV Fold Multimodal (Imaging+Genetic+Clinical) Imaging+Genetic Imaging Clinical MaxViT ResNet50 MaxViT ResNet50 MaxViT ResNet50 CoxPH 1 0.70 0.64 0.72 0.61 0.45 0.47 0.50 2 0.57 0.49 0.44 0.58 0.47 0.47 0.24 3 0.66 0.62 0.63 0.65 0.55 0.57 0.34 4 0.62 0.55 0.50 0.48 0.60 0.60 0.57 5 0.70 0.50 0.57 0.51 0.62 0.65 0.71 Mean 0.64 ± 0.06 0.56 ± 0.06 0.57 ± 0.10 0.56 ± 0.06 0.53 ± 0.08 0.55 ± 0.08 0.47 ± 0.17 4.3 Multimodal Fusion Using Dual Cross-Attention Fusing data from diverse sources is paramount in multimodal learning.,
"In our architecture, we deploy a dual cross-attention mechanism to achieve a refined interaction between histopathol- ogy image features and genetic expression profiles.",
"Specifically, this encompasses two attention operations: 1.",
Image features serve as the Query (Q) while genetic features act as both the Key (K) and Value (V).,
"Conversely, genetic features form the Query, with im- age features becoming the Key and Value.",
"Let I = {i1, i2, .",
", in} be the set of image feature vectors and G = {g1, g2, .",
", gm} denote the genetic feature vectors.",
"The enriched feature vectors for the kth image and genetic patches are: yI k = m X j=1 softmax  s(ik, g j)  g j, yG k = n X i=1 softmax (s(gk, ii)) ii.",
"Here, the scoring function s(.)",
gauges the relevance between Queries and Keys.,
"These weighted sums, after being normalised by the softmax function, are then concatenated to form a unified feature representation: yk = Concatenate(yI k, yG k ) The dual cross-attention offers two primary advantages: 1.",
Bidirectional Contextualisation: The mechanism captures intermodal dependencies in both directions— image to genetic and vice versa—providing a holistic contextualised representation.,
"Enhanced Selective Attention: By alternating the role of Query, the architecture gleans specific features from both modalities that are crucial for each other, ensuring compact and informative fused feature vectors.",
The resultant vectors yk are thus contextually enriched and opti- mised for tasks such as survival risk stratification.,
4.4 MM-SurvNet Model Training The MM-SurvNet model (Fig.,
3) employs either a pretrained Vision Transformer (MaxViT) or a CNN (ResNet50) for 4 [ Each Patient × Image Embedding (E)] Key Query Value Softmax = Matrix Multiplication Dual Cross Attention Module [ Each Patient × No.,
of Genes (PAM50)] Key Query Value Softmax + + = Concatenation 1 ... ≈≈ ... ≈≈ Fully-connected 128 ... ≈≈ ... ≈≈ Risk Score 256 32 + 4 [ Each Patient × Clinical Features (4)] Image as Query Gene as Query + 2 Fully-connected Figure 4: The proposed MM-SurvNet architecture employs a multimodal deep learning framework for survival risk prediction in cancer patients.,
Image embedding is concatenated with genetic data using dual cross-attention mechanisms and further integrated with clinical data in the final layer.,
histopathological image feature extraction.,
The multimodal data is subsequently incorporated into the MM-SurvNet model (Fig.,
4) to enhance prediction accuracy.,
"The Adam optimiser with decoupled weight decay (AdamW) with a learning rate of 0.001 and a mini-batch size of 12 governs the optimisation, which includes an early stopping criterion set at a patience of 5 and a maximum of 150 epochs [25].",
"The primary loss function is the negative log-likelihood (NLL), aligning with our survival analysis objectives, and optimisation targets the concordance index (C-index) as the key performance metric[26].",
"The NLL loss is calculated as: Loss = − P i  log(hi) −log P j∈Ri exp(log(hj))  · di P i di (2) where h = exp(log(h)) are the hazards, R is the risk set, and d represents the event.",
"The optimisation, subject to early stopping criteria, ranged from 20 to 50 minutes on an NVIDIA Tesla V100 32GB GPU.",
"The best model weights, based on the minimal NLL loss (2) were retained for testing.",
"5 Experimental Results In our study on survival risk stratification for ER+ breast can- cer patients, we integrated multimodal data—histopathological images, genetic information, and clinical data—resulting in en- hanced predictive accuracy.",
"Our method achieved a notable C-index of 0.64, superior to models leveraging a single data modality (Table 1).",
"For instance, models based on histopatho- logical image data only or on clinical information only, yielded C-indices of 0.53 and 0.47, respectively.",
"An ablation study (Ta- ble 1) highlighted the significance of our holistic, multimodal approach.",
"When integrating all feature sets, the MaxViT model Figure 5: Mean survival curves aggregated from all five cross- validation (CV) tests with 95% confidence intervals.",
"In the depicted curves, the log-rank test p<0.05.",
"surpassed ResNet50, emphasising the advantages of combining clinical, imaging and genetic data for more nuanced and accurate risk predictions.",
Kaplan-Meier survival analysis (Fig.,
"5) further demonstrates our method’s efficacy in differentiating outcomes between risk groups (log-rank test p<0.05), with an Integrated Brier Score of 0.11 for 5 years and 0.16 for 10 years [27].",
6 Discussion The findings of our study assessing multimodal data integra- tion for survival risk stratification in ER+ breast cancer patients underscore the superiority of multimodal approaches.,
"Our pro- 5 posed model, amalgamating histopathological images, genetic data and clinical parameters, achieved a significant C-index of 0.64, outshining single-modal counterparts.",
"Specifically, the MaxViT architecture consistently surpassed ResNet50 in han- dling this multimodal data.",
"While the Kaplan-Meier analysis reinforced our model’s aptitude in patient risk stratification, there was notable performance variability across cross-validation folds, suggesting potential dataset nuances or imbalances.",
"More- over, the traditional CoxPH model’s C-index of 0.47 highlighted the advantages of deep learning in capturing cancer prognosis’ intricate nature.",
"The analysis not only substantiated our model’s proficiency in stratifying patients but also highlighted significant survival differences between the high-risk and low-risk groups, as revealed in the survival curve plots.",
Future work will encompass broader data types and probe the model’s utility across diverse cancer subtypes.,
"While our study yielded promising results, it suffers from certain limitations.",
"Our sample size, though adequate for the current modeling exercise, might benefit from expansion to ensure broader generalisability.",
The study’s concentration on ER+ breast cancer means that its applicability to other cancer subtypes remains as yet untested.,
The variability in performance across different cross-validation folds suggests potential dataset imbalances or unidentified influ- encing factors.,
"Lastly, the model’s dependence on three types of data may pose challenges in settings where one or more of these data modalities is unavailable.",
7 Conclusion Our study highlights the significance of multimodal data inte- gration in advancing survival risk stratification for ER+ breast cancer patients.,
This integrative methodology enhances predic- tion accuracy and sets the foundation for personalised treatment strategies in oncology.,
"Compliance with Ethical Standards This research study was conducted retrospectively using hu- man subject data made available in open access by The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset, accessible through the National Cancer Institute’s Genomic Data Commons (GDC) portal.",
"Ethical approval was not required for this study, in accordance with the ethical policies set forth by The Cancer Genome Atlas program.",
Conflicts of Interest No funding was received for conducting this study.,
The authors have no relevant financial or non-financial interests to disclose.,
"Acknowledgement This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI), which is supported by the Australian Government.",
"Addi- tionally, data preprocessing was performed using the computa- tional cluster Katana, which is supported by Research Technol- ogy Services at UNSW Sydney.",
"References [1] H. Lindman, F. Wiklund, and K. K. Andersen, “Long-term treat- ment patterns and survival in metastatic breast cancer by intrinsic subtypes—an observational cohort study in Sweden,” BMC Can- cer, vol.",
"22, p. 1006, 2022.",
"[2] Y. Han, J. Wang, and B. Xu, “Clinicopathological characteristics and prognosis of breast cancer with special histological types: A surveillance, epidemiology, and end results database analysis,” The Breast, vol.",
"114–120, 2020.",
"[3] C. Shuai, F. Yuan, Y. Liu, C. Wang, J. Wang, and H. He, “Estrogen receptor—positive breast cancer survival prediction and analysis of resistance–related genes introduction,” PeerJ, vol.",
"9, p. e12202, 2021.",
"[4] X. Li, L. Liu, G. J. Goodall, A. W. Schreiber, T. Xu, J. Li, and T. D. Le, “A novel single-cell based method for breast cancer prognosis,” PLoS Computational Biology, vol.",
"16, p. e1008133, 2020.",
"[5] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multi- modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics,” 11 2021.",
"[6] C. Nero, F. Ciccarone, A. Pietragalla, S. Duranti, G. Daniele, G. Scambia, and D. Lorusso, “Adjuvant treatment recommenda- tions in early-stage endometrial cancer: What changes with the introduction of the integrated molecular-based risk assessment,” Frontiers in Oncology, vol.",
"11, p. 612450, 2021.",
"[7] W. Guo, W. Liang, Q. Deng, and X. Zou, “A multimodal affin- ity fusion network for predicting the survival of breast cancer patients,” Frontiers in Genetics, vol.",
"12, p. 709027, 2021.",
"B. Shvetsov, L. R. Wilkens, K. K. White, M. Chong, A. Buyum, G. Badowski, R. T. L. Guerrero, and R. Novotny, “Prediction of breast cancer risk among women of the Mariana Islands: The BRISK retrospective case—control study,” BMJ Open, vol.",
"12, p. e061205, 2022.",
"[9] K. Holli-Helenius, A. Salminen, I. Rinta-Kiikka, I. Koskivuo, N. Brück, P. Boström, and R. Parkkola, “MRI texture analysis in differentiating luminal A and luminal B breast cancer molecular subtypes—a feasibility study,” BMC Medical Imaging, vol.",
"17, p. 69, 2017.",
"[10] K. Yao, E. Schaafsma, B. Zhang, and C. Cheng, “Tumor cell in- trinsic and extrinsic features predict prognosis in estrogen receptor positive breast cancer,” PLOS Computational Biology, vol.",
"1–22, 03 2022.",
"[11] S. C. Wetstein, V. M. d. Jong, N. Stathonikos, M. Opdam, G. M. H. E. Dackus, J. P. W. Pluim, P. J. v. Diest, and M. Veta, “Deep learning-based breast cancer grading and survival analysis on whole-slide histopathology images,” Scientific Reports, vol.",
"[12] T. Wei, X. Yuan, R. Gao, L. J. Johnston, J. Zhou, Y. Wang, W. Kong, Y. Xie, Y. Zhang, D. Xu, and Z. Yu, “Survival predic- tion of stomach cancer using expression data and deep learning models with histopathological images,” Cancer Science, vol.",
"690–701, 2022.",
"[13] L. Chen, H. Zeng, X. Yu, Y. Huang, Y. Luo, and X. Ma, “Histopathological images and multi-omics integration predict molecular characteristics and survival in lung adenocarcinoma,” Frontiers in Cell and Developmental Biology, vol.",
"[14] X. Wu, Y. Shi, M. Wang, and A. Li, “CAMR: cross-aligned multimodal representation learning for cancer survival prediction,” Bioinformatics, vol.",
"39, p. btad025, 2023.",
"He, B. Hu, C. Zhu, W. Xu, X. Hao, B. Dong, X. Chen, Q. Dong, and X. Zhou, “A novel multimodal radiomics model for predicting prognosis of resected hepatocellular carcinoma,” Frontiers in Oncology, vol.",
"12, p. 745258, 2022.",
"6 [16] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multimodal fusion using sparse CCA for breast cancer survival prediction,” IEEE International Symposium on Biomedical Imaging, pp.",
"1429– 1432, 2021.",
"[17] R. Vanguri, J. Luo, A. Aukerman, J. V. Egger, C. J. Fong, N. Hor- vat, A. Pagano, J. d. A.",
"B. Araújo-Filho, L. Geneslaw, H. Rizvi, R. E. Sosa, K. M. Boehm, S. Yang, F. M. Bodd, K. Ventura, T. J. Hollmann, M. S. Ginsberg, J. Gao, M. D. Hellmann, J. L. Sauter, and S. P. Shah, “Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer,” Nature Cancer, vol.",
"1151–1164, 2022.",
"[18] W. Lingle, B. J. Erickson, M. L. Zuley, R. Jarosz, E. Bonac- cio, J. Filippini, J. M. Net, L. Levi, E. A. Morris, G. G. Figler, P. Elnajjar, S. Kirk, Y. Lee, M. Giger, and N. Gruszauskas, “The cancer genome atlas breast invasive carcinoma collection (TCGA- BRCA),” The Cancer Imaging Archive, 2016.",
"[19] P. Bankhead, M. B. Loughrey, J.",
"A. Fernández, Y. Dombrowski, D. G. McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray, H. G. Coleman, J.",
"A. James, M. Salto-Tellez, and P. W. Hamil- ton, “QuPath: Open source software for digital pathology image analysis,” Scientific Reports, vol.",
"7, p. 16878, 2017.",
"[20] M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley, X. Guan, C. Schmitt, and N. E. Thomas, “A method for normalizing histology slides for quantitative analysis,” IEEE International Symposium on Biomedical Imaging (ISBI), pp.",
"1107– 1110, 2009.",
"[21] B. Li and C. N. Dewey, “RSEM: Accurate transcript quantification from RNA-Seq data with or without a reference genome,” BMC Bioinformatics, vol.",
"12, p. 323, 2011.",
"[22] J. S. Parker, M. E. Mullins, M. C. Cheang, S. Leung, V. David, T. L. Vickery, S. R. Davies, C. Fauron, X.",
"He, Z. Hu, J. Quacken- bush, I. J. Stijleman, J. Palazzo, J. S. Marron, A.",
"B. Nobel, E. R. Mardis, T. O. Nielsen, M. J. Ellis, C. M. Perou, and P. S. Bernard, “Supervised risk predictor of breast cancer based on intrinsic sub- types,” Journal of Clinical Oncology, vol.",
"1160–1167, 2009.",
"[23] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li, “MaxViT: Multi-axis vision transformer,” European Con- ference on Computer Vision (ECCV), p. 459–479, 2022.",
"[24] A. D. Jones, J. P. Graff, M. A. Darrow, A. D. Borowsky, K. Ol- son, R. Gandour-Edwards, A. Mitra, D. Wei, G. Gao, B. Durbin- Johnson, and H. H. Rashidi, “Impact of pre-analytical variables on deep learning accuracy in histopathology,” Histopathology, vol.",
"39–53, 2019.",
"[25] I. Loshchilov and F. Hutter, “Decoupled weight decay regular- ization,” International Conference on Learning Representations (ICLR), 2019.",
"[26] E. Longato, M. Vettoretti, and B.",
"Di Camillo, “A practical per- spective on the concordance index for the evaluation and selection of prognostic time-to-event models,” Journal of Biomedical Infor- matics, vol.",
"108, p. 103496, 2020.",
"[27] K. J. Jager, P. C. van Dijk, C. Zoccali, and F. W. Dekker, “The analysis of survival data: The Kaplan–Meier method,” Kidney International, vol.",
"560–565, 2008.",
"Raktim Kumar Mondol is a PhD candidate in Computer Science and Engineering, spe- cializing in computer vision and bioinformat- ics.",
"He completed his MEng in Engineering with High Distinction from RMIT University, Australia.",
"Mondol’s research interests in- clude histopathological image analysis, clini- cal prognosis prediction, and enhancing clini- cal understanding through the interpretability of computational models.",
Ewan Millar is a Senior Staff Specialist Histopathologist with NSW Health Pathology at St George Hospital Sydney with expertise in breast cancer pathology and translational research and a strong interest in AI and digi- tal pathology applications.,
"Arcot Sowmya is Professor in the School of Computer Science and Engineering, UNSW.",
"Her major research interest is in the area of Machine Learning for Computer Vision and includes learning object models, fea- ture extraction, segmentation and recogni- tion based on computer vision, machine learning and deep learning.",
"In recent years, applications in the broader health area are a focus, including biomedical informatics and rapid diagnostics in the real world.",
"All of these areas have been supported by com- petitive, industry and government funding.",
"Erik Meijering (Fellow, IEEE), is a Profes- sor of Biomedical Image Computing in the School of Computer Science and Engineer- ing.",
His research focusses on the develop- ment of innovative computer vision and ma- chine learning (in particular deep learning) methods for automated quantitative analysis of biomedical imaging data.,
"BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion Raktim Kumar Mondol1, Ewan K.A.",
"Millar2, Arcot Sowmya1, and Erik Meijering1,* 1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia 2Department of Anatomical Pathology, NSW Health Pathology, St. George Hospital *Correspondence: erik.meijering@unsw.edu.au Abstract Breast cancer is a significant health concern affecting millions of women worldwide.",
Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes.,
"Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to obtain a holistic profile and achieve survival risk stratification of ER+ breast cancer patients.",
We employ multiple self-supervised feature extractors (DINO and MoCoV3) pretrained on histopathological patches to capture detailed image features.,
These features are then fused by a variational autoencoder and fed to a self-attention network generating patient-level features.,
"A co-dual-cross-attention mechanism combines the histopathological features with genetic data, enabling the model to capture the interplay between them.",
"Additionally, clinical data is incorporated using a feed-forward network, further enhancing predictive performance and achieving comprehensive multimodal feature integration.",
"Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge.",
"Our model achieves a mean concordance index of 0.77 and a time- dependent area under the curve of 0.84, outperforming state-of-the-art methods.",
"It predicts risk (high versus low) with prognostic significance for overall survival in univariate analysis (HR=2.99, 95% CI: 1.88–4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80–4.68, p<0.005).",
"Keywords Multimodal Fusion · Breast Cancer · Whole Slide Images · Deep Neural Network · Survival Prediction 1 Introduction Breast cancer poses a significant global health concern, with a high incidence rate and substantial impact on morbidity and mortality [1, 2].",
"The incidence of breast cancer varies across different regions and populations, with higher rates observed in developed countries [1].",
"The prevalence of breast cancer in Australia, affecting 1 in 8 women up to the age of 85, is a cause for concern due to its rising incidence rate over the past decade [3].",
"This trend highlights the crucial need for accurately predicting survival risks to identify high-risk patients who may benefit from more intensive treatment or monitoring, thereby potentially improving outcomes [2].",
"In breast cancer, the estrogen receptor (ER) status plays a critical role in determining treatment strategies and predicting patient prognosis.",
"ER+ breast cancer, which includes Luminal A and Luminal B subtypes, is distinguished by the presence of ERs on cancer cells, making it responsive to hormonal therapies such as tamoxifen [4].",
"While Luminal A tumours are usually low-grade with a favorable prognosis (Ki-67<14%), Luminal B tumours are usually higher grade and pose a higher recurrence risk and worse outcome (Ki-67≥14%) [4, 5].",
"The inherent heterogeneity of breast cancer poses challenges for prediction of prognosis and treatment decisions, particularly in post-menopausal ER+ breast cancer, with previous studies reporting conflicting results on the survival difference between Luminal A and B metastatic breast cancer patients [6, 7].",
A common critical clinical dilemma is the selection of those early ER+ breast cancer patients at high risk of recurrence who may benefit from the addition of chemotherapy to endocrine therapy.,
"Therefore, accurate survival risk prediction models specifically tailored for ER+ breast cancer are essential for personalised treatment decisions.",
"Traditional methods for survival risk prediction often rely on clinicopathological risk factors (such as age, tumour size, grade, lymph node metastasis and clinical stage), which may fail to fully capture the complex biology of cancer [4, 7–10].",
"To ad- dress this issue, molecular markers and gene expression profiles have been identified as potential prognostic factors that provide valuable insights into tumour biology and potential therapeu- tic targets [11–15].",
"Over the past decade, the integration of genomic testing into treatment decision-making processes has been enhanced by the utilisation of several commercial gene panels, such as Prosigna/PAM50, OncotypeDx and Endopredict among others.",
"In addition, histopathological imaging, which offers in-depth insights into the cellular and tissue characteristics of tumours, plays a vital role in both the diagnosis and prognosis of breast cancer [16].",
"However, to address the varied nature of the disease effectively, it is essential to consider all available data modalities.",
"Therefore, integrating imaging, genetic and clini- copathological information into a single risk prediction model could potentially enhance risk prognostication in the clinic [9].",
"In this study, we propose a novel multimodal survival risk pre- diction model that significantly enhances the prognosis of ER+ arXiv:2402.10717v2 [cs.CV] 3 Jun 2024 2 Patient Details Medical Tests Treatment Diagnosis Age Group: Post Menopausal Gender: Female Ethnicity: Caucasian Clinical Exam Imaging Core Biopsy Breast-Conserving Surgery (Lumpectomy) Tumor Size: T2 Tumor Grade: G2 Hormone Receptor Status: ER+ HER2 Status: HER2- Lymph Node Status: LN- Cell Proliferation: Ki67 20% Genetic Testing PAM50/ROR, OncotypeDX etc.",
Endocrine Therapy & Radiotherapy High Risk Low Risk Adjuvant Chemotherapy Chemotherapy Not Required Multimodal Risk Prediction Invasive Nature: IDC Ensures Adequate Treatment Avoids Excessive Treatment & Toxicity Figure 1: Illustration of the clinical management pathway in treating breast cancer patients.,
"This example concerns a postmenopausal patient who has been diagnosed with breast cancer, specifically invasive ductal carcinoma (IDC).",
"The process begins with an initial diagnosis through clinical examination, imaging and core biopsy.",
"Following this, surgery is performed to completely excise the tumour, and postoperative tumour histopathological classification is performed to assess key factors including tumour size (e.g.",
"T2), grade (e.g.",
"G2), hormone receptor status (e.g.",
"ER+), HER2 status (e.g.",
"HER2-), lymph node status (e.g.",
"LN-), and proliferation index (e.g.",
Subsequent treatments may include hormone therapy and radiotherapy.,
"Additional molecular tests, like genetic testing, are utilised to determine specific cancer molecular subtypes and further assess risk of recurrence.",
The proposed final step in this pathway is the application of our BioFusionNet model.,
"This model combines tumour characteristics, pathology and genetic testing to determine high and low risk patients, thereby guiding personalised treatment decisions and efficiently preventing both under-treatment and over-treatment.",
"For example, low-risk patients might undergo lumpectomy with hormone therapy and radiotherapy, whereas high-risk patients are advised to have chemotherapy in addition to these treatments.",
"Whilst this pathway mirrors current clinical practice, our study streamlines the integration of all available critical data to derive an automated single risk prediction score.",
"breast cancer by integrating histopathology images, genetic pro- files, and clinical data.",
"By combining these data modalities, our model captures the complex interplay between cellular, molec- ular, and clinical factors, thereby improving its predictive ac- curacy.",
"Moreover, we introduce a weighted Cox loss function specifically designed to handle imbalanced survival data, further enhancing the model’s performance.",
"The model is thoroughly evaluated using metrics such as the concordance index (C-index) and time-dependent area under the curve (AUC) score, and its performance is compared to existing methods to establish its efficacy.",
The model’s ability to accurately predict survival risks and categorise patients into distinct risk groups has the potential to inform personalised treatment decisions and improve patient outcomes.,
"Finally, we provide explainable analyses to eluci- date the influence of different genes and clinical factors on risk prediction, offering valuable insights into the underlying predic- tive mechanisms.",
The main contributions of this study are as follows: 1.,
"A novel multimodal survival risk prediction model in- tegrating histopathology images, genetic profiles, and clinical data for ER+ breast cancer prognosis.",
"A weighted Cox loss function designed to handle im- balanced survival data, improving predictive accuracy.",
Explainable analyses providing insights into the influ- ence of genes and clinical factors on risk prediction.,
"2 Background Cancer risk prediction is of paramount importance due to its potential for guiding personalised screening, prevention and treatment strategies, ultimately leading to improved patient out- comes and reduced mortality (Fig.",
"This approach is espe- cially vital in the context of ER+ breast cancer, where accurate risk prediction is essential for identifying individuals at higher recurrence risk.",
These patients may benefit from more aggres- sive treatments like chemotherapy [17–20].,
"On the other hand, risk models are equally critical in recognising lower-risk pa- tients, potentially sparing them from unnecessary treatments and their side effects [21].",
"Online algorithms such as Predict1 and Adjuvant2 are used clinically to estimate the risk of recurrence and the benefit of adding chemotherapy to endocrine therapy, which is a major treatment dilemma.",
"Developments in deep learning, such as the Cox proportional hazards deep neural network, have revolutionised cancer re- search by improving survival data modeling and treatment rec- ommendation systems [22].",
"Multimodal data fusion, which combines information from diverse sources such as imaging, ge- nomics and clinical data, has gained attention in cancer research due to its ability to provide a comprehensive understanding of the disease and improve predictive outcomes [23–25].",
"Cross- attention transformer mechanisms that integrate histopatholog- ical images and genomic data capture complementary infor- mation from different modalities, leading to improved survival prediction [26].",
"In this evolving landscape, models such as MultiDeepCox- SC [27], MCAT [28], MultiSurv[29], HFBSurv [30], Pathomic Fusion [31], and TransSurv [32] exemplify significant progress in multimodal analysis.",
These models harness unique strategies to integrate diverse data types for enhanced survival prediction.,
"MultiDeepCox-SC combines histopathological image-derived 1https://breast.predict.nhs.uk/tool 2https://oncoassist.com/adjuvant-tools/ 3 risk scores with clinical data and gene expression through the Cox proportional hazards (CoxPH) model, providing a detailed risk assessment.",
"MCAT utilises a genomic-guided co-attention layer to map relationships between whole slide images and genomic features, enhancing interpretability in computational pathology.",
MultiSurv simplifies the integration of multimodal data by merging feature vectors into a unified representation for predicting survival probabilities.,
"In contrast, HFBSurv applies an hierarchical framework with attentional factorised bilinear modules, systematically processing information from lower to higher complexity levels.",
"Pathomic Fusion adopts a gating- based attention mechanism, effectively filtering out noise to focus on salient features across modalities.",
"Finally, TransSurv employs cross-attention transformers to combine histopatholog- ical and genomic data, capturing complementary information that significantly improves predictive accuracy.",
"Despite these advances, the integration of multimodal data, marked by its inherent heterogeneity and dimensional variabil- ity, remains a significant challenge, highlighting the need for advanced methodologies to effectively integrate and leverage diverse data sources for robust survival risk assessment [33–35].",
"To address this, BioFusionNet introduces a unique strategy by combining self-supervised learning models, specifically DINO and MoCoV3, for feature extraction.",
This method is further enhanced by a co-dual-cross-attention mechanism for effective multimodal fusion.,
"The co-attention component facilitates syn- chronised learning from multiple data types by highlighting mutually informative features, whereas the dual cross-attention mechanism provides a deeper interaction layer, allowing for more effective integration of these features.",
"Additionally, Bio- FusionNet mitigates issues of data imbalance through the imple- mentation of a weighted Cox loss function.",
"This comprehensive approach to multimodal fusion marks a significant advancement in predicting survival risks for ER+ breast cancer, addressing notable gaps in current research.",
2.1 Data Collection Our study used hematoxylin-and-eosin-stained (H&E) formalin- fixed paraffin-embedded (FFPE) digital slides from The Can- cer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA).,
Whole-slide images (WSIs) from the TCGA-BRCA data col- lection were downloaded from the GDC Portal (accessed 25 August 2023).,
"In this work, we chose a subset of 249 cases from the TCGA-BRCA dataset, in order to maintain the propor- tions of Luminal A and B subtypes as well as the proportion of survival events within each subtype.",
"Among the 249 cases, 83 had survival events, while the remaining cases were censored.",
The survival events were distributed as follows: 54 events in the Luminal A subtype and 29 events in the Luminal B sub- type.,
"For each subtype, we selected cases such that the total was about three times the number of survival events, resulting in 149 Luminal A cases and 100 Luminal B cases.",
"By preserving these proportions, we obtained a subset representative of key characteristics in the full dataset.",
"Additionally, we obtained transcriptome-wide RNA-sequencing data representing mRNA expression levels for a total of 20,438 genes in the reference genome from the TCGA dataset.",
These data were processed using RNA-sequencing by expectation maximisation (RSEM) and were downloaded from the cBioPortal platform[36].,
"This dataset included a range of clinical information for each pa- tient, such as tumour grade, tumour size, lymph node status, age at diagnosis and molecular subtypes.",
"Overall, patients who had WSIs, RNA-sequencing and clinical data available were included in the study.",
2.2 Data Preparation 2.2.1 Slide Annotation An expert breast pathologist manually annotated the selected slides using QuPath [37].,
"The annotation was performed for localisation of the tumour outline, excluding any necrosis but including stroma and tumour infiltrating lymphocytes (TILs).",
The pathologist was blinded to any molecular or clinical features during annotation.,
"2.2.2 Image Data Preparation The images were first downsampled to 0.25 µm/pixel, corre- sponding to approximately 40× magnification.",
"The annotated tumour regions were processed semi-automatically with QuPath to create 224×224-pixel patches, resulting in approximately 500 nonoverlapping patches per sample.",
"To address staining inconsistencies, vector-based colour normalisation was applied [38].",
"2.2.3 RNA-Sequencing Data Preparation From the extensive set of 20,438 genes, we selected genes featured in various commercial assays, namely Oncotype DX, Mammaprint, Prosigna (PAM50), EndoPredict, BCI (Breast Cancer Index), and Mammostrat [39–44], as these are the most relevant genetic markers to our study’s objectives.",
This resulted in a subset of 138 genes.,
"The RNA-sequencing data obtained from the TCGA dataset had already undergone processing using RSEM, and no further normalization was applied to the gene expression values.",
"2.2.4 Clinical Data Preparation From the clinical data, we selected variables based on their es- tablished relevance in breast cancer prognosis and treatment outcomes [45–48].",
"Specifically, we included tumour grade (cat- egorised as grade 1&2 versus grade 3), tumour size (>20 mm versus ≤20 mm), patient age (>55 versus ≤55) and lymph node status (positive versus negative).",
The decision to binarise the clinical data was made to facilitate CoxPH analysis.,
Binarisation also simplifies both univariate and multivariate hazard analyses and makes it easier to understand how each clinical factor affects the chance of survival.,
"2.3 Proposed Model The proposed deep learning model, which we call BioFusion- Net, is an innovative feature extraction and multimodal fusion framework designed to leverage and integrate diverse data types, including histology images, genomic features and clinical data for enhanced cancer outcome prediction (Figs.",
"The essence of BioFusionNet lies in its capability to fuse these data modalities into a cohesive tensor representation, effectively cap- turing both bimodal and trimodal interactions.",
This approach is aimed at surpassing the performance of traditional unimodal and existing multimodal representations in survival risk prediction.,
"2.3.1 Feature Extraction Using DINO and MoCoV3 Histopathological images, rich in phenotypic information, are pivotal for understanding cancer pathology.",
BioFusionNet 4 [Each Patient × Image Embedding (256)] .,
Output Image Features (Patient Level) [No.,
"of Patches (N) × Image Embedding (256)] 500 × 3 × 224 ×224 Preprocessed Histopathology Images Skip Connection Image Patches ... Key Query Value Softmax Aggregated Output Self-Attention Module MoCoV3 Pretrained on 15M Histo Patches (Multiple Datasets) DINO Pretrained on 2M Histo Patches (TCGA-BRCA) DINO Pretrained on 33M Histo Patches (Multiple Datasets) + Encoder Decoder σ μ Latent Space (z) Total Loss = Reconstruction Loss (MSE) + KL Divergence 500 × 384 500 × 384 500 × 1152 500 × 1152 500 × 384 Variational Autoencoder (VAE) SSL Base Model ViT Small (Patch16) + = Concatenation = Mean = Standard Deviation 500 × 256 Feed-Forward Network Encoder Decoder = Matrix Multiplication σ μ Legend: Figure 2: BioFusionNet Stage 1: The proposed model integrates self-supervised image feature extraction methods, namely DINO and MoCoV3, pretrained on three distinct datasets.",
Features are concatenated and fed to a Variational AutoEncoder (VAE).,
"Subsequently, the latent space of the VAE is utilised to feed a self-attention network, which aggregates patch-level features into a comprehensive patient-level representation.",
"utilises two advanced self-supervised learning models, DINO (self-DIstillation with NO labels) and MoCoV3 (Momentum Contrast version 3), both based on the Vision Transformer (ViT) architecture, to extract morphological features from histology images crucial for identifying cancer-related patterns.",
"DINO: The DINO framework employs a dual-network architec- ture, consisting of a student and a teacher network, both being ViTs.",
"The student network learns by attempting to replicate the output of the teacher network, which in turn is an exponential moving average of the student’s parameters.",
"The core process involves generating multiple augmented views (I1, I2, .",
", Ik) of a given input image (I), which are then processed by these net- works.",
"The resultant feature vectors from the student (Fs) and teacher (Ft) networks are utilised to compute the distillation loss as follows: LD = k X i=1 CrossEntropy Fs(Ii), Softmax Ft(Ii) τ !",
", (1) where τ represents the temperature scaling parameter.",
"Notably, DINO is pretrained on a broad range of datasets, including BACH, CRC, MHIST, PatchCamelyon and CoNSeP, compris- ing 33 million patches (DINO33M), and 2 million patches from TCGA-BRCA (DINO2M) [49, 50].",
"The output func- tions for DINO33M and DINO2M, denoted as fDINO33M(x) and fDINO2M(x) respectively, convert an input image x of size 224 × 224 × 3 into a 1 × 384 feature vector.",
"We utilised these two models for feature extraction: DINO33M trained on diverse datasets providing a broad perspective and enabling the model to recognise a wide array of general histopathological features, and DINO2M specifically trained on breast cancer data for more spe- cialised and precise feature extraction relevant to breast cancer pathology.",
"MoCoV3: The MoCoV3 framework, which incorporates ViTs, represents a significant advancement in self-supervised learning through its adoption of the momentum contrast (MoCo) ap- proach [51].",
"At the heart of this framework lies the contrastive learning mechanism, designed to differentiate between positive and negative pairs, thereby enhancing the model’s feature learn- ing capabilities.",
"MoCoV3’s architecture is defined by two main components: a query encoder that processes the current batch of images, and a key encoder updated via a momentum-based moving average of the query encoder’s parameters: θk ←mθk + (1 −m)θq, (2) where θk and θq are the parameters of the key and query encoders respectively and m is the momentum coefficient.",
"This enables the key encoder to maintain a queue of encoded keys represent- ing previously seen images, thus enhancing the model’s ability to maximise agreement between differently augmented views of the same image (positive pairs) and minimise similarity with other images (negative pairs).",
"The framework uses the InfoNCE loss [52]: LInfoNCE = −log exp(q · k+/τ) PK i=0 exp(q · ki/τ) , (3) where q and k+ are the query and positive key feature vectors, ki are the negative key vectors, K is the number of negative 5 [ Each Patient × Image Embedding (256)] [ Each Patient × No.",
of Genes (138)] Fully-connected 128 + 4 Risk Score 256 32 [ Each Patient × Clinical Features (4)] Fully-connected Fully-connected 512 ... ≈≈ ... ≈≈ ... ≈≈ ... ≈≈ Transformer Encoder Co Dual Cross Attention Co Attention Dual Cross Attention Multi-Head Self-Attention Feed-Forward Network (FFN) Transformer Encoder Key Query Value Softmax Cross Attention + Layer Normalization Dropout Matrix Multiplication Concatenation + + + + x1 x2 xout fin fout I G AIG AGI CIG CGI DIG DGI Tcat Figure 3: BioFusionNet Stage 2: The proposed model fuses image embeddings generated from Stage 1 with genetic data through a co-dual-cross-attention mechanism.,
"This fusion is subsequently combined with clinical data using a feed-forward network (FFN), leading to the generation of the final risk score output.",
keys and τ is the temperature parameter.,
"MoCoV3 has been pretrained on an extensive collection of 15 million histology patches from over 30 thousand WSIs derived from the TCGA and Pathology AI Platform (PAIP) datasets, encompassing a wide variety of cancer types and histological features.",
This endows the model with a robust and versatile capability to extract meaningful features from histopathological data [53].,
"Similar to DINO, the output function of MoCoV3, fMoCoV3(x), transforms an input image x of dimensions 224 × 224 × 3 into a 1 × 384 feature vector.",
"2.3.2 Unimodal Feature Integration Extracted features from DINO33M, DINO2M and MoCoV3 are concatenated to form a comprehensive 1 × 1152 feature vector fcat(x) = fDINO33M(x) ⊕fDINO2m(x) ⊕fMoCoV3(x) (Fig.",
"Following this, we employ a VAE to encode the integrated image features into a 256-dimensional feature in latent space.",
The latent feature vector is then passed through a self-attention model and aggregated using sum pooling to generate patient- level features.,
2.3.3 Feature Fusion Using Variational Autoencoding Our VAE consists of an encoder and a decoder.,
"The encoder function fenc maps the concatenated feature vector fcat(x) to the latent space by generating the mean µ and standard deviation σ of the latent representation: (µ, σ) = fenc(fcat(x)).",
"(4) With 500 patches per patient, the latent space is structured as a matrix of size 500 × 256.",
"This is achieved by sampling z using the reparameterization trick: z = µ + σ · ϵ, ϵ ∼N(0, I), z ∈R500×256.",
(5) The decoder fdec then attempts to reconstruct the input from the latent variable z: ˆx = fdec(z).,
"(6) The VAE is optimised using a loss function that combines mean squared error (MSE) for reconstruction accuracy and Kullback- Leibler (KL) divergence for distribution regularisation: LVAE = MSE(ˆx, x) + β · KL(N(µ, σ2)∥N(0, I)), (7) where β balances the reconstruction and regularisation terms.",
"The MSE term ensures that the reconstructed image closely resembles the original input, while the KL divergence term encourages the latent distribution to approximate a standard normal distribution.",
"This enables BioFusionNet to effectively blend the features from the different self-supervised models, enhancing the overall feature representation.",
"6 2.3.4 Patch-to-Patient Aggregation To aggregate the patch-level features from the latent space of the VAE into a comprehensive patient-level representation capturing the interdependencies among image patches, our model uses self-attention (Fig.",
"The self-attention module computes a weighted sum of the key (K), query (Q), and value (V) vectors: yi = 500 X j=1 Softmax  s(Qi, Kj)  Vj, (8) where s(Qi, Kj) is the attention function that determines the rel- evance between each query and key pair.",
This offers two signifi- cant benefits.,
"First, by leveraging the VAE’s latent vectors, the model focusses on the most pertinent image features, resulting in a more precise feature representation.",
"Second, by consid- ering all latent representations, the self-attention mechanism contextualises each patch within the broader histopathology of the patient.",
"This aggregation process effectively combines the patch-level features of all 500 patches, taking into account their relevance and interdependencies, to generate a comprehensive patient-level representation.",
"2.3.5 Multimodal Fusion Using Co Dual Cross Attention To integrate patient-level image embeddings with genetic fea- ture data, our model uses a co-dual-cross-attention mechanism (Fig.",
This is achieved using a complex architecture compris- ing co-attention and dual-cross-attention modules.,
"The co-attention module applies linear transformations to the image embeddings I and genetic features G, yielding their re- spective query (Q), key (K), and value (V) vectors, and computes co-attention scores scaled by √dk, where dk is the dimensional- ity of the keys: AIG = Softmax  QIKT G/ p dk  VG, (9) AGI = Softmax  QGKT I / p dk  VI, (10) where AIG represents the attention from images to genetic fea- tures, and AGI the attention from genetic features to images.",
"The Softmax function normalises these scores, facilitating an effec- tive weighting of feature importance in the fusion process.",
This bidirectional attention prepares the ground for more complex in- teractions in the subsequent stage of the co-dual-cross-attention mechanism.,
"Subsequently, the dual-cross-attention module fur- ther refines the integration of the image and genetic features in two distinct stages.",
The first stage concerns the interaction between the co-attended image features (AIG) and co-attended genetic features (AGI).,
"The cross attention is computed as: CIG = Softmax  AIG(AGI)T/ p dk  AIG, (11) CGI = Softmax  AGI(AIG)T/ p dk  AGI, (12) where CIG represents the cross-attention output when image fea- tures attend to genetic features, and CGI the reverse.",
This stage is crucial for enhancing each modality by integrating contextu- ally relevant information from the other.,
"In the second stage, the outputs from the first stage are further refined by reapplying them to their respective original features.",
"This enhances the depth of the multimodal integration: DIG = Softmax  CIGIT/ p dk  I, (13) DGI = Softmax  CGIGT/ p dk  G, (14) Algorithm 1 Weighted Cox Loss Require: r: risks (log hazard ratios), e: events, w: weights Ensure: LWCox: negative log likelihood loss 1: Compute Ew = PN i=1 wiei as total weighted events 2: Sort samples by descending r and align e, w 3: for i ∈1, .",
", N do ▷N is the number of samples 4: Compute hazard ratio hi = exp(ri) 5: end for 6: Init weighted cumulative hazard Hw0 = 0 7: for i ∈1, .",
", N do 8: Update cumulative sum Hwi = Hwi−1 + wihi 9: end for 10: for i ∈1, .",
", N do 11: Compute uncensored log likelihood ui = wi(ri −log(Hwi)) 12: end for 13: Compute c = u ⊙e ▷⊙is the element-wise product 14: Compute loss LWCox = −1 Ew PN i=1 ci 15: return LWCox where DIG and DGI denote the refined cross-attention outputs, further enhancing the original image and genetic data with ad- ditional contextual insights.",
"By sequentially processing the attended features, the model achieves a richer and more contextu- ally informed representation of the fused image and genetic data, suited for complex tasks like risk prediction.",
"The concatenated output Tcat = DIG ⊕DGI is fed to a Transformer Encoder, which employs multiple layers of self-attention and a feed-forward network (FFN) to achieve a deeper assimilation and transforma- tion of the fused features.",
"The Transformer Encoder consists of N identical layers, each containing two sublayers: a multihead self-attention mechanism and a position-wise fully connected FFN.",
"In the multihead self-attention sublayer, the input is first linearly projected into a number of different subspaces equal to the number of attention heads.",
"The self-attention mechanism is then applied to each subspace independently, allowing the model to jointly attend to information from different representa- tion subspaces at different positions.",
The outputs from all heads are then concatenated and linearly projected back to the original dimension.,
"The second sublayer is a position-wise FFN, which consists of two linear transformations with a ReLU activation in between: FFN(x) = max(0, xW1 + b1)W2 + b2 (15) where W1, W2, b1, and b2 are learnable parameters.",
"A residual connection is employed around each of the two sublayers, fol- lowed by layer normalisation.",
"The output of the Transformer Encoder represents the deeply integrated features from both the imaging and genetic modalities and is further processed by four fully-connected layers, the third of which also integrates the clinical information (Fig.",
The integration of clinical variables at this stage is crucial due to their dimensionality and characteristics.,
"Clinical data, comprising only four features, may be overshadowed by the higher-dimensional features from histopathological images and genetic profiles if introduced ear- lier in the model.",
"By integrating these clinical variables in a later layer of the network, closer to the output, their impact is more effectively mapped onto the model’s survival risk pre- diction.",
"This approach, referred to as ‘late fusion’, allows the clinical variables to have a more significant influence on the 7 Table 1: Performance comparison of multimodal and unimodal models for cancer risk prediction using C-index.",
"Fold Imaging+Genetic+Clinical Imaging+Genetic Imaging Clinical Genetic BioFusionNet BioFusionNet BioFusionNet CoxPH MLP CoxPH MLP 1 0.78 0.73 0.58 0.52 0.66 0.62 0.66 2 0.71 0.72 0.69 0.42 0.64 0.55 0.73 3 0.72 0.69 0.61 0.59 0.69 0.51 0.66 4 0.81 0.75 0.70 0.64 0.70 0.62 0.75 5 0.82 0.65 0.69 0.72 0.66 0.63 0.67 Mean ± Std 0.77 ± 0.05 0.71 ± 0.04 0.65 ± 0.05 0.58 ± 0.11 0.67 ± 0.02 0.59 ± 0.05 0.69 ± 0.04 final output compared to ‘early fusion’, where clinical variables are integrated in earlier layers of the network.",
This multimodal integration results in a holistic representation of both the phenotypic and genotypic information of ER+ breast cancer.,
"Finally, the network employs a linear output layer that predicts the survival risk score.",
"2.4 Proposed Loss Function For training BioFusionNet, we propose a novel loss function termed the weighted Cox loss (computed by Algorithm 1), which is tailored to address the challenges of imbalanced survival data (a common issue in survival analysis): LWCox = − 1 PN i=1 wiei N X i=1 wiei(ri −log(Hwi)), (16) where ei denotes the event occurrence, wi the assigned weight, ri the log hazard ratio, Hwi the weighted cumulative hazard and N the number of samples.",
"Unlike the traditional Cox proportional hazards loss (LCox) [22], the proposed loss uses weighting to mitigate the effects of uneven distribution of events within the dataset.",
"In this work we used wi = 3, considering that censored data (denoted as ‘0’) is almost three times as prevalent as event data (denoted as ‘1’), and thus the sensitivity of the loss function to the latter should be enhanced accordingly, mitigating the bias towards censored data.",
2.5 Model Training The training of BioFusionNet is divided into two distinct stages as follows (Figs.,
"2 and 3): Stage 1: Feature Extraction: The self-supervised pretrained models DINO33M, MoCoV3 and DINO2M extract features from histopathology image patches, which are concatenated and then fed into a VAE to produce embeddings, which in turn are processed by a self-attention module to produce a patient- level feature vector (Fig.",
The VAE was optimised using AdamW with a learning rate of 0.0001 and a batch size of 12.,
"The employed loss function is a combination of MSE and KL divergence, targetting the construction of an advanced latent space to generate detailed patient-level features.",
"Stage 2: Risk Prediction: The proposed co-dual-cross-attention mechanism, followed by multiple FFNs and a final output node that uses a linear activation function, predicts the patient-level risk from the image-based, genetic and clinical information (Fig.",
"Here, training was performed using the proposed weighted Cox loss (LWCox), which was optimised using the Adam algorithm with a learning rate of 0.001 and a batch size of 12.",
"To mitigate overfitting, an early stopping mechanism based on the validation loss was implemented.",
This involved halting the training process after a patience period of 10 epochs if no improvement was observed.,
This two-stage training approach for BioFusionNet was moti- vated by two key factors.,
"First, the computational complexity of training sophisticated models, especially those incorporating elements like multiple feature extractors, VAE, and various at- tention blocks, can be significantly high.",
"An end-to-end training process often requires increased memory usage and even more data, which may not be feasible always.",
"By adopting a two-stage training process, we effectively manage this complexity, break- ing down the training into more manageable parts.",
"Second, a modular design facilitates conducting ablation experiments.",
"By training in stages, we can isolate the effects of specific modules or features, systematically analysing their impact and optimising their configuration.",
This modular approach enhances our ability to refine and improve the learning architecture.,
Both training stages used a dataset comprising 199 training sam- ples and 50 validation samples within a five-fold cross-validation framework.,
The trained model predicts a continuous risk score for every patient within each validation fold.,
"For survival analy- sis, we employed the median risk score θopt, derived from each training set, as a threshold to classify patients in the validation set into two categories: high risk (risk score > θopt) and low risk (risk score < θopt).",
"We note that θopt is not a user-defined param- eter, but its value depends on and is automatically calculated from the specific training set used in each fold, and thus varies depending on the dataset.",
"To achieve optimal performance, we used Optuna3 for hyperparameter optimisation, tuning key parameters including learning rate, weight decay, number of neurons, number of layers, and dropout for each model.",
"2.6 Evaluation Metrics To quantitatively evaluate survival risk score prediction, we employed the concordance index (C-index) and the area under the curve (AUC) as our primary metrics.",
"The C-index assesses the concordance between predicted survival times and observed outcomes, especially in the presence of censored data: C-index = nP i=1 nP j=1 I(yi < yj, δi = 1)I( ˆf(xi) < ˆf(xj)) nP i=1 nP j=1 I(yi < yj, δi = 1) .",
"(17) where n is the number of patients, yi and yj denote the observed survival times, δi indicates whether the event was observed (not censored), ˆf(xi) represents the predicted risk for the ith patient, 3https://optuna.org/ 8 Table 2: Performance comparison of multimodal fusion methods for cancer risk prediction.",
"Method Fold C-index AUC Value Mean ± Std Value Mean ± Std MultiSurv [29, 54, 55] 1 0.71 0.63 ± 0.07 0.74 0.63 ± 0.09 2 0.59 0.52 3 0.60 0.61 4 0.69 0.70 5 0.54 0.57 MultiDeepCox-SC [27] 1 0.71 0.60 ± 0.08 0.68 0.58 ± 0.08 2 0.68 0.66 3 0.55 0.49 4 0.58 0.57 5 0.50 0.50 HFBSurv [30] 1 0.58 0.54 ± 0.07 0.51 0.49 ± 0.04 2 0.47 0.51 3 0.56 0.45 4 0.45 0.44 5 0.62 0.53 PathomicFusion [31] 1 0.63 0.52 ± 0.08 0.68 0.47 ± 0.23 2 0.43 0.10 3 0.56 0.54 4 0.50 0.63 5 0.46 0.38 MCAT [28] 1 0.71 0.70 ± 0.04 0.70 0.71 ± 0.04 2 0.69 0.67 3 0.64 0.65 4 0.70 0.69 5 0.76 0.72 TransSurv [32][26] 1 0.70 0.69 ± 0.04 0.68 0.66 ± 0.04 2 0.61 0.60 3 0.69 0.65 4 0.69 0.67 5 0.74 0.72 BioFusionNet (Proposed) 1 0.78 0.77 ± 0.05 0.82 0.84 ± 0.05 2 0.71 0.93 3 0.72 0.79 4 0.81 0.81 5 0.82 0.83 and I is the indicator function that returns 1 when its condition is true and 0 otherwise.",
"The time-dependent AUC offers a dynamic view of the model accuracy over time t and incorporates weights ωi, and is calculated using the following formula: AUC(t) = nP i=1 nP j=1 ωiI(yi ≤t)I(y j > t)I( ˆf(xj) ≤ˆf(xi)) nP i=1 ωiI(yi ≤t) nP i=1 I(yi > t) .",
"(18) Both the C-index and AUC values range from 0 to 1, with higher values indicating better performance.",
"In addition to these metrics, we also compared the computational properties of the models in terms of the number of trainable parameters, memory usage, and floating-point operations (FLOPS).",
3 Experimental Results 3.1 Comparison Across Modalities The effectiveness of BioFusionNet in cancer risk prediction was evaluated by comparing its C-index performance across differ- ent modality configurations.,
"The results (Table 1) show that the mean performance in the cross-validation experiments consis- tently increased from using only imaging data, to using imaging and genetic data, to combining imaging, genetic and clinical data.",
"Two traditional methods, specifically CoxPH and the Mul- tiLayer Perceptron (MLP), were also evaluated and showed no Table 3: Comparison of computational properties.",
"Method Parameters Size FLOPS MultiSurv 1.79 M 6.99 MB 2.67 G MultiDeepCox-SC 0.71 M 2.72 MB 1.18 G HFBSurv 0.79 M 3.08 MB 2.65 G PathomicFusion 102.28 M 397.12 MB 3.87 G MCAT 2.06 M 8.06 MB 2.67 G TransSurv 1.97 M 7.72 MB 2.67 G BioFusionNet (Proposed) 4.37 M 17.20 MB 12.11 G consistent advantage when using only clinical or only genetic data, and their performance was inferior to that of BioFusion- Net.",
"This suggests that BioFusionNet’s architecture and training process allow it to effectively extract and integrate complemen- tary information from the three different modalities, leading to improved predictive performance, although it remains elusive exactly how the information in each modality complements the information in the other.",
3.2 Comparison With State-of-the-Art Fusion Methods The performance of BioFusionNet was compared with several state-of-the-art multimodal fusion methods for cancer risk pre- diction in terms of both C-index and AUC (Table 2).,
"For this experiment we included methods using concatenation (Multi- Surv), Cox proportional hazards with image-derived risk scores (MultiDeepCox-SC), hierarchical attention (HFBSurv), gating attention (PathomicFusion), co-attention (MCAT) and cross at- tention (TransSurv).",
"To ensure fair comparison, we used the same hyperparameter optimisation framework (Optuna) for all models.",
The AUC was calculated using average values over 5-year and 10-year periods.,
"The proposed model consistently outperformed all these previous methods, showing substantial improvements in both metrics.",
"However, BioFusionNet is com- putationally demanding, requiring 12.11 G (Giga) FLOPS, the highest among the compared models, although it is more mem- ory efficient than PathomicFusion (Table 3).",
3.3 Evaluation of Loss Functions The performance of two different loss functions was compared using the C-index across five cross-validation folds for Bio- FusionNet and MoCoV3 (Table 4).,
"The results show that the mean performance improved for both methods when using the weighted Cox loss (LWCox) proposed in this paper, compared to the traditional Cox loss (LCox).",
3.4 Univariate and Multivariate Hazard Analysis A comprehensive hazard analysis was conducted to evaluate the overall survival (OS) in the TCGA dataset of ER+ patients.,
Both univariate and multivariate analyses were performed (Table 5).,
"The analysis encompassed various parameters, including tumour grade, tumour size, age, lymph node (LN) status, subtype and the risk predictions made by BioFusionNet.",
"In the multivariate analysis, positive LN status was associated with a hazard ratio (HR) of 1.87 (95% CI: 1.32–2.64), demonstrating a significant effect on survival (p < 0.005).",
"Additionally, patients over the age of 55 had a HR of 1.77 (95% CI: 1.07–2.91), also showing a significant impact on survival (p = 0.03).",
"However, no signif- icant associations were found between tumour grade, size, or 9 (a) CoxPH (Clinical) (b) MCAT (Multimodal) (c) BioFusionNet (Multimodal) (d) HFBSurv (Multimodal) (e) MultiSurv (Multimodal) (f) PathomicFusion (Multimodal) Figure 4: Performance comparison of BioFusionNet and other methods using Kaplan-Meier survival curves.",
Table 4: Performance comparison of loss functions for cancer risk prediction using two different methods.,
Loss Method C-index (Fold) Mean ± Std 1 2 3 4 5 LCox BioFusionNet 0.69 0.54 0.59 0.75 0.80 0.67 ± 0.10 MoCoV3 0.66 0.57 0.57 0.78 0.80 0.67 ± 0.11 LWCox (Proposed) BioFusionNet 0.78 0.71 0.72 0.81 0.82 0.77 ± 0.05 MoCoV3 0.70 0.66 0.66 0.77 0.72 0.70 ± 0.04 subtype and survival outcomes in this analysis.,
"Notably, the BioFusionNet-predicted risk group (high vs. low) demonstrated a significant correlation with OS, with a HR of 2.91 (95% CI: 1.80–4.68) (p < 0.005).",
"Univariate analysis indicated that tumour grade, size, age, and subtype were not statistically significant, whereas LN status (HR of 1.84, 95% CI: 1.33–2.55, p < 0.005) and BioFusionNet risk group (HR of 2.99, 95% CI: 1.88–4.78, p < 0.005) were significant predictors of survival.",
"We note that the LN status had 51 missing values, which were imputed using a fixed value of 2.",
"Kaplan-Meier survival analysis further sup- ported the results, showing a significant difference in survival probabilities between the high- and low-risk groups as predicted by BioFusionNet (log-rank test p = 6.45e-7) (Fig.",
3.5 Ablation Study We also evaluated the performance of various versions of Bio- FusionNet for ER+ breast cancer risk stratification.,
"The re- sults (Table 6) show that the base model, BioFusionNet-B0 (MoCoV3, ViT Small) with LWCox, achieved the lowest C- index, and incorporating single cross-attention (SCA), dual cross-attention (DCA), or co-attention (CoA) yielded slight im- provements, as did BioFusionNet-B1 (DINO33M, ViT Small) and BioFusionNet-B2 (DINO2M, ViT Small), both with DCA and LWCox.",
"Combining BioFusionNet-B0, B1, and B2 with just LWCox also resulted in slightly better performance than the base model, as did the inclusion of SCA and VAE.",
More substantial improvements of the combined model were obtained with the inclusion of DCA or CoA instead of SCA.,
"The best performance was achieved by the combined model using VAE, CoA, DCA and late clinical data fusion (C-FusionLate) with LWCox, which clearly outperformed the same model using early clinical data fusion (C-FusionEarly) or LCox.",
"3.6 Interpretability of BioFusionNet BioFusionNet utilises a self-attention mechanism to analyse histopathological image patches, identifying regions of high and low attention within both high-risk and low-risk patient pro- files.",
Visual inspection of the results (Fig.,
"5) reveals that regions with high attention contain distinct cellular patterns crucial for synthesising features from patch-level to patient-level, whereas areas of low attention typically exhibit less cellular atypia.",
This shows the model capacity to pinpoint clinically relevant features within tissue morphology.,
"Additionally, SHAP analysis (Fig.",
"6) reveals the influence of individual genes on the model predic- tions, ranked from high to low, providing interpretability of the risk assessment process.",
"From this analysis, gene SLC39A6 (an estrogen regulated Zinc transporter protein with a role in epithe- lial to mesenchymal transition (EMT)) was identified as the most important predictor, with high expression levels producing high SHAP values, indicating positive impact on the model’s cancer risk prediction.",
"Other influential genes include ERBB2 (the gene for HER2), ESR1 (the gene for ER), with low expression levels producing high SHAP values therefore positive impact on the model.",
"Moreover, the distribution of SHAP values for clinical features (Fig.",
"7) indicates that higher values of clinical parameters—such as positive LN status, higher tumour grade, 10 Table 5: Univariate and multivariate analysis for overall survival (OS) in the TCGA dataset of ER+ patients.",
Parameter Risk Group Cutoff #Patients/Group Multivariate (n=249) Univariate (n=249) HR 95% CI p HR 95% CI p Tumour Grade 3 vs. 1 & 2 64 vs. 185 0.83 0.46–1.49 0.54 1.13 0.67–1.91 0.65 Tumour Size >20 vs. ≤20 (mm) 167 vs. 82 1.45 0.88–2.37 0.14 1.37 0.86–2.19 0.19 Age >55 vs. ≤55 159 vs. 90 1.77 1.07–2.91 0.03 1.47 0.91–2.36 0.11 LN Status* pos.,
110 vs. 88 1.87 1.32–2.64 <0.005 1.84 1.33–2.55 <0.005 Subtype lum B vs. A 100 vs. 149 1.43 0.88–2.34 0.15 1.38 0.88–2.18 0.16 BioFusionNet high vs. low 132 vs. 117 2.91 1.80–4.68 <0.005 2.99 1.88–4.78 <0.005 *LN Status had 51 missing values which were imputed with a fixed value for both multivariate and univariate analysis.,
"Table 6: Ablation study of BioFusionNet Model C-index BioFusionNet-B0 (MoCoV3, ViT Small) + C-FusionLate + LWCox 0.65 ± 0.05 BioFusionNet-B0 (MoCoV3, ViT Small) + SCA + C-FusionLate + LWCox 0.69 ± 0.04 BioFusionNet-B0 (MoCoV3, ViT Small) + DCA + C-FusionLate + LWCox 0.70 ± 0.03 BioFusionNet-B0 (MoCoV3, ViT Small) + CoA + C-FusionLate + LWCox 0.70 ± 0.04 BioFusionNet-B1 (DINO33M, ViT Small) + DCA + C-FusionLate + LWCox 0.68 ± 0.02 BioFusionNet-B2 (DINO2M, ViT Small) + DCA + C-FusionLate + LWCox 0.67 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + C-FusionLate + LWCox 0.67 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + SCA + C-FusionLate + LWCox 0.69 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + VAE + SCA + C-FusionLate + LWCox 0.68 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + DCA + C-FusionLate + LWCox 0.75 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + C-FusionLate + LWCox 0.70 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + Clinic-FLate + LCox 0.67 ± 0.10 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + C-FusionEarly + LWCox 0.74 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + C-FusionLate + LWCox 0.77 ± 0.03 increased tumour size, and postmenopausal age group—tend to have a positive impact on the model’s output.",
"In this context, a ‘positive impact’ implies that the model associates these values with a higher likelihood of predicting patients at high risk.",
"4 Discussion and Conclusion As demonstrated by the experimental results, the proposed Bio- FusionNet is highly effective for cancer risk prediction, show- ing superior performance compared to alternative approaches.",
"Clearly, the multimodal fusion of imaging, genetic and clinical data allows the model to achieve substantially higher C-index scores compared to unimodal and dual-modal configurations, as well as compared to the traditional unimodal CoxPH and MLP.",
"Furthermore, BioFusionNet outperforms existing multimodal fusion methods such as MultiSurv, HFBSurv, PathomicFusion, MCAT and TransSurv, and achieves the highest mean C-index (0.77 ± 0.05) and AUC (0.84 ± 0.05).",
"Partly, the superior per- formance of BioFusionNet is due to the introduction of the proposed weighted Cox loss function instead of using the tradi- tional Cox loss.",
"Univariate and multivariate analyses showed the significant impact of age and BioFusionNet predictions on survival outcomes, while other clinical parameters such as tu- mour grade, size, lymph node status and subtype did not exhibit a significant correlation with survival outcomes.",
Kaplan-Meier analysis revealed a distinct separation in survival probabilities between the high-risk and low-risk groups identified by Bio- FusionNet.,
"In addition, the results of the ablation experiment confirmed the importance of attention mechanisms in improving prediction accuracy, with the combined model configuration util- ising VAE, CoA and DCA and the weighted Cox loss showing the highest performance.",
"BioFusionNet also presents a significant advancement in the interpretation of histopathological images, leveraging a self- attention mechanism to distinguish critical regions in patient profiles.",
"A key contribution is the model’s ability to align high- attention areas with distinct cellular patterns, crucial for tran- sitioning from patch-level to patient-level analysis, thereby en- hancing the diagnostic process.",
SHAP analysis amplifies this by clarifying the influence of specific genes and clinical fea- tures on the model’s predictions.,
"We observed that elevated SLC39A6 gene expression correlates with a high-risk prediction in ER+ breast cancers, where previous studies have shown con- flicting findings, associating high SLC39A6 levels with good prognosis [56, 57], while others associated it with increased pro- liferation and lymph node involvement [58–60].",
"Similarly, our model identified high ESR1 expression as indicative of low risk, aligning with literature that associates ESR1 positivity with en- hanced responsiveness to endocrine therapy and, consequently, a better prognosis in ER+ breast cancer patients [61].",
"In con- trast, our analysis revealed an unexpected association between ERBB2 overexpression and a favorable prognosis in ER+ breast tumours, contrasting with the established view that ERBB2 over- expression indicates a poor prognosis [62, 63].",
"As our study was specifically tailored to analyze ER+ samples, excluding the HER2-enriched subtype (known for its high ERBB2 expression and aggressiveness) likely influenced the findings.",
"Moreover, the SHAP analysis for clinical factors (LN positivity, higher tumour grade and size, and postmenopausal age) significantly influences our model’s ability to identify patients at increased risk, highlighting the critical role of these factors in breast cancer prognosis.",
"Our analysis provides a transparent understanding of how each gene and clinical feature contributes to the model’s predictions, providing actionable insights for clinical decision- making.",
"While the risk assessment process mirrors current clinical practice, BioFusionNet streamlines the integration of all available data (patient features, tumour features and molecular features) to derive an automated single risk prediction score as a potential clinical oncology tool of the future.",
"While insightful, this study has certain limitations.",
"We primarily opted for OS as the key outcome measure, instead of disease-free survival (DFS).",
"This choice was made because DFS presented challenges such as a lower rate of events and a higher degree of data censorship, which could have limited the depth of the analysis.",
"While OS is a feasible choice, it potentially overlooks critical insights into early-stage disease progression, typically highlighted by DFS.",
"Moreover, the study’s reliance on specific datasets such as TCGA for ER+ patients may affect the broad applicability of our findings.",
Another shortcoming of this study 11 Low Risk High Risk Luminal A Luminal B Risk Type High Attention Low Attention High Attention Low Attention TCGA-A7-A13H TCGA-AR-A2LK TCGA-B6-A0IN TCGA-A2-A259 Figure 5: Visualisation of model-derived attention regions and associated risk types in Luminal A and Luminal B breast cancer patients.,
"The figure presents raw histopathological image patches processed with BioFusionNet, which identifies areas of high and low attention, subsequently categorising patients into high and low risk.",
Figure 6: SHAP analysis of genetic features.,
The x-axis represents the SHAP value; colour intensity indicates gene expression level.,
The plot is sorted vertically by the features’ overall importance.,
Figure 7: SHAP value distribution of clinical features.,
"In this distribu- tion, higher clinical values showing positive impact on the model, as indicated by its SHAP values.",
"is the inherent limitations of the clinical data, which, during univariate analysis, identified tumour size, grade, and age as in- significant while only LN Status emerged as significant.",
"Despite its limitations, the effectiveness of deep learning algorithms in analysing this clinical data arises from their ability to uncover complex patterns and interactions within dataset.",
Future re- search should therefore aim to validate these findings across a wider range of datasets to bolster the model’s generalisability.,
"Incorporating organ-level data, such as mammograms, could fur- ther enhance the predictive accuracy of our model.",
"Additionally, extending the application of BioFusionNet to other cancer types and clinical scenarios could yield more comprehensive insights, making the research more universally relevant and applicable.",
"Finally, a limitation of our model is that it is computationally demanding, primarily due to its extensive use of attention mech- anisms.",
Whether the better performance justifies the higher computational cost depends on user needs and resources.,
Fur- ther research may provide ways to reduce the computational requirements of the model while retaining its high performance.,
Data Availability TCGA image data and clinical data are publicly available at https://portal.gdc.cancer.gov/.,
Code Availability Our work is fully reproducible and source code is publicly avail- able on GitHub at https://github.com/raktim-mondol/ BioFusionNet.,
"Acknowledgement This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI), which is supported by the Australian Government.",
"Addi- tionally, data preprocessing was performed using the computa- tional cluster Katana, which is supported by Research Technol- ogy Services at UNSW Sydney.",
"12 Compliance with Ethical Standards This research study was conducted retrospectively using hu- man subject data made available in open access by The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset, accessible through the National Cancer Institute’s Genomic Data Commons (GDC) portal.",
"Ethical approval was not required for this study, in accordance with the ethical policies set forth by The Cancer Genome Atlas program.",
References [1] Y.,
"B. Shvetsov, L. R. Wilkens, K. K. White, M. Chong, A. Buyum, G. Badowski, R. T. L. Guerrero, and R. Novotny, “Prediction of breast cancer risk among women of the Mariana Islands: the BRISK retrospective case–control study,” BMJ Open, vol.",
"12, p. e061205, 2022.",
"[2] W. Guo, W. Liang, Q. Deng, and X. Zou, “A multimodal affin- ity fusion network for predicting the survival of breast cancer patients,” Frontiers in Genetics, vol.",
"12, p. 709027, 2021.",
"[3] M. D. J. Peters, I. Ramsey, K. Kennedy, G. Sharplin, and M. Eck- ert, “Culturally safe, high-quality breast cancer screening for transgender people: a scoping review protocol,” Journal of Ad- vanced Nursing, vol.",
"276–281, 2021.",
"[4] C. Shuai, F. Yuan, Y. Liu, C. Wang, J. Wang, and H. He, “Estrogen receptor—positive breast cancer survival prediction and analysis of resistance–related genes introduction,” PeerJ, vol.",
"9, p. e12202, 2021.",
"[5] K. Holli-Helenius, A. Salminen, I. Rinta-Kiikka, I. Koskivuo, N. Brück, P. Boström, and R. Parkkola, “MRI texture analysis in differentiating luminal A and luminal B breast cancer molecular subtypes—a feasibility study,” BMC Medical Imaging, vol.",
"17, p. 69, 2017.",
"[6] H. Lindman, F. Wiklund, and K. K. Andersen, “Long-term treat- ment patterns and survival in metastatic breast cancer by intrinsic subtypes—an observational cohort study in Sweden,” BMC Can- cer, vol.",
"22, p. 1006, 2022.",
"[7] Y. Han, J. Wang, and B. Xu, “Clinicopathological characteristics and prognosis of breast cancer with special histological types: a surveillance, epidemiology, and end results database analysis,” The Breast, vol.",
"114–120, 2020.",
"[8] M. A. Han, E. C. Hwang, and J. H. Jung, “Prognostic factors of mortality in patients with cancer infected with COVID-19: a systematic review protocol,” BMJ Open, vol.",
"7, p. e071810, 2023.",
"[9] C. Nero, F. Ciccarone, A. Pietragalla, S. Duranti, G. Daniele, G. Scambia, and D. Lorusso, “Adjuvant treatment recommenda- tions in early-stage endometrial cancer: what changes with the introduction of the integrated molecular-based risk assessment,” Frontiers in Oncology, vol.",
"11, p. 612450, 2021.",
"[10] A. N. Zahari, N. S. Mohamad, and M. H. Mahmud, “Impacts of clinicopathological factors on metabolic parameters of 18F fluorodeoxyglucose PET/CT in the staging of breast cancer,” Jour- nal of Sustainability Science and Management, vol.",
"166–173, 2022.",
"[11] X. Li, L. Liu, G. J. Goodall, A. Schreiber, T. Xu, J. Li, and T. D. Le, “A novel single-cell based method for breast cancer prognosis,” PLoS Computational Biology, vol.",
"1–20, 2020.",
"[12] M. I. Jaber, L. Beziaeva, C. W. Szeto, and S. C. Benz, “Deep learning-based risk stratification for HER2-negative breast cancer patients,” bioRxiv, p. 10.1101/2021.05.26.445720, 2021.",
"[13] M. Garutti, G. Griguolo, A. Botticelli, G. Buzzatti, C. D. Angelis, L. Gerratana, C. Molinelli, V. Adamo, G. Bianchini, L. Biganzoli, G. Curigliano, M. D. Laurentiis, A. Fabi, A. Frassoldati, A. Gen- nari, M. Scaltriti, F. Perrone, G. Viale, C. Zamagni, A. Zam- belli, L. D. Mastro, S. D. Placido, V. Guarneri, P. Marchetti, and F. Puglisi, “Definition of high-risk early hormone-positive HER2-negative breast cancer: a consensus review,” Technology in Cancer Research & Treatment, vol.",
"8, p. 1898, 2022.",
"[14] B. Lu, E. Natarajan, H. R. B. Raghavendran, and U. D. Markan- dan, “Molecular classification, treatment, and genetic biomarkers in triple-negative breast cancer: a review,” Technology in Cancer Research & Treatment, vol.",
"22, p. 15330338221145246, 2023.",
"[15] L. Guo, D. Kong, J. Liu, L. Zhan, L. Luo, W. Zheng, Q. Zheng, C. Chen, and S. Sun, “Breast cancer heterogeneity and its implica- tion in personalized precision therapy,” Experimental Hematology & Oncology, vol.",
"1, p. 3, 2023.",
"[16] J. C. Wei, A.",
"A. Suriawinata, B. Ren, Y. Zhang, M. Lisovsky, L. J. Vaickus, C. R. Brown, M. J. Baker, N. Tomita, L. Torresani, J.",
"Z. Wei, and S. Hassanpour, “A petri dish for histopathology image analysis,” Artificial Intelligence in Medicine, pp.",
"11–24, 2021.",
"[17] E. Schaafsma, B. Zhang, M. Schaafsma, C. Tong, L. Zhang, and C. Cheng, “Impact of oncotype DX testing on ER+ breast cancer treatment and survival in the first decade of use,” Breast Cancer Research, vol.",
"1, p. 74, 2021.",
"[18] A. D. Caluwé, L. Buisseret, P. Poortmans, D. V. Gestel, R. Sal- gado, C. Sotiriou, D. Larsimont, M. Paesmans, L. Craciun, S. Dri- sis, C. Vandekerckhove, F. Reyal, I. Veys, D. Eiger, M. Piccart, E. Romano, and M. Ignatiadis, “Neo-CheckRay: radiation therapy and adenosine pathway blockade to increase benefit of immuno- chemotherapy in early stage luminal B breast cancer, a random- ized phase II trial,” BMC Cancer, vol.",
"21, p. 899, 2021.",
"[19] S. Paik, G. Tang, S. Shak, C. Kim, J.",
"B. Baker, W. Kim, M. Cronin, F. L. Baehner, D. Watson, J. Bryant, J. P. Costantino, C. E. Geyer, D. L. Wickerham, and N. Wolmark, “Gene expression and benefit of chemotherapy in women with node-negative, estrogen recep- tor–positive breast cancer,” Journal of Clinical Oncology, vol.",
"3726–3734, 2006.",
"[20] K. Lee, S. H. Sim, E. J. Kang, J. H. Seo, H. D. Chae, K. S. Lee, J. Y. Kim, J. S. Ahn, Y. H. Im, S. Park, Y. H. Park, and I. H. Park, “The role of chemotherapy in patients with HER2-negative isolated locoregional recurrence of breast cancer: a multicen- ter retrospective cohort study,” Frontiers in Oncology, vol.",
"11, p. 653243, 2021.",
"[21] Y. Naoi, R. Tsunashima, K. Shimazu, and S. Noguchi, “The multi- gene classifiers 95GC/42GC/155GC for precision medicine in ER-positive HER2-negative early breast cancer,” Cancer Science, vol.",
"1369–1375, 2021.",
"[22] J. Katzman, U. Shaham, A. Cloninger, J. Bates, T. Jiang, and Y. Kluger, “DeepSurv: personalized treatment recommender sys- tem using a Cox proportional hazards deep neural network,” BMC Medical Research Methodology, vol.",
"18, p. 24, 2018.",
"He, B. Hu, C. Zhu, W. Xu, X. Hao, B. Dong, X. Chen, Q. Dong, and X. Zhou, “A novel multimodal radiomics model for predicting prognosis of resected hepatocellular carcinoma,” Frontiers in Oncology, vol.",
"12, p. 745258, 2022.",
"[24] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multimodal fusion using sparse CCA for breast cancer survival prediction,” IEEE International Symposium on Biomedical Imaging, pp.",
"1429– 1432, 2021.",
"[25] R. Vanguri, J. Luo, A. Aukerman, J. V. Egger, C. J. Fong, N. Hor- vat, A. Pagano, J. d. A.",
"B. Araújo-Filho, L. Geneslaw, H. Rizvi, R. E. Sosa, K. M. Boehm, S. Yang, F. M. Bodd, K. Ventura, T. J.",
"13 Hollmann, M. S. Ginsberg, J. Gao, M. D. Hellmann, J. L. Sauter, and S. P. Shah, “Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer,” Nature Cancer, vol.",
"1151–1164, 2022.",
"[26] S. Deng, Y. Suo, S. Liu, X. Ma, H. Chen, X. Liao, J. Zhang, and W. W. Y. Ng, “MFCSA-CAT: a multimodal fusion method for cancer survival analysis based on cross-attention transformer,” International Conference on Computer Information Science and Artificial Intelligence, vol.",
"12566, p. 1256608, 2023.",
"[27] T. Wei, X. Yuan, R. Gao, L. J. Johnston, J. Zhou, Y. Wang, W. Kong, Y. Xie, Y. Zhang, D. Xu, and Z. Yu, “Survival predic- tion of stomach cancer using expression data and deep learning models with histopathological images,” Cancer Science, vol.",
"690–701, 2022.",
"[28] R. J. Chen, M. Y. Lu, W.-H. Weng, T. Y. Chen, D. F. Williamson, T. Manz, M. Shady, and F. Mahmood, “Multimodal co-attention transformer for survival prediction in gigapixel whole slide im- ages,” IEEE/CVF International Conference on Computer Vision, pp.",
"3995–4005, 2021.",
"[29] L. A. Vale-Silva and K. Rohr, “Long-term cancer survival predic- tion using multimodal deep learning,” Scientific Reports, vol.",
"11, p. 13505, 2021.",
"[30] R. Li, X. Wu, A. Li, and M. Wang, “HFBSurv: Hierarchical multimodal fusion with factorized bilinear models for cancer survival prediction,” Bioinformatics, vol.",
"2587– 2594, 2022.",
"[31] R. J. Chen, M. Y. Lu, J. Wang, D. F. Williamson, S. J. Rodig, N. I. Lindeman, and F. Mahmood, “Pathomic Fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis,” IEEE Transactions on Medical Imaging, vol.",
"757–770, 2022.",
"[32] Z. Lv, Y. Lin, R. Yan, Y. Wang, and F. Zhang, “TransSurv: Transformer-based survival analysis model integrating histopatho- logical images and genomic data for colorectal cancer,” IEEE/ACM Transactions on Computational Biology and Bioinfor- matics, vol.",
"3411–3420, 2023.",
"[33] S. Garg, H. SS, and S. Kumar, “On-device document classifica- tion using multimodal features,” ACM India Joint International Conference on Data Science & Management of Data, p. 203–207, 2021.",
"[34] K. Xu, Z. Lin, J. Zhao, P. Shi, W. Deng, and H. Wang, “Multi- modal deep learning for social media popularity prediction with attention mechanism,” ACM International Conference on Multi- media, p. 4580–4584, 2020.",
"[35] S. Qiu, X. Cui, Z. Ping, N. Shan, Z. Li, X. Bao, and X. Xu, “Deep learning techniques in intelligent fault diagnosis and prognosis for industrial systems: a review,” Sensors, vol.",
"3, p. 1305, 2023.",
"[36] B. Li and C. N. Dewey, “RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome,” BMC Bioinformatics, vol.",
"12, p. 323, 2011.",
"[37] P. Bankhead, M. B. Loughrey, J.",
"A. Fernández, Y. Dombrowski, D. G. McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray, H. G. Coleman, J.",
"A. James, M. Salto-Tellez, and P. W. Hamil- ton, “QuPath: open source software for digital pathology image analysis,” Scientific Reports, vol.",
"7, p. 16878, 2017.",
"[38] M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley, X. Guan, C. Schmitt, and N. E. Thomas, “A method for normalizing histology slides for quantitative analysis,” IEEE International Symposium on Biomedical Imaging, pp.",
"1107–1110, 2009.",
"[39] C. Mazo, S. Barron, and C. Mooney, “Multi-gene prognostic signatures and prediction of pathological complete response to neoadjuvant chemotherapy in ER-positive, HER2-negative breast cancer patients,” Cancers, vol.",
"5, p. 1133, 2020.",
"[40] P. Blanchette, D. Sivajohanathan, J. M. Bartlett, A. Eisen, H. Feilotter, R. C. Pezo, G. Turashvili, and P. E. Williams, “Clin- ical utility of multigene profiling assays in early-stage invasive breast cancer: an Ontario Health (Cancer Care Ontario) clinical practice guideline,” Current Oncology, vol.",
"2599– 2616, 2022.",
"[41] K. Almstedt, S. Mendoza, M. Otto, M. Battista, J. Steetskamp, A. S. Heimes, S. Krajnak, A. Poplawski, A. Gerhold-Ay, A. Hasen- burg, C. Denkert, and M. Schmidt, “Endopredict® in early hor- mone receptor-positive, HER2-negative breast cancer,” Breast Cancer Research and Treatment, vol.",
"137–146, 2020.",
"[42] S. Jahn, A. Bösl, O. Tsybrovskyy, C. Gruber-Rossipal, R. Helf- gott, F. Fitzal, M. Knauer, M. Bali´c, Z. Jasarevic, F. Offner, and F. Moinfar, “Clinically high-risk breast cancer displays markedly discordant molecular risk predictions between the mammaprint and endopredict tests,” British Journal of Cancer, vol.",
"1744–1746, 2020.",
"[43] S. P. Somashekhar, S. Zaveri, D. G. Vijay, P. S. Dattatreya, R. Ku- mar, F. Islahi, and C. Bahl, “Individualized chemotherapy benefit prediction by endopredict in patients with early breast cancer in an Indian cohort,” JCO Global Oncology, no.",
"1363–1369, 2020.",
"[44] R. Buus, I. Šestak, R. Kronenwett, S. Ferree, C. A. Schnabel, F. L. Baehner, E. Mallon, J. Cuzick, and M. Dowsett, “Molecular drivers of Oncotype DX, Prosigna, EndoPredict, and the Breast Cancer Index: a TransATAC study,” Journal of Clinical Oncology, vol.",
"126–135, 2021.",
"[45] J. Warwick, L. Tabár, B. Viták, and S. W. Duffy, “Time-dependent effects on survival in breast carcinoma,” Cancer, vol.",
"1331–1336, 2004.",
"[46] C. Woo, H. Silberman, S. Nakamura, W. Ye, R. Sposto, W. J. Colburn, J. Waisman, and M. J. Silverstein, “Lymph node status combined with lymphovascular invasion creates a more powerful tool for predicting outcome in patients with invasive breast cancer,” American Journal of Surgery, vol.",
"337–340, 2002.",
"[47] H. Bor, E. N. Maina, B. Nyambega, K. Patel, C. O. Ol- wal, W. Nalyanya, and Y. Gavamukulya, “The potential of differentiation-related gene-1 (DRG1) as a biomarker for metas- tasis of estrogen receptor-positive breast cancer,” Journal of Ad- vances in Medicine and Medical Research, pp.",
"162–169, 2021.",
"[48] S. Naz, M. Siddiqui, A. I. Memon, A. M. Bhatti, Z. I. Hussain, and Iqra, “Analysis of breast cancer receptors status and molecular subtypes among female population,” Pakistan Journal of Medical and Health Sciences, vol.",
"656–658, 2023.",
"[49] M. Kang, H. Song, S. Park, D. Yoo, and S. Pereira, “Bench- marking self-supervised learning on diverse pathology datasets,” IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pp.",
"3344–3354, 2023.",
"[50] R. J. Chen and R. G. Krishnan, “Self-supervised vision transform- ers learn visual concepts in histopathology,” Annual Conference on Neural Information Processing Systems, 2021.",
"[51] X. Chen, S. Xie, and K. He, “An empirical study of training self-supervised vision transformers,” CoRR, p. 2104.02057, 2021.",
"[52] A. van den Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,” arXiv, p. 1807.03748, 2018.",
"[53] X. Wang, S. Yang, J. Zhang, M. Wang, J. Zhang, W. Yang, J. Huang, and X. Han, “Transformer-based unsupervised con- trastive learning for histopathological image classification,” Medi- cal Image Analysis, vol.",
"81, p. 102559, 2022.",
"14 [54] J. Venugopalan, L. Tong, H. R. Hassanzadeh, and M. D. Wang, “Multimodal deep learning models for early detection of Alzheimer’s disease stage,” Scientific Reports, vol.",
"11, p. 3254, 2021.",
"[55] S. Steyaert, Y. L. Qiu, Y. Zheng, P. Mukherjee, H. Vogel, and O. Gevaert, “Multimodal deep learning to predict prognosis in adult and pediatric brain tumors,” Communications Medicine, vol.",
"1, p. 44, 2023.",
"[56] M. Althobiti, K. A. El-sharawy, C. Joseph, M. Aleskandarany, M. S. Toss, A. R. Green, and E. A. Rakha, “Oestrogen-regulated protein SLC39A6: A biomarker of good prognosis in luminal breast cancer,” Breast Cancer Research and Treatment, vol.",
"621–630, 2021.",
"[57] L. Liu, J. Yang, and C. Wang, “Analysis of the prognostic signifi- cance of solute carrier (SLC) family 39 genes in breast cancer,” Bioscience Reports, vol.",
"8, p. BSR20200764, 2020.",
"[58] C. Hogstrand, P. Kille, M. L. Ackland, S. Hiscox, and K. M. Taylor, “A mechanism for epithelial-mesenchymal transition and anoikis resistance in breast cancer triggered by zinc channel ZIP6 and STAT3 (signal transducer and activator of transcription 3),” Biochemical Journal, vol.",
"229–237, 10 2013.",
"[59] S. U. Gerold, K. M. Taylor, I.",
"A. Muraina, D. Brethour, T. Nim- manon, S. Ziliotto, P. Kille, and C. Hogstrand, “Zinc transporter ZIP10 forms a heteromer with ZIP6 which regulates embryonic development and cell migration,” Biochemical Journal, vol.",
"2531–2544, 2016.",
"[60] K. M. Taylor, H. E. Morgan, K. Smart, N. M. Zahari, S. Pumford, I. O. Ellis, J. F. Robertson, and R. I. Nicholson, “The emerging role of the LIV-1 subfamily of zinc transporters in breast cancer,” Molecular Medicine, vol.",
"396–406, 2007.",
"[61] A. Sappok and U. Mahlknecht, “Ribavirin restores ESR1 gene expression and tamoxifen sensitivity in ESR1 negative breast cancer cell lines,” Clinical Epigenetics, vol.",
"3, p. 8, 2011.",
"[62] W. Xu, M. Marcu, X. Yuan, E. Mimnaugh, C. Patterson, and L. Neckers, “Chaperone-dependent E3 ubiquitin ligase CHIP mediates a degradative pathway for c-ErbB2Neu,” Cell Biology, vol.",
"12847–12852, 2002.",
"[63] W. Xia, S. Bacus, P. Hegde, I. Husain, J. Strum, L. Liu, G. Paulazzo, L. Lyass, P. Trusk, J. Hill, J. Harris, and N. L. Spector, “A model of acquired autoresistance to a potent ErbB2 tyrosine kinase inhibitor and a therapeutic strategy to prevent its onset in breast cancer,” Proceedings of the National Academy of Sciences, vol.",
"7795–7800, 2006.",
"Raktim Kumar Mondol is a PhD candidate in Computer Science and Engineering, spe- cializing in computer vision and bioinformat- ics.",
"He completed his MEng in Engineering with High Distinction from RMIT University, Australia.",
"Mondol’s research interests in- clude histopathological image analysis, clini- cal prognosis prediction, and enhancing clini- cal understanding through the interpretability of computational models.",
Ewan Millar is a Senior Staff Specialist Histopathologist with NSW Health Pathology at St George Hospital Sydney with expertise in breast cancer pathology and translational research and a strong interest in AI and digi- tal pathology applications.,
"Arcot Sowmya is Professor in the School of Computer Science and Engineering, UNSW.",
"Her major research interest is in the area of Machine Learning for Computer Vision and includes learning object models, fea- ture extraction, segmentation and recogni- tion based on computer vision, machine learning and deep learning.",
"In recent years, applications in the broader health area are a focus, including biomedical informatics and rapid diagnostics in the real world.",
"All of these areas have been supported by com- petitive, industry and government funding.",
"Erik Meijering (Fellow, IEEE), is a Profes- sor of Biomedical Image Computing in the School of Computer Science and Engineer- ing.",
His research focusses on the develop- ment of innovative computer vision and ma- chine learning (in particular deep learning) methods for automated quantitative analysis of biomedical imaging data.,
"On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset Abien Fred M. Agarap abienfred.agarap@gmail.com ABSTRACT This paper presents a comparison of six machine learning (ML) algorithms: GRU-SVM[4], Linear Regression, Multilayer Percep- tron (MLP), Nearest Neighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on the Wisconsin Diagnostic Breast Cancer (WDBC) dataset[20] by measuring their classification test accuracy, and their sensitivity and specificity values.",
The said dataset consists of features which were computed from digitized images of FNA tests on a breast mass[20].,
"For the implementation of the ML algorithms, the dataset was partitioned in the follow- ing fashion: 70% for training phase, and 30% for the testing phase.",
The hyper-parameters used for all the classifiers were manually assigned.,
Results show that all the presented ML algorithms per- formed well (all exceeded 90% test accuracy) on the classification task.,
The MLP algorithm stands out among the implemented algo- rithms with a test accuracy of ≈99.04%.,
CCS CONCEPTS • Computing methodologies →Supervised learning by clas- sification; Supervised learning by regression; Support vector machines; Neural networks; KEYWORDS artificial intelligence; artificial neural networks; classification; lin- ear regression; machine learning; multilayer perceptron; nearest neighbors; softmax regression; supervised learning; support vector machine; wisconsin diagnostic breast cancer dataset ACM Reference Format: Abien Fred M. Agarap.,
On Breast Cancer Detection: An Application of Machine Learning Algorithms on the Wisconsin Diagnostic Dataset.,
"In ICMLSC 2018: ICMLSC 2018, The 2nd International Conference on Machine Learning and Soft Computing, February 2–4, 2018, Phu Quoc Island, Viet Nam.",
"ACM, New York, NY, USA, 5 pages.",
"https://doi.org/10.1145/3184066.3184080 1 INTRODUCTION Breast cancer is one of the most common cancer along with lung and bronchus cancer, prostate cancer, colon cancer, and pancreatic cancer among others[2].",
"Representing 15% of all new cancer cases in the United States alone[1], it is a topic of research with great Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
Copyrights for components of this work owned by others than the author(s) must be honored.,
Abstracting with credit is permitted.,
"To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
Request permissions from permissions@acm.org.,
"ICMLSC 2018, February 2–4, 2018, Phu Quoc Island, Viet Nam © 2018 Copyright held by the owner/author(s).",
Publication rights licensed to Associa- tion for Computing Machinery.,
ACM ISBN 978-1-4503-6336-5/18/02...$15.00 https://doi.org/10.1145/3184066.3184080 value.,
The utilization of data science and machine learning approaches in medical fields proves to be prolific as such approaches may be considered of great assistance in the decision making process of medical practitioners.,
"With an unfortunate increasing trend of breast cancer cases[1], comes also a big deal of data which is of sig- nificant use in furthering clinical and medical research, and much more to the application of data science and machine learning in the aforementioned domain.",
"Prior studies have seen the importance of the same research topic[17, 21], where they proposed the use of machine learning (ML) algorithms for the classification of breast cancer using the Wisconsin Diagnostic Breast Cancer (WDBC) dataset[20], and even- tually had significant results.",
"This paper presents yet another study on the said topic, but with the introduction of our recently-proposed GRU-SVM model[4].",
"The said ML algorithm combines a type of recurrent neural network (RNN), the gated recurrent unit (GRU)[8] with the support vector machine (SVM)[9].",
"Along with the GRU-SVM model, a number of ML algorithms is presented in Section 2.4, which were all applied on breast cancer classification with the aid of WDBC[20].",
"2 METHODOLOGY 2.1 Machine Intelligence Library Google TensorFlow[3] was used to implement the machine learning algorithms in this study, with the aid of other scientific computing libraries: matplotlib[12], numpy[19], and scikit-learn[15].",
2.2 The Dataset The machine learning algorithms were trained to detect breast cancer using the Wisconsin Diagnostic Breast Cancer (WDBC) dataset[20].,
"According to [20], the dataset consists of features which were computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.",
The said features describe the characteristics of the cell nuclei found in the image[20].,
Figure 1: Image from [20] as cited by [21].,
"Digitized im- ages of FNA: (a) Benign, (b) Malignant.",
"There are 569 data points in the dataset: 212 – Malignant, 357 – Benign.",
"Accordingly, the dataset features are as follows: (1) radius, arXiv:1711.07831v4 [cs.LG] 7 Feb 2019 ICMLSC 2018, February 2–4, 2018, Phu Quoc Island, Viet Nam Abien Fred M. Agarap (2) texture, (3) perimeter, (4) area, (5) smoothness, (6) compactness, (7) concavity, (8) concave points, (9) symmetry, and (10) fractal dimension.",
"With each feature having three information[20]: (1) mean, (2) standard error, and (3) “worst” or largest (mean of the three largest values) computed.",
"Thus, having a total of 30 dataset features.",
"2.3 Dataset Preprocessing To avoid inappropriate assignment of relevance, the dataset was standardized using Eq.",
"1. z = X −µ σ (1) where X is the feature to be standardized, µ is the mean value of the feature, and σ is the standard deviation of the feature.",
The stan- dardization was implemented using StandardScaler().fit_transform() of scikit-learn[15].,
2.4 Machine Learning (ML) Algorithms This section presents the machine learning (ML) algorithms used in the study.,
"The Stochastic Gradient Descent (SGD) learning algo- rithm was used for all the ML algorithms presented in this section except for GRU-SVM, Nearest Neighbor search, and Support Vec- tor Machine.",
The code implementations may be found online at https://github.com/AFAgarap/wisconsin-breast-cancer.,
2.4.1 GRU-SVM.,
"We proposed a neural network architecture[4] combining the gated recurrent unit (GRU) variant of recurrent neural network (RNN) and the support vector machine (SVM), for the purpose of binary classification.",
"z = σ(Wz · [ht−1,xt ]) (2) r = σ(Wr · [ht−1,xt ]) (3) ˜ht = tanh(W · [rt ∗ht−1,xt ]) (4) ht = (1 −zt ) ∗ht−1 + zt ∗˜ht (5) where z and r are the update gate and reset gate of a GRU-RNN respectively, ˜ht is the candidate value, and ht is the new RNN cell state value[8].",
"In turn, the ht is used as the predictor variable x in the L2-SVM predictor function (given by siдn(wx + b)) of the network instead of the conventional Softmax classifier.",
The learning parameter W of the GRU-RNN is learned by the L2-SVM using the loss function given by Eq.,
The computed loss is then minimized through Adam[13] optimization.,
The same optimization algorithm was used for Softmax Regression (Section 2.4.5) and SVM (Section 2.4.6).,
"Then, the decision function f (x) = siдn(wx + b) produces a vector of scores for each cancer diagnosis: -1 for benign, and +1 for malignant.",
"In order to get the predicted labels y for a given data x, the arдmax function is used (see Eq.",
y′ = arдmax  siдn(wx + b) (6) The arдmax function shall return the indices of the highest scores across the vector of predicted classes siдn(wx + b).,
2.4.2 Linear Regression.,
"Despite an algorithm for regression problem, linear regression (see Eq.",
7) was used as a classifier for this study.,
This was done by applying a threshold for the output of Eq.,
subjecting the value of the regressand to Eq.,
"8. hθ (x) = n Õ i=0 θi · xi + b (7) f  hθ (x) = ( 1 hθ (x) ≥0.5 0 hθ (x) < 0.5 (8) To measure the loss of the model, the mean squared error (MSE) was used (see Eq.",
"L(y,θ,x) = 1 N N Õ i=0  yi −(θi · xi + b)2 (9) where y represents the actual class, and (θ · x +b) represents the predicted class.",
"This loss is minimized using the SGD algorithm, which learns the parameters θ of Eq.",
The same method of loss minimization was used for MLP and Softmax Regression.,
2.4.3 Multilayer Perceptron.,
The perceptron model was devel- oped by Rosenblatt (1958)[16] based on the neuron model by Mc- Culloch & Pitts (1943)[14].,
"The multilayer perceptron (MLP)[7] consists of hidden layers (composed by a number of perceptrons) that enable the approximation of any functions, that is, through activation functions such as tanh or sigmoid σ. hθ (x) = n Õ i=0 θixi + b (10) f  hθ (x) = hθ (x)+ = max(0, hθ (x)) (11) For this study, the activation function used for MLP was ReLU[11] (see Eq.",
"11), while there were three hidden layers that each con- sists of 500 nodes (500-500-500 architecture).",
"As for the loss, it was computed using the cross entropy function (see Eq.",
2.4.4 Nearest Neighbor.,
This is a form of an optimization prob- lem that seeks to find the closest point pi ∈p to a query point qi ∈q.,
"In this study, both the L1 (Manhattan, see Eq.",
"12) and L2 (Euclidean, see Eq.",
13) norm were used to measure the distance between p and q.,
∥p −q∥1 = n Õ i=1 |pi −qi | (12) ∥p −q∥2 = v t n Õ i=1 (pi −qi)2 (13) The code implementation was based on the work of Damien (2017)[10] in GitHub.,
"A learning algorithm such as SGD and Adam[13] is not applicable to Nearest Neighbor search, as it is practically a geometric approach for classification.",
"On Breast Cancer Detection ICMLSC 2018, February 2–4, 2018, Phu Quoc Island, Viet Nam 2.4.5 Softmax Regression.",
This is a classification model gen- eralizing logistic regression to multinomial problems.,
"But unlike linear regression (Section 2.4.2) that produces raw scores for the classes, softmax regression produces a probability distribution for the classes.",
This is accomplished using the Softmax function (see Eq.,
"P(ˆy | x) = e ˆyi Ín i=0 e ˆyi (14) L(y, ˆy) = − n Õ i=0 yi · loд  ˆyi  (15) The loss is measured by using the cross entropy function (see Eq.",
"15), where y represents the actual class, and ˆy represents the predicted class.",
2.4.6 Support Vector Machine.,
"Developed by Vapnik[9], the sup- port vector machine (SVM) was primarily intended for binary clas- sification.",
"Its main objective is to determine the optimal hyperplane f (w,x) = w · x +b separating two classes in a given dataset having input features x ∈Rp, and labels y ∈{−1, +1}.",
"SVM learns by solving the following constrained optimization problem: min 1 p wT w + C p Õ i=1 ξi (16) s.t y′ i(w · x + b) ≥1 −ξi (17) ξi ≥0,i = 1, ...,p (18) where wT w is the Manhattan norm, ξ is a cost function, and C is the penalty parameter (may be an arbitrary value or a selected value using hyper-parameter tuning).",
"Its corresponding unconstrained optimization problem is the following: min 1 p wT w + C p Õ i=1 max  0, 1 −y′ i(wixi + b) (19) where wx + b is the predictor function.",
The objective of Eq.,
"19 is known as the primal form problem of L1-SVM, with the standard hinge loss.",
"The problem with L1-SVM is the fact that it is not differentiable[18], as opposed to its variation, the L2-SVM: min 1 p ∥w∥2 2 + C p Õ i=1 max  0, 1 −y′ i(wixi + b)2 (20) The L2-SVM is differentiable and provides more stable results than its L1 counterpart[18].",
"2.5 Data Analysis There were two phases of experiment for this study: (1) training phase, and (2) test phase.",
The dataset was partitioned by 70% (train- ing phase) / 30% (testing phase).,
"The parameters considered in the experiments were as follows: (1) Test Accuracy, (2) Epochs, (3) Num- ber of data points, (4) False Positive Rate (FPR), (5) False Negative Rate (FNR), (6) True Positive Rate (TPR), and (7) True Negative Rate (TNR).",
"3 RESULTS AND DISCUSSION All experiments in this study were conducted on a laptop computer with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4, 16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU.",
Table 1 shows the manually-assigned hyper-parameters used for the ML algorithms.,
Table 2 summarizes the experiment results.,
"In addition to the reported results, the result from [21] was put into compari- son.",
[21] implemented the SVM with Gaussian Radial Basis Function (RBF) as its kernel for classification on WDBC.,
Their experiment revealed that their SVM had its highest test accuracy of 89.28% with its free parameter σ = 0.6.,
"However, their experiment was based on a 60/40 partition (training/testing respectively).",
"Hence, we would not be able to draw a fair comparison between the current study and [21].",
"Comparing the results of this study on an intuitive sense may perhaps be close to a fair comparison, recalling that the partition done in this study was 70/30.",
"With a test accuracy of ≈96.09%, the L2-SVM in this study bares superiority against the findings of [21] (SVM with Gaussian RBF, having a test accuracy of 89.28%).",
"But then again, it was based on a higher training data of 10% (70% vs 60%).",
Figure 2 shows the training accuracy of the ML algorithms: (1) Figure 2: Plotted using matplotlib[12].,
Training accuracy of the ML algorithms on breast cancer detection using WDBC.,
Figure 3: Plotted using matplotlib[12].,
Scatter plot of mean fea- tures (x0 −x9) in the WDBC.,
"GRU-SVM finished its training in 2 minutes and 54 seconds with ICMLSC 2018, February 2–4, 2018, Phu Quoc Island, Viet Nam Abien Fred M. Agarap Table 1: Hyper-parameters used for the ML algorithms.",
"Hyper-parameters GRU-SVM Linear Regression MLP Nearest Neighbor Softmax Regression SVM Batch Size 128 128 128 N/A 128 128 Cell Size 128 N/A [500, 500, 500] N/A N/A N/A Dropout Rate 0.5 N/A None N/A N/A N/A Epochs 3000 3000 3000 1 3000 3000 Learning Rate 1e-3 1e-3 1e-2 N/A 1e-3 1e-3 Norm L2 N/A N/A L1, L2 N/A L2 SVM C 5 N/A N/A N/A N/A 5 Table 2: Summary of experiment results on the ML algorithms.",
Parameter GRU-SVM Linear Regression MLP L1-NN L2-NN Softmax Regression SVM Accuracy 93.75% 96.09375% 99.038449585420729% 93.567252% 94.736844% 97.65625% 96.09375% Data points 384000 384000 512896 171 171 384000 384000 Epochs 3000 3000 3000 1 1 3000 3000 FPR 16.666667% 10.204082% 1.267042% 6.25% 9.375% 5.769231% 6.382979% FNR 0 0 0.786157% 6.542056% 2.803738% 0 2.469136% TPR 100% 100% 99.213843% 93.457944% 97.196262% 100% 97.530864% TNR 83.333333% 89.795918% 98.732958% 93.75% 90.625% 94.230769% 93.617021% Figure 4: Plotted using matplotlib[12].,
Scatter plot of error fea- tures (x10 −x19) in the WDBC.,
Figure 5: Plotted using matplotlib[12].,
Scatter plot of worst fea- tures (x20 −x29) in the WDBC.,
"an average training accuracy of 90.6857639%, (2) Linear Regression finished its training in 35 seconds with an average training accuracy of 92.8906257%, (3) MLP finished its training in 28 seconds with an average training accuracy of 96.9286785%, (4) Softmax Regression finished its training in 25 seconds with an average training accuracy of 97.366573%, and (5) L2-SVM finished its training in 14 seconds with an average training accuracy of 97.734375%.",
"There was no recorded training accuracy for Nearest Neighbor search since it does not require any training, as the norm equations (Eq.",
13) are directly applied on the dataset to determine the “nearest neighbor” of a given data point pi ∈p.,
"The empirical evidence presented in this section draws a qual- itative comparability with, and corroborates the findings of [21].",
"Hence, a testament to the effectiveness of ML algorithms on the diagnosis of breast cancer.",
"While the experiment results are all commendable, the performance of the GRU-SVM model[4] war- rants a discussion.",
The mid-level performance of GRU-SVM with a test accuracy of 93.75% is hypothetically attributed to the fol- lowing information: (1) the non-linearities introduced by the GRU model[8] through its gating mechanism (see Eq.,
"4) to its output may be the cause of a difficulty in generalizing on a linearly-separable data such as the WDBC dataset, and (2) the sensitivity of RNNs to weight initialization[5].",
"Since the weights of the GRU-SVM model are assigned with arbitrary values, it will also prove limited capability of result reproducibility, even when using an identical configuration[5].",
"Despite the given arguments, it does not necessarily revoke the fact that GRU-SVM is comparable with the presented ML al- gorithms, as what the results have shown.",
"In addition, it was a expected that the upper hand goes to the linear classifiers (Linear Regression and SVM) as the utilized dataset was linearly separa- ble.",
"The linear separability of the WDBC dataset is shown in a On Breast Cancer Detection ICMLSC 2018, February 2–4, 2018, Phu Quoc Island, Viet Nam naive method of visualization (see Figure 3, Figure 4, and Figure 5).",
"Visually speaking, it is palpable that the scattered features in the mentioned figures may be easily separated by a linear function.",
"4 CONCLUSION AND RECOMMENDATION This paper presents an application of different machine learning algorithms, including the proposed GRU-SVM model in [4], for the diagnosis of breast cancer.",
"All presented ML algorithms exhibited high performance on the binary classification of breast cancer, i.e.",
determining whether benign tumor or malignant tumor.,
"Conse- quently, the statistical measures on the classification problem were also satisfactory.",
"To further substantiate the results of this study, a CV technique such as k-fold cross validation should be employed.",
"The application of such a technique will not only provide a more accurate measure of model prediction performance, but it will also assist in determin- ing the most optimal hyper-parameters for the ML algorithms[6].",
"5 ACKNOWLEDGMENT Deep appreciation is given to the family and friends of the author (in arbitrary order): Myra M. Maranan, Faisal E. Montilla, Corazon Fabreag-Agarap, Crystal Love Fabreag-Agarap, Michaelangelo Milo L. Lim, Liberato F. Ramos, Hyacinth Gasmin, Rhea Jude Ferrer, Ma.",
"Pauline de Ocampo, and Abqary Alon.",
REFERENCES [1] [n. d.].,
https://seer.cancer.gov/statfacts/html/breast.html [2] 2017.,
Cancer Statistics.,
(Mar 2017).,
"https://www.cancer.gov/about-cancer/ understanding/statistics [3] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San- jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven- berg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.",
TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.,
http://tensorflow.org/ Software available from tensorflow.org.,
[4] Abien Fred Agarap.,
A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data.,
arXiv preprint arXiv:1709.03082 (2017).,
[5] Abdulrahman Alalshekmubarak and Leslie S Smith.,
A novel approach combining recurrent neural network and support vector machines for time series classification.,
"In Innovations in Information Technology (IIT), 2013 9th International Conference on.",
"IEEE, 42–47.",
"[6] Yoshua Bengio, Ian J Goodfellow, and Aaron Courville.",
Deep learning.,
"Nature 521 (2015), 436–444.",
[7] Christopher M Bishop.,
Neural networks for pattern recognition.,
Oxford university press.,
"[8] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.",
Learning phrase representations using RNN encoder-decoder for statistical machine translation.,
arXiv preprint arXiv:1406.1078 (2014).,
[9] C. Cortes and V. Vapnik.,
Support-vector Networks.,
"Machine Learning 20.3 (1995), 273–297.",
https://doi.org/10.1007/BF00994018 [10] Aymeric Damien.,
"2017, August 29.",
"(2017, August 29).",
"https: //github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_ BasicModels/nearest_neighbor.py Accessed: November 17, 2017.",
"[11] Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung.",
Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.,
"Nature 405, 6789 (2000), 947–951.",
[12] J. D. Hunter.,
Matplotlib: A 2D graphics environment.,
"Computing In Science & Engineering 9, 3 (2007), 90–95.",
https://doi.org/10.1109/MCSE.2007.55 [13] Diederik Kingma and Jimmy Ba.,
Adam: A method for stochastic optimiza- tion.,
arXiv preprint arXiv:1412.6980 (2014).,
[14] Warren S McCulloch and Walter Pitts.,
A logical calculus of the ideas immanent in nervous activity.,
"The bulletin of mathematical biophysics 5, 4 (1943), 115–133.",
"[15] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour- napeau, M. Brucher, M. Perrot, and E. Duchesnay.",
Scikit-learn: Machine Learning in Python.,
"Journal of Machine Learning Research 12 (2011), 2825–2830.",
[16] Frank Rosenblatt.,
The perceptron: A probabilistic model for information storage and organization in the brain.,
"Psychological review 65, 6 (1958), 386.",
"[17] Gouda I Salama, M Abdelhalim, and Magdy Abd-elghany Zeid.",
Breast cancer diagnosis on three different datasets using multi-classifiers.,
"Breast Cancer (WDBC) 32, 569 (2012), 2.",
[18] Yichuan Tang.,
Deep learning using linear support vector machines.,
arXiv preprint arXiv:1306.0239 (2013).,
"[19] Stéfan van der Walt, S Chris Colbert, and Gael Varoquaux.",
The NumPy array: a structure for efficient numerical computation.,
"Computing in Science & Engineering 13, 2 (2011), 22–30.",
"[20] William H Wolberg, W Nick Street, and Olvi L Mangasarian.",
Breast cancer Wisconsin (diagnostic) data set.,
UCI Machine Learning Repository [http://archive.,
edu/ml/] (1992).,
"[21] Elias Zafiropoulos, Ilias Maglogiannis, and Ioannis Anagnostopoulos.",
A support vector machine approach to breast cancer diagnosis and prognosis.,
"Artificial Intelligence Applications and Innovations (2006), 500–507.",
"The RSNA Abdominal Traumatic Injury CT (RATIC) Dataset Authors: Jeffrey D. Rudie, Hui-Ming Lin, Robyn L. Ball, Sabeena Jalal, Luciano M. Prevedello, Savvas Nicolaou, Brett S. Marinelli, Adam E. Flanders, Kirti Magudia, George Shih, Melissa A. Davis, John Mongan, Peter D. Chang, Ferco H. Berger, Sebastiaan Hermans, Meng Law, Tyler Richards, Jan-Peter Grunz, Andreas Steven Kunz, Shobhit Mathur, Sandro Galea-Soler, Andrew D. Chung, Saif Afat, Chin-Chi Kuo, Layal Aweidah, Ana Villanueva Campos, Arjuna Somasundaram, Felipe Antonio Sanchez Tijmes, Attaporn Jantarangkoon, Leonardo Kayat Bittencourt, Michael Brassil, Ayoub El Hajjami, Hakan Dogan, Muris Becircic, Agrahara G. Bharatkumar, Eduardo Moreno Júdice de Mattos Farina, Dataset Curator Group, Dataset Contributor Group, Dataset Annotator Group, Errol Colak.",
"Affiliations: Department of Radiology, Scripps Clinic Medical Group and University of California San Diego (J.D.R.",
"), Department of Medical Imaging, St. Michael’s Hospital, Unity Health Toronto (H.M.L., S.H., S.M., E.C.",
"), The Jackson Laboratory, Bar Harbor, ME (R.L.B.",
"), Department of Radiology, Vancouver General Hospital, Vancouver, Canada, Department of Radiology, (S.J., S.N.",
"), Department of Radiology, The Ohio State University, Columbus, Ohio (L.M.P.",
"), Memorial Sloan Kettering Cancer Center, New York, NY (B.S.M.",
"), Department of Radiology, Thomas Jefferson University, Philadelphia, PA (A.E.F.",
"), Duke University School of Medicine, Durham, NC (K.M.",
"), Department of Radiology, Weill Cornell Medicine, New York, NY (G.S.",
"), Department of Radiology and Biomedical Imaging, Yale University School of Medicine, New Haven, CT (M.A.D.",
"), Department of Radiology and Biomedical Imaging, University of California San Francisco, San Francisco, CA (J.M.",
"), Departments of Radiological Sciences and Computer Science, University of California, Irvine, CA (P.D.C.",
"), Sunnybrook Health Sciences Centre, University of Toronto (F.H.B), Alfred Health, Monash University, Melbourne, Australia (M.L.",
"), University of Utah, Salt RSNA RATIC Dataset Arxiv 5/29/2024 2 Lake City, Utah (T.R.",
"), Department of Diagnostic and Interventional Radiology, University Hospital of Würzburg, Würzburg, Germany (JP.G., A.S.K.",
"), Medical Imaging Department, Mater Dei Hospital, Msida, Malta (S.",
"), Department of Diagnostic Radiology, Queen's University, Kingston, Canada (A.D.C.",
"), School of Computing, Queen's University (M.H.",
"), Department of Diagnostic and Interventional Radiology, Eberhard-Karls-University Tübingen, Tübingen, Germany (S.A.), Division of Nephrology, Department of Internal Medicine, China Medical University Hospital and College of Medicine, China Medical University, Taichung, Taiwan (C.C.K.",
"), Big Data Center, China Medical University Hospital, Taichung, Taiwan (C.C.K.",
"), AKI- CARE (Clinical Advancement, Research and Education) Center, Department of Internal Medicine, China Medical University Hospital, Taichung, Taiwan (C.C.K.",
"), Department of Medical Imaging Liverpool Hospital, Sydney, Australia (L.A.), Hospital Universitario Ramón y Cajal, Madrid, Spain (A.V.C.",
"), Department of Radiology, Gold Coast University Hospital, Griffith University, Gold Coast, Queensland, Australia (A.S.), Department of Medical Imaging, Clínica Santa María, Santiago, Chile (F.A.S.T.",
"), Department of Radiology, Faculty of Medicine, Chiang Mai University, Chiang Mai, Thailand (A.J.",
"), Department of Radiology, University Hospitals Cleveland Medical Center and Associate Professor, Case Western Reserve University School of Medicine, Cleveland, OH (L.K.B.",
"), Department of Radiology, Tallaght University Hospital, Dublin, Ireland (M.B.",
"), Radiology Department, Arrazi Hospital, CHU Mohamed VI Cadi Ayyad University, Marrakech, Morocco (A.E.H.",
"), Department of Radiology, Koç University School of Medicine, Istanbul, Türkiye (H.D.",
"), Clinical Center University of Sarajevo, Sarajevo, Bosnia and Herzegovina (M.B.",
"), Department of Diagnostic Radiology, Medical College of Wisconsin, Milwaukee, WI (A.G.B.",
"), Department of Diagnostic Imaging, Universidade Federal de São Paulo, São Paulo, Brazil (E.M.J.M.F.",
"), Department of Medical Imaging, University of Toronto, Toronto, Canada (S.M., E.C.",
"), RSNA RATIC Dataset Arxiv 5/29/2024 3 Dataset Curator Group: Matthew Aitken, Patrick Chun-Yin Lai, Priscila Crivellaro, Jayashree Kalpathy-Cramer, Zixuan Hu, Reem Mimish, Aeman Muneeb, Mitra Naseri, Maryam Vazirabad, Rachit Saluja Dataset Contributor Group: Nitamar Abdala, Jason Adleberg, Waqas Ahmad, Christopher O. Ajala, Emre Altinmakas, Robin Ausman, Miguel Ángel Gómez Bermejo, Deniz Bulja, Jeddi Chaimaa, Lin-Hung Chen, Sheng- Hsuan Chen, Hsiu-Yin Chiang, Rahin Chowdhury, David Dreizin, Zahi Fayad, Yigal Frank, Sirui Jiang, Belma Kadic, Helen Kavnoudias, Alexander Kagen, Felipe C. Kitamura, Nedim Kruscica, Michael Kushdilian, Brian Lee, Jennifer Lee, Robin Lee, Che-Chen Lin, Karun Motupally, Eamonn Navin, Andrew S. Nencka, Christopher Newman, Akdi Khaoula, Shady Osman, William Parker, Jacob J.",
"Peoples, Marco Pereañez, Christopher Rushton, Navee Sidi Mahmoud, Xueyan Mei, Beverly Rosipko, Muhammad Danish Sarfarz, Adnan Sheikh, Maryam Shekarforoush, Amber Simpson, Ashlesha Udare, Victoria Uram, Emily V. Ward, Conor Waters, Min-Yen Wu, Wanat Wudhikulprapan, Adil Zia Dataset Annotator Group: Claire K. Sandstrom, Angel Ramon Sosa Fleitas, Joel Kosowan, Christopher J Welman, Sevtap Arslan, Mark Bernstein, Linda C. Chu, Karen S. Lee, Chinmay Kulkarni, Taejin Min, Ludo Beenen, Betsy Jacobs, Scott Steenburg, Sree Harsha Tirumani, Eric Wallace, Shabnam Fidvi, Helen Oliver, Casey Rhodes, Paulo Alberto Flejder, Adnan Sheikh, Muhammad Munshi, Jonathan Revels, Vinu Mathew, Marcela De La Hoz Polo, Apurva Bonde, Ali Babaei Jandaghi, Robert Moreland, M. Zak Rajput, James T. Lee, Nikhil Madhuripan, Ahmed Sobieh, Bruno Nagel Calado, Jeffrey D Jaskolka, Lee Myers, Laura Kohl, Matthew Wu, Wesley Chan, Facundo Nahuel Diaz RSNA RATIC Dataset Arxiv 5/29/2024 4 Abstract: The RSNA Abdominal Traumatic Injury CT (RATIC) dataset is the largest publicly available collection of adult abdominal CT studies annotated for traumatic injuries.",
"This dataset includes 4,274 studies from 23 institutions across 14 countries.",
The dataset is freely available for non- commercial use via Kaggle at https://www.kaggle.com/competitions/rsna-2023-abdominal- trauma-detection.,
"Created for the RSNA 2023 Abdominal Trauma Detection competition, the dataset encourages the development of advanced machine learning models for detecting abdominal injuries on CT scans.",
"The dataset encompasses detection and classification of traumatic injuries across multiple organs, including the liver, spleen, kidneys, bowel, and mesentery.",
Annotations were created by expert radiologists from the American Society of Emergency Radiology (ASER) and Society of Abdominal Radiology (SAR).,
"The dataset is annotated at multiple levels, including the presence of injuries in three solid organs with injury grading, image-level annotations for active extravasations and bowel injury, and voxelwise segmentations of each of the potentially injured organs.",
"With the release of this dataset, we hope to facilitate research and development in machine learning and abdominal trauma that can lead to improved patient care and outcomes.",
RSNA RATIC Dataset Arxiv 5/29/2024 5 Introduction Trauma is the most common cause of fatal injuries in Americans under the age of 45 and claims six million lives globally each year (1).,
Early accurate diagnosis and grading of traumatic injuries is critical in guiding clinical management and improving patient outcomes.,
"Computed tomography plays a central role in the initial evaluation of hemodynamically stable patients (2,3).",
"For blunt and penetrating abdominal trauma, the American Association for the Surgery of Trauma (AAST) organ injury grading system is the most well recognized system for grading solid organ injuries (4,5) and is critical in triaging patients between surgery, minimally invasive intervention, and conservative management (6).",
"While the AAST grading system is an important guide to assess solid organ injury, rapid interpretation of trauma studies is challenging given the large number of images needed to review and potential for subtle findings.",
"In fact, diagnostic errors in the interpretation of trauma are common (7) and there is high inter-rater variability in the AAST grading system (8,9).",
"Furthermore, the large variation in protocols used at different hospitals, including a single portal venous phase, multiphasic imaging, and split bolus approaches (10), can further complicate this task.",
"Automated assessment of traumatic abdominal injuries is an excellent use case for artificial intelligence (AI) algorithms given the potential to prioritize studies that may require more expedient interpretations as well as augment radiologist accuracy and efficiency, which may be particularly true in areas where subspecialists are in short supply.",
"Recent work on AI based assessment of abdominal trauma includes studies on automated detection of splenic (11-14) and liver (15) injury, hemoperitoneum (16), and pneumoperitoneum (17).",
"However, prior studies have typically been limited in scope to single organs and single institutions, and realistically not generalizable into clinical practice.",
"Thus, there is a need for large multi-institutional publicly available annotated abdominal trauma datasets to address this challenge.",
"RSNA RATIC Dataset Arxiv 5/29/2024 6 The Radiological Society of North America (RSNA) collaborated with the American Society of Emergency Radiology (ASER) and Society of Abdominal Radiology (SAR) to curate a large, publicly available expert-labeled dataset of abdominal CT images for traumatic injuries focusing on injuries to the liver, spleen, kidneys, bowel and mesentery, and active extravasation.",
"This dataset was used for the RSNA 2023 Abdominal Trauma Detection competition which attracted 1,500 competitors from around the world to develop innovative machine learning (ML) models that detect traumatic injuries on abdominal CT. Dataset Description and Usage The RSNA Abdominal Traumatic Injury CT (RATIC) dataset is composed of CT scans of the abdomen and pelvis from 4,274 patients with a total of 6,481 image series from 23 institutions across 14 countries and 6 continents.",
A detailed breakdown of patient demographics and injuries across the different institutions are provided in Table 1.,
The composition of the demographics and injuries are found in Table 2 with a breakdown of injury severity in Table 3.,
CT images are in Digital Imaging and Communications in Medicine (DICOM) format.,
Study level injury annotations and demographic information are provided in four comma-separated value files.,
"The train_2024.csv file contains information about the presence of traumatic abdominal injuries (liver, kidney, spleen, bowel/mesenteric and active extravasation) for each patient.",
The image_level_labels_2024.csv file provides image level labels for bowel/mesenteric injuries and active extravasation.,
The train_series_meta_2024.csv file contains information regarding the phase of imaging and anatomical coverage of each CT series.,
The train_demographics_2024.csv file contains information about patient demographics.,
Pixel-level segmentations of abdominal organs are provided in Neuroimaging Informatics Technology Initiative (NIfTI) format for a subset of 206 series from the training set.,
A flowchart of how the RATIC dataset was curated and annotated is shown in Figure 1 with a detailed description provided in Supplementary Materials.,
"In brief, sites provided initial RSNA RATIC Dataset Arxiv 5/29/2024 7 labels for the presence of different traumatic injuries based on clinical reports.",
Radiologist annotators recruited from the ASER and SAR then annotated solid organ injury grades and locations of bowel/mesenteric injuries and active extravasation.,
Ground truth labels for the grading of solid organ injuries were established through best of 3 majority voting and collapsed into low (AAST I - III) and high grade (IV and V) injury groups.,
Image level labels for bowel/mesenteric injuries and active extravasation were based on the consensus of different annotators.,
"Voxel-wise segmentations (Figure 2) were manually corrected after training a nnU- Net (18) on the TotalSegmentator dataset (19), focusing only on the organs being evaluated in the challenge: (1) liver, (2) spleen, (3) left kidney, (4) right kidney, and (5) bowel (representing a combination of esophagus, stomach, duodenum, small bowel, and colon).",
"Discussion We curated a large, high-quality dataset of abdominal trauma CTs with contributions from 23 institutions in 14 countries and 6 continents.",
This represents the largest and most diverse publicly available dataset of abdominal trauma CT scans.,
"This dataset provides annotations relating to injuries of the liver, spleen, kidneys, bowel, and mesentery, as well as active extravasation.",
"This rich dataset has further utility for investigators as other injuries such as hematomas, fractures, and lower thoracic injuries are present within the dataset but were not explicitly annotated due to intent of the challenge to focus on critical findings of the highest clinical importance for trauma patients.",
We chose broad inclusion criteria for the contributed CT scans.,
Our initial survey of potential contributing sites showed great variety in the protocols used for imaging abdominal trauma patients.,
"In fact, some institutions had multiple protocols and selected a protocol based on the severity of the trauma.",
"Aspects that varied across protocols included the parts of the body that were imaged, phases of imaging, and slice thickness.",
Stringent inclusion criteria that limited contributed scans to a single homogenous protocol (e.g.,
"thin slice, multiphasic CT scans of the RSNA RATIC Dataset Arxiv 5/29/2024 8 abdomen and pelvis) would severely constrain the size and potential generalizability of this dataset.",
"For this reason, we widened the inclusion criteria to facilitate a larger and more diverse dataset that could then be used to train more robust ML models.",
"Biphasic (arterial and portal venous), split bolus, and portal venous phase protocols were considered acceptable.",
Participating sites were asked to enrich the dataset with representative injuries given the relatively low prevalence of traumatic abdominal injuries on CT encountered in clinical practice.,
"Despite this request, the number of cases with injuries submitted was lower than the organizing committee had anticipated.",
"Addressing class imbalances in curated datasets is particularly important in improving ML model robustness and reducing bias (20,21).",
"An explicit effort was made by the organizing committee to reduce potential biases in the dataset by considering factors such as sex, age, injuries, and contributing site when assigning scans to the train, public test, and private test datasets.",
A challenge we faced in curating this dataset was the dramatic differences in z-axis coverage in the contributed CT scans.,
"For example, some sites imaged from the skull vertex to feet while many sites limited imaging to the abdomen and pelvis.",
"To reduce the size of the dataset and to help ML model training by reducing the search space, we decided to limit scans to the abdomen and pelvis, using an upper bound of the mid-heart and lower bound of the proximal femurs using an automated pipeline (22) and manually reviewed the processed scans.",
"Similar to prior challenges, we wanted to maximize use of the data and ensure high quality labels while not overburdening annotators.",
Contributing sites pre-labelled submitted scans with information extracted from the clinical report that allowed annotators to focus on the abnormal scans.,
We considered a variety of annotation strategies that ranged from study to pixel level annotations.,
Our past experience with the cervical spine fracture detection challenge (23) showed that the prize-winning models relied on study level annotations and segmentations rather than bounding boxes (24).,
"In addition, recent work has shown that strongly supervised models trained on slice level labels from the RSNA Brain CT Hemorrhage dataset labels (25) do not outperform RSNA RATIC Dataset Arxiv 5/29/2024 9 weakly-supervised models trained on study level labels (26).",
"Pixel level annotations of injuries, including bounding boxes, would be a time consuming task with likely poor reproducibility as abdominal injuries can be quite complex with ill-defined borders.",
We settled on providing segmentations of the relevant abdominal organ systems to assist with localization and organ labels at the study level.,
"Image level labels were provided for bowel/mesenteric injuries and active extravasation as these injuries can be subtle, present in variable anatomic locations, and present on a limited number of images.",
Individual annotators were assigned a single organ system to annotate rather than providing annotations for multiple organ systems on their assigned CT scans.,
The organizing committee felt this would improve the efficiency of the annotation process and label quality by allowing an annotator to focus on a single task and AAST injury grading scale.,
The annotators provided granular labels using the AAST grading scale for solid organ injuries.,
"Due to the well documented issues with inter-rater agreement in the grading of solid organ injuries with AAST (9) and to help model training, AAST grades I - III were collapsed into low grade injuries while grades IV and V were considered high grade injuries.",
This grouping of injury grades still provides more information than a binary label for injury and reflects many clinical practices where grade IV and V injuries are more likely to undergo surgery or endovascular treatment (4-6).,
"Rather than assigning a fixed number of CT scans for annotation, we utilized the crowd sourcing mode on the annotation platform.",
This allowed annotators to label as many cases as they wanted with the public scoreboard providing motivation.,
Each solid organ injury label was annotated independently by 3 radiologists and the final ground truth labels were established by majority.,
In scenarios where all 3 annotators assigned different gradings (i.e.,
"no injury, low grade, and high grade), a member of the organizing committee adjudicated the case and assigned the final ground truth label.",
We felt that this approach would improve annotation quality by generating labels with better inter-rater agreement and avoiding the problem of a poor-quality annotation in a single annotation scheme.,
"With an RSNA RATIC Dataset Arxiv 5/29/2024 10 approach that relies on a single annotator per scan, it can be difficult to detect poor quality annotators following completion of the trial of training cases.",
There are several limitations of this dataset.,
Ground truth labels for the grading of solid organ injuries were established through best of 3 majority voting.,
"While this represents an improvement over a single annotator, there are inherent issues with AAST grading as a result of inter-rater variability.",
We recognize the absence of delayed phase imaging as a limitation since it forms part of the AAST imaging criteria for grade II - IV renal injuries in terms of collecting system injuries.,
Delayed phase imaging was not included as it was not part of the routine protocol for most contributing sites and we were concerned that its inclusion in cases with renal injuries would bias models potentially through spurious associations rather than truly detecting collecting system injuries.,
"Finally, ground truth labels for solid organ injuries, bowel and mesenteric injuries, and active extravasation were made using a web-based annotation platform which is limited when compared to real world clinical practice with access to high resolution monitors, multi-planar thin slice imaging, clinical information, and prior imaging examinations.",
"In summary, the RATIC dataset represents the largest and most geographically diverse, publicly available expert-annotated dataset of abdominal traumatic injury CT studies.",
"With the release of this dataset, we hope to facilitate research and development in machine learning and abdominal trauma that can lead to improved patient care and outcomes.",
This dataset is made freely available to all researchers for non-commercial use.,
RSNA RATIC Dataset Arxiv 5/29/2024 11 References 1.,
"Larsen JW, Søreide K, Søreide JA, Tjosevik K, Kvaløy JT, Thorsen K. Epidemiology of abdominal trauma: An age- and sex-adjusted incidence analysis with mortality patterns.",
Elsevier; 2022;53(10):3130–3138.,
doi: 10.1016/j.injury.2022.06.020.,
"Soto JA, Anderson SW. Multidetector CT of Blunt Abdominal Trauma.",
"Radiological Society of North America, Inc.; 2012; doi: 10.1148/radiol.12120354.",
"Federle MP, Goldberg HI, Kaiser JA, Moss AA, Jeffrey RB, Mall JC.",
Evaluation of abdominal trauma by computed tomography.,
Radiological Society of North America; 1981;138(3):637–644.,
doi: 10.1148/radiology.138.3.6450962.,
"Kozar RA, Crandall M, Shanmuganathan K, et al.",
"Organ injury scaling 2018 update: Spleen, liver, and kidney.",
J Trauma Acute Care Surg.,
2018;85(6):1119. doi: 10.1097/TA.0000000000002058.,
"Dixe de Oliveira Santo I, Sailer A, Solomon N, Borse R, Cavallo J, Teitelbaum J, Chong S, Roberge EA, Revzin MV.",
"Grading Abdominal Trauma: Changes in and Implications of the Revised 2018 AAST-OIS for the Spleen, Liver, and Kidney.",
RadioGraphics.,
2023 Aug 17;43(9):e230040.,
"Padia SA, Ingraham CR, Moriarty JM, et al.",
Society of Interventional Radiology Position Statement on Endovascular Intervention for Trauma.,
Journal of Vascular and Interventional Radiology.,
2020;31(3):363-369.e2.,
doi: 10.1016/j.jvir.2019.11.012.,
"Patlas MN, Dreizin D, Menias CO, et al.",
Abdominal and Pelvic Trauma: Misses and Misinterpretations at Multidetector CT: Trauma/Emergency Radiology.,
RadioGraphics.,
Radiological Society of North America; 2017; https://pubs.rsna.org/doi/10.1148/rg.2017160067.,
"Accessed January 30, 2024.",
RSNA RATIC Dataset Arxiv 5/29/2024 12 8.,
"Pretorius EJ, Zarrabi AD, Griffith-Richards S, et al.",
Inter-rater reliability in the radiological classification of renal injuries.,
World J Urol.,
2018;36(3):489–496.,
doi: 10.1007/s00345- 017-2166-6.,
"Adams-McGavin RC, Tafur M, Vlachou PA, et al.",
Interrater Agreement of CT Grading of Blunt Splenic Injuries: Does the AAST Grading Need to Be Reimagined?,
Can Assoc Radiol J J Assoc Can Radiol.,
2023;8465371231184425. doi: 10.1177/08465371231184425 10.,
"Kim SJ, Ahn SJ, Choi SJ, Park DH, Kim HS, Kim JH.",
Optimal CT protocol for the diagnosis of active bleeding in abdominal trauma patients.,
Am J Emerg Med.,
2019;37(7):1331– 1335. doi: 10.1016/j.ajem.2018.10.011.,
"Chen H, Unberath M, Dreizin D. Toward automated interpretable AAST grading for blunt splenic injury.",
Emergency Radiology.,
2023 Feb;30(1):41-50.,
"Wang J, Wood A, Gao C, Najarian K, Gryak J.",
Automated Spleen Injury Detection Using 3D Active Contours and Machine Learning.,
Multidisciplinary Digital Publishing Institute; 2021;23(4):382. doi: 10.3390/e23040382.,
"Cheng C-T, Lin H-S, Hsu C-P, et al.",
The three-dimensional weakly supervised deep learning algorithm for traumatic splenic injury detection and sequential localization: an experimental study.,
Int J Surg.,
2023;109(5):1115. doi: 10.1097/JS9.0000000000000380.,
"Hamghalam M, Moreland R, Gomez D, et al.",
Machine Learning Detection and Characterization of Splenic Injuries on Abdominal Computed Tomography.,
Canadian Association of Radiologists Journal.,
doi:10.1177/08465371231221052 15.,
"Farzaneh N, Stein EB, Soroushmehr R, Gryak J, Najarian K. A deep learning framework for automated detection and quantitative assessment of liver trauma.",
BMC Med Imaging.,
2022;22(1):39. doi: 10.1186/s12880-022-00759-9.,
"Dreizin D, Zhou Y, Fu S, et al.",
A Multiscale Deep Learning Method for Quantitative Visualization of Traumatic Hemoperitoneum at CT: Assessment of Feasibility RSNA RATIC Dataset Arxiv 5/29/2024 13 and Comparison with Subjective Categorical Estimation.,
Radiol Artif Intell.,
Radiological Society of North America; 2020;2(6):e190220.,
doi: 10.1148/ryai.2020190220.,
"Winkel DJ, Heye T, Weikert TJ, Boll DT, Stieltjes B.",
Evaluation of an AI-Based Detection Software for Acute Findings in Abdominal Computed Tomography Scans: Toward an Automated Work List Prioritization of Routine CT Examinations.,
Invest Radiol.,
2019;54(1):55–59.,
doi: 10.1097/RLI.0000000000000509.,
"Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH.",
nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.,
Nature Methods.,
Nature Publishing Group; 2020;1–9.,
doi: 10.1038/s41592-020-01008-z.,
"Wasserthal J, Breit HC, Meyer MT, Pradella M, Hinck D, Sauter AW, Heye T, Boll DT, Cyriac J, Yang S, Bach M. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images.",
Radiology: Artificial Intelligence.,
2023 Sep;5(5).,
"Johnson JM, Khoshgoftaar TM.",
Survey on deep learning with class imbalance.,
J Big Data.,
2019;6(1):27. doi: 10.1186/s40537-019-0192-5 21.,
"Ren M, Zeng W, Yang B, Urtasun R. Learning to Reweight Examples for Robust Deep Learning.",
Proceedings of the 35th International Conference on Machine Learning.,
PMLR; 2018. p. 4334–4343.,
https://proceedings.mlr.press/v80/ren18a.html.,
"Accessed December 25, 2023.",
"Chang PD et al., DeepATLAS: One-Shot Localization for Biomedical Data.",
Under Review.,
"Lin HM, Colak E, Richards T, et al.",
The RSNA Cervical Spine Fracture CT Dataset.,
Radiology: Artificial Intelligence.,
2023;5(5):e230034.,
doi: 10.1148/ryai.230034.,
"Zixuan Hu et al., More Than Just a Game - Real World Validation of the Prize-Winning Models from the 2022 RSNA Cervical Spine Fracture Detection Challenge.",
Under Review RSNA RATIC Dataset Arxiv 5/29/2024 14 25.,
"Flanders AE, Prevedello LM, Shih G, et al.",
Construction of a Machine Learning Dataset through Collaboration: The RSNA 2019 Brain CT Hemorrhage Challenge.,
Radiology: Artificial Intelligence.,
2020;2(3):e190211.,
doi: 10.1148/ryai.2020190211.,
"Teneggi J, Yi PH, Sulam J. Examination-level Supervision for Deep Learning–based Intracranial Hemorrhage Detection at Head CT. Radiology: Artificial Intelligence.",
Radiological Society of North America; 2023;e230159.,
doi: 10.1148/ryai.230159.,
Mason D. SU-E-T-33: Pydicom: An Open Source DICOM Library.,
Medical Physics.,
2011;38(6Part10):3493–3493.,
doi: 10.1118/1.3611983.,
"RSNA RATIC Dataset Arxiv 5/29/2024 15 Tables Site ID Sex Age (y) Total Cases Nega5ve Injury Posi5ve Injury Male Female UNK Total Posi5ve Liver Injury Spleen Injury Kidney Injury Bowel Injury Ac5ve Extravasa5on Site 1 195 55 0 52.9±20.6 (18-90) 250 202 48 12 24 6 9 20 Site 2 49 1 45 37.1±13.8 (19-71) 37 UNK 95 76 19 10 9 4 0 1 Site 3 72 22 0 45.1±16.5 (18-88) 94 76 18 9 8 6 1 2 Site 4 200 93 0 41.8±17.8 (20-90) 293 147 146 48 54 78 17 28 Site 5 16 4 0 41.8±20.9 (20-85) 20 12 8 3 5 5 0 1 Site 6 343 190 0 56.3±21.4 (18-90) 533 436 97 40 37 21 13 38 Site 7 141 50 0 47.2±20.0 (18-90) 191 155 36 11 11 9 7 8 Site 8 148 45 0 46.2±20.4 (20-90) 193 107 86 26 48 23 12 20 Site 9 109 51 0 44.5±19.6 (19-90) 160 119 41 18 12 14 1 8 Site 10 96 38 0 43.7±17.8 (18-90) 134 68 66 29 14 17 8 17 Site 11 17 21 0 51.9±22.2 (20-90) 38 38 0 0 0 0 0 0 Site 12 102 31 0 42.1±17.9 (18-90) 133 85 48 28 15 16 4 6 Site 13 127 63 0 49.8±21.6 (18-90) 190 113 77 33 37 11 6 18 Site 14 179 68 0 46.9±20.8 247 135 112 46 52 33 14 43 RSNA RATIC Dataset Arxiv 5/29/2024 16 (18-90) Site 15 110 52 0 46.2±19.2 (18-90) 162 101 61 24 24 13 3 13 Site 16 117 77 0 46.4±19.7 (18-90) 194 125 69 17 30 23 2 16 Site 17 99 34 0 38.4±16.8 (18-90) 133 61 72 38 26 24 11 17 Site 18 63 11 0 47.9±18.8 (18-85) 74 50 24 9 5 5 2 7 Site 19 116 61 0 56.1±21.1 (18-90) 177 104 73 13 21 24 3 32 Site 20 162 145 0 57.1±20.6 (19-90) 14 UNK 307 232 75 18 27 9 1 20 Site 21 253 95 0 43.5±19.9 (18-90) 3 UNK 348 331 17 4 6 5 2 2 Site 22 68 37 0 44.4±20.6 (18-90) 105 90 15 8 5 4 0 1 Site 23 155 48 0 42.4±18.2 (18-88) 203 95 108 47 47 20 17 18 Total 2,937 1,292 45 48.0±20.6 (18-90) 54 UNK 4,274 2,958 1,316 491 517 370 133 336 Table 1.",
The distribution of positive and negative cases for abdominal injury with breakdown of injury class per each institution.,
Age is represented by mean ± standard deviation and the range is provided in parentheses.,
The number of cases with unknown age and sex is denoted by UNK.,
"RSNA RATIC Dataset Arxiv 5/29/2024 17 Usage Sex Age (y) Total Cases Nega5ve Injury Posi5ve Injury Male Female UNK Total Posi5ve Liver Injury Spleen Injury Kidney Injury Bowel Injury Ac5ve Extravasa5on Train 2,164 949 34 47.9±21.0 (18-90) 40 UNK 3,147 2,237 910 340 372 217 71 215 Public 282 121 1 48.2±19.6 (18-90) 6 UNK 404 - - - - - - - Private 491 222 10 48.0±19.5 (18-90) 8 UNK 723 - - - - - - - Table 2.",
The distribution of positive and negative cases for abdominal injury with breakdown of injury class for the training portion of the dataset.,
The composition of the public and private test sets is confidential.,
The demographic distribution is also provided.,
Age is represented by mean ± standard deviation and the range is provided in parentheses.,
The number of cases with unknown age and sex is denoted by UNK.,
"The Usage Liver Spleen Kidneys Bowel Injury Images Ac5ve Extravasa5on Images Low High Low High Low High Train 273 67 210 162 141 76 7,232 8,400 Table 3.",
Distribution of injuries for the training portion of the dataset.,
RSNA RATIC Dataset Arxiv 5/29/2024 18 Figures Figure 1: Summary of the data curation and annotation process.,
* Bowel/mesenteric injuries were reviewed by 2 annotators.,
RSNA RATIC Dataset Arxiv 5/29/2024 19 Figure 2.,
Example of abdominal organ segmentation with each color representing different organs.,
(A) Axial CT DICOM image demonstrating a splenic laceration (arrow).,
"(B) Image illustrating the segmentations for the liver (red), spleen (green), left kidney (blue), and gastrointestinal tract (brown) in the axial plane.",
(C) Segmentation masks overlaying the corresponding CT image.,
(D) Segmentation masks overlaying the corresponding organs on a reconstructed coronal CT DICOM image.,
DICOM = Digital Imaging and Communications in Medicine.,
RSNA RATIC Dataset Arxiv 5/29/2024 20 Supplementary Materials Data Collection An initial survey of potential contributing sites was conducted to determine the protocols used for the imaging of abdominal traumatic injuries.,
"This survey demonstrated significant variation in imaging protocols used in terms of anatomical coverage, phases of imaging, and slice thickness.",
This survey also indicated that a strict inclusion criteria would result in a substantially smaller dataset when compared to broader inclusion.,
"To help ensure a representative dataset that could lead to the development of more generalizable models, arterial, portal venous, split bolus, and multi-phasic protocol imaging were all deemed acceptable.",
Non-contrast imaging was not included as it was felt to offer little value to model training and is not a standard practice at most institutions.,
"While delayed phase imaging is important in evaluating integrity of the collecting system, the majority of the potential contributing sites did not perform it routinely.",
"Furthermore, the organizing committee expressed reservations about introducing bias and the potential for models to form spurious correlations if delayed phase imaging was more common in scans with renal injuries.",
The minimal acceptable anatomical coverage for these phases of imaging was the entire liver while no upper bound on anatomical coverage was defined.,
The maximum allowable slice thickness was 5.0 mm with a preference for thinner slice images.,
The minimum patient age was 18 years and post-laparotomy scans were excluded.,
No patient was to be represented more than once in the submitted scans.,
"Details about compliance of local ethics regulations, scan identification, and de-identification were left to the discretion of each contributing site based on their own local privacy policies.",
"Contributing sites were asked to enrich the dataset with 50% of submitted cases with at least one of the following: solid organ injuries (liver, splenic, right renal, left renal), bowel and mesenteric injuries, or active extravasation.",
The remainder of the dataset was to be split equally between normal cases and cases with traumatic findings but none of the above listed injuries.,
The contributing sites were requested to submit de-identified CT images in DICOM format.,
"These RSNA RATIC Dataset Arxiv 5/29/2024 21 sites were requested to submit a comma separated values file which included patient sex and age, as well as binary labels indicating the presence or absence of solid organ injuries (liver, splenic, right renal, left renal), bowel and mesenteric injuries, and active extravasation with information being extracted from the clinical radiology report.",
A brief note describing the site(s) of active extravasation was requested to help focus subsequent expert annotation of the scans.,
Providing AAST grading of the solid organs was not necessary for the contributing sites.,
Standardization of Anatomic Coverage and Determination of Imaging Phase The contributed CT scans had extensive variability in anatomical coverage which ranged from imaging of the upper abdomen to continuous imaging from the skull vertex to toes.,
"Manual cropping of the scans to a standardized range was deemed not feasible given the size of the dataset, available human resources, and time constraints.",
An automated pipeline (22) was used to crop the CT scans with an upper bound of the mid-heart and lower bound of the proximal femur.,
The deep learning-based algorithm was conditioned to map each voxel in a CT volume to its corresponding coordinate within a learned whole-body standard atlas space.,
"In turn, these predictions are used to project fixed upper and lower field-of-view bounds, defined once in atlas space, onto each individual exam.",
The phase of imaging is not readily apparent based on series descriptions and time stamps of the submitted DICOM images.,
The same automated pipeline was utilized to determine aortic attenuation in Hounsfield units to serve as a surrogate for the phase of imaging.,
The processed scans were manually reviewed by a radiologist to ensure appropriate anatomic coverage.,
This review showed that scans which were imaged from the skull to feet often included more of the thorax than desired following processing but it still represented a dramatic improvement in coverage when compared to the original scan.,
RSNA RATIC Dataset Arxiv 5/29/2024 22 Technical and Visual Validation A technical validation process was performed on each contributed study to ensure DICOM integrity and fidelity.,
"This involves verifying that every DICOM file in a series has been assigned identical values for the specified DICOM elements, namely: Accession Number, Acquisition Number, Bits Allocated, Bits Stored, Columns, Frame of Reference UID, Patient ID, Pixel Spacing, Rows, Series Number, Slice Thickness, Study Date, Study Time, and Study Instance UID.",
"In addition, any series with missing images or extra images using the Instance Number and Patient Image Position DICOM elements were flagged for review.",
Any DICOM images that were corrupted or not in the axial plane were removed.,
"Each exam was scrutinized to ensure it had a distinct Patient ID, Accession Number, and Frame of Reference UID to mitigate the risk of data leakage.",
Scans were manually reviewed by a radiologist as an opportunity to flag scans for removal from the dataset.,
Any CT series with insufficient coverage of the abdomen (i.e.,
"less than the entire liver), incorrect phase (e.g.",
"non-contrast, delayed phase), acute post-operative changes, non-soft tissue window images, and significant motion artifacts were removed from the dataset.",
An “incomplete FOV” was applied to CT series where the entire liver was imaged but not the entirety of the spleen and/or kidneys.,
Annotation of Cases Only CT scans flagged as positive by the contributing sites for the relevant abdominal traumatic injuries underwent annotation.,
"Scans without these injuries did not undergo review by the annotation team as contributing sites had access to clinical reports, which was deemed as being superior to volunteer annotators in establishing ground truth for these scans.",
A call for volunteer annotators was sent by email to the membership of the American Society of Emergency Radiology and the Society of Abdominal Radiology.,
Radiologists with subspeciality training and/or professional experience in abdominal trauma were invited to RSNA RATIC Dataset Arxiv 5/29/2024 23 participate as annotators.,
"Volunteers were provided with a written guide, instructional video, and if relevant, an AAST injury grading scale.",
A set of 12 training cases was provided to each volunteer and those that passed 9 of the training cases were able to continue as annotators.,
The ground truth of the training cases was established by members of the organizing committee.,
The training sample for solid organ injuries included both positive and negative cases with representation of each AAST grade.,
Training for active extravasation and bowel/mesenteric injuries included an equal number of positive and negative cases.,
Each label was annotated independently by multiple radiologists as prior AI challenges showed that a single labeler can lead to poor quality annotations.,
A majority vote for each label established the ground truth.,
Annotators were assigned to one type of annotation (e.g.,
"liver, spleen, active extravasation) rather than having them annotate every label for each scan.",
The organizing committee felt that was more efficient for annotators as they did not require to constantly switch between AAST grading scales.,
"A web-based annotation platform (md.ai, New York) which shares many features of a Picture Archiving and Communication System (PACS) workstation was used.",
"Rather than assigning a fixed number of cases to annotators, annotations were performed in a crowd sourcing mode where volunteers were free to determine how many scans they wanted to annotate.",
A public scoreboard that ranked annotators was used to motivate the volunteers.,
"Liver, Spleen, and Kidneys The exam level annotation of liver, spleen, and renal injuries provided by the contributing sites was not made available to annotators.",
Each solid organ injury underwent annotation independently by 3 radiologists using the AAST grading system.,
Annotators had the option to dispute the contributing site label for scans they deemed as being normal.,
The individual AAST gradings were collapsed into low (AAST I - III) and high grade (IV and V) injury groups due to poor inter-rater observation for assigning AAST scores (8) and to facilitate model training.,
In RSNA RATIC Dataset Arxiv 5/29/2024 24 scenarios where all 3 annotators assigned different gradings (i.e.,
"no injury, low grade, and high grade), a member of the organizing committee adjudicated the case and assigned the final ground truth label.",
"For multi-phasic imaging, the solid organ injury labels were applied to every series.",
Bowel/Mesenteric Injury and Active Extravasation Annotators were instructed to provide image level labels for bowel/mesenteric injuries and active extravasation.,
"For the active extravasation annotation task, 3 radiologists independently reviewed each scan.",
Bowel/mesenteric injuries underwent independent review by 2 radiologists rather than 3 because of limited resources and time constraints.,
The annotators were instructed to apply image level labels to both series in a multi-phasic scan.,
Any image that was labeled as positive by at least 2 annotators was considered as positive for active extravasation and by at least 1 annotator was considered positive for bowel/mesenteric injury.,
"Segmentation To provide anatomical context to injuries, voxel-level segmentations of abdominal organs on a subset of the dataset are provided.",
These cases were enriched for the presence of high- grade or multiple injuries to better cover atypical appearances of injured organs.,
"Cases with two or more injuries were selected for segmentation using stratified random sampling, where injuries included renal (left and/or right), liver, splenic, bowel/mesenteric, and active extravasation.",
"Samples were stratified on the type of injury, site, age group (18-35, 36-55, 55+), and sex.",
"For sampling purposes, cases missing age or sex were randomly assigned to an age group or sex, respectively.",
"Of the cases with two or more injuries, we sampled cases with at least one severe (high grade) injury and then also sampled cases with only low grade injuries.",
"In total, 148 studies and 245 series with two or more injuries were sampled for segmentation.",
Thirteen studies and a total of 30 series were removed from the sampled pool due to errors in converting them to NIfTI format.,
Nine series from seven studies were removed due to segmentations failing as a result of RSNA RATIC Dataset Arxiv 5/29/2024 25 cases having DICOM PixelData processed with a different binary value representation.,
"A total of 206 segmentations (128 studies and 206 series) are provided, comprising 92 cases with at least one severe (high grade) injury and 36 cases with only low grade injuries.",
"nnU-Net (18), a 3D fully-convolutional network (3D Res-U-Net), was used to train a preliminary network on the TotalSegmentator dataset (19), focusing only on the organs being evaluated in the challenge.",
"This consisted of (1) liver, (2) spleen, (3) left kidney, (4) right kidney, and (5) bowel.",
"Bowel represented a combination of esophagus, stomach, duodenum, small bowel, and colon.",
"The 3D patch size was auto selected to be 128 x 128 x 128 (x, y, z dimensions) and training was performed on a RTX 3090 GPU (CUDA version 11.2; NVIDIA, Santa Clara, CA; 24 GB memory) for 1,000 epochs using a combination of cross entropy and Dice loss function (1:1).",
"This network was applied to the 206 training cases and then manually corrected by one of five radiologists (JDR, EC, PL, PC, BM with 8, 14, 1,5 and 2 years of experience respectively).",
Data Structure CT images in DICOM format are organized with a [patient_id]/[series_id]/[image_instance_number].dcm hierarchy.,
Study level injury annotations and demographic information are provided in CSV files.,
Patient_id is a unique study level identifier and matches the last token of the Study Instance UID.,
"For example, for a Study Instance UID value of 1.2.123.12345.1.2.3.201, the Patient ID would be 201.",
"Similarly, series_id is a series level identifier that matches the last token of the Series Instance UID.",
Image_instance_number corresponds to the Instance Number data element and indicates the position of an image within the series.,
"The dataset contains 141 CT scans (95 in the training set, 9 in the public test set, and 37 in the private test set) where DICOM PixelData was processed with a different binary value representation that will appear unusual when opened using the pydicom Python library (27) or other DICOM viewers.",
The organizing committee decided against altering the DICOM PixelData RSNA RATIC Dataset Arxiv 5/29/2024 26 to remediate this issue in order to maintain DICOM file integrity.,
"Instead, a potential solution was provided on the Kaggle platform (https://www.kaggle.com/competitions/rsna-2023-abdominal- trauma-detection/discussion/427217).",
The train_demographics_2024.csv file contains information about patient demographics.,
Patient_id is a unique study level identifier and matches the last token of the Study Instance UID.,
The Age and Sex columns correspond to patient age and sex.,
Patient ages above 89 have been replaced with 90+ to maintain compliance with HIPAA.,
The train_series_meta_2024.csv file contains information regarding the phase of imaging and anatomical coverage of each CT series.,
The aortic_hu column indicates the attenuation value of the abdominal aorta in Hounsfield units which acts as a proxy for the phase of imaging.,
"For a multiphasic CT scan, the higher value indicates arterial phase imaging and the lower portal venous phase imaging.",
"The incomplete_organ column indicates if any of the liver, spleen, or kidneys were not completely imaged for a particular series.",
The train_2024.csv file contains information about traumatic abdominal injuries for each scan.,
"The liver, spleen, and kidney columns can contain one of three values: healthy (i.e.",
"not injured), low grade injury, and high grade injury.",
The bowel and extravasation columns are binary labels that indicate bowel/mesenteric injury and active extravasation respectively.,
"The any_injury column indicates if any of the liver, spleen, kidney, bowel/mesentery, and active extravasation are positive for injury.",
The image_level_labels_2024.csv file provides image level labels for bowel/mesenteric injuries and active extravasation.,
"The injury_name column can have one of two values, Active_Extravasation or Bowel, and indicates if that particular injury is present on the image with the corresponding Instance Number in DICOM metadata.",
The segmentation files are named according to Series Instance UID and represent a subset of the training set.,
"Segmentation labels have the following values: (1) liver, (2) spleen, (3) left kidney, (4) right kidney, and (5) bowel consisting of esophagus, stomach, duodenum, small RSNA RATIC Dataset Arxiv 5/29/2024 27 bowel and large bowel.",
"Please note, the NIFTI and DICOM files may not be in the same orientation.",
Header information in both NiFTI and DICOM metadata should be used to determine the appropriate orientation.,
Aligning the orientation of the NiFTI and DICOM file data requires consideration of the affine matrix.,
The train_dicom_tags_2024.parquet file provides the metadata extracted from the header of every DICOM image in the dataset.,
"Kaggle Competition Due to the heterogeneity of the dataset, where each institution exhibits distinct DICOM metadata features, data harmonization is crucial for standardizing data input.",
A white list approach is used to address this step by restricting the number of DICOM elements and acts to perform another step of de-identification.,
"The selected DICOM elements to retain are: Bits Allocated, Bits Stored, Columns, Content Date, Content Time, Frame of Reference UID, High Bit, Image Orientation Patient, Image Position Patient, Instance Number, kVP, Patient ID, Patient Position, Photometric Interpretation, Pixel Data, Pixel Representation, Pixel Spacing, Rescale Intercept, Rescale Slope, Rows, SOP Instance UID, Samples Per Pixel, Series Instance UID, Series Number, Slice Thickness, Study Instance UID, Window Center, and Window Width.",
"All identifying DICOM elements, including Patient ID, Study Instance UID, Series Instance UID, and SOP Instance UID were de-identified using the pydicom Python library (24).",
"Additionally, given the substantial size of the dataset, each DICOM file underwent compression using the Run-Length Encoding Lossless (RLELossless) method.",
"The dataset was partitioned into train, public test, and private test sets and included 723 cases in the private test set, 404 cases in the public test set, and 3,147 cases in the training set.",
All segmentation cases were assigned to the training set.,
"Similar to sampling for segmentation cases, test set cases were selected using stratified random sampling, stratified on injury for positive cases and for both positive and negative cases, stratified on site, age group (18-35, 36- RSNA RATIC Dataset Arxiv 5/29/2024 28 55, 55+), and sex.",
"For sampling purposes, injuries included kidney (left and/or right), liver, splenic, bowel/mesenteric, and active extravasation.",
Cases missing age or sex were randomly assigned to an age group or sex during the sampling process.,
A single site did not include any positive cases so all cases from that site were assigned to the training set.,
"From cases not used in segmentation, we sampled cases with two or more injuries, one set with at least one high grade injury and another set with only low grade injuries.",
"Next, negative cases and positive cases with only one injury were sampled using stratified random sampling as described above, with negative cases stratified only on site, age group, and sex.",
"After these sets were assigned to the test set, an additional 500 negative cases not yet assigned to train or test sets were randomly sampled for inclusion in the test set.",
"To ensure a reasonable number of training cases for bowel extravasation, some cases initially assigned to the test set were randomly selected and assigned to the training set.",
All other negative cases were assigned to the training set.,
"From the test set, stratified random sampling was used to select approximately 65% of the cases for the private test set with the remaining cases assigned to the public test set.",
"Study Identification, Data Extraction, and Data De-identification Contributing sites are listed in alphabetical order and do not correspond to the order in Table 1.",
"Alfred Health, Australia The institutional archive was searched for all contrast-enhanced (split bolus) abdominal CT scans performed for trauma between January 2020 and January 2023.",
"The URN, Age, Sex, Accession Number, Report Text and Interpretation Code were retrieved.",
"These studies were filtered using the interpretation code (normal, abnormal, and critical) to assist with classification.",
Radiology reports were reviewed and findings abstracted as present or not present.,
"When extravasation was present, the location, series and image number were recorded.",
"Studies were sent to XNAT (NRG, Washington University School of Medicine) and de-identified using a DicomEdit script and RSNA RATIC Dataset Arxiv 5/29/2024 29 a custom written Python script.",
The desired axial series were manually identified and exported from XNAT.,
"Chiang Mai University, Thailand The Faculty of Medicine at Chiang Mai University searched their PACS backup archive (Synapse Radiology PACS version 5.7.000; FUJIFILM Medical systems) using RIS (Envision.Net) for CTs of the abdomen performed on patients with abdominal trauma in the emergency room between January 1, 2018 and December 31, 2022.",
Radiology findings were reviewed by two radiologists.,
Images were downloaded from the PACS in DICOM format.,
"China Medical University Hospital (CMUH), Taichung, Taiwan This study was approved by the Ethics Committee and Institutional Review Board of CMUH (IRB No.",
CMUH112-REC1-074).,
"The Big Data Center of CMUH obtained all data from the iHi Data Platform that includes the carefully verified electronic health data of administrative and demographic information; diagnoses, medical and surgical procedures, and prescriptions; laboratory measurements; physiologic monitoring records; and data on hospitalization; catastrophic illness status; registry data; and National Death Registry from patients who sought care at CMUH between 2003 and 2020.",
Patients with trauma who received care in the Emergency Department (ED) of CMUH between 2012 and 2020 were initially included from the CMUH’s Trauma Registry.,
"To identify the source population of patients with organ injuries in the abdomen, we selected patients aged 18 years or older who had the ICD diagnosis codes for abdominal organ injuries (ICD-9: 863, 864, 865, 866, 867, 868) and underwent an abdominal computed tomography (CT) scan during their emergency visit.",
"To identify the source population of adult patients with bone fractures, we excluded those who had organ injuries-related ICD codes during the same ED visit and kept those who had the ICD diagnosis for rib, lumbosacral spine, femur, and pelvis fractures (ICD-9: 805, RSNA RATIC Dataset Arxiv 5/29/2024 30 806, 807, 808, 820, 821).",
"To obtain the source population of control patients without any organ injury or fractures, we excluded all patients admitted to ED with ICD codes for organ-related injuries or fractures and those without any CT scan during their ED visits.",
"Next, for each source population, we manually reviewed the CT reports and annotated the key words of organ injuries (e.g., hepatic, splenic, renal, and bowel or mesenteric injuries, or active extravasation), traumatic injuries to other structures without organ injury, and negative for any traumatic injury.",
"Furthermore, we kept only the records from patients who had consented on providing data for research use or who had died before the data retrieval date.",
"Finally, we randomly selected cases with 50% being organ injuries cases, 25% fracture cases, and 25% normal controls.",
The DICOM images of the axial and soft tissue window kernel were downloaded from the institutional PACS.,
"The de-identification process complied with ISO-certified CMUH's data security specifications pipeline for ISO-29100, ISO-29191, and ISO-27001.",
"We performed irreversible hash calculations on the patient's personal information in the DICOM file (including name, birthday, medical record number, national identity number, and any identifiable information), and re-established an index value based on the serial number to preserve links between CT data.",
"A total of 42 DICOM tags were retained, all of which were imaging parameters during CT acquisition such as kVP.",
"Clinica Santa Maria, Santiago, Chile This retrospective study received institutional approval with a waiver for informed consent.",
A database search was performed using the institutional RIS (AGFA Healthcare Enterprise Imaging; version 8.1.2).,
"The search included abdomen and pelvis contrast-enhanced CTs obtained between January 1, 2017 and March 31, 2023, with the following keywords: fracture, laceration, contusion, hematoma, active bleeding, bleeding, or active extravasation.",
"Radiology reports were manually reviewed to assess exclusion criteria, including no history of trauma, recent surgery, and patient age less than 18 years.",
"Reports were classified as positive or negative for liver injury, RSNA RATIC Dataset Arxiv 5/29/2024 31 splenic injury, renal injury, bowel or mesenteric injury, active extravasation, hemorrhage or traumatic fluid collections, lower rib fractures, spinal fractures, and proximal femora or pelvis fractures.",
CT scans with equivocal reports were excluded.,
Positive CT scans and a proportional number of negative scans were downloaded from the institutional PACS and de-identified.,
A body radiologist reviewed the extracted images to ensure optimal image quality and the absence of private health information.,
"Eberhard Karls University Tübingen, Germany University Hospital Tuebingen searched their institutional RIS (mesalvo, Mannheim, Germany) for emergency department contrast CT scans performed between 2018 and 2023.",
Studies were included according to inclusion criteria and radiology reports were manually reviewed and categorized as positive or negative for trauma.,
CT scans with equivocal reports were excluded.,
Studies with axial section thickness greater than 3.0 mm were excluded.,
"For all cases, patient sex and age were collected.",
"For studies positive for trauma, additional information on injuries were extracted from the report.",
"Images in the axial plane were downloaded in DICOM format and de-identified from PACS (syngo.via, Siemens Healthineers, Erlangen, Germany) to a central workstation.",
"Gold Coast, Australia Following approval by the Gold Coast Hospital and Health Service, a PACS search to identify all trauma CT whole body scans of Emergency Department patients performed between January 1, 2013, and December 31, 2022.",
"Studies not meeting inclusion criteria (age > 18 years, biphasic protocol covering solid abdominal organs) were discarded.",
"The study reports were manually reviewed and scans were categorized as either positive or negative for visceral or bony injury, and further classified by type of injury.",
Thin slice axial sequences were downloaded from the PACS in DICOM format and underwent de-identification.,
"RSNA RATIC Dataset Arxiv 5/29/2024 32 Hospital Universitario Ramón y Cajal, Spain With the approval of the Hospital Ramón y Cajal review board, reports from emergency department abdominal and pelvis CT scans with intravenous contrast performed between January 2013 and March 2023 were exported from the institutional archive and manually reviewed by a board-certified radiologist and a senior resident.",
Reports were classified as positive or negative for traumatic abdominal injury.,
The specific abdominal traumatic injuries were recorded for each positive scan.,
Positive CT scans and an equal number of negative scans that include axial images (maximum of 3.0 mm) with arterial and venous phase were downloaded from PACS (Fujifilm Synapse 3D v6.1) and de-identified.,
Each CT scan was manually reviewed to assess diagnostic image quality and to ensure the absence of protected health information.,
"Koç University, Türkiye Following approval of the Koç University institutional review board, a search was performed of the institutional PACS (Sectra IDS7; Sectra AB) for contrast-enhanced abdominal CT studies containing the word “trauma” obtained between 2016 and 2023.",
The corresponding radiology reports were manually reviewed by a radiologist and categorized as positive or negative for abdominal trauma findings.,
CT scans with equivocal reports and patients younger than 18 years of age were excluded.,
"In all cases, data including patient age, study date, and patient sex were collected.",
"For the positive cases, the locations of positive trauma findings were noted.",
Axial images were anonymized and manually exported from the PACS as DICOM images.,
The images were then uploaded to MD.ai for final curation and processing.,
"Kingston Health Sciences Centre and Queen’s University, Kingston, Canada Kingston Health Sciences Centre retrospective search was performed using the institutional RIS (GE Healthcare’s Centricity Universal Viewer) and Nuance mPower (Nuance Communications) RSNA RATIC Dataset Arxiv 5/29/2024 33 for contrast-enhanced CT of the chest, abdomen, and pelvis requested by the emergency department, and obtained between June 1, 2012, and April 25, 2023.",
Prior approval of the Queen’s University Health Sciences & Affiliated Teaching Hospitals Research Ethics Board was obtained.,
"The search keyword was “trauma.” Images were reviewed by an abdominal radiologist, and cases which were not obtained in the setting of acute trauma or did not utilize the institutional trauma protocol were excluded.",
The location and grading of injuries was documented.,
Images of positive cases and negative cases were downloaded from the PACS (GE Healthcare’s Centricity Universal Viewer) using RSNA Anonymizer.,
Custom scripts written in the Python programming language were used to extract the axial arterial and portal venous phase series with a section thickness of 2.5 mm or less.,
Images were de-identified using RSNA Anonymizer.,
Extracted images were manually reviewed to ensure the absence of protected health information.,
"Marrakech University Hospital, University Caddi Ayyad, Morocco Faculty of Medicine and Pharmacy of Marrakech, Caddi Ayyad University searched their PACS backup archive (Syngo Plaza, Siemens) using RIS (Hosix.net) for abdominal trauma CT obtained between January 1, 2021 and December 31, 2022.",
"Radiology reports were searched for a specific keyword, manually reviewed, and categorized as positive or negative.",
Images were downloaded from the PACS in DICOM format.,
Images were de-identified using RSNA Anonymizer.,
"Mater Dei Hospital, Malta A retrospective search was performed using the institutional RIS (GE Healthcare’s Centricity Universal Viewer) for CT poly-trauma (CTPOLTRA code) studies performed between January 2015 and February 2023, including patients that were 18 years of age or older.",
Radiology reports were individually reviewed and categorized as positive or negative for acute intra-abdominal injuries.,
The positive cases were then documented according to the types of injuries present.,
A spectrum of injury severity was also obtained.,
"Positive and negative CT cases were downloaded RSNA RATIC Dataset Arxiv 5/29/2024 34 and anonymized from the local PACS (GE Healthcare’s Centricity Universal Viewer), which were then again de-identified using RSNA Anonymizer.",
Each of the extracted studies was reviewed in order to ascertain the absence of protected health information.,
"Medical College of Wisconsin A process for extracting, preparing, and analyzing computed tomography (CT) scans from emergency department (ED) patients for radiological review was developed.",
"The initial step involved the execution of an SQL query to the electronic medical record (Epic Clarity) to retrieve data of subjects who underwent abdominal, pelvis, or chest CT scans with multi-volume axial scans.",
"Following this, the corresponding DICOM images for each subject were accessed from the PACS system via a research-dedicated query/retrieve process.",
De-identification proceeded using Pandas dataframe manipulation in conjunction with Python's hash function.,
"This process resulted in two distinct spreadsheets: one containing participant keying information and the other featuring only hashed IDs alongside corresponding reports, supplemented with additional columns for RSNA-specified injuries and biological landmarks.",
Radiologists then labeled the correct injuries and landmarks for each subject.,
"Mount Sinai Health System, New York City Mount Sinai Health System utilized Nuance mPower to search for all CT studies between January 1, 2016 and February 1, 2023 leveraging our department’s dedicated trauma CT angiography exam description.",
The resultant studies were then annotated manually by reviewing reports for findings included in the dataset inclusion criteria.,
Patients younger than 18 and older than 89 years-old were excluded.,
Corresponding accession numbers with radiology reports facilitated batch transfer of image data to an on-premises XNATs server and de-identified by stripping all RSNA RATIC Dataset Arxiv 5/29/2024 35 DICOM meta-data tags containing personal health information.,
A key was created using anonymized study IDs linked to radiology report annotations.,
"NSW Health, Australia A retrospective search was conducted within our institutions PACS for CT abdominal and pelvic trauma imaging.",
Each report was manually reviewed and categorized into positive or negative for trauma findings.,
"These were then further divided into subgroups based on the location of the trauma (liver, spleen, renal, bowel, extravasation, peritoneal, rib, lumbosacral, pelvic, femoral).",
Inclusion criteria were > 18 years of age and CT images conducted using a biphasic protocol.,
Axial DICOM images were downloaded and anonymized using the RSNA Anonymizer tool.,
"Tallaght University Hospital, Dublin, Ireland This project received institutional approval with a waiver for informed consent.",
A data protection impact assessment was carried out and approved by our local data protection office.,
"A retrospective search of our institutional PACS for CT of the thorax, abdomen and pelvis performed with the clinical indication of trauma performed between January 2020 and December 2022.",
CT scans without biphasic protocol assessing the upper abdominal solid viscera and scans of patients younger than 18 years of age were excluded.,
Reports were reviewed and relevant findings were recorded on a spreadsheet.,
A selection of normal and abnormal studies was made.,
Images were downloaded from PACS and de-identified using anonymizer software.,
A further round of de-identification was then performed on RSNA Anonymizer.,
"Thomas Jefferson University Hospital, USA Following approval of the Thomas Jefferson University institutional review board, a corpus of radiology reports between 2021 and 2023 was extracted from the dictation system (Nuance Powerscribe, Burlington, Vermont) using the mPower natural language processing search tool.",
"RSNA RATIC Dataset Arxiv 5/29/2024 36 The searches were filtered for all emergency department encounters with an indication of ""trauma"" using two CT body examination codes (CT CHEST ABDOMEN PELVIS WO CONTRAST and CT CHEST ABDOMEN PELVIS WITH CONTRAST) over a two year for 15 hospitals in the Jefferson network, yielding a more heterogeneous population and set of acquisition devices.",
Each search query was expanded to be inclusive of a variety of injuries of each type (e.g.,
"INDICATION:(""trauma"") & IMPRESSION:(""liver injury"" or ""liver laceration"" or ""liver contusion"" or ""subcapsular hematoma"" or ""AAST"" or ""portal vein injury"" or ""hepatic arterial injury"").",
"Eight searches were performed, each focusing on a specific feature needed for the dataset: liver, splenic, renal, bowel injuries, active extravasation, hemorrhage, and fracture mentioned in the impression of the report only.",
"A final search for controls was performed where ""no acute injury"", ""no acute abnormality"" or ""no traumatic abnormality"" was found in the impression section of the report.",
Each search yielded between fifty and one-hundred exams.,
"The exams were then randomized and combined, and duplicates were removed yielding one exam per patient.",
The reports were double-checked and coded for each of the eight search categories.,
The adjudicated examinations were then extracted from the PACS archive (Philips Intellispace) and de-identified using RSNA Anonymizer.,
"The RSNA Anonymizer utility also removed any secondary capture objects from the examinations, as well as any scanned documents or dose images.",
"Universidade Federal de São Paulo, Brazil All radiology reports from abdominal CT scans performed between 2012 and 2021 were exported from the RIS and searched for trauma-related words to pre-label the studies.",
Reports were manually reviewed to correct any inconsistencies in the pre-labeling process while labeling the organs affected by the trauma.,
The studies that met the inclusion criteria were downloaded from PACS using an in-house developed script.,
"RSNA RATIC Dataset Arxiv 5/29/2024 37 Unity Health Toronto, Canada Unity Health Toronto searched their institutional RIS (syngo, Siemens Medical Solutions USA) using Nuance mPower (Nuance Communications) for contrast enhanced trauma CT chest, abdomen and pelvis studies obtained between January 1, 2018 and April 15, 2023.",
The studies that met the inclusion criteria were manually reviewed by a radiologist and categorized as positive or negative for the relevant traumatic abdominopelvic injury.,
The location and grade of injury was recorded in positive cases and a balanced dataset was created.,
"Patient age, sex, and study date were collected for all studies.",
"Axial soft tissue window images were downloaded from PACS (Carestream PACS, Carestream Health, Rochester, NY, U.S.) in DICOM format and de-identified using RSNA Anonymizer.",
"University Hospital Würzburg, Würzburg, Germany With approval from the local institutional review board (IRB number: 20230227 01), two radiologists of University Hospital Würzburg with 6 and 9 years of training in the field searched the institutional database for biphasic trauma scans (late arterial phase covering the thorax and solid abdominal organs plus portal venous phase of the abdomen and pelvis).",
Radiology reports between September 2019 and December 2022 were manually reviewed.,
"The presence of liver, splenic, renal, bowel, lower rib, lumbosacral spine, pelvic, and proximal femur injuries were determined in dichotomous fashion (present/absent).",
"In addition, the presence and location of active extravasation were noted as well as the presence of hemorrhage/traumatic fluid.",
Patient age and sex were also recorded.,
"CT scans with equivocal reports, a different scan protocol, or of patients younger than 18 years were excluded.",
A random sample of negative cases that equaled the number of positive scans was added to generate a balanced dataset.,
"Images in axial orientation were downloaded from the local PACS (Merlin, Phönix-PACS) in DICOM format and de-identified using RSNA Anonymizer.",
"RSNA RATIC Dataset Arxiv 5/29/2024 38 University Hospitals Cleveland Medical Center, USA Following approval of the University Hospitals Cleveland Medical Center institutional review board, our research team queried our PACS (Sectra) for exams with the Current Procedural Terminology (CPT) code for CTs including the abdomen and pelvis with a trauma designation presenting to the emergency room between April 1, 2013 and April 1, 2023 for patients between the ages of 18 to 89.",
Access to these results were via the Power BI Cloud Service.,
The final radiology reports were consecutively reviewed and categorized as positive or negative for abdominal trauma as well as the specific location of the trauma.,
Equivocal findings were reviewed individually by returning to the original images by our research team.,
"Additional data including the patient age, study date, and accession number were also collected.",
"Images formatted in the axial, coronal, and sagittal planes were downloaded from the PACS to a central workstation where the studies were anonymized using the RSNA anonymizer tool.",
"University of Sarajevo, Sarajevo, Bosnia and Herzegovina A comprehensive retrospective search was conducted using the institutional PACS, Radiology Information System, and Hospital Information System scans with contrast media acquired between January 2012 and March 2023.",
"Rigorous peer-reviewed analysis of CT reports was undertaken to apply exclusion criteria, which included patients under 18 years of age and unenhanced CT scans.",
This meticulous process involved assessment by two experienced radiologists.,
"For each positive case identified, meticulous documentation of the precise location of the injury was carried out.",
"University of Utah, USA University of Utah searched their institutional RIS (Radiant; Epic Systems Corporation) using Nuance mPower (Nuance Communications) to identify CT abdomen and pelvis studies for the AI challenge.",
"The searches used included the following: AAST, trauma AND ""bowel injury"", RSNA RATIC Dataset Arxiv 5/29/2024 39 “shattered kidney” OR ""renal laceration"" OR ""kidney laceration"", “shattered spleen” OR ""spleen laceration"" OR ""splenic laceration"".",
"Since liver injury was relatively common among our patient population, there was not a dedicated search for liver injury.",
"The results of this search were manually reviewed and categorized as either abdominal organ trauma, traumatic findings without abdominal organ injury, and normal CT scan in a trauma patient.",
"Studies were dated from July 1, 2013 through May 18, 2023.",
Additional data including patient age (if 89 years old or younger) and patient sex were also collected.,
CT scans with equivocal reports were excluded.,
"Images from the CT abdomen/pelvis exam in the portal venous phase formatted in the axial plane using the soft tissue kernel were downloaded from the PACS (IntelliSpace PACS; Philips Healthcare) as well as arterial phase CT abdomen/pelvis, arterial phase CT chest, or delayed phase CT abdomen/pelvis if available.",
Images were downloaded to a local workstation where the studies were anonymized.,
Vancouver General Hospital Abdominal trauma cases from Vancouver General Hospital were collected in a systematic manner.,
"The institutional Radiology Information System (RIS) at Vancouver General Hospital, was queried for multiphasic abdominal trauma CT scans performed between January 1, 2015, and December 31, 2022.",
The search was facilitated by the hospital RIS system with a focus on emergency department cases.,
"Subsequently, the collected data underwent a comprehensive review process.",
"An emergency and trauma radiologist meticulously examined the radiology reports, categorizing them as either positive or negative for abdominal trauma.",
"For positive cases, details regarding the patient age, gender and site of abdominal trauma were systematically recorded.",
Exclusion criteria were applied to ensure the integrity and relevance of the dataset.,
"CT scans with equivocal reports, descriptions of prior abdominal surgeries were excluded from the study.",
"To achieve a balanced dataset, a random sample of negative cases was selected, matching the number of RSNA RATIC Dataset Arxiv 5/29/2024 40 positive scans.",
The actual CT images were retrieved in DICOM format from the PACS at Vancouver General Hospital.,
"Prior to further analysis, all images underwent a rigorous de- identification process.",
The RSNA Anonymizer tool was employed to strip any personally identifiable information.,
"To ensure compliance with privacy regulations and data security, a radiologist conducted a manual review of the de-identified images, confirming the absence of private health information, details of prior abdominal surgeries, and any indication of intravenous contrast media administration.",
Using convolutional neural networks for the classification of breast cancer images.,
"Eric Bonnet Centre National de Recherche en G´enomique Humaine (CNRGH) CEA, Universit´e Paris–Saclay, Evry, France eric.bonnet@cea.fr March 2024 Abstract Motivation: Breast cancer is the leading cancer type in women world- wide.",
An important part of breast cancer staging is the assessment of the sentinel axillary node for early signs of tumor spreading.,
"However, this as- sessment by pathologists is not always easy and retrospective surveys often requalify the status of a high proportion of sentinel nodes.",
"Convolutional Neural Networks (CNNs) are a class of deep learning algorithms that have shown excellent performances in the most challenging visual classification tasks, with numerous applications in medical imaging.",
In this study I compare twelve different CNNs and different hardware acceleration de- vices for the detection of breast cancer from microscopic images of breast cancer tissue.,
Convolutional models are trained and tested on two pub- lic datasets.,
"The first one is composed of more than 300,000 images of sentinel lymph node tissue from breast cancer patients, while the second one has more than 220,000 images from inductive breast carcinoma tissue, one of the most common forms of breast cancer.",
"Four different hardware acceleration cards were used, with an off–the–shelf deep learning frame- work.",
The impact of transfer learning and hyperparameters fine-tuning are tested.,
"Results: Hardware acceleration device performance can improve train- ing time by a factor of five to twelve, depending on the model used.",
"On the other hand, increasing convolutional depth will augment the training time by a factor of four to six times, depending on the acceleration device used.",
"Increasing the depth and the complexity of the model generally improves performance, but the relationship is not linear and also depends on the architecture of the model.",
The performance of transfer learning is always worse compared to a complete retraining of the model.,
"Fine-tuning the hyperparameters of the model improves the results, with the best model showing a performance comparable to state–of–the–art models.",
"1 arXiv:2108.13661v3 [q-bio.QM] 29 Apr 2024 Availability: All the models tested in this study, including the code for tuning some of the models, are available on GitHub (https://github.",
com/erbon7/pcam_analysis).,
Introduction Pathologists have been making diagnoses using digital images of glass micro- scope slides for many decades.,
"In recent years, advances in slide scanning tech- niques has allowed the full digitization of microscopic stained tissue sections.",
"There are many advantages to the digitalization of such images, including stan- dardization, reproducibility, the ability to create workflows, remote diagnostics, immediate access to archives and easier sharing among expert pathologists [1].",
"Breast cancer is the leading cancer type in women worldwide, with an estimated 2 million new cases and 627,000 deaths in 2018.",
Breast cancer staging refers to the process of describing the tumor growth or spread.,
Accurate staging by pathologists is an essential task that will determine the patient’s treatment and his chances of recovery (prognosis).,
"An important part of breast cancer staging is the assessment of the sentinel axillary node, a tissue commonly used for the de- tection of early signs of tumor spreading (metastasis).",
"However, sentinel lymph nodes assessment by pathologists is not always easy and optimal.",
"For instance, a retrospective survey performed in 2012 by expert pathologists requalified the status of a high proportion of sentinel nodes [2].",
"Recently, deep learning algo- rithms have made major advances in solving problems that have resisted the machine learning and artificial intelligence community such as speech recogni- tion, the activity of potential drug molecules, brain circuits reconstruction and the prediction of the effects of non-coding RNA mutation on gene expression and disease [3].",
Convolutional neural networks (CNNs) are a class of deep neural networks characterized by a shared-weight architecture of convolution kernels (or filters) that slide along input features and provide translation equivariant features known as feature maps.,
"One of the main advantages of CNNs is that the network learns to optimize the filters through automated learning, requiring very little pre-processing compared to other machine learning techniques.",
"Since their introduction in the 1990’s [4], CNNs have shown excellent performances in the most challenging visual classification tasks and are currently dominating this research field [5].",
"When applied to medical imaging, CNNs demonstrated excellent performance and have been successfully used for the identification of retinal diseases from fundus images [6, 7, 8], tuberculosis from chest radiography images [9, 10] and malignant melanoma from skin images [11].",
"CNNs have also been used for the detection of lymph node metastases in women with breast can- cer in an algorithm competition known as CAMELYON16 (Cancer Metastases in Lymph Nodes Challenge), with the best models showing equal or slightly better performances than a panel of pathologists [12].",
"In this study, I use a dataset of more than 300,000 lymph node images derived from CAMELYON, known as the PCAM (Patch CAMELYON) dataset [13] and the IDC dataset, 2 composed of more than 220,000 images derived from whole slide images of inva- sive ductal carcinoma tissue [14, 15], one of the most common forms of breast cancer.",
"I used these datasets to characterize and analyze the performance of different CNNs network architectures and GPU accelerators, using a standard, off–the–shelf, deep learning computational library.",
Material and methods The PCAM dataset was downloaded from the original website (https://github.,
com/basveeling/pcam).,
"All images have a size of 96 x 96 pixels, in three col- ors.",
"The training set has 262,144 images (80 % of the total), the validation set has 32,768 images (10 %) and the test set also has 32,768 images (10 %).",
All datasets have a 50/50 balance between positive (tumor present) and negative (tumor absent) samples.,
The patches of 96 x 96 pixels images were automati- cally extracted from the CAMELYON dataset [13].,
"For each image, a positive label indicates that the 32 x 32 pixel center of the image contains at least one pixel annotated as tumor tissue (Figure 1).",
Figure 1: Normal and tumor example images from the PCAM dataset.,
The red rectangle corresponds to the 32 x 32 pixels center.,
"The presence of at least one pixel of tumor tissue in this region dictates a positive label (1), otherwise the image is labeled as negative(0).",
The IDC dataset was downloaded from http://andrewjanowczyk.com/wp-static/IDC_regular_ps50_idx5.zip.,
"In total in this dataset there are 220,177 images of 50 x 50 pixels in three colors.",
Images classified as tumor by a pathologist are labelled as 1 while normal tissue images are labelled 0.,
The ratio between the two classes is 70 % normal and 30 % tumor for this dataset (Figure 2).,
All models were coded in Python (v3.7.5) using the TensorFlow (v2.4.1) and integrated Keras API (v2.4.0).,
Some additional statistics and performance indicators were calculated with the Sci-Kit Learn Python package [16].,
The 3 Figure 2: Normal and tumor example images from the Inductive Ductal Carci- noma (IDC) dataset.,
Each image has a size of 50 x 50 pixels.,
"model performance was tested on four different GPU architectures: Nvidia Tesla K80 with 32 Gb of RAM, Nvidia Pascal P100 with 16 Gb of RAM, Nvidia Volta V100 with 32 Gb of RAM and Nvidia Ampere A100 with 80Gb of RAM.",
"For each network architecture tested in this study, the same procedure is used: the model is trained on the training set for 15 epochs, with an evaluation on the validation set after each epoch.",
"Depending on the accurracy value of the model, the weights are saved after each epoch to keep the best model, which is then evaluated on the test set.",
"For some models, I used the KerasTuner framework with the Hyperband algorithm to optimize some hyperparameters [17].",
"Results Twelve different CNN models were used in this study, with different levels of depth and number of parameters (Table 1).",
The models B2 and B6 were coded from scratch and have a relatively simple architecture.,
The B2 model has only two convolutional layers while the B6 model is slightly deeper and more complex with six convolutional layers.,
The architecture for the B6 model was inspired by the Kaggle models of F. Marazzi (https://bit.ly/35wINGv) and H. Mello (https://bit.ly/3xwI6cl).,
Note that the B6 model includes batch normal- ization [18] and dropout steps to improve performance.,
All the other models are CNN models that are part of the TensorFlow Keras library (Table 1).,
"They were developed and tested by several research groups on the Imagenet Challenge, a competition with hundreds of object categories and millions of images [25].",
"For instance, InceptionV3 is a model created in 2015 with a very deep architecture (94 convolutional layers) that performs very well 4 Table 1: CNN models architecture and parameters.",
Model name Depth Param.,
Description or reference B2 10 9.4 M Basic model with two convolutional layers B6 28 2.4 M Basic model with six convolutional layers IV3 189 23.9M Inception V3 [19] VGG19 25 143.7 M VGG19 [20] RN50 107 25.6 M ResNet50 [21] MN 55 4.3 M MobileNet [22] DN121 242 8.1 M DenseNet121 [23] DN169 338 14.3 M DenseNet169 [23] DN201 402 20.2 M DenseNet201 [23] ENB0 273 7.2 M EfficientNet V2 B0 [24] ENB1 337 8.2 M EfficientNet V2 B1 [24] ENB2 252 10.2 M EfficientNet V2 B2 [24] on various computer vision tasks [19].,
"As for most of the models available in the Keras llibrary, it is possible to load the model pre-weighted with ImageNet training weights, thus enabling transfer learning (TF).",
TF is a popular approach in deep learning where pre-trained models are used as the starting point on com- puter vision and natural language processing tasks in order to save computing and time resources.,
In this study I have used models both pre-trained with im- agenet weights and fully re-trained with the two datasets.,
"For the pre-trained version, only the last layers of the model are re-trained with the dataset (global average pooling layer, dense layer and final output).",
"Of course, given that the number of training parameters is much greater in the case of the fully re-trained model, the computation time needed for training the model is also expected to be much longer.",
Table 1 is detailing the architecture and parameters for each of the models used in this study.,
Note that some CNN models could not be used with the IDC dataset because the images are smaller than the minimum size required by these models.,
"The computational running time was analysed for the for B2, B6 and the more complex InceptionV3 (IV3) model, both fully re-trained (F) and with transfer learning (TL) on the PCAM dataset.",
The results are shown in Table 2.,
Note that the time corresponds to the average time observed for one epoch.,
We can compare the model architecture and the hardware GPUs acceleration effects.,
"As expected, the running time is increasing with the complexity and depth of the model.",
"The IV3-F model takes 4 to 10 times longer to train than the simple 2 convolutional layers B2 model, depending on the GPU card utilised.",
The B6 CNN model is taking 1.7 to 2 times longer than the B2 model to train.,
"With the InceptionV3 model, using transfer learning is obviously saving a lot of training time, as a full model training is taking ∼3 times longer to train on all GPU models.",
"In fact, even though the IVF-TL model (transfer learning) is 5 Table 2: Run time in seconds for one epoch on different GPU architectures.",
NbCU: number of CUDA cores.,
Pp: processing power in GFlops.,
TL: transfer learning.,
F: full retraining.,
"GPU card NbCU Pp B2 B6 IV3-TL IV3-F K80 (2014) 4992 2400 325 562 468 1333 P100 (2016) 3584 5000 70 121 143 393 V100 (2017) 5120 7000 47 78 98 279 A100 (2020) 6912 8000 26 57 100 270 much more complex, the running time is comparable to the B2 and B6 models.",
"Regarding the different GPU cards tested here, more recent and powerful GPU cards decrease the computing time quite drastically, with an acceleration factor between 5 and 12 times for the most recent architecture tested here (A100) on all the CNN models compared to the oldest model tested here (K80).",
It is worth noting that the deepest model tested here can be fully trained in about one hour with a V100 or A100 GPU card.,
The performance of all the models on the PCAM and IDC datasets is de- scribed in Table 3 and 4.,
All the indicators are measured on the test sets.,
"Most of the model show a very good performance, with AUC scores around 0.90 or above.",
"However, when we look at the details, there are clear differences.",
"For instance the AUC of the simple 2-layers B2 model is 0.85, increasing to 0.91 with the B6 model, which is slightly more complex.",
"If we take the best performing models in terms of AUC score, for the PCAM dataset we have VGG19 (0.95), followed by the MobileNet (MN, 0.93) and the EfficientNet V2 B2 (ENB2, 0.93).",
"For the IDC dataset, the best performing models are again VGG19 (0.95), to- gether with MobileNet (MN, 0.95), and followed by the B6 (0.94), DenseNet 121 (DN121, 0.94) and EfficientNet V2 B2 (ENB2, 0.94).",
"Looking at the AUC values versus depth and number of parameters of the models for the PCAM dataset (Figure 3), we can see that the VGG19 model has a high AUC value but a low depth, while the ENB0 model has a very high depth but a very low AUC.",
"For the number of parameters, VGG19 has a very high number of param- eters and the best AUC, but models with a much lower number of parameters, such as MN or ENB2, also have high AUC values, in fact very close to the value for VGG19.",
"Taken together, these results show that the more complex models perform better than very simple models (such as the B2 model), but the relationship is not entirely straightforward.",
"It is worth noting that the best models are all models that were fully re- trained on the data, not the models that used transfer learning.",
"In fact, the models that are fully retrained have higher AUC values on average (Figure 4) for both the PCAm and the IDC datasets, and if we look at the average of the AUC values there is a statistically significant difference (Welch Two Sample t-test, for the PCAM dataset t = -2.5527, df = 16.285, p-value = 0.02108, for the IDC dataset, t = -3.5998, df = 8.4843, p-value = 0.006335).",
The reason 6 Table 3: Performance of the models on the PCAM test set.,
AUC: area under the ROC curve.,
CNN model Loss Accuracy AUC Transfert learning IV3 0.46 0.78 0.88 VGG19 0.42 0.80 0.89 RN50 0.52 0.74 0.82 MN 0.46 0.79 0.88 DN121 0.38 0.82 0.91 DN169 0.49 0.79 0.90 DN201 0.39 0.82 0.91 ENB0 0.53 0.73 0.81 ENB1 0.52 0.74 0.82 ENB2 0.54 0.73 0.80 Full training B2 0.47 0.78 0.85 B6 0.44 0.84 0.91 IV3 0.42 0.84 0.91 VGG19 0.43 0.86 0.95 RN50 0.57 0.82 0.91 MN 0.53 0.84 0.93 DN121 0.76 0.82 0.92 DN169 0.46 0.83 0.91 DN201 0.75 0.80 0.88 ENB0 0.99 0.70 0.84 ENB1 0.89 0.80 0.92 ENB2 0.94 0.84 0.93 7 Table 4: Performance of the models on the invasive ductal carcinoma (IDC) breast cancer test set.,
AUC: area under the ROC curve.,
"CNN model Loss Accuracy AUC Transfert learning VGG19 0.38 0.83 0.88 RN50 0.46 0.79 0.82 MN 0.56 0.71 0.66 DN121 0.33 0.85 0.91 DN169 0.35 0.85 0.90 DN201 0.34 0.82 0.91 ENB0 0.48 0.78 0.81 ENB1 0.49 0.79 0.81 ENB2 0.52 0.75 0.75 Full training B2 0.31 0.86 0.92 B6 0.28 0.87 0.94 VGG19 0.25 0.90 0.95 RN50 0.28 0.88 0.93 MN 0.26 0.89 0.95 DN121 0.27 0.88 0.94 ENB2 0.28 0.88 0.94 DN201 0.31 0.88 0.93 ENB0 0.35 0.86 0.92 ENB1 0.43 0.84 0.91 DN169 0.35 0.85 0.90 8 Figure 3: Dotplots showing the AUC score versus the depth of the model, i.e.",
the total number of layers in the model (A) or the total number of parameters of the model (B) for the PCAM dataset.,
for this is probably that the visual structures (i.e.,
the filters) learned with the ImageNet dataset are not adapted to the PCAM images.,
"Indeed, the object categories in the ImageNet dataset (such as ”balloon”, ”strawberry”, etc.)",
are very different from the types and structures seen in digital pathology images.,
Fine tuning of the hyperparameters was done for the Inception V3 and the VGG19 models on the PCAM dataset.,
Two hyperparameters (Adam learn- ing rate and batch size) were fine-tuned using the Keras Tuner with the hy- perband algorithm.,
The performance is improved in the two models with an AUC of 0.95 for the Inception V3 model and an AUC of 0.96 for the VGG19 model.,
These performances are comparable to current state–of–the–art models for computational pathology analysis.,
It is within the top 5 best algorithms of the CAMELYON16 challenge [12] and is within the top 10 best models for the PCAM dataset (https://tinyurl.com/3rhk6ph6).,
"The current best PCAM models have an AUC around 0.97 and implement rotation equivariant strategies [26, 27, 28].",
"Indeed, histology images are typically symmetric under rotation, meaning that each orientation is equally as likely to appear.",
"Rotation– equivariance removes the necessity lo learn this type of transformation from the data, thus allowing more discriminative features to be learned and also reducing the number of parameters of the model.",
"Conclusion Precise staging by expert pathologists of breast cancer axillary nodes, a tissue commonly used for the detection of early signs of tumor spreading, is an essen- tial task that will determine the patient’s treatment and his chances of recovery.",
9 Figure 4: Boxplots showing the AUC score for different CNN models for fully re-trained models (F) or with transfer learning (TL).,
"However, it is a difficult task that was shown to be prone to misclassification.",
"Al- gorithms, and in particular deep learning based convolutional neural networks, can help the experts in this task by analyzing fully digitized slides of microscopic stained tissue sections.",
"In this study, I evaluated twelve different CNN architec- tures and different hardware acceleration devices for breast cancer classification on two different public datasets consisting of hundreds of thousands of images.",
"The performance of hardware acceleration devices can improve the training time by a factor of five to twelve, depending on the model used.",
"On the other hand, increasing the convolutional depth increases the training time by a factor of four to six, depending on the acceleration device used.",
"More complex models tend to perform better than very simple ones, especially when fully retrained on the digital pathology dataset, but the relationship between model complexity and performance is not straightforward.",
Transfer learning from imagenet always performs worse than fully retraining the models.,
"Fine-tuning the hyperparame- ters of the model improves the results, with the best model tested in this study showing very high performance, comparable to current state–of–the–art models.",
Acknowledgements I wish to thank Claude Scarpelli and Jean-Fran¸cois Deleuze for their general support and discussions.,
I would also like to thank Christine M´enach´e and Xavier Delaruelle for giving me access to the new FENIX infrastucture of the CEA cluster computer at the Tr`es Grand Centre de Calcul (TGCC).,
"Last, I would like to thank the TGCC support team for their patient and competent answers to my multiple questions and requests.",
10 References [1] Jon Griffin and Darren Treanor.,
Digital pathology in clinical use: where are we now and what is holding us back?,
"Histopathology, 70(1):134–145, 2017.",
"[2] JHMJ Vestjens, MJ Pepels, Maaike de Boer, George Florimond Borm, Carolien HM van Deurzen, Paul J van Diest, JAAM Van Dijck, EMM Adang, Johan WR Nortier, EJ Th Rutgers, et al.",
Relevant impact of central pathology review on nodal classification in individual breast cancer patients.,
"Annals of Oncology, 23(10):2561–2566, 2012.",
"[3] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.",
Deep learning.,
"Nature, 521(7553):436–444, 2015.",
"[4] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel.",
Backprop- agation applied to handwritten zip code recognition.,
"Neural computation, 1(4):541–551, 1989.",
[5] Matthew D Zeiler and Rob Fergus.,
Visualizing and understanding con- volutional networks.,
"In European conference on computer vision, pages 818–833.",
"Springer, 2014.",
"[6] Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin Siew Wei Tan, Nguyen D Quang, Alfred Gan, Haslina Hamzah, Renata Garcia- Franco, Ian Yew San Yeo, Shu Yen Lee, et al.",
Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes.,
"Jama, 318(22):2211–2223, 2017.",
"[7] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al.",
Identifying medical diagnoses and treatable diseases by image-based deep learning.,
"Cell, 172(5):1122–1131, 2018.",
"[8] Philippe M Burlina, Neil Joshi, Michael Pekala, Katia D Pacheco, David E Freund, and Neil M Bressler.",
Automated grading of age-related macu- lar degeneration from color fundus images using deep convolutional neural networks.,
"JAMA ophthalmology, 135(11):1170–1176, 2017.",
[9] Paras Lakhani and Baskaran Sundaram.,
Deep learning at chest radiogra- phy: automated classification of pulmonary tuberculosis by using convolu- tional neural networks.,
"Radiology, 284(2):574–582, 2017.",
"[10] Daniel SW Ting, Paul H Yi, and Ferdinand Hui.",
Clinical applicability of deep learning system in detecting tuberculosis with chest radiography.,
"Radiology, 286(2):729–731, 2018.",
"11 [11] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swet- ter, Helen M Blau, and Sebastian Thrun.",
Dermatologist-level classification of skin cancer with deep neural networks.,
"nature, 542(7639):115–118, 2017.",
"[12] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al.",
Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer.,
"Jama, 318(22):2199–2210, 2017.",
"[13] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.",
Rotation equivariant cnns for digital pathology.,
"In International Conference on Medical image computing and computer-assisted interven- tion, pages 210–218.",
"Springer, 2018.",
[14] Andrew Janowczyk and Anant Madabhushi.,
Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases.,
"Journal of Pathology Informatics, 7(1):29, 2016.",
"[15] Angel Cruz-Roa, Ajay Basavanhally, Fabio Gonz´alez, Hannah Gilmore, Michael Feldman, Shridar Ganesan, Natalie Shih, John Tomaszewski, and Anant Madabhushi.",
Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks.,
"In Medical Imaging 2014: Digital Pathology, volume 9041, page 904103.",
"SPIE, 2014.",
"[16] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al.",
Scikit-learn: Machine learning in python.,
"the Journal of Machine Learning Research, 12:2825–2830, 2011.",
"[17] Tom O’Malley, Elie Bursztein, James Long, Fran¸cois Chollet, Haifeng Jin, Luca Invernizzi, et al.",
Keras tuner.,
"https://github.com/keras-team/ keras-tuner, 2019.",
[18] Sergey Ioffe and Christian Szegedy.,
Batch normalization: Accelerating deep network training by reducing internal covariate shift.,
"In International conference on machine learning, pages 448–456.",
"PMLR, 2015.",
"[19] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbig- niew Wojna.",
Rethinking the inception architecture for computer vision.,
"In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 2818–2826, 2016.",
[20] Karen Simonyan and Andrew Zisserman.,
Very deep convolutional networks for large-scale image recognition.,
"arXiv preprint arXiv:1409.1556, 2014.",
"[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
Deep residual learning for image recognition.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.",
"12 [22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.",
Mobilenets: Efficient convolutional neural networks for mobile vision applications.,
"arXiv preprint arXiv:1704.04861, 2017.",
"[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Wein- berger.",
Densely connected convolutional networks.,
"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700– 4708, 2017.",
[24] Mingxing Tan and Quoc Le.,
Efficientnetv2: Smaller models and faster training.,
"In International conference on machine learning, pages 10096– 10106.",
"PMLR, 2021.",
"[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bern- stein, et al.",
Imagenet large scale visual recognition challenge.,
"International Journal of Computer Vision, 115:211–252, 2015.",
[26] Taco Cohen and Max Welling.,
Group equivariant convolutional networks.,
"In International conference on machine learning, pages 2990–2999.",
"PMLR, 2016.",
"[27] Simon Graham, David Epstein, and Nasir Rajpoot.",
Dense steerable filter cnns for exploiting rotational symmetry in histology images.,
"IEEE Trans- actions on Medical Imaging, 39(12):4124–4136, 2020.",
"[28] Maurice Weiler, Fred A Hamprecht, and Martin Storath.",
Learning steerable filters for rotation equivariant cnns.,
"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 849–858, 2018.",
"MeisenMeister: A Simple Two Stage Pipeline for Breast Cancer Classification on MRI Benjamin Hamm1,2 , Yannick Kirchhoff1,3,4 , Maximilian Rokuss1,3 , Klaus Maier-Hein1,2,3,4,5 1German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany 2Medical Faculty, Heidelberg University, Germany 3Faculty of Mathematics and Computer Science, Heidelberg University 4HIDSS4Health, Karlsruhe/Heidelberg, Germany 5Pattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Heidelberg, Germany {benjamin.hamm, yannick.kirchhoff, maximilian.rokuss}@dkfz-heidelberg.de INTRODUCTION The ODELIA Breast MRI Challenge 2025 addresses a critical issue in breast cancer screening: improving early detection through more efficient and accurate interpretation of breast MRI scans.",
"Even though methods for general-purpose whole- body lesion segmentation [1] as well as multi–time-point analysis [2] exist, breast cancer detection remains highly challenging, largely due to the limited availability of high- quality segmentation labels.",
"Therefore, developing robust classification-based approaches is crucial for the future of early breast cancer detection, particularly in applications such as large-scale screening.",
"In this write-up, we provide a com- prehensive overview of our approach to the challenge.",
We begin by detailing the underlying concept and foundational assumptions that guided our work.,
"We then describe the iterative development process, highlighting the key stages of experimentation, evaluation, and refinement that shaped the evolution of our solution.",
"Finally, we present the reasoning and evidence that informed the design choices behind our final submission, with a focus on performance, robustness, and clinical relevance.",
We release our full implementation publicly at https://github.com/MIC-DKFZ/MeisenMeister MATERIALS AND METHODS Divide And Conquer Pipeline We initiated our approach by designing a customized Divide- and-Conquer pipeline to address the computational challenges posed by high-resolution breast MRI volumes.,
"Given the ex- tremely high spatial resolution and large dimensionality of the input scans, processing an entire volume in a single forward pass is impractical due to memory limitations.",
"Although the challenge organizers provided a unilateral cropping script to isolate a single breast per scan, our analysis showed that the resulting subvolumes remained too large for efficient model inference.",
"To mitigate this, we developed a dedicated dataset and segmentation model, presented as Divide and Conquer: A Large-Scale Dataset and Model for Left–Right Breast MRI Segmentation [3].",
"To improve performance and generalizabil- ity, we employed an active learning strategy that leveraged Fig.",
The pipeline follows a divide-and-conquer strategy to efficiently and accurately analyze high-resolution breast MRI volumes.,
(1) The original high- resolution 3D MRI is first resampled to a lower resolution to reduce com- putational complexity.,
(2) A segmentation model is applied to the resampled image to delineate the breast regions.,
Bounding boxes are then derived from the segmentation masks to identify the spatial extent of each breast.,
"(3) Using these predicted bounding boxes, the corresponding high-resolution regions of interest (ROIs) are cropped directly from the original image, preserving anatomical detail.",
"(4) Each high-resolution ROI is processed independently by a final prediction model, enabling focused, high-accuracy analysis at the level of individual breasts.",
"predictions from non–contrast-enhanced T1-weighted images as pseudo-ground truth for co-registered sequences, building on previously demonstrated reliability of cross-sequence an- notation transfer [4].",
"This model enables accurate localization of individual breasts and the extraction of tight bounding boxes, thereby enabling efficient, ROI-focused processing in subsequent stages.",
A hierarchical overview of this pipeline is illustrated in Figure1.,
"Public Data To enhance the challenge data, we sought to incorporate publicly available datasets.",
A primary challenge was iden- tifying datasets with benign-labeled cases and harmonizing imaging modalities across varying acquisition protocols.,
"We ultimately integrated two external datasets into our pipeline: the Advanced-MRI-Breast-Lesions (AMBL) dataset [5], which includes both malignant and benign cases, and the Duke- Breast-Cancer-MRI (DUKE) dataset [6], which contains only arXiv:2510.27326v1 [cs.CV] 31 Oct 2025 malignant cases.",
"While DUKE does not include benign annota- tions, we utilized it in a separate experimental setting described later in the paper.",
Model Training We utilized the nnU-Net framework [7] to create a classification training framework.,
Given the heterogeneity in available input modalities—particularly the variable number of post-contrast images and the optional presence of T2-weighted sequences—we conducted a series of ablation studies to determine the optimal input configuration.,
"Specifically, we investigated (i) how many post-contrast phases were necessary for robust performance, and (ii) whether the inclusion of T2-weighted images contributed meaningful gains.",
"After finalizing the input channel configuration, we proceeded to ablate various backbone architectures for the classification model.",
"Specifically, we compared three architectures: ResNet- 18 [8], ResEncL (a scaled-up variant of the nnU-Net encoder) and ResEncL with Squeeze-and-Excitation (SE) blocks [9].",
A key consideration during this process was architectural compatibility with supervised segmentation-based pretraining.,
We first conducted supervised pretraining on the MAMMA- MIA [10] dataset using the standard nnU-Net framework.,
"For this, we trained a single model (no ensembling) on the full dataset for 2000 epochs.",
"While this prolonged training clearly led to overfitting, it served as a valuable initialization point for our downstream task.",
"Following pretraining, we explored several fine-tuning strategies on the target classification task.",
"Specifically, we compared: (i) linear probing, where only the classification head is trained; (ii) full fine-tuning of all model weights; and (iii) fine-tuning with learning rate warm-up, including two variants—one with a warm-up from 1e-4 to 1e-2, and another from 1e-5 to 1e-3.",
"After finalizing our fine-tuning strategy, we performed an ablation of data augmentation (DA) techniques.",
Effective augmentation was essential due to the limited number of training samples and the high susceptibility of models to overfitting in this setting.,
"However, applying overly aggressive DA—particularly intensity-based transformations—can obscure diagnostically relevant features, effectively reducing the task to class distribution matching rather than promoting the learning of meaningful representations.",
"To balance augmentation strength and fidelity, we systematically ablated individual DA components using only fold 0 (with CAM held out as the test center), as this fold showed good signal and resource constraints prevented full 5-fold ablation.",
The full list of compared DA techniques can be seen in TableII.,
"These include contrast adjustment, gamma correction, Gaussian blur, Gaussian noise, multiplicative brightness, simulated low resolution, full 3D spatial transformations, in-plane spatial transformations, scaling, and elastic deformations.",
Each technique was tested in isolation to assess its effect on generalization and inform the final augmentation strategy used in our pipeline.,
"In addition, we found that cropping away non-informative background regions by setting them to zero using the segmentation masks generated by our Divide-and-Conquer model led to significant performance gains.",
"Finally, we explored the potential of incorporating the DUKE dataset into our training pipeline, given its availability.",
"However, DUKE contains only malignant cases, making it incompatible with the original multi-class classification task.",
"To address this, we reformulated the problem as a binary classification task with two labels: healthy and lesion-present (i.e., benign + malignant).",
"At inference time, we mapped the binary predictions back to the original three-class challenge setting.",
"Specifically, if the model predicted lesion-present, we assigned its probability to the malignant class and attributed the remaining probability to benign.",
"Conversely, if the model predicted healthy, we assigned that probability to the healthy class and the complement to benign.",
This approach inherently prevents the model from explicitly predicting the benign class.,
"While this might appear limiting, we thought it could reduce confusion between healthy and benign, as well as between malignant and benign, ultimately improving overall class separation.",
"Given that benign is a minority class in the dataset, we hoped this strategy could still yield a strong overall performance under macro-averaged metrics, despite the lack of a dedicated prediction path for benign cases.",
RESULTS AND DISCUSSION The results of our incremental pipeline improvements are summarized in TableI.,
"We report the mean AUROC, averaged across five cross-validation folds, where each fold corresponds to holding out one acquisition center as the test set.",
"In- cluding the AMBL dataset provides a modest gain, while switching to the ResEncL backbone further improves perfor- mance.",
"Fine-tuning strategies with learning rate warm-up offer clear advantages over linear probing, with warm-up schedules producing the largest single improvement—highlighting the impact of pretraining.",
Data augmentation and background masking contribute additional gains.,
"The final configura- tion—combining ResEncL, warm-up fine-tuning, and data augmentation—achieves a strong baseline, which is further enhanced by the background masking step.",
The results of our data augmentation ablation study are summarized in TableII.,
"Some techniques—particularly gamma transformation, elastic deformation, and simulated low resolu- tion—led to performance degradation.",
"These findings under- score the importance of carefully selecting augmentations in medical imaging tasks, where overly strong or unrealistic dis- tortions can obscure clinically relevant features.",
"The baseline model without augmentation achieves an AUROC of 0.826, which is exceeded by several well-calibrated augmentations, most notably Gaussian noise (0.851) and scaling (0.843).",
TABLE I ABLATION RESULTS ACROSS KEY COMPONENTS OF OUR PIPELINE.,
"REPORTED AUROC VALUES ARE AVERAGED ACROSS 5 CROSS-VALIDATION FOLDS, EACH CORRESPONDING TO ONE HELD-OUT ACQUISITION CENTER.",
BOLD ENTRIES INDICATE THE SETTING WE COMMIT TO USE FURTHER.,
Mean Setting Scheme AUROC Data Only Odelia Data 0.623 Add AMBL Data 0.649 Networks ResNet18 0.608 ResEncL SE 0.631 ResEncL 0.649 Finetuning (Mama Mia) Linear Probing 0.651 Full Finetuning 0.725 Warmup 1e-4 1e-2 0.729 Warmup 1e-5 1e-3 0.738 Data Augmentation Data Augmentation 0.749 Preprocessing Mask Background 0.765 ResEncL + Warmup + DA 0.736 TABLE II ABLATION STUDY OF INDIVIDUAL DATA AUGMENTATION TECHNIQUES.,
EACH AUGMENTATION WAS APPLIED IN ISOLATION USING THE RESENCL BACKBONE WITH WARM-UP FINE-TUNING.,
"REPORTED AUROC VALUES CORRESPOND TO FOLD 0, WHERE CAM WAS USED AS THE HELD-OUT TEST CENTER.",
"GREEN CELLS INDICATE IMPROVEMENT OVER THE NO-AUGMENTATION BASELINE (0.826), WHILE RED CELLS INDICATE A PERFORMANCE DROP.",
Augmentation Technique AUROC Contrast Transform 0.837 Gamma Transform 0.823 Gaussian Blur Transform 0.819 Gaussian Noise Transform 0.851 Multiplicative Brightness Transform 0.827 Simulate Low Resolution Transform 0.812 Spatial Transform 0.836 Spatial Transform Inplane 0.813 Scaling 0.843 Elastic Deform 0.797 No Augmentation 0.826 Batch size results are summarized in TableIII.,
"Reducing the batch size increases gradient noise during training, which can serve as a beneficial regularizer.",
The results show a clear trend: smaller batch sizes consistently improve performance.,
"A batch size of 1 yielded the highest mean AUROC of 0.765, averaged across five cross-validation folds.",
"While training with very small batch sizes can introduce instability or slow convergence, in our setting it proved to be an effective and simple regularization strategy that improved generalization.",
"Given the variability in available MRI sequences across datasets, it was important to identify a modality combination that works for all centers.",
"Including T2-weighted images alongside pre- and post-contrast sequences resulted in de- graded performance (0.584), suggesting that the T2 modality either introduced noise or lacked sufficient cross-dataset con- sistency.",
Using only pre-contrast and two post-contrast phases TABLE III EFFECT OF BATCH SIZE ON CLASSIFICATION PERFORMANCE USING THE RESENCL BACKBONE WITH WARM-UP FINE-TUNING.,
"REPORTED AUROC VALUES ARE AVERAGED ACROSS 5 CROSS-VALIDATION FOLDS, EACH CORRESPONDING TO ONE HELD-OUT ACQUISITION CENTER.",
"SMALLER BATCH SIZES LED TO IMPROVED PERFORMANCE, WITH A BATCH SIZE OF 1 ACHIEVING THE HIGHEST MEAN AUROC.",
"Mean Batch Size AUROC 4 0.745 2 0.740 1 0,765 (middle and last) significantly improved performance (0.636).",
"The best results were achieved using the first and second post- contrast phases together with the pre-contrast scan (0.649), indicating that early dynamic enhancement patterns are par- ticularly informative for classification.",
"Results are shown in TableIV, TABLE IV EFFECT OF INPUT CHANNEL CONFIGURATION ON CLASSIFICATION PERFORMANCE.",
"REPORTED AUROC VALUES ARE AVERAGED ACROSS 5 CROSS-VALIDATION FOLDS, EACH CORRESPONDING TO ONE HELD-OUT ACQUISITION CENTER.",
"THE COMBINATION OF PRE-CONTRAST AND EARLY POST-CONTRAST PHASES YIELDS THE HIGHEST PERFORMANCE, WHILE ADDING T2 LEADS TO DEGRADATION.",
"Input Configuration AUROC Pre + Post middle + Post last + T2 0.584 Pre + Post middle + Post last 0.636 Pre + Post 1 + Post 2 0.649 To compare performance under different task formulations and preprocessing variants, we evaluated both the original three-class classification task (”Regular Task”) and a simplified binary reformulation (”Binary Task”) as described earlier.",
The results are summarized in TableV.,
"Each row incrementally adds components to the baseline configuration (ResEncL with warm-up fine-tuning), allowing us to isolate their effects.",
"For the regular task, the best performance (0.776) was achieved with the inclusion of data augmentation, background masking and applying isotropic spacing.",
"In contrast to our initial hypothesis, the binary task formulation did not consistently benefit from the incremental addition of components.",
"Its best performance (0.757) was achieved when isotropic spacing was included, yet this still lagged behind the corresponding result in the regular task (0.776).",
These findings suggest that the trade-off between the number of benign cases and the confusion among the remaining classes did not yield the expected benefit.,
"CONCLUSION Overall, we found it challenging to extract a stable and informative training signal from the available data, even after incorporating additional public datasets.",
"Models exhibited a strong tendency to overfit, and training outcomes were highly TABLE V Mean Setting Scheme AUROC Regular Task ResEncL + Warmup + Data Augmentation 0.765 + Mask Background + Isotropic Spacing 0.776 Binary Task ResEncL + Warmup + Data Augmentation 0.722 + Mask Background + Isotropic Spacing 0.757 sensitive to initialization and data splits, with substantial performance variance across runs using identical settings.",
"This instability suggests a high level of noise in the learning process, making reproducibility difficult.",
There remain several promising directions we have yet to explore.,
"These include multi-resolution feature aggregation (e.g., in the spirit of HRNet [11]), additional regularization strategies to further combat overfitting, improved model selec- tion techniques during training, and more extensive pretraining approaches.",
"Additionally, gaining access to more data through secure federated learning [12], [13] or by leveraging efficient annotation tools such as nnInteractive [14] could be highly beneficial, as the challenge remains primarily a data limitation rather than a lack of strong architectural solutions.",
"REFERENCES [1] M. Rokuss, Y. Kirchhoff, S. Akbal, B. Kovacs, S. Roy, C. Ulrich, T. Wald, L. T. Rotkopf, H.-P. Schlemmer, and K. Maier-Hein, “Le- sionlocator: Zero-shot universal tumor segmentation and tracking in 3d whole-body imaging,” in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp.",
30 872–30 885.,
"[2] M. R. Rokuss, Y. Kirchhoff, S. Roy, B. Kovacs, C. Ulrich, T. Wald, M. Zenk, S. Denner, F. Isensee, P. Vollmuth et al., “Longitudinal segmentation of ms lesions via temporal difference weighting,” in International Conference on Medical Image Computing and Computer- Assisted Intervention.",
"Springer, 2024, pp.",
"[3] M. Rokuss, B. Hamm, Y. Kirchhoff, and K. Maier-Hein, “Divide and conquer: A large-scale dataset and model for left-right breast mri segmentation,” 2025.",
"Available: https://arxiv.org/abs/ 2507.13830 [4] T. Wald, B. Hamm, J. C. Holzschuh, R. El Shafie, A. Kudak, B. Ko- vacs, I. Pfl¨uger, B. von Nettelbladt, C. Ulrich, M. A. Baumgartner et al., “Enhancing deep learning methods for brain metastasis detection through cross-technique annotations on space mri,” European Radiology Experimental, vol.",
"1–14, 2025.",
"[5] D. Daniels, D. Last, K. Cohen, Y. Mardor, and M. Sklair-Levy, “Standard and delayed contrast-enhanced mri of malignant and benign breast lesions with histological and clinical supporting data (advanced-mri- breast-lesions) (version 2),” https://doi.org/10.7937/C7X1-YN57, 2024, the Cancer Imaging Archive [dataset].",
"[6] A. Saha, M. R. Harowicz, L. J. Grimm, J. Weng, E. H. Cain, C. E. Kim, S. V. Ghate, R. Walsh, and M. A. Mazurowski, “Dynamic contrast- enhanced magnetic resonance images of breast cancer patients with tumor locations [data set],” The Cancer Imaging Archive, vol.",
"[7] F. Isensee, P. F. Jaeger, S. A.",
"A. Kohl, J. Petersen, and K. H. Maier-Hein, “nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,” Nature Methods, vol.",
"203–211, 2021.",
"[8] K. He, X. Zhang, S. Ren, and J.",
"Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp.",
"[9] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp.",
"[10] L. Garrucho, K. Kushibar, C.-A.",
"Reidel, S. Joshi, R. Osuala, A. Tsirikoglou, M. Bobowicz, J. Del Riego, A. Catanese et al., “A large- scale multicenter breast cancer dce-mri benchmark dataset with expert segmentations,” Scientific data, vol.",
"1, p. 453, 2025.",
"[11] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., “Deep high-resolution representation learning for visual recognition,” IEEE transactions on pattern analysis and machine intelligence, vol.",
"3349–3364, 2020.",
"[12] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B.",
"A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in Artificial intelligence and statistics.",
"PMLR, 2017, pp.",
1273– 1282.,
"[13] B. Hamm, Y. Kirchhoff, M. Rokuss, P. Schader, P. Neher, S. Param- pottupadam, R. Floca, and K. Maier-Hein, “Efficient privacy-preserving medical cross-silo federated learning,” Authorea Preprints, 2025.",
"[14] F. Isensee, M. Rokuss, L. Kr¨amer, S. Dinkelacker, A. Ravindran, F. Stritzke, B. Hamm, T. Wald, M. Langenberg, C. Ulrich et al., “nninteractive: Redefining 3d promptable segmentation,” arXiv preprint arXiv:2503.08373, 2025.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis This work proposes a new semantic foundation for circuit description languages used in quantum programming, with particular focus on the Proto-Quipper family of calculi inspired by the Quipper language.",
"The central goal is to provide a clean and compositional way to reason about both program behavior and the quantum circuits produced during execution, especially with respect to resource usage.",
Existing denotational models for Proto-Quipper typically rely on presheaf-based constructions.,
"While powerful, these models tend to blur the distinction between the value computed by a program and the circuit generated as a side effect.",
"As a result, it becomes difficult to extract precise information about circuit structure, such as its size or interface, and to justify advanced type systems that aim to control circuit resources.",
"To overcome this limitation, the paper introduces a monadic interpretation of circuit construction based on indexed monads.",
"In this approach, program evaluation produces a pair consisting of a value and a circuit, and these two components are kept separate throughout the semantics.",
"Indexed monads are essential here because the type of the generated circuit depends on the input and output wire interfaces, which vary across computations.",
"Building on this idea, the authors define a refined calculus called Proto-Quipper-C.",
This language extends Proto-Quipper-M by enriching function types with explicit information about the circuit data captured by closures.,
Making this information part of the type system is crucial for determining the interface of circuits generated by higher-order functions and for enabling modular reasoning about circuit composition.,
The denotational semantics of Proto-Quipper-C is formulated using a parameterized Freyd category induced by the circuit indexed monad.,
"Programs are interpreted compositionally, with values and circuits handled in distinct semantic components.",
"The model is shown to be sound and computationally adequate with respect to the operational semantics, meaning that it precisely characterizes program evaluation and circuit generation.",
"The framework is further extended to support effect typing, allowing quantitative properties of circuits—such as size or width—to be tracked using abstract circuit algebras.",
This extension naturally accommodates circuit optimizations while preserving soundness.,
"Finally, the paper outlines how dependent types could be incorporated into the system using families constructions, opening the door to even more expressive reasoning about quantum programs and their resource requirements.",
"Overall, the proposed monadic semantics provides a flexible and conceptually clear foundation for circuit description languages, supporting precise resource analysis and extensible type systems for quantum programming.",
"On Circuit Description Languages, Indexed Monads, and Resource Analysis This paper presents a new denotational semantics for circuit description languages (CDLs) inspired by Quipper and its formal core calculi in the Proto-Quipper family.",
The proposed approach is based on indexed monads and is designed to clearly separate the value produced by a program from the quantum circuit generated as a side effect during its execution.,
"Traditional semantic models for Proto-Quipper rely heavily on presheaf constructions, where program values and circuits are intertwined into a single mathematical object.",
"While expressive, this coupling makes it difficult to reason modularly about circuit properties such as size, width, or resource usage.",
"In particular, it limits the ability to justify advanced type systems that aim to control or bound the resources required by generated circuits.",
"To address this issue, the paper introduces a monad-based framework in which circuit construction is modeled as an indexed monad.",
"Each program is interpreted as producing both a value and a circuit, but these two components are kept explicitly separate.",
This separation enables clearer semantic reasoning and supports richer forms of effect typing that capture quantitative properties of circuits.,
"A revised calculus, called Proto-Quipper-C, is presented.",
It extends Proto-Quipper-M by annotating function types with explicit information about the circuit resources captured by closures.,
This modification is essential for determining the interface of circuits produced by higher-order functions and enables a sound monadic interpretation.,
The denotational semantics of Proto-Quipper-C is defined using a parameterized Freyd category induced by the circuit indexed monad.,
The model is shown to be sound and computationally adequate with respect to the operational semantics.,
"Moreover, the framework naturally generalizes to effect-typed systems, allowing circuit metrics such as size and width to be tracked compositionally, even in the presence of simple circuit optimizations.",
"Finally, the paper discusses possible extensions of the framework to dependent types, showing how families constructions can be used to enrich the semantics further.",
"Overall, the work provides a flexible and conceptually clean foundation for reasoning about resource usage in quantum circuit description languages.",
