UNIT 1 & 2 NOTES Structure of a Biological Neuron (6-marks answer â€“ concise) A biological neuron is the basic information-processing unit of the nervous system. In neural networks and encoders, the artiï¬cial neuron is modeled based on this biological structure. The main structural components are: 1. Dendrites â€“ Branched structures that receive signals from other neurons through synapses. They act as input terminals. 2. Cell Body (Soma) â€“ Contains the nucleus and cytoplasm; integrates/sums all incoming signals from dendrites. 3. Nucleus â€“ Present inside the soma; controls metabolic activities and functioning of the neuron. 4. Axon â€“ A long ï¬ber that carries the processed electrical impulse away from the cell body toward other neurons. 5. Myelin Sheath â€“ Fatty insulating layer around the axon (in many neurons) that increases speed of impulse conduction. 6. Synapse â€“ Junction between axon terminal of one neuron and dendrite of another; enables electro- chemical signal transmission using neurotransmitters. Link to neural networks: dendrites = inputs, synaptic strength = weights, soma = summation + activation, axon = output. Artiï¬cial Neuron Model â€“ McCullochâ€“Pitts (MCP) Neuron (6-marks answer â€“ concise) The McCullochâ€“Pitts neuron is a simple mathematical model of a biological neuron used in neural networks. It converts multiple inputs into a single output using weighted summation and thresholding. 1. Inputs (xâ‚, xâ‚‚, â€¦, xâ‚™) External signals are applied to the neuron, analogous to dendrites of biological neurons. 2. Weights (wâ‚, wâ‚‚, â€¦, wâ‚™) Each input is multiplied with a weight representing synaptic strength or importance of that input. 3. Summation Function (âˆ‘) The neuron computes the net input net = à·ğ‘¤à¯œ à¯¡ à¯œà­€à¬µ ğ‘¥à¯œ+ ğ‘ where b is bias (or threshold adjustment). 4. Threshold / Bias (bk) A constant value used to shift the activation level; controls when the neuron should ï¬re. 5. Activation / Transfer Function T The net value is passed through a threshold activation function producing output: o If net â‰¥ Î¸ â†’ neuron ï¬res (output = 1) o If net < Î¸ â†’ neuron does not ï¬re (output = 0) 6. Output f(net) Produces a binary decision (0/1) or (+1/âˆ’1) depending on activation function; used in logic gates and perceptrons. Linear Separability A problem is linearly separable when a single straight line (or hyperplane) can separate two classes completely. ï‚· Classes can be divided by one straight boundary ï‚· Examples: AND, OR logic functions ï‚· Single perceptron/single neuron can solve it Nonlinear Separability A problem is nonlinearly separable when no straight line can separate classes. ï‚· Requires curved or complex boundary ï‚· Example: XOR problem ï‚· Single neuron cannot solve it â€” needs multilayer network Key limitation Single MCP neuron/perceptron â†’ only solves linearly separable problems Multilayer neural networks â†’ solve nonlinear problems Steps of Hebbian learning rule: 1. Initialize all weights to small random values or zero. 2. Apply an input vector (x) to the neuron. 3. Compute the neuron output (y). 4. Update each weight using ( \Delta w_i = \eta x_i y ). 5. Add weight change to old weight: ( w_i = w_i + \Delta w_i ). 6. Repeat the above process for all training patterns. Advantages 1. Simple learning rule; easy to implement 2. Biologically realistic and supports associative learning Disadvantages 1. Weights may grow indeï¬nitely (no upper limit) 2. Does not include error-correction; poor for complex tasks Perceptron Learning Rule Steps 1. Initialize weights and bias 2. Apply input vector and compute output ğ‘¦ 3. Compare with target output ğ‘¡ 4. If error exists, update ğ‘¤à¯œ= ğ‘¤à¯œ+ ğœ‚(ğ‘¡âˆ’ğ‘¦)ğ‘¥à¯œ 5. Repeat for all patterns until error becomes zero Advantages 1. Simple and easy to implement 2. Guarantees convergence for linearly separable problems Disadvantages 1. Works only for linearly separable data 2. Produces binary output only (hard-limited activation) Delta Learning Rule (Gradient Descent Rule) Steps 1. Initialize weights and bias 2. Apply inputs and compute output ğ‘¦ 3. Compute error ğ‘’= ğ‘¡âˆ’ğ‘¦ 4. Update weights Î”ğ‘¤à¯œ= ğœ‚ğ‘’ğ‘¥à¯œ 5. Repeat for all patterns to minimize mean square error Advantages 1. Can be used for continuous-valued outputs 2. Forms basis of backpropagation in multilayer networks Disadvantages 1. May get stuck in local minima 2. Convergence can be slow and depends on learning rate Widrowâ€“HoÆ¯ Learning Rule (LMS Rule / ADALINE) Steps 1. Initialize weights 2. Apply input and compute ADALINE output (before activation) 3. Calculate error using desired output 4. Update weights using Î”ğ‘¤à¯œ= ğœ‚(ğ‘¡âˆ’ğ‘¦)ğ‘¥à¯œ 5. Repeat until mean squared error is minimized Advantages 1. Minimizes mean square error optimally 2. More stable and accurate than perceptron rule Disadvantages 1. Assumes linear activation (works for ADALINE only) 2. Computationally heavier due to error minimization EBPA (Error Backpropagation Algorithm) is a supervised learning algorithm used to train multilayer feedforward neural networks. It works by propagating inputs forward through the network to produce outputs, comparing them with the desired outputs, computing the error, and then back-propagating this error layer by layer to adjust the connection weights using gradient descent so that future outputs become more accurate. In short: EBPA adjusts network weights in the direction that reduces output error, using forward pass + backward pass. Key points ï‚· used for multilayer perceptrons ï‚· requires continuous activation functions ï‚· minimizes mean square error ï‚· based on gradient descent ï‚· backbone of modern deep learning training âœ” Working of Error Backpropagation Algorithm (EBPA) 1. Initialize weights Random small values are assigned to all connection weights of the hidden and output layers. 2. Forward pass â€“ hidden layer Inputs are applied and outputs of hidden layer neurons are computed using the current hidden layer weights. 3. Forward pass â€“ output layer Using hidden layer outputs and output layer weights, outputs of output neurons are calculated. 4. Compute error The diÆ¯erence between desired output and actual network output is found. 5. Backward pass â€“ update output layer weights Output-layer weights are updated ï¬rst using the gradient descent rule. 6. Update hidden layer weights Hidden-layer weights are then updated using the backpropagated error and gradient descent. 7. Iterate until convergence Steps 2â€“6 are repeated until the total error becomes minimum / within tolerance. the 8 steps from the EBPA ï¬‚ow diagram, in short points: 1. Initialize weights (W, V) with small random values. 2. Present an input pattern (z) to the network. 3. Compute hidden layer outputs (y = f(Vz)). 4. Compute output layer outputs (o = f(Wy)). 5. Calculate error for the pattern: (E = \frac{1}{2}\lVert d - o \rVert^2). 6. Compute error signals for output and hidden layers (Î´ terms). 7. Update weights o Output layer: (W = W + \eta \delta_o y^T) o Hidden layer: (V = V + \eta \delta_y z^T) 8. Check stopping condition o if (E < E_{\text{max}}) â†’ stop o else â†’ repeat for remaining patterns/cycles SETTING OF DESIGN CONSIDERATIONS AND PARAMETER VALUES Initialization of weights ï‚· Weights are initialized to small random values ï‚· Common ranges: (+0.5 to âˆ’0.5) or (+1 to âˆ’1) ï‚· Very large initial weights â†’ neuron saturation â†’ slow learning ï‚· Unequal scaling of inputs may bias learning ï‚· Goal: similar eÆ¯ective input to each hidden node Frequency of weight updates Per-pattern (online) updating ï‚· Weights updated after every sample ï‚· Requires more computation ï‚· Suitable for real-time / streaming applications Per-epoch (batch) updating ï‚· Weights updated after full training set ï‚· Computationally cheaper ï‚· Not suitable for online systems Choice of learning rate (Î·) ï‚· Controls step size of learning ï‚· Large Î· â†’ fast but oscillatory/unstable ï‚· Small Î· â†’ slow but stable ï‚· Typical range: 0.1 â€“ 0.9 Improvement strategies ï‚· Start large then decrease ï‚· Start small then increase when helpful ï‚· Use 2nd derivative of error to adapt step size Momentum ï‚· Added term in weight update to smooth learning ï‚· Prevents oscillation and helps escape local minima ï‚· Typical momentum factor Î± â‰ˆ 0.9 ï‚· EÆ¯ective weight change depends on: o current gradient o previous weight change Generalizability ï‚· Network must perform well on unseen test data ï‚· Overtraining reduces test accuracy ï‚· Monitor validation error ï‚· Stop when: o training error â†“ but test error â†‘ (overï¬tting point) Network size ï‚· Too few nodes â†’ underï¬tting / weak learning ï‚· Too many nodes â†’ overï¬tting / high computation ï‚· Input nodes = number of input features ï‚· Output nodes = number of classes ï‚· Hidden nodes chosen by trial and error Sample size ï‚· More weights require more training samples Thumb rule Samples = 5â€“10 Ã— number of weights Necessary condition (Baumâ€“Haussler) ğ‘ƒ> âˆ£ğ‘Šâˆ£ 1 âˆ’ğ‘ SuÆ¯icient condition ğ‘ƒ> âˆ£ğ‘Šâˆ£ ğ‘ log á‰€ ğ‘› 1 âˆ’ğ‘á‰ Non-numeric inputs ï‚· EBPA requires numeric inputs ï‚· Non-numeric data must be encoded ï¬rst: o one-hot encoding o label encoding o embedding representations ï‚· Scaling/normalization improves learning ï¿½ ï¿½ ï¿½ Performance Evaluation of Error Backpropagation Algorithm (EBPA) ê¾† 1. Processing Time The total training time of EBPA depends on: ï‚· number of weights to be trained ï‚· value of learning rate (Î·) ï‚· number of nodes in the network ï‚· number of input samples ï‚· complexity of activation function used ï‚· number of epochs/iterations More nodes, more weights, and complex activation functions â†’ longer training time. ê¾† 2. Error Measurement (a) Error per sample / per neuron ğ¸= 1 2 (ğ‘‘à¯âˆ’ğ‘œà¯)à¬¶ where ğ‘‘à¯= desired output ğ‘œà¯= actual output (b) Error for all samples and neurons ğ¸= à·à·1 2 à¯„ à¯à­€à¬µ à¯‰ à¯£à­€à¬µ (ğ‘‘à¯âˆ’ğ‘œà¯)à¬¶ (c) Error based on Euclidean distance ğ¸= 1 ğ‘ƒğ¾à·à·1 2 à¯„ à¯à­€à¬µ à¯‰ à¯£à­€à¬µ (ğ‘‘à¯âˆ’ğ‘œà¯)à¬¶ (d) Error based on Hamming distance ğ¸= 1 ğ‘ƒğ¾à·à·1 2 à¯„ à¯à­€à¬µ à¯‰ à¯£à­€à¬µ âˆ£ğ‘‘à¯âˆ’ğ‘œà¯âˆ£ ê¾† 3. Implementation Hardware implementation Software implementation Very high speed Slower Hardware implementation Software implementation Cannot support all weight magnitudes Can support any weight magnitude DiÆ¯icult to implement & debug for large networks Easy to implement & debug Suitable for small, high-speed systems Suitable for large networks where speed is less critical ê¾† 4. Generalizability (Very Important) ï‚· Network must perform well on unseen test data, not only training data ï‚· Excessive training â†’ overï¬tting ï‚· Overï¬tting eÆ¯ect: o training error â†“ continues decreasing o test error â†‘ starts increasing Solution ï‚· continuously monitor test error ï‚· stop training when: test error increases even if training error decreases (early stopping principle). ë£… ë£† Local Minima Problem in EBPA ï‚· Ideal error curve should be single smooth parabola ï‚· In practice, the error surface contains many local minima ï‚· Training using gradient descent may get stuck in one of these ï‚· At a local minimum, gradient = 0, so algorithm stops updating ï‚· Training is assumed complete, but global minimum is not reached Goal: Reach global minimum, not get trapped in local minima. ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ Why it happens? ï‚· Error surface is non-convex in multilayer networks ï‚· Weight space is high-dimensional ï‚· Gradient descent follows steepest local slope only âœ” Solution â€” Momentum Term To avoid getting stuck and reduce oscillations: Î”ğ‘¤(ğ‘¡) = ğœ‚ğ›¿+ ğ›¼Î”ğ‘¤(ğ‘¡âˆ’1) Where ï‚· ğœ‚= learning rate ï‚· ğ›¼= momentum factor (typically 0.9) ï‚· Î”ğ‘¤(ğ‘¡âˆ’1)= previous weight change EÆ¯ect of momentum ï‚· Acts like averaging of error curve (without expensive computation) ï‚· Helps system roll past shallow local minima ï‚· Prevents drastic oscillations ï‚· Speeds convergence Intuition ï‚· If gradient direction stays same â†’ momentum increases step size ï‚· If gradient direction ï¬‚ips â†’ momentum reduces step size Point of DiÆ¯erence Supervised Learning Unsupervised Learning Reinforcement Learning Availability of output Target/desired output is provided No target output is provided No target output; reward/penalty given Presence of teacher Teacher present No teacher Environment acts as teacher via rewards Data type used Labeled data Unlabeled data Interaction data (stateâ€“ actionâ€“reward) Main goal Minimize error between actual and desired output Discover hidden structure/patterns in data Maximize cumulative reward Error signal Explicit error signal computed No error signal Reward signal instead of error Output type Classiï¬cation or regression Clusters or feature groups Policy or optimal action strategy Learning method Error-correction learning Self-organization learning Trial-and-error learning Example applications Handwriting recognition, spam detection Market segmentation, anomaly detection Game playing, robotics, self-driving cars Algorithms used Perceptron, Backpropagation, SVM, k-NN K-means, PCA, SOM Q-learning, TD learning, Policy gradients Feedback type Direct and explicit None Delayed and evaluative feedback Simple Learning Algorithm Deï¬nition The simple learning algorithm is the earliest learning rule used for a single neuron. Weights are increased when the output is wrong and kept same when the output is correct. Explanation ï‚· Inputs are applied to the neuron ï‚· Output is compared with target output ï‚· If output is correct â†’ no change in weights ï‚· If output is incorrect â†’ weights are adjusted in the direction of error ï‚· Process is repeated for all patterns until correct classiï¬cation is obtained Steps 1. Initialize weights and bias 2. Apply an input pattern 3. Compute neuron output 4. Compare with desired output 5. If output wrong â†’ modify weights 6. If output correct â†’ keep weights same 7. Repeat for all patterns Weight update formula ğ‘¤à¯œ(ğ‘›ğ‘’ğ‘¤) = ğ‘¤à¯œ(ğ‘œğ‘™ğ‘‘) + ğœ‚(ğ‘¡âˆ’ğ‘¦)ğ‘¥à¯œ âœ” Advantages 1. Simple and easy to implement (no complex error backpropagation) 2. Learns clusters automatically without target output (unsupervised) 3. Requires fewer computations since only the winning neuron is updated âœ– Disadvantages 1. Sensitive to initial weights and may converge to poor clusters 2. Only one neuron wins â€” may lead to dead neurons (some neurons never learn) 3. Cannot handle overlapping or complex class structures eÆ¯ectively Learning Vector Quantization (LVQ) Deï¬nition Learning Vector Quantization is a supervised competitive learning algorithm in which prototype vectors (codebook vectors) represent classes, and learning occurs by moving prototypes toward correctly classiï¬ed samples and away from misclassiï¬ed samples. Explanation ï‚· Each class is represented by one or more reference/prototype vectors ï‚· An input vector is compared with all prototypes ï‚· The nearest prototype is found (winner neuron) ï‚· If winnerâ€™s class = input class â†’ move prototype towards input ï‚· If winnerâ€™s class â‰  input class â†’ move prototype away from input ï‚· Repeated until classiï¬cation accuracy improves Steps 1. Initialize prototype/codebook vectors for each class 2. Present an input sample with known class label 3. Find the prototype vector closest to the input (minimum distance) 4. If prototype class = input class â†’ move prototype towards input 5. If prototype class â‰  input class â†’ move prototype away from input 6. Repeat for all training samples Update formulas If correctly classiï¬ed ğ‘¤à¯¡à¯˜à¯ª= ğ‘¤à¯¢à¯Ÿà¯—+ ğœ‚(ğ‘¥âˆ’ğ‘¤à¯¢à¯Ÿà¯—) If misclassiï¬ed ğ‘¤à¯¡à¯˜à¯ª= ğ‘¤à¯¢à¯Ÿà¯—âˆ’ğœ‚(ğ‘¥âˆ’ğ‘¤à¯¢à¯Ÿà¯—) Where ï‚· ğ‘¤= prototype vector , ğ‘¥= input vector, ğœ‚= learning rate âœ” Advantages 1. Simple and intuitive; easy to implement 2. Directly works with class labels (supervised competitive learning) 3. Decision boundaries are easy to interpret âœ– Disadvantages 1. Sensitive to initial prototype selection 2. May misclassify overlapping classes 3. Performance depends on learning rate choice Self-Organizing Map (SOM) Deï¬nition Self-Organizing Map is an unsupervised competitive learning algorithm that maps high-dimensional input data to a low-dimensional (usually 2-D) grid, preserving the topological structure of the data. Explanation ï‚· No target/teacher is used (unsupervised) ï‚· Each neuron on the grid has an associated weight (prototype) vector ï‚· For each input, the best matching unit (BMU) (nearest neuron) is found ï‚· BMU and its neighbors move closer to the input vector ï‚· Over time, the map organizes itself so that similar inputs lie close together Steps 1. Initialize weights of all neurons randomly 2. Present an input vector to the network 3. Compute distance between input vector and all neuron weight vectors 4. Select the Best Matching Unit (BMU) (minimum distance) 5. Update BMU and its neighboring neurons towards the input 6. Decrease learning rate and neighborhood size gradually 7. Repeat for all input samples over many iterations Update formula ğ‘¤à¯¡à¯˜à¯ª= ğ‘¤à¯¢à¯Ÿà¯—+ ğœ‚à¯—â„(ğ‘¡)à¯—(ğ‘¥âˆ’ğ‘¤à¯¢à¯Ÿà¯—) Where ğ‘¤= weight vector, ğ‘¥= input vector, ğœ‚= learning rate, â„(ğ‘¡)= neighborhood function (larger at start, shrinks with time) âœ” Advantages 1. Works without target output (unsupervised) 2. Preserves topology â€” similar inputs map close together 3. Useful for visualizing high-dimensional data âœ– Disadvantages 1. No direct control on number of clusters formed 2. Training time may be high for large data 3. Quality depends on neighborhood and learning-rate schedule Adaptive Resonance Theory (ART) Deï¬nition Adaptive Resonance Theory (ART) is an unsupervised learning neural network that performs stable clustering of input patterns while allowing the network to learn new patterns without forgetting old ones. Explanation ï‚· ART groups similar input patterns into clusters (categories) ï‚· Each category has a prototype vector ï‚· When a new input is presented: o it is compared with existing categories o if similarity is above vigilance level, it is assigned to that category o if not, a new category is created ï‚· Thus, ART avoids catastrophic forgetting while still learning new data Steps 1. Present an input vector to the network 2. Compare input with each category prototype 3. Select the best matching category 4. Check vigilance parameter (similarity test) 5. If similarity â‰¥ vigilance â†’ update that category 6. If similarity < vigilance â†’ create new category 7. Repeat for remaining inputs Update idea (qualitative) ï‚· For accepted category, weights move towards input ï‚· Vigilance parameter controls: o high vigilance â†’ many small clusters o low vigilance â†’ few broad clusters âœ” Advantages 1. Stable learning â€” does not forget previously learned patterns 2. Can learn new patterns dynamically 3. Vigilance parameter allows control over cluster granularity âœ– Disadvantages 1. Performance highly sensitive to vigilance parameter 2. May create too many categories at high vigilance 3. Implementation is more complex than SOM/LVQ Hopï¬eld Network è†† î€± Deï¬nition A Hopï¬eld Network is a recurrent neural network used as associative memory, where stored patterns correspond to stable states (attractors) of the network. It is fully connected, has symmetric weights, and no self-connections. è†† î€± Explanation ï‚· Every neuron is connected to every other neuron ï‚· Neuron outputs are binary or bipolar ï‚· When an input pattern (possibly noisy/incomplete) is applied, neurons update their states ï‚· With each update, the energy function decreases ï‚· Finally, the network settles into a stable minimum-energy state ï‚· This stable state corresponds to one of the stored patterns è¸° è¸± è¸² è¸³ Thus, Hopï¬eld networks can retrieve complete patterns from partial/noisy inputs (content-addressable memory). è†† î€± Steps (Working) 1. Initialize symmetric weights; set diagonal weights to zero 2. Store patterns by adjusting weights (e.g., Hebbian rule) 3. Present an input pattern to the network 4. Compute neuron net input using weighted sum 5. Update neuron states asynchronously or synchronously using activation rule 6. Recompute energy; network keeps updating 7. Stop when neuron states stop changing (stable state reached) è†† î€± Advantages 1. Works as associative memory â€” retrieves stored patterns from noisy inputs 2. Guaranteed convergence to a stable state due to energy minimization 3. Simple implementation; binary/bipolar neurons and symmetric weights è†† î€± Disadvantages 1. Low storage capacity (~0.138N patterns for N neurons) 2. May converge to spurious states/local minima instead of correct memory 3. Not suitable for large network sizes; sensitive to noise and weight errors Brain-State-in-a-Box (BSB) Network è†† î€± Deï¬nition The BSB network is a recurrent auto-associative neural network used for pattern storage and recall, similar to Hopï¬eld but with continuous-valued neuron activations constrained inside a box-shaped region (typically between â€“1 and +1). è†† î€± Explanation ï‚· It stores patterns as stable equilibrium states ï‚· Neurons are recurrently connected ï‚· Neuron activations are continuous (not just binary) ï‚· After each update, the state is projected back into a bounded region ï‚· When a noisy or partial input is applied, the network trajectory moves toward the nearest stored pattern ï‚· Convergence happens within a state box (hence the name) è¸° è¸± è¸² è¸³ BSB therefore performs auto-associative memory with continuous activation values. è†† î€± Steps (Working) 1. Initialize weight matrix using stored training patterns 2. Present an input pattern (possibly noisy/incomplete) 3. Compute new state using: ğ‘¥(ğ‘¡+ 1) = ğ‘¥(ğ‘¡) + ğœ‚(ğ‘Šğ‘¥(ğ‘¡) + ğ‘âˆ’ğ‘¥(ğ‘¡)) 4. Clip/limit each neuron output to lie within bounds (e.g., â€“1 to +1) 5. Repeat state updates iteratively 6. Stop when states stop changing â†’ stable pattern recalled è†† î€± Advantages 1. Stores and recalls patterns like Hopï¬eld, but with continuous activations 2. Better convergence properties than binary Hopï¬eld networks 3. Handles analog/noisy input data more eÆ¯ectively è†† î€± Disadvantages 1. Training and weight design are more complex than Hopï¬eld 2. Sensitive to choice of parameters (learning rate, bounds) 3. Storage capacity is still limited and network may converge to spurious states