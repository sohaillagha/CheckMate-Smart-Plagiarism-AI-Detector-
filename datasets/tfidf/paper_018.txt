Deep Learning and Computational Physics (Lecture Notes) Deep Ray, Orazio Pinti and Assad A. Oberai1 1Department of Aerospace and Mechanical Engineering, University of Southern California, Los Angeles, California, USA arXiv:2301.00942v1 [cs.LG] 3 Jan 2023 Contents Preface 3 1 Introduction 4 1.1 Computational physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2.1 Examples of ML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2.2 Types of ML algorithms based leaning task . . . . . . . . . . . . . . . . . 5 1.3 ArtiÔ¨Åcial Intelligence, Machine Learning and Deep Learning . . . . . . . . . . . . 6 1.4 Machine learning and computational physics . . . . . . . . . . . . . . . . . . . . . 7 2 Introduction to deep neural networks 9 2.1 MLP architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.2 Activation functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.1 Linear activation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.2 RectiÔ¨Åed linear unit (ReLU) . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.3 Leaky ReLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.4 Logistic function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.5 Tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2.6 Sine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3 Expressivity of a network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3.1 Universal approximation results . . . . . . . . . . . . . . . . . . . . . . . . 15 2.4 Training, validation and testing of neural networks . . . . . . . . . . . . . . . . . 15 2.5 Generalizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.5.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.6 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.7 Some advanced optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . 20 2.7.1 Momentum methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.7.2 Adam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.7.3 Stochastic optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.8 Calculating gradients using back-propagation . . . . . . . . . . . . . . . . . . . . 23 2.9 Regression versus classiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3 Residual neural networks 27 3.1 Vanishing gradients in deep networks . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.2 ResNets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.3 Connections with ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.4 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 1 4 Solving PDEs with MLPs 33 4.1 Finite diÔ¨Äerence method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.2 Spectral collocation method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.3 Physics-informed neural networks (PINNs) . . . . . . . . . . . . . . . . . . . . . . 38 4.4 Extending PINNs to a more general PDE . . . . . . . . . . . . . . . . . . . . . . 40 4.5 Error analysis for PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.6 Data assimilation using PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5 Convolutional Neural Networks 44 5.1 Functions and images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.2 Convolutions of functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.2.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.2.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.3 Discrete convolutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.4 Connection to Ô¨Ånite diÔ¨Äerences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.5 Convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.5.1 Average and Max Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 5.5.2 Convolution for inputs with multiple channels . . . . . . . . . . . . . . . . 51 5.6 Convolution Neural Network (CNN) . . . . . . . . . . . . . . . . . . . . . . . . . 51 5.7 Transpose convolution layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.8 Image-to-image transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6 Operator Networks 57 6.1 The problem with PINNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6.2 Parametrized PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6.3 Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.4 Deep Operator Network (DeepONet) Architecture . . . . . . . . . . . . . . . . . 59 6.5 Training DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.6 Error Analysis for DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.7 Physics-Informed DeepONets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.8 Fourier Neural Operators - Architecture . . . . . . . . . . . . . . . . . . . . . . . 64 6.9 Discretization of the Fourier Neural Operator . . . . . . . . . . . . . . . . . . . . 66 6.10 The Use of Fourier Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 7 Probabilistic Deep Learning 70 7.1 Key elements of Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 70 7.2 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 7.2.1 Cumulative distribution function . . . . . . . . . . . . . . . . . . . . . . . 72 7.2.2 Probability density function . . . . . . . . . . . . . . . . . . . . . . . . . . 74 7.2.3 Examples of Important RVs . . . . . . . . . . . . . . . . . . . . . . . . . . 75 7.2.4 Expectation and variance of RVs . . . . . . . . . . . . . . . . . . . . . . . 76 7.2.5 Pair of RVs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 7.3 Unsupervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . 79 7.3.1 GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 7.4 Supervised probabilistic deep learning algorithms . . . . . . . . . . . . . . . . . . 82 7.4.1 Conditional GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 2 Preface These notes were compiled as lecture notes for a course developed and taught at the University of the Southern California. They should be accessible to a typical engineering graduate student with a strong background in Applied Mathematics. The main objective of these notes is to introduce a student who is familiar with concepts in linear algebra and partial diÔ¨Äerential equations to select topics in deep learning. These lecture notes exploit the strong connections between deep learning algorithms and the more conventional techniques of computational physics to achieve two goals. First, they use concepts from computational physics to develop an understanding of deep learning algorithms. Not surprisingly, many concepts in deep learning can be connected to similar concepts in computational physics, and one can utilize this connection to better understand these algorithms. Second, several novel deep learning algorithms can be used to solve challenging problems in computational physics. Thus, they oÔ¨Äer someone who is interested in modeling a physical phenomena with a complementary set of tools. 3 Chapter 1 Introduction This course deals with topics that lie at the interface of computational physics and machine learning. Before we can appreciate the need to combine both these important concepts, we need to understand what each of them mean on their own. 1.1 Computational physics Computational physics plays a fundamental role in solving many problems in Ô¨Åelds of science and engineering. To gain an understanding of this concept, we brieÔ¨Çy outline the key steps involved in solving a physical problem: 1. Consider a physical phenomena and collect measurements of some observable of interest. For example, the measurements of the water height and wave direction obtain from ocean bouys, when studying oceanic waves. 2. Based on the observations, postulate a physical law. For instance, you observe that the mass of Ô¨Çuid is a closed-system is conserved for all time. 3. Write down a mathematical description of the law. This could make use of ordinary diÔ¨Äerential equations (ODEs), partial diÔ¨Äerential equations (PDEs), integral equations, etc. 4. Once the mathematical model is framed, solve for the solution of the system. There are two ways to obatin this: (a) In certain situations an exact analytical form of the solution can be obtained. For instance one could solve ODEs/PDEs using separation of variables, Laplace transforms, Fourier transforms or integration factors. (b) In most scenarios, exact expressions of the solution cannot be obtained and must be suitable approximated using a numerical algorithm. For instance, one could use forward or backward Euler, mid-point rule, or Runge-Kutta schemes for solving systems of ODEs; or one could use Ô¨Ånite diÔ¨Äerence/volume/element methods methods for solving PDEs. 5. Once the algorithm to evaluate the solution (exactly or approximately) is designed, use it to validate the mathematical model, i.e., see if the predictions are in tune with the data collected. All these steps broadly describe what computational physics entails. 4 1.2 Machine learning Unlike computational physics, machine learning (ML) does not require the postulation of a physical law. The general steps involved are: 1. Collect data by observing the physical phenomena, by real-time measurements of some observable or by using an numerical solver approximating the phenomenon. 2. Train a suitable algorithm using the collected data, with the aim of discovering a pattern or relation between the various samples. See Section 1.2.1 for some concrete examples. 3. Once trained, use the ML algorithm to make future predictions, and validate it with additional collected data. 1.2.1 Examples of ML 1. Regression algorithms: Given the set of pairwise data {(xi, yi) : 1 ‚â§i ‚â§N} which corresponds to some unknown function y = f(x), Ô¨Åt a polynomial (or any other basis) to this data set in order to approximate f. For instance, Ô¨Ånd the coeÔ¨Écients a, b of the linear Ô¨Åt Àúf(x; a, b) = ax + b to minimize the error E(a, b) = N X i=1 |yi ‚àíÀúf(xi)|2. If (a‚àó, b‚àó) = arg mina,b E(a, b), then we can consider Àúf‚àó(x) := Àúf(x; a‚àó, b‚àó) to be the approxi- mation of f(x) (see Figure 1.1(a)). 2. Decision trees: We are given a dataset from a sample population, containing the features: age, income. Furthermore, the data is divided into two groups; an individual in Group A owns a house while an individual in Group B does not. Then, given a features of a new data point, we would like to predict the probability of this new individual owning a house. Decision trees can be used to solve this classiÔ¨Åcation problem. The way a typical decision tree works is by making cuts the maximize the group-based separation for the samples in the dataset (see Figure 1.1(b)). Then, based on these cuts, the algorithm determines the probability of belonging to a particular class/group for a new point. 3. Clustering algorithms: Given a set of data with a number of features per sample, Ô¨Ånd cluster/patterns in the data (see Figure 1.1(c)). 1.2.2 Types of ML algorithms based leaning task Broadly speaking, there are four types of ML algorithms: 1. Supervised learning: Given the data S = {(xi, yi) : 1 ‚â§i ‚â§N}, predict ÀÜy for some new ÀÜx, such that (ÀÜx, ÀÜy) /‚ààS. For instance, given a set of images and image labels (e.g. dog, cat, cow, etc), train a classiÔ¨Åcation ML algorithm to learn the relation between images and labels, and use it to predict the label of a new image. 2. Unsupervised learning: Given the data S = {xi ‚àà‚Ñ¶x : 1 ‚â§i ‚â§N}, Ô¨Ånd a relation among diÔ¨Äerent regions of ‚Ñ¶x. For instance, Ô¨Ånd clusters in the dataset, or Ô¨Ånd an expression for the probability distribution px(x) governing the spread of this data and generate new samples from it. 5 (a) Linear regression (b) Decision tree (c) Clustering Figure 1.1: Examples of ML 3. Semi-supervised learning: This family of methods falls between the supervised and unsupervised learning families. They typically make use of a combination of labelled and unlabelled data for training. For example let‚Äôs say we are given 10,000 images that are unlabeled and only 50 images that are labeled. Can we use this dataset to develop an image classiÔ¨Åcation algorithm? 4. Re-inforcement learning: The methods belonging to this family learn driven by rewards or penalties for decisions taken. Thus, a suitable path/policy is learned to maximize the reward. These kinds of methods are used to train algorithms to play chess or Go. In this course, we will primarily focus on the Ô¨Årst two types of ML algorithms. 1.3 ArtiÔ¨Åcial Intelligence, Machine Learning and Deep Learning At times, the terms ArtiÔ¨Åcial Intelligence (AI), ML and Deep Learning (DL) are used inter- changeably. In reality, these are three related but diÔ¨Äerent concepts. This can be understood by looking at the Venn diagram in Figure 1.2. Figure 1.2: The relation between AI, ML and DL AI refers to a system with human-like intelligence. While ML is a key component of an AI system, there are other ingredients involved. A self-driving car is a prototypical example of AI. 6 Let‚Äôs take a closer look at the design of such a system (see Figure 1.3). A car is mounted with a camera which takes lives images/video of the road ahead. These frames are then passed to an ML algorithm which performs a semantic segmentation, i.e., segments out diÔ¨Äerent regions of the frame and classiÔ¨Åes the type of object (car, tree, road, sky, etc) in each segment. Once this segmentation is done, it is passed to a decision system that decides the next action of the car should be based on this segmented image. This information then passes though a control module that actually controls the mechanical actions of the car. This entire process is mimics what a real driver would do, and is thus artiÔ¨Åcial intelligence. On the other hand, machine learning (ML) are the components of this system that are trained using data. That is they learn through data. In the example above, the Semantic Segmenter is one such system. There are many ML algorithms that can perform this task using data, and we will learn some in this course. The Decision System could also be an ML component - where the appropriate decision to be made is learnt from prior data. However, it could be non-ML. Perhaps a rules based expert system. Figure 1.3: Schematic of AI system for a self-driving car. Some illustration taken from [32]. DL is a subset of ML algorithms. The simplest form of a DL architecture, known as a feed- forward network. It comprises a number of layers of non-linear transformations. The non-linear transformations are applied (component-wise) to an aÔ¨Éne transformation of an intermediate output. This architecture is loosely motivated by how signals are transmitted by the central nervous in living organisms. We will study the DL architecture in greater detail in Chapter 2. 1.4 Machine learning and computational physics Now that we have a better understanding of computational physics and ML, the next obvious question would be ‚Äúwhy do we need to look at a combination of the two?‚Äù We list down a few motivations below: ‚Ä¢ For complex patterns of ‚Äúphysical‚Äù data, ML provides an alternate route to representing mathematical laws. Consider a physical process that contains two important components. Of these, one is well understood and has a trusted mathematical model, and the other is poorly understood and does not have a mathematical description. In this scenario, one may use computational physics for the Ô¨Årst component and ML for the second. A concrete example of this would be a system governed by conservation of energy and a complex constitutive model. For the former we may have a well understood mathematical model, while for the latter we may have to rely on ML to develope a model. ‚Ä¢ ML in general is very data hungry. But the knowledge of physics can help restrict the 7 manifold on which the input and solution/predictions lie. With such constraints, we can reduce the amount of data required to train the ML algorithm. ‚Ä¢ Tools for analyzing computational physics (functional analysis, numerical analysis, notions of convergence to exact solutions, probabilistic frameworks) carry over to ML. Applying these tools to ML helps us better understand and design better ML algorithms. We brieÔ¨Çy summarize the various topics that will be covered in this course: ‚Ä¢ Deep Neural Networks (MLPs) and their convergence. ‚Ä¢ Resnets and their connections with non-linear ODEs (Neural ODEs). ‚Ä¢ Recurnets and their connections with nonlinear ODEs. ‚Ä¢ Convolutional neural networks and their connection to PDEs. ‚Ä¢ Stochastic gradient descent and how it is related to ODEs. ‚Ä¢ Deep Learning algorithms for solving PDEs. ‚Ä¢ Deep Learning algorithms for approximation operators. ‚Ä¢ Generative adversarial algorithms and their connection to computational physics. 8 Chapter 2 Introduction to deep neural networks In this chapter, we will take a closer look at the simplest network architecture that is available known as multilayer perceptron (MLP). 2.1 MLP architecture Let us deÔ¨Åne our objective as the approximation of a function f : x ‚ààRd 7‚Üíy ‚ààRD using an MLP, which we denote as F. Computing units of an MLP, called artiÔ¨Åcial neurons, are stacked in a number of consecutive layers. The zeroth layer of F is called the source layer, which is not a computing layer but is only responsible for providing an input (of dimension d) to the network. The last layer of F is known as the output layer, which outputs the network‚Äôs prediction (of dimension D). Every other layer in between is known as a hidden layer. The number of neurons in a layer deÔ¨Ånes the width of that layer. A schematic of an MLP with 2 hidden layers is shown in Figure 2.1. x(l) i = œÉ(W (l) i j x(l‚àí1) j +b(l) i ) x(0) x(1) x(2) x(3) Act. func. Act. func. Out. func. Source Layer Hidden Layer 1 Hidden Layer 2 Output Layer Figure 2.1: MLP with 2 hidden layers To understand the operations occurring inside an MLP, let us deÔ¨Åne some notations. We consider a network with L hidden layers, with the width of layer (l) denoted as Hl for l = 0, 1, ..., L+1. Note that for consistency with the function f that we are trying to approximate, we must have H0 = d and HL+1 = D. Let us denote the output vector for l-th layer by x(l) ‚ààRHl, which will serve as the input to the next layer. We set x(0) = x ‚ààRd which will be the input signal provided by the input layer. In each layer l, 1 ‚â§l ‚â§L + 1, the i-th neuron performs an 9 aÔ¨Éne transformation on that layers input x(l‚àí1) followed by a non-linear transformation x(l) i = œÉ  W (l) ij x(l‚àí1) j | {z } Einstein sum +b(l) i  , 1 ‚â§i ‚â§Hl, 1 ‚â§j ‚â§Hl‚àí1 (2.1) where W (l) ij and b(l) i are respectively known as the weights and bias associated with i-th neuron of layer l, while the function œÉ(.) is known as the activation function, and plays a pivotal role in helping the network to represent non-linear complex functions. If we set W (l) ‚ààRHl‚àí1√óHl to be the weight matrix for layer l and b(l) ‚ààRHl to be the bias vector for layer l, then we can re-write the action of the whole layer as x(l) = œÉ  A(l)(x(l‚àí1))  , A(l)(x(l‚àí1)) = W (l)x(l‚àí1) + b(l) (2.2) where the activation function is applied component-wise. Thus, the action of the whole network F : Rd 7‚ÜíRD can be mathematically seen as a composition of alternating aÔ¨Éne transformations and component-wise activations F(x) = A(L+1) ‚ó¶œÉ ‚ó¶A(L) ‚ó¶œÉ ‚ó¶A(L‚àí1) ‚ó¶¬∑ ¬∑ ¬∑ ‚ó¶œÉ ‚ó¶A(1)(x). (2.3) We make a few remarks here: 1. For simplicity of the representation, we assume that the same activation function is used across all layers of the network. However, this is not a strict rule. In fact, there is recent evidence that suggests that alternating activation function from layer to layer leads to better neural networks [33]. 2. At times, there might be an output function O instead of an activation function at the end of the output layer, which is typically used to reformulate the output into a suitable form. We will see examples of such functions later in the course. 3. We will use the term depth of the network to denote the number of computing layers in the MLP, i.e. the number of hidden layers and the output layer, which would be L + 1 as per the notations used above. The parameters of the network is all the weights and biases, which we will represent as Œ∏ = {W (l), b(l)}L+1 l=1 ‚ààRNŒ∏ where NŒ∏ denotes the total number of parameters of the network. The network F(x; Œ∏) represents a family of parameterized functions, where Œ∏ needs to suitably chosen such that the network approximates the target function f(x) at the input x. Question 2.1.1. Prove that NŒ∏ = PL+1 l=1 (Hl‚àí1 + 1)Hl. 2.2 Activation functions The activation function is perhaps the most important component of an MLP. A large number of activations are available in literature, each with its own advantages and disadvantages. Let us take a look at a few of these options (also see Figure 2.2). 10 Œæ œÉ(Œæ) (a) Linear Œæ œÉ(Œæ) (b) ReLU Œ± = 0.1 Œæ œÉ(Œæ) (c) Leaky ReLU 1 Œæ œÉ(Œæ) (d) Logistic 1 ‚àí1 Œæ œÉ(Œæ) (e) Tanh 1 ‚àí1 Œæ œÉ(Œæ) (f) Sine Figure 2.2: Examples of activation functions 11 2.2.1 Linear activation The simplest activation corresponds to œÉ(Œæ) = Œæ. Some features of this function are ‚Ä¢ The function is inÔ¨Ånitely smooth, but all derivatives beyond the second derivative are zero. ‚Ä¢ The range of the function is (‚àí‚àû, ‚àû). ‚Ä¢ Using the linear activation function (in all layers) will reduce the entire network to a single aÔ¨Éne transformation of the input x. In other words, the network will be nothing more that a linear approximation of the target function f, which is not useful if f is highly non-linear. 2.2.2 RectiÔ¨Åed linear unit (ReLU) This function is piecewise linear and deÔ¨Åned as œÉ(Œæ) = max{0, Œæ} = ( Œæ, if Œæ ‚â•0 0, if Œæ < 0 (2.4) This is one of the most popular activation functions used in practice. Some features of this function are: ‚Ä¢ The function is continuous, while its derivative will be piecewise constant with a jump Œæ = 0. The second derivative will be a dirac function concentrated at Œæ = 0. In other words, the higher-order derivates (greater than 1) are not well-deÔ¨Åned. ‚Ä¢ The range of the function is [0, ‚àû). 2.2.3 Leaky ReLU The ReLU activation leads to a null output from a neuron if the aÔ¨Éne transformation of the neuron is negative. This can lead to the phenomena of dying neurons [15] while training a neural network, where neurons drops out completely from the network and no longer contribute to the Ô¨Ånal prediction. To overcome this challenge, a leaky version ReLU was designed œÉ(Œæ; Œ±) = ( Œæ, if Œæ ‚â•0 Œ±Œæ, if Œæ < 0 (2.5) where Œ± becomes a network hyper-parameter. Some features of this function are: ‚Ä¢ The derivates of Leaky ReLU behave in the same way as those for ReLU. ‚Ä¢ The range of the function is (‚àí‚àû, ‚àû). 2.2.4 Logistic function The Logistic or Sigmoid activation function is given by œÉ(Œæ) = 1 1 + e‚àíŒæ (2.6) and has the following properties ‚Ä¢ The function is inÔ¨Ånitely smooth and monotonic. ‚Ä¢ The range of the function is (0, 1), i.e., the function is bounded. Such a function is useful in representing probabilities. ‚Ä¢ Since the derivative quickly decays to zero away from Œæ = 0, this activation function can lead to slow convergence of the network while training. 12 2.2.5 Tanh The tanh function is can be seen as a symmetric extension of the logistic function œÉ(Œæ) = eŒæ ‚àíe‚àíŒæ eŒæ + e‚àíŒæ (2.7) and has the following properties ‚Ä¢ The function is inÔ¨Ånitely smooth and monotonic. ‚Ä¢ The range of the function is (‚àí1, 1), i.e., the function is bounded. Note that it maps zeros input to zero, while pushing positive (negative) inputs to +1 (-1). ‚Ä¢ Similar to the logistic function, the derivative of tanh quickly decays to zero away from Œæ = 0 and can thus lead to slow convergence while training networks. 2.2.6 Sine Recently, the sine function, i.e., œÉ(Œæ) = sin(Œæ) has been proposed as an eÔ¨Écient activation function [27]. It has the best features of all the activation function discussed above: ‚Ä¢ The function is inÔ¨Ånitely smooth. ‚Ä¢ The range of the function is (‚àí1, 1), i.e., the function is bounded. ‚Ä¢ None of the derivatives of this function decay to zero. Question 2.2.1. Can you think of an MLP architecture with the sine activation function, which leads to an approximation very similar to a Fourier series expansion? 2.3 Expressivity of a network Let us try to understand the eÔ¨Äects of NŒ∏ increases. To see this, let us consider a simple example using the ReLU activation function, i.e., œÉ(Œæ) = max{Œæ, 0}. We set d = D = 1, L = 1 and the parameters W (1) = 2 1  , b(1) = ‚àí2 0  , W (2) =  1 1  , b(2) = 0. as shown in Figure 2.3(a). Then the various layer outputs are x(1) 1 = max{2x(0) 1 ‚àí2, 0}, x(1) 2 = max{x(0) 1 , 0}, x(2) 1 = max{2x(0) 1 ‚àí2, 0} + max{x(0) 1 , 0}. Notice that while the the output x(1) of the hidden layer (see Figures 2.3(b) and (c)) have only one corner/kink, the Ô¨Ånal output ends up having two kinks (see Figures 2.3(d)). We generalize this formulation to a bigger network with L hidden layers each of width H. Then one can expect that x(1) i , 1 ‚â§i ‚â§H will have a single kink, with the location and angle of the kink depending on the weights and bias associated with each neuron of the hidden layer. The vector x(1) is passed to the next hidden layer, where each neuron will combine the single kinks and give an output with possibly H kinks. Once again, the location and angles of the H kinks in the output from each neuron of the second hidden layer will be diÔ¨Äerent. The location of the kinks will be diÔ¨Äerent because each neuron is allowed a diÔ¨Äerent bias, and therefore can induce a diÔ¨Äerent shift. Continuing this argument, one can expect the number of kinks to increase as H, H2, H3 as it passes through the various hidden layers with width H. In general the total number of kinks can grow as HL. In other words, the networks have the ability to become more expressive as the depth (and width) of the network is increased. 13 w = 2 b = ‚àí2 w = 1 b = 0 w = 1 b = 0 w = 1 (0) (1) (2) (a) MLP with L = 1,W = 2 x(0) 1 x(1) 1 0 4 4 One kink (b) x(1) 1 vs x(0) 1 x(0) 1 x(1) 2 0 4 4 One kink (c) x(1) 2 vs x(0) 1 x(0) 1 x(2) 1 0 4 4 Two kinks (d) x(2) 1 vs x(0) 1 Figure 2.3: Examples to understand the expressivity of neural networks 14 2.3.1 Universal approximation results To quantify the expressivity of networks in a mathematically rigorous manner, we look at some results about the approximation properties of MLPs. For these results, we assume K ‚äÇRd is a closed and bounded set. Theorem 2.3.1 (Pinkus, 1999 [23]). Let f : K ‚ÜíR, i.e., D = 1, be a continuous function. Then given an œµ > 0, there exists an MLP with a single hidden layer (L = 1), arbitrary width H and a non-polynomial continuous activation œÉ such that max x‚ààK |F(x; Œ∏) ‚àíf(x)| ‚â§œµ. Theorem 2.3.2 (Kidger, 2020 [9]). Let f : K ‚ÜíRD be a continuous vector-valued function. Then given an œµ > 0, there exists an MLP with arbitrary number of hidden layers L, each having width H ‚â•d + D + 2, a continuous activation œÉ (with some additional mild conditions), such that max x‚ààK ‚à•F(x; Œ∏) ‚àíf(x)‚à•‚â§œµ. Theorem 2.3.3 (Yarotsky, 2021 [33]). Let f : K ‚ÜíR be a function with two continuous derivates, i.e., f ‚ààC2(K). Consider an MLP with ReLU activations and H ‚â•2d + 10. Then there exists a network with this conÔ¨Åguration such that the error converges as max x‚ààK |F(x; Œ∏) ‚àíf(x)| ‚â§C(NŒ∏)‚àí4 where C is a constant depending on the number of network parameters. Numerical results like those mentioned above help demystify the ‚Äúblack-box‚Äù nature of neural network, and serve as useful practical guidelines when designing network architectures. 2.4 Training, validation and testing of neural networks Now that we have a better understanding of the architecture of MLPs, we would now like to discuss how the parameters of these networks are set to approximate some target function. We restrict our discussions to the framework of supervised learning. Let us assume that we are given a dataset of pairwise samples S = {(xi, yi) : 1 ‚â§i ‚â§N} corresponding to a target function f : x 7‚Üíy. We wish to approximate this function using the neural network F(x; Œ∏, Œò) where Œ∏ are the network parameters deÔ¨Åned before, while Œò corresponds to the hyper-parameters of the network such as the depth L + 1, width H, type of activation function œÉ, etc. The strategy to design a robust network involves three steps: 1. Find the optimal values of Œ∏ (for a Ô¨Åxed Œò) in the training phase. 2. Find the optimal values of Œò in the validation phase. 3. Test the performance of the network on unseen data on the testing phase. To accomplish these three tasks, it is Ô¨Årst customary to split the dataset S into three distinct parts: a training set with Ntrain samples, a validation set with Nval samples and test set with Ntest samples, with N = Ntrain + Nval + Ntest. Typically, one uses around 60% of the samples as training samples, 20% as validation samples and the remaining 20% for testing. 15 Splitting the dataset is necessary as neural networks are heavily over-parameterized functions. The large number of degrees of freedom available to model the data can lead to over-Ô¨Åtting the data. This happens when the error or noise present in the data drives the behavior of the network more than the underlying input-output relation itself. Thus, a part of the data is used to determine Œ∏, and another part to determine the hyper-parameters Œò. The remainder of the data is kept aside for testing the performance of the trained network on unseen data, i.e., the network‚Äôs ability to generalize well. Now let us discuss how this split is used during the three phases in further details: Training: Training the network makes use of the training set Strain to solve the following optimization problem: Find Œ∏‚àó= arg min Œ∏ Œ†train(Œ∏), where Œ†train(Œ∏) = 1 Ntrain Ntrain X i=1 (xi,yi)‚ààStrain ‚à•yi ‚àíF(xi; Œ∏, Œò)‚à•2 for some Ô¨Åxed Œò. The optimal Œ∏‚àóis obtained using a suitable gradient based algorithm (will be discussed later). The function Œ†train is referred to as the loss function. In the example above we have used the mean-squared loss function. Later we will consider other types of loss functions. Validation: Validation of the network involves using the validation set Sval to solve the following optimization problem: Find Œò‚àó= arg min Œò Œ†val(Œò), where Œ†val(Œò) = 1 Nval Nval X i=1 (xi,yi)‚ààSval ‚à•yi ‚àíF(xi; Œ∏‚àó, Œò)‚à•2. The optimal Œò‚àóis obtained using a techniques such as (random or tensor) grid search. Testing: Once the "best" network is obtained, characterized by Œ∏‚àóand Œò‚àó, it is evaluated on the test set Stest to estimate the networks performance on data not used during the Ô¨Årst two phases. Œ†test = 1 Ntest Ntest X i=1 (xi,yi)‚ààStest ‚à•yi ‚àíF(xi; Œ∏‚àó, Œò‚àó)‚à•2. This testing error is also known as the (approximate) generalizing error of the network. Let‚Äôs see an example to better understand how such a network is obtained Example 2.4.1. Let us consider an MLP where all hyper-parameters are Ô¨Åxed except for the following Ô¨Çexible choices œÉ ‚àà{ReLU, tanh}, L ‚àà{10, 20}. We use the following algorithm 1. For each possible œÉ, L pair: (a) Find Œ∏‚àó= arg minŒ∏ Œ†train(Œ∏) (b) With this Œ∏‚àó, evaluate Œ†val(Œò) 2. Select Œò‚àóto be the one that gave the smallest value of Œ†val(Œò). 3. Finally, report Œ†test for this Œò‚àóand the corresponding Œ∏‚àó. 16 2.5 Generalizability If we train a network that has a small value of Œ†train and Œ†val, does it ensure that Œ†test will be small? This question is addressed by studying the generalizability of the trained network, i.e., it capability to perform well on data not seen while training/validating the network. If the network is trained to overÔ¨Åt the training data, the network will typically lead to poor predictions on test data. Typically, if Strain, Sval and Stest are chosen from the same distribution of data, then a small value of Œ†train, Œ†val can lead to small values of Œ†test. Let us look at the commonly used technique to avoid data overÔ¨Åtting, called regularization. 2.5.1 Regularization Neural networks, especially MLPs, are almost always over-parametrized, i.e., NŒ∏ ‚â´N where N is the number of training samples. This would lead to a highly non-linear network model, for which the loss function Œ†(Œ∏) (where we omit the subscript "train" for brevity) can have a landscape with many local minimas (see Figure 2.4(a)). Then how do we determine which minima leads to a better generalization? To nudge the choice of Œ∏‚àóin a more favorable direction, a regularization technique can be employed. (a) Loss function landscape (b) Network sensitivity Figure 2.4: The eÔ¨Äect of regularization on the loss function. We have assumed a scalar Œ∏ for easier illustration. The simplest method of regularization involves augmenting a penalty term to the loss function: Œ†(Œ∏) ‚àí‚ÜíŒ†(Œ∏) + Œ±‚à•Œ∏‚à•, Œ± ‚â•0 where Œ± is a regularization hyper-parameter, and ‚à•Œ∏‚à•is a suitable norm of the network parameters Œ∏. This augmentation can change the landscape of Œ†(Œ∏) as illustrated in Figure 2.4(a). In other words, such a regularization encourages the selection of a minima corresponding to smaller values of the parameters Œ∏. It is not obvious why a smaller value of Œ∏ would be a better choice. To see why this is better, consider the intermediate network output x(1) 1 = œÉ(W (1) 1j x(0) j + b(1) 1 ), which gives ‚àÇx(1) 1 ‚àÇx(0) 1 = œÉ‚Ä≤(W (1) 1j x(0) j + b(1) 1 )W (1) 11 ‚àùW (1) 11 . 17 Since this derivate scales with W (1) 11 , this implies that |‚àÇF(x) ‚àÇx(0) 1 | scales with W (1) 11 as well. If |W (1) 11 | ‚â´1, then network would be very sensitive to even small changes in the input x(0) 1 , i.e., the network would be ill-posed. As illustrated in Figure 2.4(b), using a proper regularization would help avoid over Ô¨Åtting. Let us consider some common types of regularization: ‚Ä¢ l2 regularization: Here we use the l2 norm in the regularization term ‚à•Œ∏‚à•= ‚à•Œ∏‚à•2 = NŒ∏ X i=1 Œ∏2 i !1/2 . ‚Ä¢ l1 regularization: Here we use the l1 norm in the regularization term ‚à•Œ∏‚à•= ‚à•Œ∏‚à•1 = NŒ∏ X i=1 |Œ∏i|, which promotes the sparsity of Œ∏. 2.6 Gradient descent Recall that we wish to solve the minimization problem Œ∏‚àó= arg min Œ†(Œ∏) in the training phase. This minimization problem can be solved using gradient descent (GD), also known as steepest descent. Consider the Taylor expansion about Œ∏0 Œ†(Œ∏0 + ‚àÜŒ∏) = Œ†(Œ∏0) + ‚àÇŒ† ‚àÇŒ∏ (Œ∏0) ¬∑ ‚àÜŒ∏ + ‚àÇ2Œ† ‚àÇŒ∏iŒ∏j (ÀÜŒ∏)‚àÜŒ∏i‚àÜŒ∏j for some ÀÜŒ∏ in a small neighbourhood of Œ∏0. When |‚àÜŒ∏| is small and assuming ‚àÇ2Œ† ‚àÇŒ∏iŒ∏j is bounded, we can neglect the second order term and just consider the approximation Œ†(Œ∏0 + ‚àÜŒ∏) ‚âàŒ†(Œ∏0) + ‚àÇŒ† ‚àÇŒ∏ (Œ∏0) ¬∑ ‚àÜŒ∏. In order to lower the value of the loss function as much as possible compared to its evaluation at Œ∏0, i.e. minimize ‚àÜŒ† = Œ†(Œ∏0 + ‚àÜŒ∏) ‚àíŒ†(Œ∏0), we need to choose the step ‚àÜŒ∏ in the opposite direction of the gradient, i.e.: ‚àÜŒ∏ = ‚àíŒ∑‚àÇŒ† ‚àÇŒ∏ (Œ∏0) with the step-size Œ∑ ‚â•0, also known as the learning-rate. This is yet another hyper-parameter that we need to tune during the validation phase. This is the crux of the GD algorithm, and can be summarized as follows: 1. Initialize k = 0 and Œ∏0 2. While |Œ†(Œ∏k)| > œµ1, do (a) Evaluate ‚àÇŒ† ‚àÇŒ∏ (Œ∏k) (b) Update Œ∏k+1 = Œ∏k ‚àíŒ∑ ‚àÇŒ† ‚àÇŒ∏ (Œ∏k) (c) Increment k = k + 1 18 Convergence: Assume that Œ†(Œ∏) is convex and diÔ¨Äerentiable, and its gradient is Lipschitz continuous with Lipschitz constant K. Then for a Œ∑ ‚â§1/K , the GD updates converges as ‚à•Œ∏‚àó‚àíŒ∏k‚à•2 ‚â§C k . However, in most scenarios Œ†(Œ∏) is not convex. If there is more than one minima, then what kind of minima does GD like to pick? To answer this, consider the loss function for a scalar Œ∏ as shown in Figure 2.5, which has two valleys. Let‚Äôs assume that the proÔ¨Åle of Œ†(Œ∏) in the each valley can be approximated by a (centered) parabola Œ†(Œ∏) ‚âà1 2aŒ∏2 where a > 0 is the curvature of each valley. Note that the curvature of the left valley is much smaller than the curvature of the right valley. Let‚Äôs pick a constant learning rate Œ∑ and a starting value Œ∏0 in either of the valleys. Then, ‚àÇŒ† ‚àÇŒ∏ (Œ∏0) = aŒ∏0 and the new point after a GD update will be Œ∏1 = Œ∏0(1 ‚àíaŒ∑). Similarly, it is easy to see that all subsequent iterates write Œ∏k+1 = Œ∏k(1 ‚àíaŒ∑). For convergence, we need Œ∏k+1 Œ∏k < 1 =‚áí|1 ‚àíaŒ∑| < 1. Since a > 0 in the valleys, we will need the following condition on the learning rate ‚àí1 < 1 ‚àíaŒ∑ =‚áíaŒ∑ < 2. If we Ô¨Åx Œ∑, then for convergence we need the local curvature to satisfy a < 2/Œ∑. In other words, GD will prefer to converge to a minima with a Ô¨Çat/small curvature, i.e., it will prefer the minima in the left valley. If the starting point is in the right valley, there is a chance that we will keep overshooting the right minima and bounce oÔ¨Äthe opposite wall till the GD algorithm slingshots Œ∏k outside the valley. After this it will enter the left valley with a smaller curvature and gradually move towards its minima. Figure 2.5: GD prefers Ô¨Çatter minimas. While it is clear that GD prefers Ô¨Çat minima, what is not clear is why are Ô¨Çat minima better. There is empirical evidence that the parameter values obtained at Ô¨Çat minima tend to generalize better, and therefore are to be preferred. 19 2.7 Some advanced optimization algorithms We discussed how GD can be used to solve the optimization problem involved in training neural networks. Let us look at a few advanced and popular optimization techniques motivated by GD. In general, the update formula for most optimization algorithms make use of the following formula [Œ∏k+1]i = [Œ∏k]i ‚àí[Œ∑k]i[gk]i, 1 ‚â§i ‚â§NŒ∏, (2.8) where [Œ∑k]i is the component-wise learning rate and the vector-valued function g depends/approximates the gradient. Note that the notation [.]i is used to denote the i-th component of the vector. Also note that the learning rate is allowed to depend on the iteration number k. The GD method makes use of [Œ∑k]i = Œ∑, gk = ‚àÇŒ† ‚àÇŒ∏ (Œ∏k). An issue with the GD method is that the convergence to the minima can be quite slow if Œ∑ is not suitably chosen. For instance, consider the objective function landscape shown in Figure 2.6, which has sharper gradients along the [Œ∏]2 direction compared to the [Œ∏]1 direction. If we start from a point, such as the one shown in the Ô¨Ågure, then if Œ∑ is too large (but still within the stable bounds) the updates will keep zig-zagging its way towards the minima. Ideally, for the particular situation shown in Figure 2.6, we would like the steps to take longer strides along the [Œ∏]1 compared to the [Œ∏]2 direction, thus reaching the minima faster. Figure 2.6: Zig-zagging updates with GD. Let us look at two popular methods that are able to overcome some of the issues faced by GD. 2.7.1 Momentum methods Momentum methods make use of the history of the gradient, instead of just the gradient at the previous step. The formula for the update is given by [Œ∑k]i = Œ∑, gk = Œ≤1gk‚àí1 + (1 ‚àíŒ≤1)‚àÇŒ† ‚àÇŒ∏ (Œ∏k), g‚àí1 = 0 20 where gk is a weighted moving average of the gradient. This weighting is expected to smoothen out the zig-zagging seen in Figure 2.6 by cancelling out the components of gradient along the [Œ∏]2 direction and move more smoothly towards the minima. A commonly used value for Œ≤1 is 0.9. 2.7.2 Adam The Adam optimization was introduced by Kingma and Ba [10], which makes use of the history of the gradient as well the second moment (which is a measure of the magnitude) of the gradient. For an initial learning rate Œ∑, the updates are given by gk = Œ≤1gk‚àí1 + (1 ‚àíŒ≤1)‚àÇŒ† ‚àÇŒ∏ (Œ∏k) [Gk]i = Œ≤2[Gk‚àí1]i + (1 ‚àíŒ≤2)  ‚àÇŒ† ‚àÇŒ∏i (Œ∏k) 2 [Œ∑k]i = Œ∑ p [Gk]i + œµ (2.9) where gk and Gk are the weighted running averages of the gradients and the square of the gradients, respectively. The recommended values for the hyper-parameters are Œ≤1 = 0.9, Œ≤2 = 0.999 and œµ = 10‚àí8. Note that the learning rate for each component is diÔ¨Äerent. In particular, the larger the magnitude of the gradient for a component the smaller is its learning rate. Referring back to the example in Figure 2.6, this would mean a smaller learning rate for Œ∏2 in comparison to Œ∏1, and therefore will help alleviate the zig-zag path of the optimization algorithm. Remark 2.7.1. The Adam algorithm also has additional correction steps for gk and Gk to improve the eÔ¨Éciency of the algorithm. See [10] for details. 2.7.3 Stochastic optimization We note that the training loss can be rewritten as Œ†(Œ∏) = 1 Ntrain Ntrain X i=1 Œ†i(Œ∏), Œ†i(Œ∏) = ‚à•yi ‚àíF(xi; Œ∏, Œò)‚à•2 Thus, the gradient of the loss function is ‚àÇŒ† ‚àÇŒ∏ (Œ∏) = 1 Ntrain Ntrain X i=1 ‚àÇŒ†i ‚àÇŒ∏ (Œ∏) However, taking the summation of gradients can be very expensive since Ntrain is typically very large, Ntrain ‚àº106. One easy way to circumvent this problem is to use the following update formula (shown here for the GD method) Œ∏k+1 = Œ∏k ‚àíŒ∑k ‚àÇŒ†i ‚àÇŒ∏ (Œ∏k), (2.10) where i is randomly chosen for each update step k. This is known as stochastic gradient descent. Remarkably, this modiÔ¨Åed algorithm does converge assuming that Œ†i(Œ∏) is convex and diÔ¨Äerentiable, and Œ∑k ‚àº1/ ‚àö k [19]. To illustrate why Œ∑k needs to decay, consider the toy function(s) for Œ∏ ‚ààR2 Œ†1(Œ∏) = ([Œ∏]1 ‚àí1)2 + ([Œ∏]2 ‚àí1)2, Œ†2(Œ∏) = ([Œ∏]1 + 1)2 + 0.5([Œ∏]2 ‚àí1)2, Œ†3(Œ∏) = 0.7([Œ∏]1 + 1)2 + 0.5([Œ∏]2 + 1)2, Œ†4(Œ∏) = 0.7([Œ∏]1 ‚àí1)2 + 1 2([Œ∏]2 + 1)2, Œ†(Œ∏) = 1 4 (Œ†1(Œ∏) + Œ†2(Œ∏) + Œ†3(Œ∏) + Œ†4(Œ∏)) . (2.11) 21 The contour plots of these functions in shown in Figure 2.7(a), where the black contour plots corresponds to Œ†(Œ∏). Note that the Œ∏‚àó= (0, 0) is the unique minima for Œ†(Œ∏). We consider solving with the SGD algorithm with a constant learning rate Œ∑k = 0.4 and a decaying learning rate Œ∑k = 0.4/ ‚àö k. Starting with Œ∏0 = (‚àí1.0, 2.0) and randomly selecting i ‚àà1, 2, 3, 4 for each step k, we run the algorithm for 10,000 iterations. The Ô¨Årst 10 steps with each learning rate is plotted in Figure 2.7(a). We can clearly see that without any decay in the learning rate, the SGD algorithm keeps overshooting the minima. In fact, this behaviour continues for all future iterations as can be seen in Figure 2.7(b) where the norm of the updates does not decay (we expect it to decay to |Œ∏‚àó| = 0). On the other hand, we quickly move closer to Œ∏‚àóif the learning rate decays as 1/ ‚àö k. The reason for reducing the step size as we approach closer to the minima is that far away from the minima for Œ† the gradient vector for Œ† and all the individual Œ†i‚Äôs align quite well. However, as we approach closer to the minima for Œ† this is not the case and therefore one is required to take smaller steps so as not be thrown oÔ¨Äto a region far away from the minima. (a) Function contours and paths (b) Norm of updates Figure 2.7: SGD algorithm with and without a decay in the learning rate. In practice, stochastic optimization algorithms are not used for the following reasons: 1. Although the loss function decays with the number of iterations, it Ô¨Çuctuates in a chaotic manner close the the minima and never manages to reach the minima. 2. While handling all samples at once can be computationally expensive, handling a single sample at a time severly under-utilizes the computational and memory resources. However, a compromise can be made by using mini-batch optimization. In this strategy, the dataset of Ntrain samples is split into Nbatch disjoint subsets known as mini-batches. Each mini-batch contains Ntrain = Ntrain/Nbatch samples, which also refered to as the batch-size. Thus, the gradient of the loss function can be approximated by ‚àÇŒ† ‚àÇŒ∏ (Œ∏) = 1 Ntrain Ntrain X i=1 ‚àÇŒ†i ‚àÇŒ∏ (Œ∏) ‚âà 1 Ntrain X i‚ààbatch(j) ‚àÇŒ†i ‚àÇŒ∏ (Œ∏). (2.12) 22 Note that taking Nbatch = 1 leads to the original optimization algorithms, while take Nbatch = Ntrain gives the stochastic gradient descent algorithm. One typically chooses a batch-size to maximize the amount of data that can be loaded into the RAM at one time. We deÔ¨Åne an epoch as one full pass through all samples (or mini-batches) of the full training set. The following describes the mini-batch stochastic optimization algorithm: 1. For epoch = 1, ..., J (a) Randomly shuÔ¨Ñe the full training set (b) Create Nbatch mini-batches (c) For i = 1, ¬∑ ¬∑ ¬∑ , Nbatch i. Evaluate the batch gradient using (2.12). ii. Update Œ∏ using this gradient and your favorite optimization algorithm (gradient descent, momentum, or Adam). Remark 2.7.2. There is an interesting study [31] that suggests that stochastic gradient descent might actually help in selecting minima that generalize better. In that study the authors prove that SGD prefers minima whose curvature is more homogeneous. That is, the distribution of the curvature of each of the components of the loss function is sharp and centered about a small value. This is contrast to minima where the overall curvature might be small; however the distribution of the curvature of each component of loss function is more spread out. Then they go on to show (empirically) that the more homogeneous minima tend to generalize better than their heterogeneous counterparts. 2.8 Calculating gradients using back-propagation The Ô¨Ånal piece of the training algorithm that we need to understand is how the gradients are actually evaluated while training the network. Recall the output x(l+1) of layer l + 1 is given by AÔ¨Éne transform: Œæ(l+1) i = W (l+1) ij x(l) j + b(l+1) i , 1 ‚â§i ‚â§Hl+1 (2.13) Non-linear transform: x(l+1) i = œÉ  Œæ(l+1) i  , 1 ‚â§i ‚â§Hl+1. (2.14) Given a training sample (x, y), set x(0) = x. The value of the loss/objective function (for this particular sample) can be evaluated using the forward pass: 1. For l = 1, ..., L + 1 (a) Evaluate Œæ(l) using (2.13). (b) Evaluate x(l) using (2.14). 2. Evaluate the loss function for the given sample Œ†(Œ∏) = ‚à•y ‚àíF(x; Œ∏, Œò)‚à•2. This operation can be written succinctly in the form of a computational graph as shown in Figure 2.8. In this Ô¨Ågure, the lower portion of the graph represents the evaluation of the loss function Œ†. We would of course need to repeat this step for all samples in the training set (or a mini-batch for stochastic optimization). For simplicity, we restrict the discussion to the evaluation of the loss and its gradient for a single sample. 23 In order to update the network parameters, we need ‚àÇŒ† ‚àÇŒ∏ , or more precisely ‚àÇŒ† ‚àÇW (l) , ‚àÇŒ† ‚àÇb(l) for 1 ‚â§l ‚â§L + 1. We will derive expressions for these derivatives by Ô¨Årst deriving expressions for ‚àÇŒ† ‚àÇŒæ(l) and ‚àÇŒ† ‚àÇx(l) . From the computational graph it is easy to see how each hidden variable in the network is transformed to the next. Recognizing this, and applying the chain rule repeatedly yields ‚àÇŒ† ‚àÇŒæ(l) = ‚àÇŒ† ‚àÇx(L+1) ¬∑ ‚àÇx(L+1) ‚àÇŒæ(L+1) ¬∑ ‚àÇŒæ(L+1) ‚àÇx(L) ¬∑ ¬∑ ¬∑ ‚àÇx(l+1) ‚àÇŒæ(l+1) ¬∑ ‚àÇŒæ(l+1) ‚àÇx(l) ¬∑ ‚àÇx(l) ‚àÇŒæ(l) . (2.15) In order to evaluate this expression we need to evaluate the following terms: ‚àÇŒ† ‚àÇx(L+1) = ‚àí2(y ‚àíx(L+1))T (2.16) ‚àÇŒæ(l+1) ‚àÇx(l) = W (l+1) (2.17) ‚àÇx(l) ‚àÇŒæ(l) = S(l) ‚â°diag[œÉ‚Ä≤(Œæ(l) 1 ), ¬∑ ¬∑ ¬∑ , œÉ‚Ä≤(Œæ(l) Hl)], (2.18) where the last two relations hold for any network layer l, Hl is the width of that particular layer, and œÉ‚Ä≤ denotes the derivative of the activation with respect to its argument. Using these relations in (2.15), we arrive at, ‚àÇŒ† ‚àÇŒæ(l) = ‚àÇŒ† ‚àÇx(L+1) ¬∑ S(L+1) ¬∑ W (L+1) ¬∑ ¬∑ ¬∑ S(l+1) ¬∑ W (l+1) ¬∑ S(l). (2.19) Taking the transpose, and recognizing that Œ£(l) is diagonal and therefore symmetric, we Ô¨Ånally arrive at ‚àÇŒ† ‚àÇŒæ(l) = S(l)W (l+1)T S(l+1) ¬∑ ¬∑ ¬∑ W (L+1)T S(L+1)[‚àí2(y ‚àíx(L+1))]. (2.20) This evaluation can also be represented as a computational graph. In fact, as shown in Figure 2.8, it can be appended to the original graph, where this part of the computation appear in the upper row of the graph. Note that we are now traversing in the backward direction. Hence the name back propagation. P A(1) s A(l+1) s s W(1) W(l+1) S(L+1) ùíô(ùüé) ùùÉ(ùüè) ùíô(ùüè) ùíô(ùíç) ùíô(ùíç&ùüè) ùíô(ùë≥&ùüè) ùùÉ(ùíç) ùùÉ(ùíç&ùüè) ùùÉ(ùë≥&ùüè) ùùèùö∑ ùùèùùÉ(ùüè) ùùèùö∑ ùùèùíô(ùüé) ùùèùö∑ ùùèùíô(ùíç) ùùèùö∑ ùùèùíô(ùíç&ùüè) ùùèùö∑ ùùèùíô(ùë≥&ùüè) ùùèùö∑ ùùèùùÉ(ùíç) ùùèùö∑ ùùèùùÉ(ùíç&ùüè) ùùèùö∑ ùùèùùÉ(ùë≥&ùüè) ùùèùö∑ ùùèùíô(ùüè) S(1) S(l) S(l+1) s Figure 2.8: Computational graph for computing the loss function and its derivatives with respect to hidden/latent vectors. The Ô¨Ånal step is to evaluate an explicit expression for ‚àÇŒ† ‚àÇW (l) . This can be done by recognizing, ‚àÇŒ† ‚àÇW (l) = ‚àÇŒ† ‚àÇŒæ(l) ¬∑ ‚àÇŒæ(l) ‚àÇW (l) = ‚àÇŒ† ‚àÇŒæ(l) ‚äóx(l‚àí1), (2.21) 24 where [x ‚äóy]ij = xiyj is the outer product. Thus, in order to evaluate ‚àÇŒ† ‚àÇW (l) we need x(l‚àí1) which is evaluted during the forward phase and ‚àÇŒ† ‚àÇŒæ(l) which is evaluated during back propagation. Question 2.8.1. Can you derive a similar set of expressions and the corresponding algorithm to evaluate ‚àÇŒ† ‚àÇb(l) ? Question 2.8.2. Can you derive an explicit expression for ‚àÇx(L+1) ‚àÇx(0) . That is the an expression for the derivative of the output of the network with respect to its input? This is a very useful quantity that Ô¨Ånds use in algorithms like physics informed neural networks and Wasserstein generative adversarial networks. 2.9 Regression versus classiÔ¨Åcation Till now, given the labelled dataset S = {(xi, yi) : 1 ‚â§i ‚â§N}, we have considered two types of losses ‚Ä¢ The mean square error (MSE) Œ†(Œ∏) = 1 Ntrain Ntrain X i=1 ‚à•yi ‚àíF(xi; Œ∏, Œò)‚à•2. ‚Ä¢ The mean absolute error (MAE) Œ†(Œ∏) = 1 Ntrain Ntrain X i=1 ‚à•yi ‚àíF(xi; Œ∏, Œò)‚à•. Neural networks with the above losses can be used to solve various regression problems where the underlying function is highly nonlinear and the inputs/outputs are multi-dimensional. Example 2.9.1. Given the house/apartment features such as the zip code, the number of bedrooms/bathrooms, carpet area, age of construction, etc, predict the outcomes such as the market selling price, or the number of days on the market. Now let us consider some examples of classiÔ¨Åcation problems, where the output of the network typically lies in a discrete Ô¨Ånite set. Example 2.9.2. Given the symptoms and blood markers of patients with COVID-19, predict whether they will need to be admitted to ICU. So the input and output for this problem would be x = [pulse rate, temperature, SPO2, procalcitonin, ...] y = [p1, p2] where p1 is the probability of being admitted to the ICU, while p2 is the probability of not being admitted. Note that 0 ‚â§p1, p2 ‚â§1 and p1 + p2 = 1. Example 2.9.3. Given a set of images of animals, predict whether the animal is a dog, cat or bird. In this case, the input and output should be x = the image y = [p1, p2, p2] where p1, p2, p3 is the probability of being a dog, cat or bird, respectively. 25 Since the output for the classiÔ¨Åcation problem corresponds to probabilities, we need to make a few changes to the network 1. Make use of an output function at the end of the output layer that suitably transforms the output vector into the desired form, i.e, a vector of probabilities. This is typically done using the softmax function x(L+1) i = exp (Œæ(L+1) i ) PC j=1 exp (Œæ(L+1) j ) where C is the number of classes (and also the output dimension). Verify that with this transformation, the components of the x(L+1) form a convex combination, i.e., x(L+1) i ‚àà[0, 1] and PC i=1 x(L+1) i = 1. 2. The output labels for the various samples need to be one-hot encoded. In other words, for the sample (x, y), the output label y should have dimension D = C, and whose component is 1 only for the component signifying the class x belongs to, otherwise 0. For instance, in Example 2.9.3 y = Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ [1, 0, 0]‚ä§ if x is a dog, [0, 1, 0]‚ä§ if x is a cat, [0, 0, 1]‚ä§ if x is a pig. 3. Although the MSE or MSA can still be used as the loss function, it is preferable to use the cross-entropy loss function Œ†(Œ∏) = 1 Ntrain Ntrain X i=1 C X c=1 ‚àíyci log(Fc(xi; Œ∏)), (2.22) where yci is the c-th component of the true label for the i-th sample. The loss function in (2.22) treats yc and Fc as probability distributions and measures the discrepancy between the two. It can be shown to be related to the Kullback-Liebler divergence between the two distributions. Compared to MSE, this loss function severely penalizes strongly conÔ¨Ådent incorrect predictions. This is demonstrated in Example 2.9.4 Example 2.9.4. Let us consider a binary classiÔ¨Åcation problem, i.e., C = 2. For a given x, let y = [0, 1] and let the prediction be F = [p, 1 ‚àíp]. Clearly, a small value of p is preferred. Therefore any reasonable cost function should penalize large values of p. Now let us evaluate the error using various loss functions ‚Ä¢ MSE Loss = (0 ‚àíp)2 + (1 ‚àí1 + p)2 = 2p2. ‚Ä¢ Cross-entropy Loss = ‚àí(0 log(p) + 1 log(1 ‚àíp) = ‚àílog(1 ‚àíp). Note that both losses penalize large values of p. Also when p = 0, both losses are zero. However, as p ‚Üí1 (which would lead the wrong prediction), the MSE loss ‚Üí2, while the cross-entropy loss ‚Üí‚àû. That is, it strongly penalizes incorrect conÔ¨Ådent predictions. 26 Chapter 3 Residual neural networks Residual networks (or ResNets) were introduced by He et al. [8] in 2015. In this chapter, we will discuss what these networks are, why they were introduced and their relation to ODEs. 3.1 Vanishing gradients in deep networks While training neural networks, the gradients ‚àÇŒ† ‚àÇW (l) , ‚àÇŒ† ‚àÇb(l) might become very small. For instance, consider a very deep network, say L ‚â•20. If ‚àÇŒ† ‚àÇW (l) ‚â™1 for l ‚â§¬Øl, then the contribution of Ô¨Årst ¬Øl layers of the network will be negligible, as the inÔ¨Çuence of their weights on the loss function is small. Because of this depth cut-oÔ¨Ä, the beneÔ¨Åt in terms of expressivity of deep networks is lost. So why does this happen? Recall from Section 2.8 that ‚àÇŒ† ‚àÇW (l) = ‚àÇŒ† ‚àÇŒæ(l) ‚äóx(l‚àí1) and ‚àÇŒ† ‚àÇŒæ(l) = Œ£(l) L+1 Y m=l+1 (W (m)T Œ£(m)) ‚àÇŒ† ‚àÇŒæ(L+1) . (3.1) For any matrix, A, let œÑ(A) denote the largest singular value. Then we can bound | ‚àÇŒ† ‚àÇŒæ(l) | by | ‚àÇŒ† ‚àÇŒæ(l) | ‚â§œÑ(Œ£(l)) L+1 Y m=l+1 (œÑ(W (m))œÑ(Œ£(m)))| ‚àÇŒ† ‚àÇŒæ(L+1) |. (3.2) Recall that Œ£(m) ‚â°diag[œÉ‚Ä≤(Œæ(m) 1 ), ¬∑ ¬∑ ¬∑ , œÉ‚Ä≤(Œæ(m) Hl )], where œÉ‚Ä≤ denotes the derivative of œÉ with respect to its argument. For ReLU its value is either 0 or 1. Therefore œÑ(Œ£(m))) = 1. Also, for stability we would want œÑ(W (m)) < 1. Otherwise the output of the network can become unbounded. In practise this is enforced by the regularization term. Using this in the equation above we have | ‚àÇŒ† ‚àÇŒæ(l) | ‚â§ L+1 Y m=l+1 (œÑ(W (m)))| ‚àÇŒ† ‚àÇŒæ(L+1) |, (3.3) where each term in the product is a scalar less than 1. As the number of terms increases, that is L ‚àíl ‚â´1, this product can, and does, become very small. This typically happens when 27 L ‚àíl ‚âà20, in which case | ‚àÇŒ† ‚àÇŒæ(l) |, and therefore | ‚àÇŒ† ‚àÇW (l) |, become very small. This issue is called the problem of vanishing gradients. It manifests itself in deep networks where the weights in the inner layers (say L ‚àíl > 20) do not contribute to the network. In [8], the authors demonstrate that taking a deeper network can actually lead to an increase in training and validation error (see Figure 3.1). Thus, beyond a certain point, increasing the depth of a network can be counterproductive. Based on our previous discussion on vanishing gradients we know why this is the case. Given this, we would like to come up with a network architecture that addresses the problem of vanishing gradients by ensuring ‚àÇŒ† ‚àÇŒæ(L+1) ‚âà ‚àÇŒ† ‚àÇŒæ(1) . This means requiring that when the weights of the network approach small values, the network should approach the identity mapping, and not the null mapping. This is the core idea behind a ResNet architecture. Figure 3.1: Training error (left) and test error (right) on CIFAR-10 data with ‚Äúplain" deep networks (taken from [8]). 3.2 ResNets Figure 3.2: ResNet of depth 6 with skip connections. Consider an MLP with depth 6 (as shown in Figure 3.2) with a Ô¨Åxed width H for each hidden layer. We add skip connections between the hidden layers in the following manner x(l) i = œÉ(W (l) ij x(l‚àí1) j + b(l) i ) + x(l‚àí1) i , 2 ‚â§l ‚â§L. (3.4) We can make the following observations: 1. If all weights (and biases) were null, then x(5) = x(1), which in turn would imply ‚àÇŒ† ‚àÇx(1) = ‚àÇŒ† ‚àÇx(5) , 28 P A(1) s A(l+1) s s W(1) W(l+1) S(L+1) ùíô(ùüé) ùùÉ(ùüè) ùíô(ùüè) ùíô(ùíç) ùíô(ùíç&ùüè) ùíô(ùë≥&ùüè) ùùÉ(ùíç) ùùÉ(ùíç&ùüè) ùùÉ(ùë≥&ùüè) ùùèùö∑ ùùèùùÉ(ùüè) ùùèùö∑ ùùèùíô(ùüé) ùùèùö∑ ùùèùíô(ùíç) ùùèùö∑ ùùèùíô(ùíç&ùüè) ùùèùö∑ ùùèùíô(ùë≥&ùüè) ùùèùö∑ ùùèùùÉ(ùíç) ùùèùö∑ ùùèùùÉ(ùíç&ùüè) ùùèùö∑ ùùèùùÉ(ùë≥&ùüè) ùùèùö∑ ùùèùíô(ùüè) S(1) S(l) S(l+1) I I I I I I s Figure 3.3: Computational graph for forward and backpropagation in a Resnet. i.e., we will not have the issue of vanishing gradients. 2. The computational graph for forward and back-propagation of a ResNet is shown in Figure 3.3. Looking at this graph, it is clear that the expression for ‚àÇx(l+1) ‚àÇx(l) now involves traversing two branches and adding their sum. Therefore, we have ‚àÇŒ† ‚àÇŒæ(l) = Œ£(l) L+1 Y m=l+1 (I + W (m)T Œ£(m)) ‚àÇŒ† ‚àÇŒæ(L+1) . (3.5) Now, if we assume that |W (m)| ‚â™1 via regularization, we have ‚àÇŒ† ‚àÇŒæ(l) = Œ£(l) I + L+1 X m=l+1 W (m) T Œ£(m) + higher order terms  ‚àÇŒ† ‚àÇŒæ(L+1) . (3.6) In the expression above, even if the individual matrices have small entries, their sum need not approach a zero matrix. This implies that we can create a Ô¨Ånite (and signiÔ¨Åcant) change between the gradients near the input and output layers, while still requiring the weights to be small (via regularization). Remark 3.2.1. The above analysis can be extended to cases when H is not Ô¨Åxed, but the analysis is not as clean. See [8] on how we can do this. 3.3 Connections with ODEs Let us Ô¨Årst consider the special case of a ResNet with d = D = H. Recall the relation (3.4), which we can rewrite as x(l) ‚àíx(l‚àí1) ‚àÜt = 1 ‚àÜtœÉ(W (l)x(l‚àí1) + b(l)) = 1 ‚àÜtœÉ(Œæ(l)) (3.7) 29 for some scalar ‚àÜt, where we note that Œæ(l) is a function of x(l‚àí1) parameterized by Œ∏(l) = [W (l), b(l)]. Thus, we can further rewrite (3.7) as x(l) ‚àíx(l‚àí1) ‚àÜt = V (x(l‚àí1); Œ∏(l)). (3.8) Now consider a Ô¨Årst-order system of (possibly non-linear) ODEs, where given x(0) and Àôx ‚â°dx dt = V (x, t) (3.9) we want to Ô¨Ånd x(T). In order to solve this numerically, we can uniformly divide the temporal domain with a time-step ‚àÜt and temporal nodes t(l) = l‚àÜt, 0 ‚â§l ‚â§L + 1, where (L + 1)‚àÜt = T. DeÔ¨Åne the discrete solution as x(l) = x(l‚àÜt). Then, given x(l‚àí1), we can use a time-integrator to approximate the solution x(l). We can consider a method motivated by the forward Euler integrator, where the the LHS of (3.9) is approximated by LHS ‚âàx(l) ‚àíx(l‚àí1) ‚àÜt . while the RHS is approximated using a parameter Œ∏(l) as RHS ‚âàV (x(l‚àí1); t(l)) = V (x(l‚àí1); Œ∏(l)). where we are allowing the parameters to be diÔ¨Äerent at each time-step. Putting these two together, we get exactly the relation of the ResNet given in (3.8). In other words, a ResNet is nothing but a descritization of a non-linear system of ODEs. We make some comments to further strengthen this connection. ‚Ä¢ In a fully trained ResNet we are given x(0) and the weights of a network, and we predict x(L+1). ‚Ä¢ In a system of ODEs, we are given x(0) and V (x, t), and we predict x(T). ‚Ä¢ Training the ResNet means determining the parameters Œ∏ of the network so that x(L+1) is as close as possible to yi when x(0) = xi, for i = 1, ¬∑ ¬∑ ¬∑ , Ntrain. ‚Ä¢ When viewed from the analogous ODE point of view, training means determining the right hand side V (x, t) by requiring x(T) to be as close as possible to yi when x(0) = xi, for i = 1, ¬∑ ¬∑ ¬∑ , Ntrain. ‚Ä¢ In a ResNet we are looking for "one" V (x, t) that will map xi to yi, for all 1 ‚â§i ‚â§Ntrain. 3.4 Neural ODEs Motivated by the connection between ResNets and ODEs, neural ODEs were proposed in [4]. Consider a system of ODEs given by dx dt = V (x, t) (3.10) Given x(0), we wish to Ô¨Ånd x(T). In [4], the RHS, i.e., V (x, t), is deÔ¨Åned using a feed-forward neural network with parameters Œ∏ (see Figure 3.4). The input to the network is (x, t) while the output is V (x, t) (having the same dimension as x). With this description, the system (3.10) is solved using a suitable time-marching scheme, such as forward Euler, Runge-Kutta, etc. 30 ùë•! ùë•" ùë•#$! ùë° MLP ùë£! ùë£" ùë£#$! Figure 3.4: Feed-forward neural network used to model the right hand side in a Neural ODE. The number of dependent variables = d ‚àí1. Figure 3.5: Analogy between regression problems and Neural ODEs. So how do we use this network to solve a regression problem? Assume that you are given the labelled training data S = {(xi, yi) : 1 ‚â§i ‚â§Ntrain}. Here both xi and yi are assumed to have the same dimension d ‚àí1. The key idea is to think of xi as points in the d ‚àí1-dimensional space that represent the initial state of the system, and to think of yi as points that represent the Ô¨Ånal state. Then the regression problem becomes Ô¨Ånding the RHS of (3.10) that will map the initial points to the Ô¨Ånal points with minimal amount of error. In other words, Ô¨Ånd the parameters Œ∏ such that Œ†(Œ∏) = 1 N N X i=1 |xi(T; Œ∏) ‚àíyi|2 is minimized. Here, xi(T; Œ∏) denotes the solution (at time t = T) to (3.10) with x(0) = xi and the RHS represented by a feed-forward neural network V (x, t; Œ∏). Note that yi is the output value that is measured. There is a relatively straightforward way of extending this approach to the case when xi and yi have diÔ¨Äerent dimensions. In summary, in Neural ODEs one transforms a regression problem to one of Ô¨Ånding the nonlinear, time-dependent RHS of a system of ODEs. Let us list the advantages and diÔ¨Äerences when comparing Neural ODEs to ResNets: ‚Ä¢ If we interpret the number of time-steps in the Neural ODE as the number of hidden 31 layers L in a ResNet, then the computational cost for both methods is O(L). This is the cost associated with performing one forward propagation and one backward propagation. However the memory cost (the cost associated with storing the weights of each layer), is diÔ¨Äerent. For the neural ODE all the weights are associated with the feed-forward network used to represent the function V (x, t; Œ∏). Thus the number of weights are independent of the number of time-steps used to solve the ODE. On the other hand, for a ResNet the number of weights increases linearly with the number of layers, therefore the cost of storing them scales as O(L). ‚Ä¢ In Neural ODEs, we can take the limit ‚àÜt ‚Üí0 and study the convergence, since this will not change the size of the network used to represent the RHS. However, this is not computationally feasible to do for ResNets, where ‚àÜt ‚Üí0 corresponds to the network depth L ‚Üí‚àû! ‚Ä¢ ResNet uses a forward Euler type method, but in a Neural ODE one can use any time- integrator. Especially, other higher-order explicit time-integrator like the Runge-Kutta methods that converge to the ‚Äúexact‚Äù solution at a faster rate. 32 Chapter 4 Solving PDEs with MLPs A number of numerical methods exist to solve PDEs. Some of these are: ‚Ä¢ Finite diÔ¨Äerence methods ‚Ä¢ Finite volume methods ‚Ä¢ Finite element methods ‚Ä¢ Spectral Galerkin and collocation methods ‚Ä¢ Deep neural networks! To better appreciate some of these methods, especially deep neural networks, let us consider a simple model problem describing the scalar advection-diÔ¨Äusion problem in one-dimension: Find Ô¨Ånd u(x) in the interval x ‚àà(0, l) such that adu dx ‚àíŒ∫d2u dx2 = f(x), x ‚àà(0, ‚Ñì) u(0) = 0 u(‚Ñì) = 1 (4.1) where a denotes the advective velocity, Œ∫ is the diÔ¨Äusion coeÔ¨Écient while f(x) is the source. Such equations are used to model many physical phenomena, such as the transport of pollutant by Ô¨Çuids, or modelling the Ô¨Çow of electrons through semiconductors. The multi-dimensional version of this problem will take the form a ¬∑ ‚àáu(s) ‚àíŒ∫‚àÜu(s) = f(s), s ‚àà‚Ñ¶ u(s) = g(s), s ‚àà‚àÇ‚Ñ¶ (4.2) Note that the model problem is a linear PDE (ODE in the one-dimensional case). Replacing the velocity a by u leads to the viscous Burgers equation. The solution to (4.1) for f ‚â°0 can be analytically written as u(x) = 1 ‚àíexp(ax/Œ∫) 1 ‚àíexp(a‚Ñì/Œ∫) where a‚Ñì/Œ∫ is known as the Peclet number (Pe) and measures the ratio of the strength of advection to the strength of diÔ¨Äusion. We plot the solution for varying values of a and Œ∫ in Figure 4.1. Note that for small Pe, the solution is essentially a straight line. But as Pe increases, the solution starts to bend and forming a steeper boundary layer near the right boundary. The thickness of this boundary layer is given by Œ¥ ‚âàPe √ó l. We will now consider a few methods to numerically solve this toy problem. 33 (a) Fixed Œ∫ (b) Fixed a Figure 4.1: Exact solution of (4.1) with ‚Ñì= 1. 4.1 Finite diÔ¨Äerence method The key steps of a Ô¨Ånite diÔ¨Äerence scheme are as follows: 1. Discretize the domain into a grid of points, with the goal being to Ô¨Ånd the solution at these points. 2. Approximate the derivates with Ô¨Ånite diÔ¨Äerence approximations at these points. This leads to a system of (linear or non-linear) algebraic equations. 3. Solve this system using a suitable algorithm to Ô¨Ånd the solution. Applying these steps to (4.1) leads to: 1. Discretize the domain into N + 1 points, with xi = ih, 0 ‚â§i ‚â§N where h = ‚Ñì/N. We wish to solve for u(xi) = ui. We also know from the boundary conditions that u0 = 0 and uN = 1. 2. Use the approximations du dx(xi) = ui+1 ‚àíui‚àí1 2h + O(h2) d2u dx2 (xi) = ui+1 ‚àí2ui + ui‚àí1 h2 + O(h2) Note that both the approximation used above are second order accurate. They are ‚Äúcentral diÔ¨Äerence‚Äù approximations, as they weigh points on either side of the i-th point with the same magnitude. It is worth mentioning that in the limit of large Peclet number, a central diÔ¨Äerence approximation of the advective term is not ideal since it leads to numerical instability. In such a case, an ‚Äúupwind‚Äù approximation is preferred. Applying 34 the approximations to the PDE at xi, 1 ‚â§i ‚â§N ‚àí1 aui+1 ‚àíui‚àí1 2h ‚àíŒ∫ui+1 ‚àí2ui + ui‚àí1 h2 = fi ‚áê‚áíui+1  a 2h ‚àíŒ∫ h2  | {z } Œ≥ +ui 2Œ∫ h2  | {z } Œ≤ +ui‚àí1  ‚àía 2h ‚àíŒ∫ h2  | {z } Œ± = fi Looking at each node where the solution is unknown (recall that u0 = 0 and uN = 1 are known), Œ≤u1 + Œ≥u2 = ‚àíŒ±u0 + f1 Œ±ui‚àí1 + Œ≤ui + Œ≥ui+1 = fi, ‚àÄ2 ‚â§i ‚â§N ‚àí2 Œ±uN‚àí2 + Œ≤uN‚àí1 = ‚àíŒ≥uN + fN‚àí1 (4.3) Combining all the N ‚àí1 equations in (4.3), we get the following linear system Ku = f (4.4) where the tridiagonal matrix K and the other vectors in (4.4) are deÔ¨Åned as K = Ô£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞ Œ≤ Œ≥ 0 Œ± ... ... ... ... Œ≥ 0 Œ± Œ≤ Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª ‚ààR(N‚àí1)√ó(N‚àí1), u =  u1 u2 ¬∑ ¬∑ ¬∑ uN‚àí2 uN‚àí1 ‚ä§‚ààRN‚àí1, f =  ‚àíŒ±u0 + f1 f2 f3 ¬∑ ¬∑ ¬∑ fN‚àí2 fN‚àí1 ‚àíŒ≥uN + fN‚àí1 ‚ä§‚ààRN‚àí1 3. Solve u = K‚àí1f. Note that: ‚Ä¢ In practice, we never actually invert K as it is computationally expensive. We instead use smart numerical algorithms to solve the system (4.4). For instance, one can use the Thomas tridiagonal algorithm for this particular system, which is a simpliÔ¨Åed version of Gaussian elimination. ‚Ä¢ We only obtain an approximation ui ‚âàu(xi). To reduce the approximation error, we could reduce the mesh size h. Alternatively, we could use higher-order Ô¨Ånite diÔ¨Äerence approximations which would lead to a "wider stencil" to approximate the derivates at each point. ‚Ä¢ We can think of each point where we ‚Äúapply‚Äù the PDE as a collocation point. This idea of applying the PDE at collocation points is shared by the next method we consider. It is also shared by the method with uses MLPs to solve PDEs. 4.2 Spectral collocation method Spectral collocation methods seek a solution written as an expansion in terms of a set of smooth and global basis functions. The basis functions are chosen a priori, whereas the coeÔ¨Écients of the expansion are unknowns, and are computed by requiring that the numerical solution of the PDE is exact at a set of so-called collocation points. More speciÔ¨Åcally, this approach involves the following steps. 35 1. Select a set of global basis functions with the following properties: (a) It forms complete basis in the space of functions being considered. (b) Is smooth enough so that derivatives can be evaluated. (c) Easy to evaluate. (d) Derivatives that are easy to evaluate. For instance, one can use the Chebyshev polynomials deÔ¨Åned on Œæ ‚àà(‚àí1, 1), given by the following recurrence relation T0(Œæ) = 1, T1(Œæ) = Œæ, Tn+1(Œæ) = 2ŒæTn(Œæ) ‚àíTn‚àí1(Œæ) The Ô¨Årst few Chebyshev polynomials are shown in Figure 4.2. Note that this basis satisÔ¨Åes all the required properties listed above. It is easy to evaluate at any point because one can use the recurrence relation above and the values of the two lower-order polynomials to evaluate the Chebyshev polynomial of the subsequent order. One can also take derivatives of the recurrence relation above to evaluate a recurrence relation for derivatives of all orders. Figure 4.2: First few Chebyshev polynomials. 2. Write the solution as a linear combination of the basis functions {œÜn(x)}N n=0 u(x) = N X n=0 unœÜn(x) (4.5) where un are the basis coeÔ¨Écients. For our toy problem (4.1) (assuming ‚Ñì= 1), we will use the Chebyshev polynomials œÜn(x) = Tn(2x ‚àí1), where the argument is transformed to use these functions on the interval (0, 1). 3. Evaluate the derivates for the PDE, which for our toy problem will be du dx(x) = N X n=0 unœÜ‚Ä≤ n(x) = N X n=0 un2T ‚Ä≤ n(2x ‚àí1) d2u dx2 (x) = N X n=0 unœÜ‚Ä≤‚Ä≤ n(x) = N X n=0 un4T ‚Ä≤‚Ä≤ n(2x ‚àí1) (4.6) 36 4. Use the boundary conditions of the PDE. For the speciÔ¨Åc case of (4.1), u(0) = 0 =‚áí N X n=0 unœÜn(0) = N X n=0 unTn(‚àí1) = 0, u(1) = 1 =‚áí N X n=0 unœÜn(1) = N X n=0 unTn(1) = 1 (4.7) which leads to 2 linear equations for N + 1 coeÔ¨Écients. We then consider a set of (suitably chosen) nodes xi, 1 ‚â§i ‚â§N ‚àí1 in the interior of the domain, i.e. the collocation points, and use the derivatives found in step 3. in the PDE evaluated at these N ‚àí1 nodes a N X n=0 unœÜ‚Ä≤ n(xi) ‚àíŒ∫ N X n=0 unœÜ‚Ä≤‚Ä≤ n(xi) = f(xi) =‚áí N X n=0 un  2aT ‚Ä≤ n(2xi ‚àí1) ‚àí4Œ∫T ‚Ä≤‚Ä≤ n(2xi ‚àí1)  = f(xi) (4.8) This leads to an additional N ‚àí1 equations for the N + 1 coeÔ¨Écients. Combining (4.7) and (4.8) leads to the following linear system Ku = f (4.9) where K ‚ààR(N+1)√ó(N+1), u =  u0 u2 ¬∑ ¬∑ ¬∑ uN‚àí1 uN ‚ä§‚ààRN+1, f =  0 f(x1) f(x1) ¬∑ ¬∑ ¬∑ f(xN‚àí2) f(xN‚àí1) 1 ‚ä§‚ààRN+1 5. Solve u = K‚àí1f. We need to choose the collocation/quadrature points xi properly, so that K has desirable properties that make the linear system (4.9) easier to solve. These include invertibility, positive- deÔ¨Åniteness, sparseness, etc. Remark 4.2.1. The method is called a collocation method as the PDE is evaluated at the speciÔ¨Åc collocation/quadrature points xi. Remark 4.2.2. When working with a non-linear PDE, we will end up with a non-linear systems of algebraic equations for the coeÔ¨Écients u0, ..., uN, which is typically solved by Newton‚Äôs method. Let us look at a least-square variant for Ô¨Ånding the coeÔ¨Écients of the expansion of the spectral methods. As done earlier, we still represent the solution using (4.5) and compute its derivates. Then, the coeÔ¨Écients u are found by minimizing the following loss function Œ†(u) = Œ†int(u) + ŒªŒ†bc(u) Œ†bc(u) = N X n=0 unœÜn(0) ‚àí0 2 + N X n=0 unœÜn(1) ‚àí1 2 Œ†int(u) = 1 Ntrain Ntrain X i=1 a N X n=0 unœÜ‚Ä≤ n(xi) ‚àíŒ∫ N X n=0 unœÜ‚Ä≤‚Ä≤ n(xi) ‚àíf(xi) 2 (4.10) 37 This can be solved using any of the gradient-based methods we have seen in Chapter 2. This approach is especially useful when treating non-linear PDEs. In fact, in those cases it is not be possible to write a linear system in terms of the coeÔ¨Écients such as (4.9). A few things to note here ‚Ä¢ Œª is a parameter used to scale the interior loss and boundary loss diÔ¨Äerently. ‚Ä¢ The number of interior point xi can be chosen independently of the number of basis functions. In other words, Ntrain does not have to be the same as N. ‚Ä¢ We will see in the next section how this variant of the spectral method is very similar to how deep neural networks are used to solve PDEs. 4.3 Physics-informed neural networks (PINNs) The idea of using neural networks to solve partial diÔ¨Äerential equations was introduced in 1990- 2000s by Lagaris et al. [11]. With the renewed interest in using machine learning tools in solving PDEs, this idea was rediscovered in 2019 by Raissi et al. [24], and was given the term PINNs (physics-informed neural networks). The basic idea of PINNs is similar to regression, except that the loss function Œ†(Œ∏) contains derivate operators arising in the PDE being considered. We outline the main steps below for a one-dimensional scalar PDE, which can easily be extended to multi-dimensional systems of PDEs. We recommend that the reader thinks about the similarities and diÔ¨Äerences between this method and spectral collocation method described in the previous section. 1. Select a neural network as a function representation of the PDE solution: u = F(x; Œ∏). (4.11) Some crucial properties required by this representation are: (a) Do we have completeness with the representation, i.e., can we accurately approximate the necessary class of function using the representation? The answer is yes, because of the universal approximation theorems of neural networks (see Section 2.3.1). (b) Is the representation smooth? The answer is yes if the activation function is smooth, such as tanh, sin, etc. Note that we cannot use ReLU since it does not enough number of smooth derivatives. (c) Is it easy to evaluate? The answer is yes, due to a quick forward propagation pass. (d) Is it easy to evaluate derivates? The answer is yes, due to back-propagation. This will be discussed in detail below. 2. Given the representation (4.11), we need to Ô¨Ånd Œ∏ such that the PDE is satisÔ¨Åed in some suitable form. Compare this with spectral collocation approximation given by (4.5), where we need to determine the coeÔ¨Écients un. Note that while the dependence on the coeÔ¨Écients un in (4.5) is linear, the dependence on Œ∏ in (4.11) can be highly non-linear. 3. Next we want to Ô¨Ånd the derivatives of the representation. Consider the computational graph of the network as shown in Figure 4.3. It comprises alternate steps of aÔ¨Éne transformations and component-wise nonlinear transformation. The derivative of the output with respect to the input can be evaluated by back-propagation. The graph in Figure 4.3 is obtained by 38 simply setting Œ† = x(L+1) in the graph shown in Figure 2.8. Further, once we recognize that ‚àÇx(L+1) ‚àÇx(L+1) = 1, the identity matrix, we can easily read from this graph that ‚àÇx(L+1) ‚àÇx(0) = W (L+1)S(L+1)W (L)S(L) ¬∑ ¬∑ ¬∑ W (2)S(2)W (1)S(1) Hence, the evaluation of du dx requires the extention of the original graph with a backward branch used to evaluate the derivative of the activation function for each component of the vectors Œæ(l) (see Figure 4.3). The second derivative d2u dx2 is evaluated by performing back-propagation of the extended graph. To evaluate higher order derivatives, the graph will need to be extended further in a similar manner. This is what happens behind the scenes in Pytorch when a call to "autograd" is made. A(1) s A(l+1) s s W(1) W(l+1) S(L+1) ùíô(ùüé) ùùÉ(ùüè) ùíô(ùüè) ùíô(ùíç) ùíô(ùíç&ùüè) ùíô(ùë≥&ùüè) ùùÉ(ùíç) ùùÉ(ùíç&ùüè) ùùÉ(ùë≥&ùüè) ùùèùíô(ùë≥#ùüè) ùùèùùÉ(ùüé) ùùèùíô(ùë≥#ùüè) ùùèùíô(ùüé) ùùèùíô(ùë≥#ùüè) ùùèùíô(ùíç) ùùèùíô(ùë≥#ùüè) ùùèùíô(ùíç#ùüè) ùùèùíô(ùë≥#ùüè) ùùèùíô(ùë≥#ùüè) = 1 ùùèùíô(ùë≥#ùüè) ùùèùùÉ(ùíç) ùùèùíô(ùë≥#ùüè) ùùèùùÉ(ùíç#ùüè) ùùèùíô(ùë≥#ùüè) ùùèùùÉ(ùë≥#ùüè) ùùèùíô(ùë≥#ùüè) ùùèùíô(ùüè) S(1) S(l) S(l+1) s Figure 4.3: Extended graph to evaluate derivatives with respect to network input. 4. Insert the functional representation of the solution (4.11) into the PDE to Ô¨Ånd the parame- ters Œ∏. To do this, we Ô¨Årst deÔ¨Åne a set of points S = {xi : 1 ‚â§i ‚â§Ntrain} used to train the network, analogous to the set of collocation points in the spectral collocation methods. Thereafter, we need to deÔ¨Åne the loss function (specialized to our toy problem (4.1)) Œ†(Œ∏) = Œ†int(Œ∏) + ŒªbŒ†b(Œ∏), Œ†b(Œ∏) = (F(0; Œ∏) ‚àí0)2 + (F(1; Œ∏) ‚àí1)2 , Œ†int(Œ∏) = 1 Ntrain Ntrain X i=1  aF‚Ä≤(xi; Œ∏) ‚àíŒ∫F‚Ä≤‚Ä≤(xi; Œ∏)n ‚àíf(xi) 2 . (4.12) After training the network, i.e. solving the minimization problem Œ∏‚àó= arg min Œ∏ Œ†(Œ∏), the solution writes u‚àó(x) = F(x; Œ∏‚àó). Note that this is exactly what is done for the least squares variant of the spectral collocation method, where the coeÔ¨Écients un are solved by minimizing a similar loss. We make a few remarks: ‚Ä¢ When we are able to Ô¨Ånd Œ∏‚àófor which Œ†(Œ∏‚àó) = 0, this implies Œ†int(Œ∏‚àó) = 0 and Œ†b(Œ∏‚àó) = 0. In other words, the PDE residuals are zero at the collocation points. This will lead to a good solution as long as the collocation points cover the domain well. ‚Ä¢ There are various ways to improve the accuracy of PINNs, such as 39 ‚Äì Increasing the number of collocation points. ‚Äì Changing the hyper-parameter Œªb weighting the boundary loss. ‚Äì Increasing the size of the network. That is, increasing NŒ∏. ‚Ä¢ The boundary conditions (BCs) of a diÔ¨Äerential equation carry fundamental physical properties of the phenomena we are trying to describe, and it is paramount that those are satisÔ¨Åed by our numerical solution. In the framework of PINNs, BCs are enforced as a soft constrained via the penalization term Œ†b(Œ∏). Hence, the hyper-parameter Œªb plays a crucial role in the training of the network, as it balances the interplay between the two loss terms during the minimization process. If the gradients of the diÔ¨Äerent loss terms are not adequately scaled, the convergence to a solution that satisÔ¨Åes both the BCs and the PDE itself can be extremely slow. This is particularly exacerbated for stiÔ¨ÄPDEs. To address this issue, diÔ¨Äerent self-adaptive techniques to tune the value of Œªb along the training have been proposed [29, 16, 3]. 4.4 Extending PINNs to a more general PDE Consider a general PDE: Find the solution u : ‚Ñ¶‚äÇRd ‚ÜíRD such that L(u(x)) = f(x), x ‚àà‚Ñ¶ B(u(x)) = g(x), x ‚àà‚àÇ‚Ñ¶ (4.13) where L is the diÔ¨Äerential operator, f is the known forcing term, B is the boundary operator, and g is the non-homogeneous part of boundary condition (also prescribed). As an example, we can consider the three-dimensional incompressible Navier-Stokes equation solving for the velocity Ô¨Åeld v = [v1, v2, v3] and pressure p on ‚Ñ¶= ‚Ñ¶S √ó [0, T]. Here ‚Ñ¶S is the three dimensions spatial domain and [0, T] is the time interval of interest. The equation is given by ‚àÇv ‚àÇt + v ¬∑ ‚àáv + ‚àáp ‚àí¬µ‚àÜv = f, ‚àÄ(s, t) ‚àà‚Ñ¶ ‚àá¬∑ u = 0, ‚àÄ(s, t) ‚àà‚Ñ¶ v = 0, ‚àÄ(s, t) ‚àà‚àÇ‚Ñ¶S √ó [0, T] v(s, 0) = v0(s), ‚àÄs ‚àà‚Ñ¶S. (4.14) The Ô¨Årst equation above is the balance of linear moment. The second equation enforces the conservation of mass. The third equation is the no-slip boundary condition which is used when the boundary is rigid and Ô¨Åxed. The fourth equation is the prescription of the initial velocity Ô¨Åeld. To design a PINN for (4.13), the input to the network should be the independent variables x and the output should be the solution vector u. For the speciÔ¨Åc case of the Navier-Stokes system (4.14), the input to the network would be [s1, s2, s3, t] ‚ààR4, while the output vector would be u = [v1, v2, v3, p] ‚ààR4. The steps would be the following: 1. Construct the loss functions ‚Ä¢ DeÔ¨Åne the interior residual R(u) = L(u) ‚àíf. ‚Ä¢ DeÔ¨Åne the boundary residual Rb(u) = B(u) ‚àíg. ‚Ä¢ Select suitable Nv collocation points in the interior of the domain and Nb points on the domain boundary to evaluate the residuals. These could be chosen as based on quadrature rules, such as Gaussian, Lobatto, Uniform, Random, etc. 40 Then the loss function is Œ†(Œ∏) = Œ†int(Œ∏) + ŒªbŒ†b(Œ∏) Œ†int(Œ∏) = 1 Nv Nv X i=1 |R(F(xi; Œ∏)|2 Œ†b(Œ∏) = 1 Nb Nb X i=1 |Rb(F(xi; Œ∏)|2 2. Train the network: Ô¨Ånd Œ∏‚àó= arg min Œ∏ Œ†(Œ∏), and set the solution as u‚àó= F(x; Œ∏‚àó) We make some remarks here: ‚Ä¢ It is implicitly assumed that a weight regularization term is also added to the loss Œ†(Œ∏). ‚Ä¢ Is u‚àó(x) the exact solution to the PDE? The answer is No! ‚Äì Firstly, Œ†(Œ∏‚àó) may not be zero. ‚Äì Even if Œ†(Œ∏‚àó) is identically zero, it only means that the residuals vanishes at the collocation points. However, that does not guarantee that the residuals will vanish everywhere in the domain. For that Nv, Nb ‚Üí‚àû. ‚Äì Also, with a Ô¨Åxed network (NŒ∏ Ô¨Åxed) we cannot represent all functions. For that, we will need NŒ∏ ‚Üí‚àû. ‚Ä¢ In practice, we only compute Œ†(Œ∏‚àó). Is the solution error ‚à•e‚à•= ‚à•u‚àó‚àíu‚à•related to this loss value? And if it is, can we say that this error will be small as long as the loss is small? This is what we try to answer in next section. 4.5 Error analysis for PINNs In order to evaluate the error e = u‚àó‚àíu, we need to know the exact solution u which is not available in general. We consider a way of overcoming this issue, by restricting our discussion to linear PDEs i.e., L and B are linear operators. Note that if u is the exact solution, then L(e) = L(u‚àó‚àíu) = L(u‚àó) ‚àíL(u) = L(u‚àó) ‚àíf = R(u‚àó) (4.15) and B(e) = B(u‚àó‚àíu) = B(u‚àó) ‚àíB(u) = B(u‚àó) ‚àíg = Rb(u‚àó) (4.16) Thus, (4.15) (4.16) lead to a PDE for e driven by the residuals of the MLP solution, L(e) = R(u‚àó), in ‚Ñ¶ B(e) = Rb(u‚àó), on ‚Ñ¶ (4.17) If the residuals of u‚àówere zero, then e = 0. Unfortunately, these residuals are not zero. The most that we can say is that they are small at the collocation points. However, from the theory of stability of well-posed PDEs, we have ‚à•e‚à•L2(‚Ñ¶) ‚â§C1  ‚à•R(u‚àó)‚à•L2(‚Ñ¶) + ‚à•Rb(u‚àó)‚à•L2(‚àÇ‚Ñ¶)  (4.18) 41 where C1 is a stability constant that depends on the PDE, the domain ‚Ñ¶, etc. This is a condition that hols for all well-posed PDEs. It says that if the terms driving the PDE are small, then the solution to the PDE will also be small. This equation tells us that we can control the error if we can control the residuals for the MLP solution. However, in practise we know and control Œ†int, Œ†b and not ‚à•R(u‚àó)‚à•2 L2(‚Ñ¶), ‚à•Rb(u‚àó)‚à•2 L2(‚àÇ‚Ñ¶). The question then becomes, are these quantities related. This is answered in the analysis below, ‚à•R(u‚àó)‚à•L2(‚Ñ¶) = m‚Ñ¶Œ†int(Œ∏‚àó)1/2 + ‚à•R(u‚àó)‚à•L2(‚Ñ¶) ‚àím‚Ñ¶Œ†int(Œ∏‚àó)1/2 ‚â§m‚Ñ¶Œ†int(Œ∏‚àó)1/2 + ‚à•R(u‚àó)‚à•L2(‚Ñ¶) ‚àím‚Ñ¶Œ†int(Œ∏‚àó)1/2 ‚â§m‚Ñ¶Œ†int(Œ∏‚àó)1/2 + C2(Nv)‚àíŒ± (4.19) where m‚Ñ¶is the measure of the domain ‚Ñ¶, C2 and Œ± > 0 will depend on the type of quadrature points chosen. In the equation above the Ô¨Årst line is obtained by adding and subtracting the term m‚Ñ¶Œ†int(Œ∏‚àó)1/2. The second line is obtained by using the triangle inequality. The third line is a statement of error in approximating an integral with a Ô¨Ånite sum of values evaluated at quadrature points. Similarly for the boundary residual ‚à•Rb(u‚àó)‚à•L2(‚àÇ‚Ñ¶) ‚â§m‚àÇ‚Ñ¶Œ†b(Œ∏‚àó)1/2 + C3(Nb)‚àíŒ≤ (4.20) where m‚àÇ‚Ñ¶is the measure of ‚àÇ‚Ñ¶, C3 and Œ≤ > 0 will depend on the type of boundary quadrature points. Using (4.18), (4.19) and (4.20), we get ‚à•e‚à•L2(‚Ñ¶) ‚â§C1 Ô£´ Ô£¨ Ô£≠m‚Ñ¶Œ†int(Œ∏‚àó)1/2 + m‚àÇ‚Ñ¶Œ†b(Œ∏‚àó)1/2 | {z } reduced by NŒ∏‚Üë + C2(Nv)‚àíŒ± + C3(Nb)‚àíŒ≤ | {z } reduced by Nv,Nb‚Üë Ô£∂ Ô£∑ Ô£∏ (4.21) This equation tells us that it is possible to control the error in the PINNs solution by reducing the loss functions (by increasing NŒ∏) and by increasing the number of interior and boundary collocation points. For further details about this analysis, the reader is referred to [18]. 4.6 Data assimilation using PINNs The problem of data assimilation is often encountered in the science and engineering. In this problem, we are able to make a few sparse measurements of a quantity, and using these we wish to evaluate it everywhere on a Ô¨Åne grid. We are also given a physical principal (in the form of a PDE) that the variable of interest must adhere to. Let us assume that we are given a set of sparse measurements of some quantity u on the domain ‚Ñ¶ ui = u(xi), xi ‚àà‚Ñ¶, 1 ‚â§i ‚â§M Furthermore, we are given that u satisÔ¨Åes some constraint R(u) = 0 on ‚Ñ¶. Then, data assimilation corresponds to using this information to Ô¨Ånd the value of u at any x ‚àà‚Ñ¶. We can solve this problem using PINNs. First, we represent u using a neural network F(x, Œ∏). Next, we deÔ¨Åne a loss function Œ†(Œ∏) = ŒªI M M X i=1 (ui ‚àíF(xi, Œ∏))2 | {z } data matching + 1 Nv M+Nv X i=M+1 |R(F(xi, Œ∏))|2 | {z } physical constraint + Œª‚à•Œ∏‚à•2 | {z } smoothness 42 where xi, M + 1 ‚â§i ‚â§M + Nv are some collocation points chosen to evaluate the residual, while ŒªI, Œª are hyper-parameters. Then we train the network by Ô¨Ånding Œ∏‚àó= arg min Œ∏ Œ†(Œ∏), and set the PINNs solution as u‚àó= F(x; Œ∏‚àó). 43 Chapter 5 Convolutional Neural Networks In the previous chapters, we have seen how to construct neural networks using fully-connected layers. We will now look at a diÔ¨Äerent class of layers, called convolution layers, which are very useful when handling inputs which are images. These tasks include classifying images into categories, performing semantic segmentation on images, and transforming images from one type to another. 5.1 Functions and images Consider a function u(x) deÔ¨Åned on x ‚àà[a, b] √ó [c, d] ‚äÇR2. Then we can visualize the discretized version of this function as an image U ‚ààRN1√óN2, where U[i, j] = u(ih, jh), 1 ‚â§i ‚â§N1, 1 ‚â§j ‚â§N2, h = pixel size. (5.1) Note that the image in (5.1) deÔ¨Ånes a grayscale image where the value of u at each pixel is just the intensity. If we work with color images, then it would be a three-dimensional tensor, with the third dimension corresponding to the red, blue and green channels. In other words, U ‚ààRN1√óN2√ó3. If we want to use a fully-connected neural network (MLP) which takes as input a colored 2D image of size 100 √ó 100, then the input dimension after unravelling the entire image as a single vector would be 3 √ó 104, which is very large. This would in turn lead to very large connected layers which is not computationally feasible. Secondly, when unravelling the image, we lose all spatial context of the initial image. Finally, one would expect that local operations, such as edge detection, would be the same in any region of the image. Consider the weights for a fully connected layer. These would be represented by the matrix Wij, where the i index represent the output of the linear transform and the j index represents the input. If the operation was the same for every output index, we would apply the same operation for every i and therefore not need the matrix. To address all these issues, we can use the convolution operator on functions. 5.2 Convolutions of functions The convolution operator maps functions to functions. Consider the function u(x), x ‚ààRd, and a suÔ¨Éciently smooth kernel function g(x) which decays as |x| ‚Üí‚àû. Then the convolution operator is given by u(x) = Z Rd g(y ‚àíx)u(y)dy (5.2) 44 We can interpret the convolution operator as sampling u by varying x. For example, in 1D, let u(x) and g(x) be as shown in Figure 5.1, and u(x) = Z R g(y ‚àíx)u(y)dy. Consider a point x0. Then g(y ‚àíx0) shifts the kernel to the location x0 which will sample the function u in the orange shaded region. Similarly, for another point x1, g(y ‚àíx1) shifts the kernel to the location x1 which will sample the function u in the green shaded region. So as the kernel moves, it samples u in diÔ¨Äerent windows. Note that the same operation is applied regardless of the value of x. Lets now consider a few typical kernel functions. (a) Kernel g(x) (b) Sampling Figure 5.1: Sampling with shifted kernel in 1D. 5.2.1 Example 1 A popular choice is the Gaussian kernel g(Œæ) = œÅ(|Œæ|), where for any scalar r, œÅ(r) = exp(‚àír2/2œÉ2) p (2œÄœÉ2)d . which is used as a smoothing/blurring Ô¨Ålter. Here d is the dimension while œÉ denotes the spread. This kernel in 1D is shown in 5.1(a) for œÉ = 0.2. Some important properties of this kernel are: ‚Ä¢ It is isotropic, as it only depends on the magnitude of Œæ. ‚Ä¢ The integral over the whole domain is unity. ‚Ä¢ It is parameterized by œÉ, which represents a frequency cut-oÔ¨Ä, as scales Ô¨Åner than œÉ are Ô¨Åltered out by the convolution. This smoothing eÔ¨Äect is demonstrated in Figure 5.2 45 Figure 5.2: 1D convolution with Gaussian kernel. 5.2.2 Example 2 Let us consider another example of a kernel that would produce the derivative of a smooth version of u. In 2D, we want this to look like u(x) = ‚àÇ ‚àÇx1 Z ‚àû ‚àí‚àû Z ‚àû ‚àí‚àû œÅ(|y ‚àíx|)u(y)dy1dy2  = Z ‚àû ‚àí‚àû Z ‚àû ‚àí‚àû ‚àÇœÅ(|y ‚àíx|) ‚àÇx1 u(y)dy1dy2 = Z ‚àû ‚àí‚àû Z ‚àû ‚àí‚àû  ‚àí‚àÇœÅ(|y ‚àíx|) ‚àÇy1  | {z } required kernel u(y)dy1dy2 (5.3) This kernel is shown in both 1D and 2D in Figure 5.3. Note that the action of this kernel look like a smoothed Ô¨Ånite diÔ¨Äerence operation. That is the region to the left of the center of the kernel is weighted by a negative value and the region to the right is weighted by a positive value. 5.3 Discrete convolutions To evaluate the discrete convolution in 2D, consider (5.2) and discretize it using quadrature. Then, we will have U[i, j] = N1 X m=1 N2 X n=1 g[m ‚àíi, n ‚àíj]U[m, n] (5.4) where we have absorbed the measure h2 into the deÔ¨Ånition of the kernel. As in the continuous case, we will assume that g vanishes after a certain distance g[p, q] = 0, |p|, |q| > ¬ØN (measure of width of the kernel). 46 (a) 1D (b) 2D, with a derivative along the 1-direction Figure 5.3: Derivative kernel. The blue curve denotes the Gaussian kernel and the orange curve denotes the derivative. Thus, the limits of the sum are reduced by exclusing all the pixels over which the convolution will be zero, U[i, j] = i+ ¬Ø N X m=i‚àí¬Ø N j+ ¬Ø N X n=j‚àí¬Ø N g[m ‚àíi, n ‚àíj]U[m, n]. (5.5) Let m‚Ä≤ = m ‚àíi and n‚Ä≤ = n ‚àíj. Then U[i, j] = ¬Ø N X m‚Ä≤=‚àí¬Ø N ¬Ø N X n‚Ä≤=‚àí¬Ø N g[m‚Ä≤, n‚Ä≤]U[i + m‚Ä≤, j + n‚Ä≤]. (5.6) This is precisely how a convolution is applied in deep learning. Thus, the convolution is entirely determined by g[m, n], |m|, |n| ‚â§¬ØN, which become the weights of the convolution layer, with the number of weight being ( ¬ØN + 1)2. Let‚Äôs consider some examples: ‚Ä¢ A smoothing kernel would be 1 8 Ô£Æ Ô£∞ 1 4 1 1 4 1 3 1 1 4 1 1 4 Ô£π Ô£ª‚âàGaussian kernel with some œÉ ‚Ä¢ Kernels that lead to the derivative along the x-direction and y-direction are given by Ô£Æ Ô£∞ 0 0 0 ‚àí1 0 1 0 0 0 Ô£π Ô£ª and Ô£Æ Ô£∞ 0 1 0 0 0 0 0 ‚àí1 0 Ô£π Ô£ª ‚Ä¢ Similarly, the second derivatives anlong the x and y-directions are given by kernels of the form Ô£Æ Ô£∞ 0 0 0 ‚àí1 2 ‚àí1 0 0 0 Ô£π Ô£ª and Ô£Æ Ô£∞ 0 ‚àí1 0 0 2 0 0 ‚àí1 0 Ô£π Ô£ª 47 ‚Ä¢ While the Laplacian is given by the kernel of the type Ô£Æ Ô£∞ 0 ‚àí1 0 ‚àí1 4 ‚àí1 0 ‚àí1 0 Ô£π Ô£ª Remark 5.3.1. We can have diÔ¨Äerent ¬ØN in diÔ¨Äerent directions. That is, we can have kernels with diÔ¨Äerent widths along each direction. 5.4 Connection to Ô¨Ånite diÔ¨Äerences There is a very strong connection between the concept of convolution and the stencil of a Ô¨Ånite diÔ¨Äerence scheme. This is made clear in the discussion below. Say some function u(x1, x2) is represented on a Ô¨Ånite grid, where the grid points are indexed by (i, j) with a grid size h. Then we use the notation U[i, j] = u(xi 1, xj 2). Using Taylor series expansion about (i, j) U[i + 1, j] ‚àíU[i ‚àí1, j] 2h = u(xi+1 1 , xj 2) ‚àíu(xi‚àí1 1 , xj 2) 2h = ‚àÇ ‚àÇx1 u(xi 1, xj 2) + O(h2). The operation above is identical to the operation of a discrete convolution with weights given by Ô£Æ Ô£∞ 0 0 0 ‚àí1 0 1 0 0 0 Ô£π Ô£ª1 2h ‚âà‚àÇu ‚àÇx1 . Thus we may say that this convolution operation approximates a derivative along the 1-direction. Similarly, we can show that U[i + 1, j] ‚àí2U[i, j] + U[i ‚àí1, j] h2 = ‚àÇ ‚àÇx2 1 u(xi 1, xj 2) + O(h2). and thus the convolution with the kernel given by Ô£Æ Ô£∞ 0 0 0 1 ‚àí2 1 0 0 0 Ô£π Ô£ª1 h2 ‚âà‚àÇu ‚àÇx2 1 , approximates the computation of the second derivative along the 1-direction. 5.5 Convolution layers The key things to remember are: ‚Ä¢ Each convolution layer consists of several discrete convolutions, each with its own kernel. ‚Ä¢ The weights of the kernel, which determine its action (smoothing, Ô¨Årst derivative, second derivative etc.), are learnable parameters and are determined when training the network. Thus the way to think about the learning process is that the network learns the operations (convolutions) that are appropriate for its task. The task can be a classiÔ¨Åcation problem, for example. 48 Let us assume we have an N1√óN2 image as an input. Then, we will have multiple convolutions in a convolution layer, each of which will generate a diÔ¨Äerent image, as shown in Figure 5.4(a). The trainable parameters of this layer are the weights of each convolution kernel. Assuming the width of the kernels is ¬ØN in each direction, and there are P kernels, then the number of trainable weights will be P √ó (2 ¬ØN + 1)2. Next let us consider the size of the output image after applying a single kernel operation. Note that we will not be able to apply the kernel on the boundary pixels since there are no pixel-values available beyond the image boundary (see Figure 5.4(b)). Thus, we will have to skip ¬ØN pixels at each boundary when applying the kernel, leading to an output image of shape (N1 ‚àí¬ØN + 1) √ó (N2 ‚àí¬ØN + 1). One way to overcome this is by padding the image with ¬ØN pixels with zero value on each edge. Now we can apply the kernel on the boundary pixels and the output image will be the same size as the input image, as can be seen in Figure 5.4(c). Another feature of convolutions is known as the stride which determines the number of pixels by which the kernel is shifted as we move over the image. In the examples above, the stride was 1 in both directions. In practice, we can choose a stride > 1 which will further shrink the size of the output image. For instance, if stride was taken as S in each direction (with zero-padding applied), then the output image size would reduce by a factor of S in each direction (see Figure 5.4(d)). (a) Action of a convolution layer (b) Convolution without padding (c) Convolution with zero-padding (d) Convolution with zero-padding and stride 2 Figure 5.4: Action of a convolution layer/kernel. 49 5.5.1 Average and Max Pooling Pooling operations are generally used to reduce the size of an image, and allowing you to step through diÔ¨Äerent scales of the image. If applied on an image of size N √ó N over patches of size S √ó S, the new image will have dimensions N S √ó N S , where S is the stride of the pooling operation. This is shown in Figure 5.5 for S = 2. Note it is typical to select the patch of pixels over which the max or average is computed to be (S √ó S), where S is the stride. This is true for Figure 5.5 (b) but not for 5.5 (a). Also, we show in Figure 5.6 how pooling allows us to move through various scales of the image, where the image gets coarser as more pooling operations are applied. Note that pooling operations do not have any trainable parameters. The pooling operation has strong analog in similar operators that are used when designing multigrid preconditioners for solving linear systems of algebraic equations. (a) Stride 1 (b) Stride 2 Figure 5.5: Max pooling applied to an image over patches of size (2 √ó 2). (a) Original (b) After 1 pooling op. (c) After 2 pooling op. (d) After 3 pooling op. (e) After 4 pooling op. (f) After 5 pooling op. Figure 5.6: Max pooling applied repeatedly to an image over patches of size (2 √ó 2) with stride 2. 50 5.5.2 Convolution for inputs with multiple channels Assume that the input to a convolution layer is of size N1 √ó N2 √ó C, where C is the number of channels in the input image. Then a convolution layer apply P convolutions on this input and give an output of size M1 √ó M2 √ó P. Note that both the spatial resolution as well as the number of channels of the output image might be diÔ¨Äerent from the input image. Furthermore, if a single convolution in the layer uses a kernel of width k = 2 ¬ØN + 1, then the kernel will be of the shape k √ó k √ó C, i.e., the kernel will have k √ó k weights for each of the C input channels of the input image. Each convolution will act on the input to give an output image of shape M1 √ó M2 √ó 1. The output of each convolution are stacked together to give the Ô¨Ånal output of the convolution layer. This can be written as, ¬ØU[i, j, k] = ¬Ø N X m=‚àí¬Ø N ¬Ø N X n=‚àí¬Ø N C X c=1 gk[m, n, c]U[i + m, j + n, c], 1 ‚â§i ‚â§M1, 1 ‚â§j ‚â§M2, 1 ‚â§k ‚â§P, where gk is the kernel of the k-th convolution in the layer. Note that the total number of trainable parameters will be (2 ¬ØN + 1) √ó (2 ¬ØN + 1) √ó P √ó C. This is the type of convolutional layer most frequently encountered in a convolutional neural network, which is described in the following section. 5.6 Convolution Neural Network (CNN) Now let‚Äôs put all the elements together to form the full network. Consider an image classiÔ¨Åcation problem. Then the CNN will be given by y = F(x; Œ∏) where x ‚ààRN1√óN2√óN3 is the input image with N3 channels, while y ‚ààRC is the probability vector whose i-th component of denotes the probability that the input image belongs the i-th class among a total of C classes. The y are typically one-hot encoded. The possible architecture of this network is shown in Figure 5.7. This consists of a number of convolution layers followed by pooling layers, which will reduce the spatial resolution of the input image while increasing the number of channels. The output of the Ô¨Ånal pooling layer is Ô¨Çattened to form a vector, which is then passed though a number of fully connected layers with some activation function (say ReLU). The Ô¨Ånal fully connected layer reduced the size of the vector to C (which is taken to be 10 in the Figure), which is then passed through a softmax function to generate the predicted probability vector y. Since we are solving a classiÔ¨Åcation problem, the loss function is taken to be the cross-entropy function Œ†(Œ∏) = ‚àí Ntrain X i=1 C X c=1 h y(i) c log  Fc(x(i); Œ∏) i . We train the CNN by trying to Ô¨Ånd Œ∏‚àó= arg min Œ∏ Œ†(Œ∏) with the Ô¨Ånal network being y = F(x; Œ∏‚àó). We make some important remarks: 1. The convolution operation is also a linear operation on the input, as is the case for a fully connected layer. The only diÔ¨Äerence is that in a fully-connected layer, the weights connect every pixel in the output to every pixel of the input, while in a convolution layer the weights connect one pixel of the output only to a patch of pixels in the input. Furthermore, the same weights are applied on each patch of the input. 2. In the CNN shown in Figure 5.7, the convolution layers can be interpreted as encoding information about the input image, while the fully connected layers can be interpreted as 51 Figure 5.7: Example of a CNN architecture for an image classiÔ¨Åcation problem, for 10 classes. using this information to solve the classiÔ¨Åcation problem. This is why in the literature, convolution layers are said to perform feature selection. Further the part of the network leading up to the ‚ÄúÔ¨Çattened‚Äù vector is sometimes referred to as the encoder. 3. Sometime, activation is also applied to the output of the convolution layer along with a bias x(l+1)[i, j, k] = œÉ( X m,n,c gk[m, n, c]x(l)[i + m, j + n, c] + bk) where a single bias bk is used for a given output channel k. 4. In the example above we considered an image classiÔ¨Åcation problem. That is, the network was a transform from an image to a class label. We can think of other similar cases. For example, when the network is a transform from an image to a real number. This might have several useful applications in computational physics. Consider the case where you want to create an enstrophy calculator. That is a network that will take as input images of the velocity components of a Ô¨Çuid deÔ¨Åned on a grid, and produce as output the integral of the square of the vorticity (called the enstrophy) over the entire domain. Another example would be a network that takes as input the components of the displacement of an elastic solid and produces as output the total strain energy stored within the solid. 5. The architecture we have considered allows us to transform images to vectors, which is useful in problems involving image classiÔ¨Åcation, image analysis, etc. However, there is another architecture that does the opposite, i.e., maps vectors into images. This is useful in applications involving image synthesis. Finally, by putting these two architecture together, we can transform an image to a vector and back to another image. Such image-to-image transformations are useful in applications such as image semantic segmentation. These ideas are described in the following Sections. 6. It is worth taking a moment to analyze how convolution layers act on images and why they are so useful. When dealing with images input in the context of deep learning, a Ô¨Årst naive approach could be to Ô¨Çatten the image and feed it to a regular fully connected MLP. 52 However, this would lead to diÔ¨Äerent problems. In fact, for regular images, the size of the Ô¨Çatten input would be extremely large. In that case, we would have 2 possibilities when deÔ¨Åning the architecture of the network: (a) We can size the Ô¨Årst layers of the network to have a width comparable to the (large) dimension of the input. (b) We can have a sharp decrease in the width of the second hidden layer. Either of the strategies would lead to issues. In the Ô¨Årst case, the size of the network would be too large. Hence, there would be too many trainable parameters which would require an unrealistic amount of data to train the network. In the second case, the compression of information happening between the Ô¨Årst two layers would be too aggressive, making it very hard for the network to learn how to extract the right features across diÔ¨Äerent images. Moreover, important spatial relationship among pixels (like edges, shapes, etc.) are lost by Ô¨Çattening an image. Ideally we would like to leverage these relations as much as possible, since they carry important spatial information. Convolution layers can solve both issues. They allow the input to be a 2D image, while drastically decreasing the number of learnable parameters needed for the feature extraction task. In fact, kernels introduce a limited number of parameters compared to a classic fully connected layer. Since the same kernel is applied at diÔ¨Äerent pixel locations in an image, .i.e. parameter sharing, they utilize the computational resources in an eÔ¨Écient and smart manner. 5.7 Transpose convolution layers We have seen how convolution and pooling layers can be used to scale down images. We now consider layers that do the opposite, i.e, scale up images. To understand what this operation would look like, let us look at a few examples 1. Consider a 1D image of size 4 Input =  u1, u2, u3, u4  Consider a kernel of size 3 √ó 1 k =  x, y, z  Consider a convolution layer with the kernel k, stride 1 and zero-padding layer of size 1. Then, the output of the layer acting on the input is Output =  yu1 + zu2, xu1 + yu2 + zu3, xu2 + yu3 + zu4, xu3 + yu4  The steps involved in convolution operator are: pad, dot-product, stride. Note that using padding and stride 1 have ensured the output has the same size as the input. 2. Consider another convolution with the same kernel k, zero-padding layer 1 but stride 2. Then, the output of the layer acting on the same input as earlier is Output =  yu1 + zu2, xu2 + yu3 + zu4  Note that the size of the output has reduced by a factor of 2. In other words, increasing the stride has allowed us to downsample the input. The question we want to ask is whether we can perform an upsampling in a similar way? This can indeed be done by transposing every operation in a convolution layer. 53 ‚Ä¢ Instead of using a dot-product (inner-product), we will use an outer-product. ‚Ä¢ Instead of skipping pixels in the input (stride) we will skip pixels in the output. ‚Ä¢ Instead of padding, we will need to crop the output. 3. Let us now see an example of a transpose convolution layer. Consider a 1D input image of size 2 √ó 1 Input =  u1, u2  and a kernel of size 3 √ó 1 k =  x, y, z  . if we perform the outer-product of the input with k, we will get Outer product = u1x u1y u1z u2x u2y u2z  . If we use a stride of 2, we will need to shift the rows of the outer-product by 2 Striding = u1x u1y u1z 0 0 0 0 u2x u2y u2z  . After striding is performed we need to add the entries in each column and crop the vector to get the output Output = Crop(  u1x, u1y, u1z + u2x, u2y, u2z  ) =  u1x, u1y, u1z + u2x, u2y  where we have cropped out the last few elements (by convention) to get an output which has 2 times the size of the input. 4. We consider transpose convolution in 2D applied on a 2D image of size (2 √ó 2). The kernel is taken to be of shape (3 √ó 3) with stride 2 and padding (cropping). The action of this transpose convolution is shown in Figure 5.8(a), where we Ô¨Årst obtain an image of size (5 √ó 5) which is then cropped to give an output of size (4 √ó 4). Note that the output pixels get an unequal contribution from the various patches (indicated by numbers in the Figure), which leads to checker-boarding, which is undesirable. Checker-boarding refers to pixel-to-pixel variations in the values of the output image. 5. One way to avoid checker-boarding, is by ensuring that the Ô¨Ålter size is an integer multiple of the stride. Let us repeat the previous example but with a (2 √ó 2) kernel. The operation is illustrated in Figure 5.8(b)In this case, we do not need to pad (crop) and each output pixel has an equal contribution. We make some remarks: 1. Transpose convolution layers are also called fractionally-strided layers, because for every one step in the input, we take greater than one step in the output. This is the opposite of what happens in a convolution layer. In a convolution layer, we take step greater than one in the input image, for step of uint one in the output image. 2. Transpose convolutions are a tool of choice for upscaling through learnable parameters. 3. Upscaling is typically done with a reduction in the number of channels, which is once again the opposite of what is done in convolution layers. 54 (a) kernel size (3 √ó 3), checker-boarding (b) kernel size (2 √ó 2), no checker-boarding Figure 5.8: Example of a transpose convolution operation. The cells marked with red X‚Äôs in the output are cropped out. The numbers denote the number of patches that contributed to the value in the output pixel. 55 5.8 Image-to-image transformations Image-to-image to transformations can be seen analogous to function-to-function transformations. These types of networks are typically used in computer vision, super-resolution, style transfer, and also in computational physics where we (say) map the source (RHS) Ô¨Åeld to the solution of the PDE. We will discuss a particular type of network for such transformations, which is known as U-Nets [26]. In a U-Net (see Figure 5.9), there a down is a downward branch which takes an input image and downscales the images using a number of convolution layers and pooling operations. As we go down this branch, the number of channels typically increase. After we reach the coarsest level, we have an upward branch that scales up the image and reduced the number of channels using transpose convolution type operations, to Ô¨Ånally give the output image. In addition to these branches, the U-Net also makes use of skip connections that combines information at a particular scale in the downward branch to the information in the upward branch at the same scale. These connections are similar to what are used in ResNets. In the upscaling branch of the U-net, if you consider the activation at one point, you will see they come from two diÔ¨Äerent sources. One of these is the from the same spatial scale in the down-scaling branch of the U-net, and the other is from the coarser scales of the upscaling branch of the U-net. Remark 5.8.1. The U-net architecture shares many common features with the V-cycle that is typically used in multigrid preconditioners. Remark 5.8.2. We can also think of a the U-net as an encode-decoder network with the additional feature of including skip connections. Figure 5.9: Example of a U-Net taken from [26]. 56 Chapter 6 Operator Networks 6.1 The problem with PINNs Recall that a typical MLP y = F(x; Œ∏) is a function that takes as input x ‚ààRd and gives an output y ‚ààRd with trainable weights Œ∏. Also, as we discussed in Chapter 4, a PINN is a network of the form u(x) = F(x; Œ∏) taking as input the independent variable x of the underlying PDE and giving the solution u(x) (of the PDE) as output. The network is trained by minimizing the weighted sum of the PDE and boundary residual. However, this is just one instance of the solution of the PDE for some given boundary condition and source term. For instance, if we consider the PDE ‚àá¬∑ (Œ∫‚àáu) = f(x), x ‚àà‚Ñ¶= [0, 1] √ó [0, 1] u(x) = g(x), x ‚àà‚àÇ‚Ñ¶ (6.1) and train a PINN F(x; Œ∏) to minimize the loss function Œ†(Œ∏) = 1 Nv Nv X i=1 |‚àá¬∑ (Œ∫‚àáF(xi; Œ∏)) ‚àíf(xi)|2 + Œªb Nb Nb X i=1 |F(xi; Œ∏) ‚àíg(xi)|2 Then, if Œ∏‚àó= arg min Œ∏ Œ†(Œ∏), the PINN solving (6.1) will be u(x) = F(x; Œ∏‚àó). However, if we change f or g in (6.1), we have no reason to believe that the same trained network would work. In fact, we would need to retrain the network (with perhaps the same architecture) for the new f and g. This can be quite cumbersome to do, and we would ideally like to avoid it. In this chapter, we will see ways by which we can overcome this issue. 6.2 Parametrized PDEs Assume the the source term f in (6.1) is given as a parametric function f(x; Œ±). For instance, we could have f(x1, x2; Œ±) = 4Œ±x1(1 ‚àíx1)x2(1 ‚àíx2) Then we could train a PINN that accommodates for the parametrization by considering a network that takes as input both x and Œ±, i.e., F(x, Œ±; Œ∏). This is shown in Figure 6.1 This network can be trained by minimizing the loss function Œ†(Œ∏) = 1 Na Na X j=1 " 1 Nv Nv X i=1 |‚àá¬∑ (Œ∫‚àáF(xi, Œ±j; Œ∏)) ‚àíf(xi, Œ±j)|2 + Œªb Nb Nb X i=1 |F(xi, Œ±j; Œ∏) ‚àíg(xi)|2 # 57 Note that we have to also consider collocation points for the parameter Œ± while constructing the loss function. If Œ∏‚àó= arg min Œ∏ Œ†(Œ∏), then the solution to the parameterized PDE would be u(x, Œ±) = F(x, Œ±; Œ∏‚àó). Further, for any new value of Œ± = ÀÜŒ± we could Ô¨Ånd the solution by evaluating F(x, ÀÜŒ±; Œ∏‚àó). We could use the same approach if there was a way of parameterizing the functions Œ∫(x) and g(x). ùë•! ùë•" ùõº ‚Ñ± ùë¢ Figure 6.1: Schematic of a PINN with a parameterized input. However, what if we wanted the solution for an arbitrary, non-parametric f? In order to do this, we need to Ô¨Ånd a way to approximate operators that map functions to functions. 6.3 Operators Consider a class of functions a(y) ‚ààA such that a : ‚Ñ¶Y ‚ÜíRD. The functions in this class might have certain properties, such as a ‚ààC(‚Ñ¶Y ) or a ‚ààL2(‚Ñ¶Y ). Also consider the operator N : A 7‚ÜíC(‚Ñ¶X), with u(x) = N(a)(x) for x ‚àà‚Ñ¶X. Let us see some examples of operators N. 1. Consider the PDE ‚àá¬∑ (Œ∫‚àáu) = f, x ‚àà‚Ñ¶ u = g, x ‚àà‚àÇ‚Ñ¶ (6.2) For this PDE, ‚Ñ¶X = ‚Ñ¶Y = ‚Ñ¶and the operator N maps the RHS f to the solution (temperature) u of the PDE. That is, u = N(f)(x). The input and the output to the operator are related by the equation above where it is assumed that Œ∫ and g are given and Ô¨Åxed. 2. Consider the PDE ‚àá¬∑ (Œ∫‚àáu) = f, x ‚àà‚Ñ¶ u = g, x ‚àà‚àÇ‚Ñ¶ (6.3) which is the same as the previous PDE but we are assuming that the conductivity Ô¨Åeld Œ∫ might change for the model, instead of the RHS. Then, ‚Ñ¶X = ‚Ñ¶Y = ‚Ñ¶and the operator N maps the conductivity Œ∫ to the solution u of the PDE. That is, u = N(Œ∫)(x). The input and the output to the operator are related by the equation above where it is assumed that f and g are given and Ô¨Åxed. 58 3. Once again, consider the same PDE but with conductivity and the boundary condition being allowed to change ‚àá¬∑ (Œ∫‚àáu) = f(x), x ‚àà‚Ñ¶ u(x) = g(x), x ‚àà‚àÇ‚Ñ¶ (6.4) Then, the operator N maps the boundary condition g and the conductivity Œ∫ to the solution u of the PDE. That is, u = N(Œ∫, g)(x). In this case the input to the operator are two functions (g, Œ∫) and the output is a single function. Therefore ‚Ñ¶X = ‚Ñ¶, while ‚Ñ¶Y = ‚Ñ¶√ó ‚àÇ‚Ñ¶. The input and the output are related through the solution to the PDE above where it is assumed that f is given and Ô¨Åxed. 4. Now consider the equations of linear isotropic elasticity posed on a three-dimensional domain ‚Ñ¶‚äÇR3, ‚àá¬∑  Œª(‚àá¬∑ u)I + 2¬µ‚àáS(u)  = f(x), x ‚àà‚Ñ¶ u(x) = g(x), x ‚àà‚àÇ‚Ñ¶. (6.5) Consider the operator deÔ¨Åned by u(x) = N(f)(x). Here the input function, f : ‚Ñ¶‚ÜíR3, and the output function u : ‚Ñ¶‚ÜíR3. The two are related by the equations above where Œª, ¬µ, g are given and Ô¨Åxed. 5. Now, consider a diÔ¨Äerent PDE. In particular, the advection-diÔ¨Äusion-reaction equation, ‚àÇu ‚àÇt + a ¬∑ ‚àáu ‚àíŒ∫‚àá2u + u(1 ‚àíu) = f, (x, t) ‚àà‚Ñ¶√ó (0, T] u(x, t) = g(x, t), (x, t) ‚àà‚àÇ‚Ñ¶√ó (0, T] u(x, 0) = u0(x), x ‚àà‚Ñ¶. (6.6) We want to Ô¨Ånd the operator N maps the initial condition u0 to the solution u at the Ô¨Ånal time T, i.e., u(x, T) = N(u0)(x). In this case ‚Ñ¶X = ‚Ñ¶Y = ‚Ñ¶. Further the input and the output functions are related to each other via the solution of the PDE above with a, Œ∫, f, g given and Ô¨Åxed. Remark 6.3.1. It is often useful to determine whether an operator is linear or non-linear. This is because if it is linear it can be well approximated by another linear operator. In the cases considered above the operators in examples 1 and 4 were linear whereas those in examples 2,3, and 5 were nonlinear. We are interested in networks that approximate the operator N. We will see how we can do this in the next section. These types of networks are often referred to as Operator Networks. They are two popular versions of these networks. One is referred to as a Deep Operator Network, or a DeepONet, and the other is referred to as a Fourier Neural Operator. We describe the DeepONet in the next section. 6.4 Deep Operator Network (DeepONet) Architecture Operator networks were Ô¨Årst proposed by Chen and Chen [5], where they considered only shallow networks with a single hidden layer. This idea was rediscovered and extended to deep architectures more recently in [14] and were called DeepONets. A standard DeepONet comprises two neural networks. We describe below its construction to approximate an operator N : A ‚ÜíU, where A is a set of functions of the form a : ‚Ñ¶Y ‚äÇRd ‚ÜíR while U consists of functions of the form u : ‚Ñ¶X ‚äÇRD ‚ÜíR. Furthermore, we assume that point-wise evaluations of both class of functions is possible. The architecture for the DeepONet for this operator is illustrated in Figure 6.2. It is explained below: 59 ‚Ä¢ Fix M distinct sensor points y(1), ..., y(M) in ‚Ñ¶Y . ‚Ä¢ Sample a function a ‚ààA at these sensor points to get the vector a = [a(y(1)), ..., a(y(M))]‚ä§‚àà RM. ‚Ä¢ Supply a as the input to a sub-network, called the branch net B(.; Œ∏B) : RM ‚ÜíRp, whose output would be the vector Œ≤ = [Œ≤1(a), ..., Œ≤p(a)]‚ä§‚ààRp. Here Œ∏B are the trainable parameters of the branch net. The dimension of the output of the branch is relatively small, say p ‚âà100. ‚Ä¢ Supply x as an input to a second sub-network, called the trunk net T (.; Œ∏T ) : RD ‚ÜíRp, whose output would be the vector œÑ = [œÑ1(x), ..., œÑp(x)]‚ä§‚ààRp. Here Œ∏T are the trainable parameters of the trunk net. ‚Ä¢ Take a dot product of the outputs of the branch and trunk nets to get the Ô¨Ånal output of the DeepONet e N(., .; Œ∏) : RD √ó RM ‚ÜíR which will approximate the value of u(x) u(x) ‚âàe N(x, a; Œ∏) = p X k=1 Œ≤k(a)œÑk(x). (6.7) where the trainable parameters of the DeepONet will be the combined parameters of the branch and trunk nets, i.e., Œ∏ = [Œ∏T , Œ∏M]. Figure 6.2: Schematic of a DeepONet In the above construction, once the DeepONet is trained (we will discuss the training in the following section), it will approximate the underlying operator N, and allow us to approximate the value of any N(a)(x) for any a ‚ààA and any x ‚àà‚Ñ¶X. Note that in the construction of the DeepONet, the M sensor points need to be pre-deÔ¨Åned and cannot change during the training and evaluation phases. We can make the following observations regarding the DeepONet architecture: 1. The expression in (6.7) has the form of representing the solution as the sum of a series of coeÔ¨Écients and functions. The coeÔ¨Écients are determined by the branch network, while the 60 functions are determined by the trunk network. In that sense the DeepONet construction is similar to that of what is used in the spectral method or the Ô¨Ånite element method. There is a critical diÔ¨Äerence though. In these methods, the basis functions are pre-determined and selected by the user. However, in the DeepONet these functions are determined by the trunk network and their Ô¨Ånal form depends on the data used to train the DeepONet. 2. Architecture of the branch sub-network: When points for sampling the input function are chosen randomly, the appropriate architecture for the branch network comprises fully connected layers. Further recognizing that the dimension of the input to this network can be rather large N1 ‚âà104, while the output is typically small p ‚âà102, this network can be thought of as an encoder. When points for sampling the input function are chosen on a uniform grid, the appropriate architecture for the branch network comprises convolutional layer layers. In that case this network maps an image of large dimension (N1 ‚âà104) to a latent vector of small dimension, p ‚âà102. Thus it is best represented by a convolutional neural network. 3. Broadly speaking, there are two ways of improving the experssivity of the DeepONet. These involve increase the number of network parameters in the branch and trunk sub-networks, and increasing the dimension p of the latent vectors formed by these sub-networks. 6.5 Training DeepONets Training a DeepONet is typically supervised, and requires pairwise data. The following are the main steps involved: 1. Select N1 representative function a(i), 1 ‚â§i ‚â§N1 from the set A. Evaluate the values of these functions at the M sensor points, i.e., a(i) j = a(i)(y(j)) for 1 ‚â§j ‚â§M. This gives us the vectors a(i) = [a(i)(y(1)), ..., a(i)(y(M))]‚ä§‚ààRM for each 1 ‚â§i ‚â§N1. 2. For each a(i), determine (numerically or analytically) the corresponding functions u(i) given by the operator N. 3. Sample the function u(i) at N2 points in ‚Ñ¶X, i.e., u(i)(x(k)) for 1 ‚â§k ‚â§N2. 4. Construct the training set S = n a(i), x(k), u(i)(x(k))  : 1 ‚â§i ‚â§N1, 1 ‚â§k ‚â§N2 o which will have N1 √ó N2 samples. 5. DeÔ¨Åne the loss function Œ†(Œ∏) = 1 N1N2 N1 X i=1 N2 X k=1 | e N(x(k), a(i); Œ∏) ‚àíu(i)(x(k))|2. 6. Training the DeepONet corresponds to Ô¨Ånding Œ∏‚àó= arg min Œ∏ Œ†(Œ∏). 7. Once trained, then given any new a ‚ààA samples at the M sensor points (which gives the vector a ‚ààRM), and a new point x ‚àà‚Ñ¶X, we can evaluate the corresponding prediction u‚àó(x) = e N(x, a; Œ∏‚àó). 61 Remark 6.5.1. We need not choose the same N2 points across all i in the training set. In fact, these can be chosen randomly leading to a more diverse dataset. Remark 6.5.2. The DeepONet can be easily extended to the case where the input comprises multiple functions. In this case, the trunk network remains the same, however the branch network now has multiple vectors as input. The case corresponding to two input functions, a(y) and b(y), which when sampled yield the vectors, a and b, is shown in Figure 6.3. ùíÇ ùíÉ ùíô Branch ùú∑ Trunk ùùâ ùíñ Dot Product Figure 6.3: Schematic of a DeepONet with two input functions. Remark 6.5.3. The DeepONet can be easily extended to the case where the output comprises multiple functions (say D such functions). In this case, the output of the branch and trunk network leads to D vectors each with dimension p. The solution is then obtained by taking the dot product of each one of these vectors. The case corresponding to two output functions u1(x) and u2(x) is shown in Figure 6.3. ùíÇ ùíô Branch ùú∑ùüê Trunk ùùâùüè ùíñùüè Dot Product ùú∑ùüè ùùâùüê ùíñùüê Dot Product Figure 6.4: Schematic of a DeepONet with two output functions. 6.6 Error Analysis for DeepONets There is a universal approximation theorem for a shallow version of DeepONets by Chen and Chen [5] 62 Theorem 6.6.1. Suppose ‚Ñ¶X and ‚Ñ¶Y are compact sets in RD (or more generally a Banach space) and Rd, respectively. Let N be a nonlinear, continuous operator mapping V ‚äÇC(‚Ñ¶Y ) into C(‚Ñ¶X). Then given œµ > 0, there exists a DeepONet with M sensors and a single hidden layer of width P in the branch and trunk nets such that max x‚àà‚Ñ¶X a‚ààA | e N(x, a; Œ∏) ‚àíN(a)(x)| < œµ for a large enough P and M. This result has been extended to a deeper version of the network in [14], and generalized further by removing the compactness assumptions on the spaces [12]. Recently in [20], the authors have developed an estimate of the error in a DeepONet that clearly pinpoints the diÔ¨Äerent sources of error in a DeepONet. This estimate states, the error measured in the L2(‚Ñ¶) norm is bounded as max a‚ààA ‚à•e N(x, a; Œ∏) ‚àíN(a)(x)‚à•L2(‚Ñ¶) ‚â§C  œµh + ‚àöœµt + œµs + M‚àíŒ±1 + (N2)‚àíŒ±2 (6.8) where œµh is the error made by the numerical solver used to generate the approximate target solutions u(i) in the training set (as compared to the exact solutions), œµt is the Ô¨Ånal training error/loss attained, while œµs bounds the distance of any a ‚ààA from the set of functions {a(i)}N1 i=1 used to construct the construct the training set, i.e., an estimate of how well the training samples covers the input space A. Further, since the input function is evaluated at M Ô¨Ånite sensor nodes, while the output is evaluated at N2 output nodes, this will lead to an additional discretization (or quadrature) error which is given by the last two terms in (6.8). Note that this is similar to the error estimates obtained for PINNs in (4.21). 6.7 Physics-Informed DeepONets Recall that DeepONets approximates u(x) = N(a)(x) ‚âàe N(x, a; Œ∏). Assume that the pair a and u satisfy a PDE. For example, ‚àá¬∑ (Œ∫u) = f in ‚Ñ¶ u = g on ‚àÇ‚Ñ¶ (6.9) where Œ∫ and g are prescribed. To construct the operator N that maps f to u, we need to solve the PDE externally. However, in addition to this, we can also use a PINN-type loss function and add that to the total loss. This is the idea of Physic-Informed DeepONets proposed in [30]. So for the above model PDE, the loss additional physics-based loss would would be, Œ†p(Œ∏) = 1 ¬ØN1 1 ¬ØN2 ¬Ø N1 X i=1 ¬Ø N2 X k=1 ‚àáx ¬∑  Œ∫‚àáx e N(x(k), f (i); Œ∏)  ‚àíf(i)(x(k)) 2 . (6.10) This is in addition to the standard data-driven loss function which, for this example is given by Œ†d(Œ∏) = 1 N1N2 N1 X i=1 N2 X k=1 | e N(x(k), f (i); Œ∏) ‚àíu(i)(x(k))|2. (6.11) The total loss function is a weighted sum of these two terms: Œ†(Œ∏) = Œ†d(Œ∏) + ŒªŒ†p(Œ∏), (6.12) where Œª is a hyper-parameter. A few comments are in order: 63 1. The output sensor points used in the physics-based loss function are usually distinct from the output sensor points used in the data-driven loss term. The former represent the locations at which we wish to minimize the residual of the PDE, while the latter represent the points at which the solution is available to us through external means. The total number of the output sensor points in the physics-based portion of the loss function is denoted by ¬ØN2, whereas in the data-driven loss function it is denoted by N2. 2. The set of input functions used to construct the physics-based loss function is usually distinct from the set of input functions used to construct the data-driven loss function. The former set represents the functions for which we wish to minimize the residual of the PDE, while the latter set represents the collection of input functions for which the solution is available to us through external means. The total number of functions in the set used to construct the the physics-based portion of the loss function is denoted by ¬ØN1, whereas the total number of functions in the set used to construct the data-driven portion of the loss function is denoted by N ‚àí1. As earlier, we train the network by Ô¨Ånding Œ∏‚àó= arg min Œ∏ Œ†(Œ∏) and approximate the solution for a new a by u‚àó(x) = e N(x, a; Œ∏‚àó). The advantages of adding the extra physics-based loss are: 1. It reduces the demand on the amount of data in the data-driven loss term. What is means is that we don‚Äôt have to generate as many solutions of the PDE for training the DeepONet. 2. It makes the network more robust in that it becomes more likely to produce accurate solutions for the type of input functions not included in the training set for the data-driven loss term. 6.8 Fourier Neural Operators - Architecture We now introduce and discuss Fourier Nerual Operators (FNOs) [13]. We discuss their architecture in this section, and then discuss other aspects in the following section. Our approach in developing the architecture for a FNO will be to begin with the architecture of a typical feed-forward MLP that maps a scalar to another scalar, and systematically extend it so that the extended version maps a scalar valued function to another scalar valued function. A(1) s A(l+1) s s ùíô(ùüé) ùùÉ(ùüè) ùíô(ùüè) ùíô(ùíç) ùíô(ùíç&ùüè) ùíô(ùë≥&ùüè) ùùÉ(ùíç) ùùÉ(ùíç&ùüè) ùùÉ(ùë≥&ùüè) s Figure 6.5: Computational graph for a feed-forward MLP. In Figure 6.5 we have plotted the computational graph of an MLP. We are focused only on the forward part (not the back-propagation) part of this network. For simplcity, we assume that the input x(0) = x is a scalar and the output x(L) = y is also a scalar. Further all the other hidden variables (with the exception of Œæ(L+1)) are vectors with H components. That is, the width of each layer is H. The Ô¨Årst step in this process will be to replace the input and the output with functions. The input will now be the function a(x) : ‚Ñ¶7‚ÜíR1. Similarly the output is the function u(x) : ‚Ñ¶7‚ÜíR1. This leads us to the computational graph shown in Figure 6.6. 64 A(1) s A(l+1) s s ùëé(ùíô) ùíó(ùüè) ùíñ(ùüè) ùíñ(ùíç) ùíñ(ùíç%ùüè) ùíó(ùíç) ùíó(ùíç%ùüè) ùíó(ùë≥%ùüè) s ùë¢(ùíô) Figure 6.6: Computational graph for a feed-forward Fourier Neural Operator (FNO) network. The next step is consider the variables in the hidden layers. In the MLP, these were all vectors with H components. In the FNO, these will be functions with H components. That is, v(1), ¬∑ ¬∑ ¬∑ , v(L), u(1), ¬∑ ¬∑ ¬∑ , u(L) : ‚Ñ¶7‚ÜíRH. (6.13) As shown in Figure 6.6, v(n) and u(n) are the counterparts of Œæ(n) and x(n), respectively. Further since Œæ(L+1) was a scalar, we will set v(L+1) to be a scalar valued function. We are now done with extending the input, the output and the variables in the hidden layers from vectors to functions. Next, we need to extend the operators that transform vectors to vectors within an MLP to those that transform functions to functions within an FNO. We begin with the operator A(1), which in an MLP is an aÔ¨Éne map from a vector with one component to a vector with H components. Its straightforward extension to functions is, v(1)(x) = A(1)(a)(x), (6.14) where v(1) i (x) = W (1) i a(x) + b(1) i , i = 1, ¬∑ ¬∑ ¬∑ , H. (6.15) Here W (1) i and b(1) i are the weights and biases associated with this layer. Similarly, in an MLP the operator A(L+1) is an aÔ¨Éne map from a vector with H components to a vector with 1 component. It‚Äôs straightforward extension to functions is, v(L+1)(x) = A(L+1)(u(L))(x), (6.16) where v(L+1)(x) = W (L+1) i u(L) i (x) + b(L+1), i = 1, ¬∑ ¬∑ ¬∑ , H. (6.17) Here W (L+1) i and b(L+1) are the weights and the bias associated with this layer. Next we describe the action of the activation on input functions. It is a simple extension of the activation function applied to the point-wise values of the input function. That is, u(n)(x) = œÉ(v(n))(x), (6.18) where u(n) i (x) = œÉ(v(n) i (x)), i = 1, ¬∑ ¬∑ ¬∑ , H. (6.19) Finally it remains to extend the operators A(n), n = 2, ¬∑ ¬∑ ¬∑ , L to functions. These are deÔ¨Åned as, v(n+1)(x) = A(n+1)(u(n))(x), (6.20) where v(n+1) i (x) = W (n+1) ij u(n) j (x) + b(n+1) i (6.21) + Z ‚Ñ¶ Œ∫(n+1) ij (y ‚àíx)u(n) j (y)dy, i = 1, ¬∑ ¬∑ ¬∑ , H. (6.22) 65 In the equation above the summation over the dummy index j (from 1 to H) is implied. The new term that appears in this equation is a convolution. It is motivated by the observation that a large class of linear operators can be represented as convolutions. An example is the so-called Green‚Äôs operator which maps the right hand side (also called the forcing function) of a linear PDE to its solution. The functions Œ∫(n+1) ij (z) are called the kernels of the convolution. We note that there are H2 of these functions in each layer. It is instructive to examine a speciÔ¨Åc case of a convolution. Let us consider ‚Ñ¶= [0, L1]√ó[0, L2], where we denote the two coordinates by either x1 and x2, or y1 or y2. In this case we may write the convolution as, vi(x1, x2) = Z L1 0 Z L2 0 Œ∫ij(y1 ‚àíx1, y2 ‚àíx2)uj(y1, y2) dy2dy1, i = 1, ¬∑ ¬∑ ¬∑ , H. (6.23) In the equation above, we have dropped the superscripts since they are not relevant to the discussion. Remark 6.8.1. We may interpret the FNO as a sequence of an aÔ¨Éne transform and convo- lution followed by a point-wise nonlinear activation. This combination of linear and nonlinear (activation) operations allows us to approximate nonlinear operator using this architecture. Remark 6.8.2. It is instructive to list all the trainable entities in a FNO. First we list all the trainable parameters: W (1) i , W (2) ij , ¬∑ ¬∑ ¬∑ , W (L) ij , W (L+1) i ; b(1) i , b(2) i , ¬∑ ¬∑ ¬∑ , b(L) i , b(L+1). (6.24) Thereafter, all the trainable kernel functions Œ∫(n) ij (z), n = 2, ¬∑ ¬∑ ¬∑ , L. (6.25) The neural operator introduced in this section acts directly on functions and transforms them into functions. However, when implementing this operator on a computer the functions have to be represented discretely. This is described in the following section. 6.9 Discretization of the Fourier Neural Operator The functions that appear in the neural operator described in the previous section are: a, v(1), u(1), ¬∑ ¬∑ ¬∑ , v(L), u(L), v(L+1), u. (6.26) Each of these functions is deÔ¨Åned on the domain ‚Ñ¶. We discretize this domain with N uniformly distributed points, and represent each function using its values on these points. As an example, in two dimensions, with ‚Ñ¶= [0, L1] √ó [0, L2], we represent the function a(x1, x2) as, a[m, n] = a(x1m, x2n), m = 1 ¬∑ ¬∑ ¬∑ , N1, n = 1 ¬∑ ¬∑ ¬∑ , N2. (6.27) where x1m = (m ‚àí1) √ó L1 N1 ‚àí1 (6.28) x1n = (n ‚àí1) √ó L2 N2 ‚àí1. (6.29) The same representation will be used for all other functions. 66 We now have to consider the discrete version of all operations on these functions as well. This is described below for the special case of ‚Ñ¶= [0, L1] √ó [0, L2]. We begin with the operator A(1). The discretized version is v(1)[m, n] = A(1)(a)[m, n], (6.30) where v(1) i [m, n] = W (1) i a[m, n] + b(1) i , i = 1, ¬∑ ¬∑ ¬∑ , H. (6.31) Similarly, the discretized version of the operator A(L+1) is, v(L+1)[m, n] = A(L+1)(u(L))[m, n], (6.32) where v(L+1)[m, n] = W (L+1) i u(L) i [m, n] + b(L+1), i = 1, ¬∑ ¬∑ ¬∑ , H. (6.33) Next we describe the action of the activation function on discretized input functions. It is given by u(n)[m, n] = œÉ(v(n))[m, n], (6.34) where u(n) i [m, n] = œÉ(v(n) i [m, n]), i = 1, ¬∑ ¬∑ ¬∑ , H. (6.35) Finally it remains to develop the discrete version of the operators A(n), n = 2, ¬∑ ¬∑ ¬∑ , L. These are deÔ¨Åned as, v(p+1)[m, n] = A(p+1)(u(p))[m, n], (6.36) where v(p+1) i [m, n] = W (p+1) ij u(p) j [m, n] + b(p+1) i (6.37) + N1 X r=1 N2 X s=1 Œ∫(p+1) ij [r ‚àím, s ‚àín]u(p) j [r, s]h1h2, i = 1, ¬∑ ¬∑ ¬∑ , H, (6.38) where h1 = L1 N1‚àí1 and h2 = L2 N2‚àí1. Note that the integral in the convolution is now replaced by a sum over all the grid points. Computing this integral for each value of i and m, n involves O(N1N2H) Ô¨Çops. And since this needs to be done for H diÔ¨Äerent values of i, N1 values of M, and N2 values of j, the total cost of discretizing the convolution operation is O(N2 1 N2 2 H) = O(N2H2), where N = N1 √ó N2. The factor of N2 in this cost is not acceptable and makes the implementation of this algorithm impractical. In the following section we describe how the use of Fourier Transforms (forward and inverse) overcomes this bottleneck and leads to a practical algorithm. This is also the reason that this algorithm is referred to as a ‚ÄúFourier Neural Operator." 6.10 The Use of Fourier Transforms Consider a periodic function u(x2, x2) deÔ¨Åne on ‚Ñ¶‚â°[0, L1]√ó[0, L2]. If this function is suÔ¨Éciently smooth it may be approximated by a truncated Fourier series, u(x1, x2) ‚âà N1/2 X m=‚àíN1/2 N2/2 X n=‚àíN2/2 ÀÜu[m, n]e2œÄi( mx1 L1 + nx2 L2 ). (6.39) Here N1 and N2 are even integers, the coeÔ¨Écients ÀÜu[m, n] are the Fourier coeÔ¨Écients and i = ‚àö‚àí1. We note that while the function u is real-valued the coeÔ¨Écients are complex-valued. However, 67 since u is real-valued, they obey the rule ÀÜu[‚àím, ‚àín] = ÀÜu‚àó[m, n], where (.)‚àódenotes the complex- conjugate of a complex number. The approximation can be made more accurate by increasing N1 and N2, and as these numbers tend to inÔ¨Ånity, we recover the equality. The relation above is often referred to as the inverse Fourier transform, since it maps the Fourier coeÔ¨Écients to the function in the physical space. The forward Fourier transform (which maps the function in the physical space to the Fourier coeÔ¨Écients) can be obtained from the relation above by 1. Multiplying both sides by e‚àí2œÄi( rx1 L1 + sx2 L2 ), where r and s are integers. 2. Integrating both sides over ‚Ñ¶. 3. Recognizing that the integral R ‚Ñ¶e2œÄi( (m‚àír)x1 L1 + (n‚àís)x2 L2 )dx1dx2 is non-zero only when m = r and n = s, and in that case it evaluates to L1L2. These steps yield the Ô¨Ånal relation: ÀÜu[r, s] = 1 L1L2 Z L1 0 Z L2 0 u(x1, x2)e‚àí2œÄi( rx1 L1 + sx2 L2 )dx1dx2. (6.40) We now describe how Fourier transforms can be used to evaluate the convolution eÔ¨Éciently. To do this we consider the special case of 2D convolution in (6.23). We begin with substituting uj(y1, y2) = PN1/2 m=‚àíN1/2 PN2/2 n=‚àíN2/2 ÀÜuj[m, n]e2œÄi( my1 L1 + ny2 L2 ) in this equation to get, vi(x1, x2) = Z L1 0 Z L2 0 Œ∫ij(y1 ‚àíx1, y2 ‚àíx2) X m,n ÀÜuj[m, n]e2œÄi( my1 L1 + ny2 L2 ) dy2dy1 = X m,n ÀÜuj[m, n] Z L1 0 Z L2 0 Œ∫ij(y1 ‚àíx1, y2 ‚àíx2)e2œÄi( my1 L1 + ny2 L2 ) dy2dy1 = X m,n ÀÜuj[m, n] Z L1‚àíx1 ‚àíx1 Z L2‚àíx2 ‚àíx2 Œ∫ij(z1, z2)e2œÄi( m(z1+x1) L1 + n(z2+x2) L2 ) dz2dz1 = X m,n ÀÜuj[m, n]e2œÄi( mx1 L1 + nx2 L2 ) Z L1 0 Z L2 0 Œ∫ij(z1, z2)e2œÄi( mz1 L1 + nz2 L2 ) dz2dz1 = L1L2 X m,n ÀÜuj[m, n]ÀÜŒ∫ij[‚àím, ‚àín]e2œÄi( mx1 L1 + nx2 L2 ). (6.41) In the development above, in going from the Ô¨Årst to the second line we have taken the summation outside the integral and recognized that the coeÔ¨Écients ÀÜuj[m, n] do not depend on y1 and y2. In going from the second to the third line we have introduced the variables z1 = y1 ‚àíx1 and z2 = y2 ‚àíx2. In going from the third to the fourth line we have made use of the fact that the functions Œ∫ij(z1, z2) are periodic. Finally in going from the fourth to the Ô¨Åfth line we have made use of the deÔ¨Ånition of the Fourier Transform (6.40). This Ô¨Ånal relation tells us that the convolution can be computed by: 1. Computing the Fourier Transform of uj. 2. Computing the Fourier Transform of Œ∫ij. 3. Computing the product of the coeÔ¨Écients of these two transforms. 4. Computing the inverse Fourier Transform of the product. 68 Next, we account for the fact that we will only work with the discrete forms of the functions uj and Œ∫ij. This means that we evaluate the inverse Fourier transform (6.39) at a Ô¨Ånite set of grid points. Further, it means that we have to approximate the integral in the Fourier transform (6.40). This alternate form is given by ÀÜu[r, s] = h1h2 L1L2 N1 X m=1 N2 X n=1 u[m, n]e‚àí2œÄi( rx1m L1 + sx2n L2 ). (6.42) Here h1 = L1 N1 and h2 = L2 N2 , x1m = (m ‚àí1)h1 and x2n = (n ‚àí1)h2. The Ô¨Ånal observation is that the evaluating the sums in (6.39) and (6.42) require O(N2) operations. This would make the evaluation of the convolution via the Fourier method impractical except for when N is very small. However, the use of Fast Fourier Transform (FFT) reduces this cost to O(N log N). Thus the cost of implementing the convolution reduces to O(N log NH2). This makes the implementation of Fourier Neural Operators practical. 69 Chapter 7 Probabilistic Deep Learning So far, we have considered regression and classiÔ¨Åcation problems, where for a given input x we need to compute a single output y. However, this may not be enough for many problems of interest. In fact, there may be many y‚Äôs for a given x. For example, 1. y and x might be measured with some random noise. 2. y and x might be inherently stochastic. For instance, y could be the pressure measured in a turbulent Ô¨Çow at some point x in space. 3. inverse problems can have multiple solutions. For instance, the forward/direct problems would be determining the temperature Ô¨Åeld given the head conductivity, while the inverse problem could be determining the conductivity Ô¨Åeld given the (possibly noisy) temperature Ô¨Åeld. Thus, we need to formulate a probabilistic framework to use deep learning algorithms to solve such problems. Recall, that our deterministic model was given by y = F(x; Œ∏). In the probabilistic setup, y, x and Œ∏ are treated as random variables. Before we can work with random variables we need to understand some key elements of the theory of probability that are necessary in deÔ¨Åning random variables. 7.1 Key elements of Probability Theory A random experiment is described by a procedure and a set of one or more observa- tions/measurements. For example, 1. Observe a switch and determine whether it is on or oÔ¨Ä. 2. Toss a coin 3 times and note the sequence of heads H or tails T. 3. Toss a coin 3 times and count the number of times H appears. Note that this is the same experiment as earlier but the measurement is diÔ¨Äerent. 4. Spin a spinner, and measure the Ô¨Ånal angle in radians. The outcome is the results of the experiment that cannot be broken down into smaller parts. The sample space, denoted by S, is the set of all possible outcomes of an experiment. For each of the four random experiments observed above, we have 1. S = {on, oÔ¨Ä}. 70 2. S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH}. 3. S = {0, 1, 2, 3}. 4. S = {œà : œà ‚àà(0, 2œÄ]}. Note that there is a fundamental diÔ¨Äerence between the Ô¨Årst 3 experiments, where S is discrete and countable, and the last experiment where the S is uncountable. An event is a collection of outcomes, i.e., a subset of S. Typically the outcomes in an event satisfy a condition. Let‚Äôs see some examples for the above experiments 1. A = {on} or A = {on, oÔ¨Ä} = S . 2. A are all outcomes with at least 2 H, i.e., A = {THH, HTH, HHT, HHH}. 3. A are all outcomes with at least 2 H, i.e., A = {2, 3}. If we deÔ¨Åne B to be all outcomes with 4 H, then no outcome would satisfy this condition, i.e., B = ‚àÖthe null set. 4. A are all outcomes with œà > œÄ/4, i.e, A = {œà : œà ‚àà(œÄ/4, 2œÄ]}. An event class E is a collection of all event (sets) over which probabilities can be deÔ¨Åned. When S is countable, E is all subsets of S. When S is not countable, E is the Borel Ô¨Åeld (or Borel algebra), which is the collection of all open and closed sets in S. The probability law is a rule that assigns a probability to all sets in an event class E . We list the axioms of probability, which are the requirements of a probability law. Consider a sample space S for an experiment and the corresponding event class E . Let P : E 7‚Üí[0, 1] satisfy 1. P[A] ‚â•0 for all A ‚ààE . 2. P[S] = 1. 3. If A1, A2, ... are events such that Ai ‚à©Aj = ‚àÖfor all i Ã∏= j, i.e., the events are mutually exclusive, then P[ ‚àû [ i=1 Ai] = ‚àû X i=1 P[Ai]. Any assignment P that satisÔ¨Åes the above conditions is said to be a valid probability law. Note that probability is like mass. It is non-negative (axiom 1), conserved (total mass is always 1, axiom 2), and for distinct points the total mass is obtained by adding individual masses (axiom 3). If S is countable, then it is suÔ¨Écient to deÔ¨Åne a probability law for all elements of S, i.e., for all elementary outcomes, while making sure that the probabilities are non-negative and add up to 1 (the Ô¨Årst two axioms). Let us try to assign probability laws for the Ô¨Årst three examples which have a countable S using these criteria. 1. For some p ‚àà[0, 1], deÔ¨Åne P[on] = p; P[oÔ¨Ä] = 1 ‚àíp. 2. For a fair die with no memory, P[ai] = 1/8, where ai ‚ààS, i = 1, ¬∑ ¬∑ ¬∑ , 8. 3. For a fair die with no memory, P[0] = 1/8, P[1] = 3/8, P[2] = 3/8, P[3] = 1/8. Remark 7.1.1. As an exercise, verify that the axioms are satisÔ¨Åed for each of these cases. 71 For a continuous sample space, it is suÔ¨Écient to deÔ¨Åne a probability law for all open and closed intervals, while ensuring axioms 1 and 2. Let us consider the fourth example which has an uncountable S. If the spinner is completely unbiased, then the probability is uniformly distributed. Then for b ‚â•a, we say that P[(a, b]] = (b ‚àía)/(2œÄ). Note that the probability of singleton sets in a continuous sample space is zero (for any distribution). Remark 7.1.2. From this point on, whenever we talk about the sample space S, we will im- plicitly assume that we are referring to the triplet (S, E , P). This triplet is also known as a "measure space". 7.2 Random Variables A random variable X is a function deÔ¨Åned from S to the real line with the property that the set Ab = {Œæ ‚ààS : X(Œæ) ‚â§b} belongs to E for all b ‚ààR. Note that in the measure theoretic language, we are requiring X to be a measurable function. Also note that according to the deÔ¨Ånition of Ab, we are enforcing the requirement that we should be able to evaluate P[Ab] for all b ‚ààR. Let us deÔ¨Åne random variables (RVs) for the above examples: 1. For S = {on, oÔ¨Ä} with P[on] = p, P[oÔ¨Ä] = 1 ‚àíp, deÔ¨Åne the RV X(Œæ) = ( 0 if Œæ = oÔ¨Ä 1 if Œæ = on. (7.1) This is also known as a Bernoulli Random Variable. 2. For S = {TTT, TTH, THT, HTT, THH, HTH, HHT, HHH} with P[ai] = 1/8 for all ai ‚ààS, deÔ¨Åne X(Œæ) = Number of heads in Œæ. (7.2) Note that this is the random event that was described in Experiment 3. 3. This random event is already a random variable. 4. For the spinner experiment with S = {œà : œà ‚àà(0, 2œÄ]} with P[(a, b]] = (b‚àía)/(2œÄ), deÔ¨Åne X(œà) = œà 2œÄ. (7.3) If X is deÔ¨Åned on a discrete sample space, it is called a discrete random variable, while if it is deÔ¨Åned on a continuous sample space, it is called a continuous random variable. As described above, a random variable inherits its probabilistic interpretation from the measure space used to deÔ¨Åne it. In the following sections we deÔ¨Åne the probabilistic interpretation of a random variable. 7.2.1 Cumulative distribution function The cumulative distribution function (cdf) of a random variable X is given by FX(x) = P[Œæ : X(Œæ) ‚â§x] which deÔ¨Ånes a probability on R of X taking values in the interval (‚àí‚àû, x]. Let us deÔ¨Åne the cdf for the above examples: 1. For the Bernoulli RV deÔ¨Åned by (7.1) 72 ‚Ä¢ if x < 0, then FX(x) = P[‚àÖ] = 0 ‚Ä¢ if 0 ‚â§x < 1, then FX(x) = P[{oÔ¨Ä}] = 1 ‚àíp ‚Ä¢ if x ‚â•1, then FX(x) = P[{on, oÔ¨Ä}] = 1 The full cdf is shown in Figure 7.1(a). 2. For the RV deÔ¨Åned by (7.2) ‚Ä¢ if x < 0, then FX(x) = P[‚àÖ] = 0 ‚Ä¢ if 0 ‚â§x < 1, then FX(x) = P[#ofH = 0] = P[{TTT}] = 1/8 ‚Ä¢ if 1 ‚â§x < 2, then FX(x) = P[#ofH = 0, 1] = 1/8 + 3/8 = 4/8 ‚Ä¢ if 3 ‚â§x < 3, then FX(x) = P[#ofH = 0, 1, 2] = 1/8 + 3/8 + 3/8 = 7/8 ‚Ä¢ if x ‚â•3, the FX(x) = P[#ofH = 0, 1, 2, 3] = 1 The full cdf is shown in Figure 7.1(b). 3. This random variable is the same as Example 2. 4. For the spinner experiment with the RV deÔ¨Åned by (7.3) FX(x) = P[œà : X(œà) ‚â§x] = P[{œà : œà ‚â§2œÄx}] ‚Ä¢ if x < 0, then FX(x) = P[‚àÖ] = 0 ‚Ä¢ if 0 ‚â§x < 1, then FX(x) = P[{œà ‚àà(0, 2œÄx]}] = 2œÄx 2œÄ = x ‚Ä¢ if x ‚â•1, then FX(x) = P[{œà ‚â§2œÄ}] = 1 The full cdf is shown in Figure 7.1(c). (a) Bernoulli (b) 3 coin tosses (c) Spinner Figure 7.1: Examples of cumulative distribution functions Let us discuss some properties of FX: 1. 0 ‚â§FX(x) ‚â§1. 2. limx‚Üí‚àûFX(x) = 1. 3. limx‚Üí‚àí‚àûFX(x) = 0. 73 4. FX is monotonically increasing. 5. The cdf is always continuous from the right FX(x) = lim h‚Üí0+ Fx(x + h). Note that the FX for discrete RV (see Figure 7.1) are discontinuous at Ô¨Ånitely many x. In fact, the cdf for discrete RVs can be written as a Ô¨Ånite sum of the form FX(x) = K X k=1 pkH(x ‚àíxk), K X k=1 pk = 1, where pk is the probability mass and H is the Heaviside function H(x) = ( 1 if x > 0 0 if x ‚â§0 . Remark 7.2.1. Once we have the FX we can calculate the probability that X will take values in "any" interval in R, i.e., we can compute P[a < X ‚â§b]. Note that FX(b) = P[X ‚â§b] = P[(X ‚â§a) ‚à™(a < X ‚â§b)] = P[X ‚â§a] + P[a < X ‚â§b] (mutually exclusive events) = FX(a) + P[a < X ‚â§b]. Thus, P[a < X ‚â§b] = FX(b) ‚àíFX(a). 7.2.2 Probability density function We deÔ¨Åne the probability density function (pdf). For a continuous FX, it is deÔ¨Åned as fX(x) = d dxFX(x) (7.4) which enjoys the following properties inherited from the cdf: 1. fX(x) ‚â•0, ‚àÄx ‚ààR, since FX is monotonically increasing. 2. limx‚Üí‚àí‚àûfX(x) = limx‚Üí‚àûfX(x) = 0. 3. Integrating (7.4) from (‚àí‚àû, x] gives us Z x ‚àí‚àû fX(y)dy = FX(x) ‚àí lim x‚Üí‚àí‚àûFX(x) = FX(x). 4. Also P[a < X ‚â§b] = FX(b) ‚àíFX(a) = Z b ‚àí‚àû fX(y)dy ‚àí Z a ‚àí‚àû fX(y)dy = Z b a fX(y)dy. Thus, the integral of a pdf in an interval gives the "probability mass" which is the probability that the RV lies in that interval. This is the reason why the pdf is called a "density". 74 5. Furthermore, Z ‚àû ‚àí‚àû fX(y)dy = lim x‚Üí‚àûFX(x) ‚àí lim x‚Üí‚àí‚àûFX(x) = 1. 6. For a very small h > 0, we have the interpretation P[a < X ‚â§a + h] = Z a+h a fX(y)dy ‚âàhfX(a). Note that as h ‚Üí0+, P[a < X ‚â§a + h] ‚Üí0. That is, for a continuous RV the probability of attaining a single value is zero. 7.2.3 Examples of Important RVs Let us look at some important random variables and the associated cdf, pdf (also see Figure 7.2): 1. Uniform RV: for some interval (a, b], the pdf is given by fX(x) = ( 1 b‚àía if x ‚àà(a, b] 0 other wise , while the cdf is given by FX(x) = Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ 0 if x < a x‚àía b‚àía if x ‚àà(a, b] 1 if x > b . 2. Exponential RV: used to model lifetime of devices/humans after a critical event. In this case, X represents the time to failure and P[X > x] = e‚àíŒªx where Œª > 0 is a model parameter which denotes the rate of failure. Thus, FX(x) = P[X ‚â§x] = 1 ‚àíP[X > x] = 1 ‚àíe‚àíŒªx, and fX(x) = d dxFX(x) = Œªe‚àíŒªx. 3. Gaussian RV: used to model natural things like height, weight, etc. In fact, through the Central Limit Theorem, this is also the distribution given by an aggregate of many RVs. The pdf is given by fX(x) = 1 ‚àö 2œÄœÉe‚àí1 2( x‚àí¬µ œÉ ) 2 which is parameterized by the mean ¬µ which denotes the center of this distributions, and the variance œÉ2 which denotes its spread. The corresponding cdf is given by FX(x) = 1 2  1 + erf x ‚àí¬µ œÉ ‚àö 2  , erf(x) = 2 ‚àöœÄ Z x 0 e‚àít2dt. In probabilistic Machine Learning one makes extensive use of uniform and Gaussian random variables. 75 (a) Uniform RV (a = ‚àí1, b = 1) (b) Exponential RV (Œª = 0.8) (c) Gaussian RV Figure 7.2: Continuous random variables 7.2.4 Expectation and variance of RVs Given a RV X with pdf fX, we can calculate its expected value or expectation or mean as ¬µX := E[X] = Z ‚àû ‚àí‚àû xfX(x)dx. The expectation has the following properties: ‚Ä¢ Note that if a pdf is symmetric about x = m, then E[X] = m. To see this, note that (m ‚àíx)fX(x) will be anti-symmetric about m. Thus 0 = Z ‚àû ‚àí‚àû (m‚àíx)fX(x)dx = m Z ‚àû ‚àí‚àû fX(x)dx‚àí Z ‚àû ‚àí‚àû xfX(x)dx =‚áí Z ‚àû ‚àí‚àû xfX(x)dx = m. Using this property, we can easily say the mean for a uniform RV is (a + b)/2, while for a Gaussian RV it is ¬µ. ‚Ä¢ E[c] = c for a constant c. ‚Ä¢ We can calculate the expected value of functions of RVs as E[g(X)] = Z ‚àû ‚àí‚àû g(x)fX(x)dx. ‚Ä¢ The expectation is linear, i.e., E[g(X) + ch(X)] = E[g(X)] + cE[h(X)]. The variance of a RV measures its variation about the mean. It is evaluated as VAR[X] = Z ‚àû ‚àí‚àû (x ‚àí¬µX)2fX(x)dx. Furthermore, we denote the standard deviation as œÉX := STD[X] = p VAR[X]. 76 For a uniform RV VAR[X] = Z b a  x ‚àíb + a 2 2 1 b ‚àíadx = (b ‚àía)2 12 . For a Gaussian RV, we Ô¨Årst use the property of the pdf to write Z ‚àû ‚àí‚àû e‚àí1 2( x‚àí¬µ œÉ ) 2 dx = ‚àö 2œÄœÉ. Taking a derivative with respect to œÉ on both sides lead to Z ‚àû ‚àí‚àû e‚àí1 2( x‚àí¬µ œÉ ) 2 (x ‚àí¬µ)2œÉ‚àí3dx = ‚àö 2œÄ which after a bit of algebra gives us VAR[X] = Z ‚àû ‚àí‚àû (x ‚àí¬µ)2 1 ‚àö 2œÄœÉe‚àí1 2( x‚àí¬µ œÉ ) 2 dx = œÉ2. 7.2.5 Pair of RVs In probabilistic ML we deal with multiple random variables. For example, the input, the output and the weights might all be RVs. Thus we need to extend concepts from a single RV to a vector of RVs. We do this in this section by Ô¨Årst considering a pair of RVs. Most of the concepts deÔ¨Åned for a pair of RVs carry forward to a vector of RVs. A pair of RVs is a mapping from the measure space, with event class E , of the form X : E ‚ÜíR2, where the mapping can be discrete or continuous. We will sometimes use the notation X = (X, Y ). For example, we can spin the spinner twice and measure œà1 ‚àà(0, 2œÄ], œà2 ‚àà(0, 2œÄ]. In this case, we can deÔ¨Åne the two RVs X(œà1) = œà1 2œÄ, Y (œà2) = œà2 2œÄ. Events for X are sets in R2. To compute probability of events, we need to deÔ¨Åne the joint cdf FXY : R2 ‚ÜíR, where FXY (x, y) = P[X ‚â§x, Y ‚â§y] = P[Œæ ‚ààS : X(Œæ) ‚â§x, Y (Œæ) ‚â§y]. Analogous to single RVs ‚Ä¢ Joint cdfs are non-increasing functions of x, y. In other words, for x ‚â•x‚Ä≤ and y ‚â•y‚Ä≤ FXY (x, y) ‚â•FXY (x‚Ä≤, y‚Ä≤). ‚Ä¢ limx‚Üí‚àí‚àûF(x, y) = 0, limy‚Üí‚àí‚àûF(x, y) = 0, limx,y‚Üí‚àûF(x, y) = 1. ‚Ä¢ We can calculate P[x1 < X ‚â§x2, y1 < Y ‚â§y2] = FXY (x2, y2) + FXY (x1, y1) ‚àíFXY (x1, y2) ‚àíFXY (x2, y1). For X, Y jointly continuous, we can deÔ¨Åne the joint pdf as fXY (x, y) = ‚àÇ2FXY (x, y) ‚àÇx‚àÇy which enjoys the following properties 77 ‚Ä¢ fXY (x, y) = 0 as x ‚Üí¬±‚àûor y ‚Üí¬±‚àû. ‚Ä¢ FXY (x, y) = R x ‚àí‚àû R y ‚àí‚àûfXY (r, s)drds. ‚Ä¢ R ‚àû ‚àí‚àû R ‚àû ‚àí‚àûfXY (r, s)drds = 1. ‚Ä¢ P[x1 < X ‚â§x2, y1 < Y ‚â§y2] = R x2 x1 R y2 y1 fXY (x, y)dxdy. ‚Ä¢ P[X ‚ààB] = R R B fXY (x, y)dxdy. Let us look at some important joint random variables : 1. Joint uniform RV: for some region (a, b] √ó (c, d], the joint pdf is given by fXY (x, y) = ( 1 (b‚àía)(d‚àíc) if (x, y) ‚àà(a, b] √ó (c, d] 0 otherwise . (7.5) 2. Joint Gaussian RV: the joint pdf is given by fXY (x, y) = 1 p (2œÄ)2det(Œ£) exp  ‚àí1 2(x ‚àí¬µ)‚ä§Œ£‚àí1(x ‚àí¬µ)  where x = (x, y), ¬µ = (¬µx, ¬µy) is the mean, and Œ£ is called the covariance matrix Œ£ =  œÉ2 x œÅœÉxœÉy œÅœÉxœÉy œÉ2 y  . The covariance matrix is symmetric and positive deÔ¨Ånite. We can deÔ¨Åne the marginal PDF of the RV X, which is the pdf of X assuming Y attains all possible values fX(x) = Z ‚àû ‚àí‚àû fXY (x, y)dy. Similarly, the marginal of Y is fY (y) = Z ‚àû ‚àí‚àû fXY (x, y)dx. The RVs X and Y are said to be independent if fXY (x, y) = fX(x)fY (y). Question 7.2.1. Show that the joint uniform RVs with joint pdf (7.5) are independent. Consider the function g(X), which can be scalar-, vector-, or tensor-valued, then its expected value is given by E[g(X)] = Z ‚àû ‚àí‚àû Z ‚àû ‚àí‚àû g(x)fXY (x, y)dxdy as long as the integral is deÔ¨Åned. For instance: ‚Ä¢ For g(X) = X, we have E[g(X)] = R ‚àû ‚àí‚àû R ‚àû ‚àí‚àûxfXY (x, y)dxdy. ‚Ä¢ For g(X) = X, we have a vector valued expectation E[g(X)] = [E[X], E[Y ]]. ‚Ä¢ For g(X) = X + Y , we have E[g(X)] = E[X] + E[Y ]. 78 The covariance of X is given by COV[X] = E[(X ‚àíE[X]) ‚äó(X ‚àíE[X])] where COV[X]11 = E[(X ‚àíE[X])2] = VAR[X] COV[X]22 = E[(Y ‚àíE[Y ])2] = VAR[Y ] COV[X]12 = COV[X]21 = E[(X ‚àíE[X])(Y ‚àíE[Y ])]. X and Y are said to be uncorrelated if COV[X]12 = 0. Furthermore, COV[X]12 = 0 for independent RVs. Caution: COV[X]12 = 0 does not imply the RVs are independent! Finally, we are interested in looking at the pdf of Y when we know X = ÀÜx. A good guess would be fXY (ÀÜx, y). However, this need not be a pdf as it need not integrate to unity over y. This leads us to the conditional pdf of Y when we know X = ÀÜx, fY |X(y|ÀÜx) = fXY (ÀÜx, y) R ‚àû ‚àí‚àûfXY (ÀÜx, y)dy = fXY (ÀÜx, y) fX(ÀÜx) . Similarly, we can write the conditional pdf of X given Y = ÀÜy as fX|Y (x|ÀÜy) = fXY (x, ÀÜy) fY (ÀÜy) . We note that the extension of a regression problem to probabilistic framework leads us to determining the conditional distribution of the output given an instance of the input. This will be discussed in Section 7.4. 7.3 Unsupervised probabilistic deep learning algorithms We begin with a vector of RVs X with NX components with a pdf given by fX. Let‚Äôs assume that we are given a dataset of samples {xi} sampled from the density fX, which we denote by xi ‚àºfX. For instance, these samples could correspond to RGB images of cars, with a resolution of 512 √ó 512. Note that this would mean that the samples would lie in a space with dimension NX = 512 √ó 512 √ó 3, which is quite large! We can treat each pixel of the images as a RV, taking values given by the pixel intensities (across all 3 channels). Thus, these images can be seen as samples of a NX-dimensional RV with some unknown density fX. Also, because of the inherent structure of the objects (i.e. the cars) in these images, the various components of the multidimensional RV can be expected to be highly correlated, leading to a non-trivial form of fX. This correlation also implies that it might be possible to reduce the dimension of X from NX to a smaller number and thus make the representation simpler. We are interested in using the given Ô¨Ånite set of samples {xi} to learning the density fX of the data, and generating new samples from the learned distribution. Such methods are known as generative algorithms. Although a number of generative algorithms are available, we focus on a speciÔ¨Åc type of deep learning algorithm known as Generative Adversarial Networks, or GANs for short. 7.3.1 GANs GANs were Ô¨Årst proposed by Goodfellow et al. [6] in 2014. Since then, many variants of GANs have been proposed which diÔ¨Äer based on the network architecture and the objective function 79 used to train the GAN. We begin by describing the abstract problem setup followed by the architecture and training procedure of a GAN. Consider the dataset S = {xi ‚àà‚Ñ¶X ‚äÇRNX : 1 ‚â§i ‚â§Ntrain}. We assume the samples are realizations of some RV X with density fX, i.e., xi ‚àºfX. We want to train a GAN to learn fX and generate new samples from it. A GAN typically comprises two sub-networks, a generator and a discriminator (or critic). The generator is a network of the form g(.; Œ∏g) : ‚Ñ¶Z ‚Üí‚Ñ¶X, g : z 7‚Üíx (7.6) where Œ∏g are the trainable parameters and z ‚àà‚Ñ¶Z ‚äÇRNZ is the realization of a RV Z following a simple distribution, such as an uncorrelated multivariate Gaussian with density fZ(z) = 1 p (2œÄ)2det(Œ£) exp  ‚àí1 2(z ‚àí¬µ)‚ä§Œ£‚àí1(z ‚àí¬µ)  with ¬µ = 0, Œ£ = I. Typically, NZ ‚â™NX with Z known as the latent variable of the GAN. The architecture of the generator will depend on the size/shape of x. If x is a vector, then g can be an MLP with input dimension NZ and output dimension NX. If x is an image, say of shape H √ó W √ó 3, then the generator architecture will have a few fully connected layers, followed by a reshape into a coarse image with many channels, which is pushed through a number of transpose convolution channels that gradually increase the spatial resolution and compress the number of channels to Ô¨Ånally scale up to the shape H √ó W √ó 3. This is also known as a decoder architecture, similar to the upward branch of a U-Net (see Figure 5.9.) In either case, for a Ô¨Åxed Œ∏g, the generator g transforms the RV Z to another RV, Xg = g(Z; Œ∏g) with density fg X, which corresponds to the latent density fZ pushed-forward by g. We want to choose Œ∏g such that fg X is close to the unknown target distribution fX. The critic network is of the form d(.; Œ∏d) : ‚Ñ¶X ‚ÜíR (7.7) with the trainable parameters Œ∏d. Once again, the critic architecture will depend on the shape of x. If x is a vector then d can be an MLP with input dimension NX and output dimension 1. If x is an image, then the critic architecture will have a few convolution layers, followed be a Ô¨Çattening layer and a number of fully connected layers. This is similar to the CNN architecture shown in Figure 5.7 but with a scalar output and without an output function. Figure 7.3: Schematic of a GAN The schematic of the GAN along with the inputs and outputs of the sub-networks is shown in Figure 7.3. The generator and critic play adversarial roles. The critic is trained to distinguish 80 between true samples coming from fX and fake samples generated by g with the density fg X. The generator on the other hand is trained to fool the critic by trying to generate realistic samples, i.e., samples similar to those sampled from fX. We deÔ¨Åne the objective function describing a Wasserstein GAN (WGAN) [2], which has better robustness and convergence properties compared to the original GAN. The objective function is given by Œ†(Œ∏g, Œ∏d) = 1 Ntrain Ntrain X i=1 d(xi; Œ∏d) | {z } critic value on real samples ‚àí 1 Ntrain Ntrain X i=1 d(g(zi; Œ∏g); Œ∏d) | {z } critic value on fake samples (7.8) where xi ‚ààS are samples from the true target distribution fX, while zi ‚àºfZ are passed through g to generate the fake samples. To distinguish between true and fake samples, the critic attains large positive values when evaluated on real samples and large negative values on fake generated samples. Thus, critic is trained to maximize objective function. In other words, we want to solve the problem Œ∏‚àó d(Œ∏g) = arg max Œ∏d Œ†(Œ∏g, Œ∏d) for any Œ∏g. (7.9) Note that the optimal parameters of the critic will depend on Œ∏g. Now to fool the critic, the generator g tries to minimize the objective function, Œ∏‚àó g = arg min Œ∏g Œ†(Œ∏g, Œ∏‚àó d). (7.10) Thus, training the WGAN corresponds to solving a minmax optimization problem. We note that the critic and the generator are working in an adversarial manner. That is, while the former is trying to maximize the objective function, the latter is trying to minimize it. Hence the name generative adversarial network. In practice, we need to add a stabilizing term to the critic loss. So the critic is trained to maximize Œ†c(Œ∏g, Œ∏d) = Œ†(Œ∏g, Œ∏d) ‚àíŒª ¬ØN ¬Ø N X i=1  ‚àÇd ‚àÇÀÜx(ÀÜxi; Œ∏d) ‚àí1 2 (7.11) where ÀÜxi = Œ±xi + (1 ‚àíŒ±)g(zi; Œ∏g) and Œ± is sampled from a uniform RV in (0, 1). The additional term in (7.11) is known as a gradient penalty term and is used to constraining the (norm of) gradient of the critic d with respect to its input to be close to 1, and thus be 1-Lipschitz function. For further details on this term, we direct the interested readers to [7]. The iterative Algorithm 1 is used train g and d simultaneously, which is also called alternating steepest descent, where Œ∑d and Œ∑g are the learning rates for the critic and the generator, respectively. Note that we take K > 1 optimization steps for the critic followed by a single optimization step for the generator. This is because we want to solve the inner maximization problem Ô¨Årst so that the critic is able to distinguish between real and fake samples. Although taking a very large K would lead to a more accurate solve of the minmax problem, it would also make the training algorithm computationally intractable for moderately sized networks. Thus, K is typically chosen between 4 to 6 in practice. The minmax problem is a hard optimization problem to solve, and convergence is usually reached after training for many epochs. Alternatively, the critic optimization steps can be done over mini-batches of the training data, with many mini-batches taken per epoch, leading to a similar number of optimization steps for a relatively small number of epochs. As the iterations go on, d becomes better at detecting fake samples and g becomes better at creating samples that can fool the critic. 81 Algorithm 1: Algorithm to train a GAN Input: Œ∏0 d, Œ∏0 g, K, N_epochs, Œ∑d, Œ∑g for n = 1, ..., N_epochs do ÀÜŒ∏d ‚ÜêŒ∏(n‚àí1) d for k = 1, ..., K do Maximization update: ÀÜŒ∏d ‚ÜêÀÜŒ∏d + Œ∑d ‚àÇŒ†c ‚àÇŒ∏d (Œ∏(n‚àí1) g , ÀÜŒ∏d) end Œ∏(n) d ‚ÜêÀÜŒ∏d Minimization update: Œ∏n g ‚ÜêŒ∏(n‚àí1) g ‚àíŒ∑g ‚àÇŒ† ‚àÇŒ∏g (Œ∏(n‚àí1) g , Œ∏(n) d ) end Under the assumption of inÔ¨Ånite capacity (NŒ∏g, NŒ∏d ‚Üí‚àû), inÔ¨Ånite data (Ntrain ‚Üí‚àû) and a perfect optimizer, we can prove that the generated distribution fg X converges weakly to the target distribution fX [2]. This is equivalent to saying EZ[‚Ñì(g(Z; Œ∏‚àó g))] ‚àí‚ÜíEX[‚Ñì(X)], (7.12) for every continuous, bounded function ‚Ñìon ‚Ñ¶X, i.e., ‚Ñì‚ààCb(‚Ñ¶X). Once the GAN is trained, we can use the optimized g to generate new samples from fg X ‚âàfX by Ô¨Årst sampling z ‚àºfZ, and then passing it through the generator to get the sample x = g(z; Œ∏‚àó g). Furthermore, due to the weak convergence described above, the statistics (mean, variance, etc) of the generated samples will convergence to the true statistics associated with fX. Remark 7.3.1. We make a few important remarks here: 1. Once the GAN is trained, we typically only retain the generator and don‚Äôt need the critic. The primary role of training the critic is to obtain a suitable g that can generate realistic samples. 2. The reason the term "Wasserstein" appears in the name WGAN is because one can show that solving the minmax problem is equivalent to minimizing the Wasserstein-1 distance between fg X and fX [2, 28]. The Wasserstein-1 distance is a popular metric used to measure discrepancies between two probability measures. 3. Since the dimension NZ of the latent variable is typically much smaller than the dimension NX of samples in ‚Ñ¶X, the trained generator also provides a low dimensional representation of high-dimensional data, which can be very useful in several downstream tasks [21, 22]. 7.4 Supervised probabilistic deep learning algorithms Recall the deterministic problem where given the labelled/pairwise dataset S = {(xi, yi) : xi ‚àà‚Ñ¶X ‚äÇRNX, y ‚àà‚Ñ¶Y ‚äÇRNY }Ntrain i=1 (7.13) 82 we want to Ô¨Ånd y for a new x not appearing in S. We have seen in the previous chapters how neural networks can be used to solve such a regression (or classiÔ¨Åcation) problem. Now let us consider the probabilistic version of this problem. We assume that x and y are modelled using RVs X and Y , respectively. Further, let the paired samples in (7.13) be drawn from the unknown joint distribution fXY . Then given a realization X = ÀÜx, we wish to use S to determine the conditional distribution fY |X(y|ÀÜx) and generate samples from it. There are several popular approaches to solve this probabilistic problem, such as Bayesian neural networks, variational inference, dropouts or deep Boltzman machines. But we will focus on an extension of GANs which also addresses these type of problems. 7.4.1 Conditional GANs Conditional GANs were Ô¨Årst proposed in [17] to learn conditional distributions. We will discuss a special variant of these models known as conditional Wasserstein GANS (cWGANs) which were developed in [1], and used to solve a number of physics-based (inverse) problems in [25]. Figure 7.4: Schematic of a conditional GAN The schematic of a conditional GAN is depicted in Figure 7.4. The generator is a network of the form g(.; Œ∏g) : ‚Ñ¶Z √ó ‚Ñ¶X ‚Üí‚Ñ¶Y , g : (z, x) 7‚Üíy (7.14) where z ‚àºfZ is the latent variable. Note that unlike a GAN, the generator in a conditional GAN also takes as input x. For a given value of X = ÀÜx, sampling z ‚àºfZ will generate many samples of y from some induced conditional distribution fg Y |X(y|ÀÜx). The goal is to prescribe the parameters Œ∏g such that fg Y |X(y|ÀÜx) approximates the true conditional fY |X(y|ÀÜx) for (almost) every value of ÀÜx. The critic is a network of the form d(.; Œ∏d) : ‚Ñ¶X √ó ‚Ñ¶Y ‚ÜíR (7.15) which is trained to distinguish between paired samples (x, y) generated from the true joint distribution fXY and the fake pairs (x, ÀÜy) where ÀÜy is generated by g given (real) x. 83 The objective function for a cWGAN is given by Œ†(Œ∏g, Œ∏d) = 1 Ntrain Ntrain X i=1 d(xi, yi; Œ∏d) | {z } critic value on real pairs ‚àí 1 Ntrain Ntrain X i=1 d(xi, g(zi, xi; Œ∏g); Œ∏d) | {z } critic value on fake pairs . (7.16) As earlier, the critic is trained to maximize the objective function (given by (7.9)) while the generator is trained to minimize it (given by (7.10)). Further, a stabilizating gradient penalty term needs to be included when optimizing the critic (see [25]). The generator and critic are trained using the alternating steepest descent algorithm described for GANs. Under the assumption of inÔ¨Ånite capacity (NŒ∏g, NŒ∏d ‚Üí‚àû), inÔ¨Ånite data (Ntrain ‚Üí‚àû) and a perfect optimizer, we can prove [1] that the generated conditional distribution fg Y |X(y|ÀÜx) converges in a weak sense to the target condition distribution fY |X(y|ÀÜx) (on average) for a given X = ÀÜx. 84 Bibliography [1] J. Adler and O. √ñktem, Deep bayesian inversion. https://arxiv.org/abs/1811.05910, 2018. [2] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein generative adversarial net- works, in Proceedings of the 34th International Conference on Machine Learning, D. Precup and Y. W. Teh, eds., vol. 70 of Proceedings of Machine Learning Research, International Convention Centre, Sydney, Australia, 06‚Äì11 Aug 2017, PMLR, pp. 214‚Äì223. [3] R. Bischof and M. Kraus, Multi-objective loss balancing for physics-informed deep learning. http://rgdoi.net/10.13140/RG.2.2.20057.24169, 2021. [4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud, Neural ordinary diÔ¨Äerential equations. https://arxiv.org/abs/1806.07366, 2018. [5] T. Chen and H. Chen, Universal approximation to nonlinear operators by neural net- works with arbitrary activation functions and its application to dynamical systems, IEEE Transactions on Neural Networks, 6 (1995), pp. 911‚Äì917. [6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial nets, in Advances in neural information processing systems, 2014, pp. 2672‚Äì2680. [7] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, Improved training of wasserstein gans, in Advances in neural information processing systems, 2017, pp. 5767‚Äì5777. [8] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770‚Äì778. [9] P. Kidger and T. Lyons, Universal Approximation with Deep Narrow Networks, in Proceedings of Thirty Third Conference on Learning Theory, J. Abernethy and S. Agarwal, eds., vol. 125 of Proceedings of Machine Learning Research, PMLR, 09‚Äì12 Jul 2020, pp. 2306‚Äì 2327. [10] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization. https://arxiv. org/abs/1412.6980v9, 2017. [11] I. Lagaris, A. Likas, and D. Papageorgiou, Neural-network methods for boundary value problems with irregular boundaries, IEEE Transactions on Neural Networks, 11 (2000), pp. 1041‚Äì1049. 85 [12] S. Lanthaler, S. Mishra, and G. E. Karniadakis, Error estimates for DeepONets: a deep learning framework in inÔ¨Ånite dimensions, Transactions of Mathematics and Its Applications, 6 (2022). [13] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, Fourier neural operator for parametric partial diÔ¨Äerential equations. https://arxiv.org/abs/2010.08895, 2020. [14] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis, Learning nonlinear operators via deeponet based on the universal approximation theorem of operators, Nature Machine Intelligence, 3 (2021), pp. 218‚Äì229. [15] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al., RectiÔ¨Åer nonlinearities improve neural network acoustic models, in Proc. ICML, vol. 30, 2013. [16] L. McClenny and U. Braga-Neto, Self-adaptive physics-informed neural networks using a soft attention mechanism. https://arxiv.org/abs/2009.04544, 2020. [17] M. Mirza and S. Osindero, Conditional generative adversarial nets. https://arxiv. org/abs/1411.1784, 2014. [18] S. Mishra and R. Molinaro, Estimates on the generalization error of physics-informed neural networks for approximating PDEs, IMA Journal of Numerical Analysis, (2022). [19] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation approach to stochastic programming, SIAM Journal on Optimization, 19 (2009), pp. 1574‚Äì 1609. [20] D. Patel, D. Ray, M. R. A. Abdelmalik, T. J. R. Hughes, and A. A. Oberai, Variationally mimetic operator networks. https://arxiv.org/abs/2209.12871, 2022. [21] D. V. Patel and A. A. Oberai, Gan-based priors for quantifying uncertainty in supervised learning, SIAM/ASA Journal on Uncertainty QuantiÔ¨Åcation, 9 (2021), pp. 1314‚Äì1343. [22] D. V. Patel, D. Ray, and A. A. Oberai, Solution of physics-based bayesian inverse prob- lems with deep generative priors, Computer Methods in Applied Mechanics and Engineering, 400 (2022), p. 115428. [23] A. Pinkus, Approximation theory of the mlp model in neural networks, Acta Numerica, 8 (1999), pp. 143‚Äì195. [24] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial diÔ¨Äerential equations, Journal of Computational Physics, 378 (2019), pp. 686‚Äì707. [25] D. Ray, H. Ramaswamy, D. V. Patel, and A. A. Oberai, The eÔ¨Écacy and gen- eralizability of conditional gans for posterior inference in physics-based inverse problems. https://arxiv.org/abs/2202.07773, 2022. [26] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomed- ical image segmentation, in Medical Image Computing and Computer-Assisted Intervention ‚Äì MICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, eds., Cham, 2015, Springer International Publishing, pp. 234‚Äì241. 86 [27] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein, Implicit neural representations with periodic activation functions. https://arxiv.org/abs/ 2006.09661, 2020. [28] C. Villani, Optimal Transport: Old and New, Grundlehren der mathematischen Wis- senschaften, Springer Berlin Heidelberg, 2008. [29] S. Wang, Y. Teng, and P. Perdikaris, Understanding and mitigating gradient Ô¨Çow pathologies in physics-informed neural networks, SIAM Journal on ScientiÔ¨Åc Computing, 43 (2021), pp. A3055‚ÄìA3081. [30] S. Wang, H. Wang, and P. Perdikaris, Learning the solution operator of parametric partial diÔ¨Äerential equations with physics-informed deeponets, Science Advances, 7 (2021). [31] L. Wu, C. Ma, and W. E, How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective, in Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds., vol. 31, Curran Associates, Inc., 2018. [32] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, Denseaspp for semantic segmentation in street scenes, in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 3684‚Äì3692. [33] D. Yarotsky and A. Zhevnerchuk, The phase diagram of approximation rates for deep neural networks. https://arxiv.org/abs/1906.09477, 2019. 87