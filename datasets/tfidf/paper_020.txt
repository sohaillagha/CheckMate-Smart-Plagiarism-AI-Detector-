DILIE: Deep Internal Learning for Image Enhancement Indra Deep Mastan and Shanmuganathan Raman Indian Institute of Technology Gandhinagar Gandhinagar, Gujarat, India {indra.mastan, shanmuga}@iitgn.ac.in Abstract We consider the generic deep image enhancement prob- lem where an input image is transformed into a perceptually better-looking image. Recent methods for image enhance- ment consider the problem by performing style transfer and image restoration. The methods mostly fall into two cate- gories: training data-based and training data-independent (deep internal learning methods). We perform image en- hancement in the deep internal learning framework. Our Deep Internal Learning for Image Enhancement framework enhances content features and style features and uses con- textual content loss for preserving image context in the en- hanced image. We show results on both hazy and noisy im- age enhancement. To validate the results, we use structure similarity and perceptual error, which is efﬁcient in measur- ing the unrealistic deformation present in the images. We show that the proposed framework outperforms the relevant state-of-the-art works for image enhancement. 1. Introduction Many computer vision tasks could be formulated as im- age enhancement tasks where the aim is to improve the per- ceptual quality of the image. For example, an image denois- ing method enhances image features and remove noise. The image style transfer method enhances the content image by transferring style features from a style image. Deep image enhancement is an ill-posed problem that aims to improve the perceptual quality of an image using a deep neural network [13, 28, 12, 30]. An image could be considered as the composition of content features and style features. The content features denote the objects, their structure, and their relative positions. Style features rep- resent the color and the texture information of the objects. Deep image enhancement aims to improve the quality of the content and the style features. The image features may get corrupted in various ways. For example, bad weather conditions, camera shake, and noise, etc. Let us discuss an example of a deep image en- hancement task. Consider a hazy image denoted by I. Haze particles degrade both the content features and the style fea- tures. The content features are corrupted because haze par- ticles reduce the clarity of the structure of the objects. Style features are corrupted due to gray and blueish patterns intro- duced by haze. The enhancement of content and style fea- tures can draw inspiration from the image restoration and the style transfer methods. Image enhancement task is to improve the perceptual quality of hazy image I. The challenge is the haze parti- cles are non-uniformly spread over the scene. One strategy is to utilize the content features from I and transferring the photo-realistic features from a style image S. The inter- esting observation here is that maintaining the balance be- tween the content feature and the style feature is challenging (Fig. 6). Performing deep image enhancement without using paired samples of training data was proposed as an open problem [32]. Here, paired-samples indicate the instances of the original image and corrupted image pairs. Recent ad- vancement in deep internal learning (DIL) solves the open problem for image restoration and image synthesis tasks [25, 27]. We categorize DIL methods for simplicity as: im- age reconstruction models [27], layer separation models [6], and single image GAN frameworks [17, 25]. The deep im- age enhancement of an image (content) is also performed using a style image in the style transfer [7, 16, 10]. We formulate a generic framework called Deep Internal Learning for Image Enhancement (DILIE). It does not use paired samples of training data and aims to learn features internally to perform image enhancement. Fig. 1 shows the deep image enhancement performed for hazy and noisy im- ages. The good perceptual quality of DILIE framework is due to the ability of CNN to learn good quality image statis- tics from a single image [25, 27, 6]. We have illustrated DILIE framework in Fig. 2 for hazy image enhancement. It takes the degraded image I as in- put and generates the enhanced image I∗. The main idea is to formulate the content feature enhancement (CFE) and the style feature enhancement (SFE) models separately for gen- 1 arXiv:2012.06469v1 [cs.CV] 11 Dec 2020 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 1: The ﬁgure shows that the deep internal learning-based DILIE framework output images with better perceptual quality. The style image is shown at the left corner of the content image in (a). The ﬁrst row shows that DILIE output image with better perceptual quality for the enhancement of the hazy image. The second row shows DILIE output images with better clarity for noisy image enhancement. eralizability. Fig. 2 shows CFE decomposes the hazy im- age I into environmental haze layer H and haze-free image Icfe. SFE transfers photo-realistic features from style im- age S to Icfe. We describe the DILIE framework in Sec. 3. CFE is modeled based on the type of corruption. CFE, using the image decomposition model, is used for image de- hazing and CFE using image reconstruction for image de- noising. The image decomposition model performs joint optimization to separate the degraded image into clean and corrupted features. Image reconstruction generates a clean image with pixel-based reconstruction loss. Both these ap- proaches rely upon the strong image prior captured by the encoder-decoder network. The aim of SFE is to transform the input image (con- tent) into a visually appealing output image by transferring style features from the style image. SFE is modeled based on the desired style speciﬁcation, i.e., photo-realistic style transfer [16, 31] or artistic style transfer [10]. Note that the distortions in the style transfer output lead to a lack of photo-realism. We measure the deformations using percep- tual error Pieapp [22] computed between the content image and the output image. DILIE output images with low per- ceptual error (Table 2). One of the important requirements for image enhance- ment is to preserve the context of the input image. DILIE preserves the semantics of the input image by comparing the context vectors. The context vectors represent high-level content information and are computed from feature extrac- tor VGG19 [19]. DILIE framework computes the contex- tual content loss LCL to preserve the semantics of the scene. Fig. 2 illustrates here LCL is computed between I and Icfe to preserve the contextual content features in Icfe. We propose a generic deep internal learning framework (DILIE) that addresses corruption speciﬁc image enhance- ment using image reconstruction and image decomposition, and photo-realistic feature enhancement. We summarize the major contributions as follows. • We show that utilization of contextual features improves image dehazing and outperform relevant state-of-the-art works (Table 1). • We show image enhancement for the challenging scenario where photos were taken in hazy weather (Fig. 3 and Fig. 4). DILIE also performs enhancement of the noisy images (Fig. 5). • DILIE outputs images with better visual quality and lower perceptual error (Table 2 and Fig. 6). 2. Related Work Deep Internal Learning. Recent DIL approaches perform image synthesis and image restoration without using paired samples for training [27, 25]. The aim is to learn the internal patch distribution [25] and utilize the deep image prior [27]. DIL is different from training data-based methods that use prior examples to supervise the image enhancement task [9, 19]. Content Feature Enhancement. CFE is performed us- ing image reconstruction and image decomposition mod- els. The structure of the encoder-decoder network (ED) 2 Figure 2: Image Dehazing. The ﬁgure shows the pictorial representation of DILIE framework for the enhancement of the hazy image. The hazy image I is transformed into an enhanced image I∗. The left side shows the content feature enhancement (CFE) and the right side shows the style feature enhancement (SFE). CFE performs image decomposition to output haze-free image Icfe, transmission map M and haze layer H. VGG19 network φ is used to extract features to compute contextual content loss LCL, content loss LC, and style loss LS. Image decomposition loss LID is a pixel-based loss (Eq. 3). LCL uses contextual similarity criteria on content features (Eq. 5). SFE improves style features using content loss LC and style loss LS. We describe DILIE framework in Sec. 3. provides application-speciﬁc image prior implicitly [27]. Image reconstruction models use ED for denoising, super- resolution, and inpainting. Dehazing is formulated as an image decomposition problem [6], where ED computes the image layer and haze layer separately. For simplicity, image dehazing methods could be classiﬁed into classical [5, 8], supervised method using deep learning [11], and unsuper- vised methods [6]. Style Feature Enhancement. The related works for style feature enhancement are discussed as follows. Gatys et al. proposed Neural style [7] for style feature enhancemnt. Luan et al. [16] improved Neural style [7] for photo- realism. WCT2 enhances photorealism using wavelet trans- forms [31]. STROTSS [10] uses optimal transport for more general style transfer. 3. Our Approach DILIE is a uniﬁed framework to restore the content fea- tures and synthesizes new style features for the image en- hancement task. Let us denote the input image by I. The DILIE framework is deﬁned in Eq. 1. I∗= DILIE(I, f, S, φ, α, β). (1) Here, I∗is the enhanced image. The encoder-decoder net- work f is used for the reconstruction or decomposition of input I. The style image S is used to enhance the style features of image I. The VGG19 network φ is used for image context learning [17] and the style features enhance- ment [7, 10]. DILIE framework performs content feature enhancement (CFE) and style features enhancement (SFE) separately. α and β are the parameters used for CFE and SFE. CFE enhances content features by learning deep fea- tures using encoder-decoder f (Sec. 3.1). SFE uses a style image S for photo-realistic and artistic feature enhancement (Sec. 3.2). Fig. 2 shows DILIE framework for hazy image enhance- ment. CFE performs image decomposition to decompose hazy image I into content feature Icfe, haze layer H, and transmission map M. The image composition block com- bines the decomposed image features and outputs the recon- struction of I. It is done to preserve relationship between Icfe, H, and M. SFE outputs the enhanced image as I∗. We discuss the components of the DILIE framework as fol- lows. 3.1. Content Feature Enhancement CFE could be majorly performed in the following two ways: image reconstruction (IR) and image decomposition (ID). The formulation of content feature enhancement is given in Eq. 2. Icfe = CFE(I, f, φ, α). (2) Here, Icfe denotes the output of the content feature en- hancement. The structure of the encoder-decoder network f provides an implicit image prior for the restoration of im- age features. The corruption speciﬁc image prior enables diverse applications, e.g., dark channel prior for the image dehazing [6, 8] and encoder-decoder without skip connec- tions as denoising prior [27]. The VGG network φ is used to extract the contextual features to compute the contex- 3 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 3: Hazy Image Enhancement (outdoor). The content image contains haze and the style images are clear images (bottom left corner). The style images are photo-realistic. Neural style [7] deforms the geometry of the objects. DPST [16] does not distribute image features well. WCT2 [31] output contains haze corruption, as shown by white spots. STROTSS [10] does not preserve ﬁne image features details. It could be observed that DILIE (ours) output images with better visual quality (the images are best viewed after zooming). tual content loss for preserving the context of image I in CFE output. The parameter α denotes whether CFE is used to model image decomposition (α = 1) or reconstruction (α = 2). 3.1.1 Image Decomposition Image decomposition (ID) improves the quality of images by separating image features and corrupted features. For- mally, given an image I as a combination of image feature layer and environmental noise. ID separate I into the image features layer Icfe and the image corruption layer D, where the separation is determined by a mask M. In the image de- hazing task, the mask is a transmission map that determines image features Icfe and airlight H. ID is deﬁned in Eq. 3. (θ∗ c, θ∗ d, θ∗ m) = arg min (θc,θd,θm) LID  I; fθc, fθd, fθm  . (3) Here, LID denotes the image decomposition loss. θc is the parameter of image content layer, θd is the parameter of distortion layer, and θm is the parameter of mask M. fθc, fθd, and fθm are the instances of encoder-decoder net- work. zc, zd, and zm denote the inputs for the networks. Formally, Eq. 3 models the joint optimization to compute Icfe = fθ∗c (zc), D = fθ∗ d(zd), and M = fθ∗ m(zm). We have shown LID in Eq. 4. LID  I; fθc, fθd, fθm  =  fθm(zm) ⊙fθc(zc)+ (1 −fθm(zm)) ⊙fθd(zd)  −I . (4) Here, Eq. 4 shows that the layer separation is achieved by composing image I from image features Icfe = fθ∗c (zc) and corruption layer D = fθ∗ d(zd), and then minimizing pixel-wise differences. We will discuss the image decom- position for image dehazing task in Sec. 4.1. The image decomposition in Eq. 4 does not consider the context of the input image. The abstract information of con- tent features represents the context of the image, i.e., objects and their relative positions. The feature extractor VGG19 is denoted by φ. It is used to extract the content features and style features [7]. The content features are mostly present at the higher layers of feature extractor φ denoted by φC. The style features are mostly contained at the initial layers denoted by φS. The contextual content loss LCL is deﬁned between the content features of I and Icfe = fθc(zc) as given in Eq. 5. LCL  I, φ; fθc  = −log CX  φC(fθc(zc)), φC(I)  . (5) Here, CX denotes the contextual similarity computed by ﬁnding for each feature φC(fθc(zc))i of the image Icfe, the contextually similar feature φC(I)j of the corrupted image I, and then sum over all the features in φC(fθc(zc)). We call the strategy above as the contextual similarity criterion. The key observation is that high-level content information (image context) is similar in both Icfe and I. LCL maxi- mizes the contextual similarity between Icfe and I to im- prove performance1. 3.1.2 Image Reconstruction Image reconstruction model (IR) uses encoder-decoder f to reconstruct the desired image. IR is described in Eq. 6. θ∗= arg min θ LIR(I; fθr), where LIR(I; fθr) = fθ(zr) −T (I) . (6) Here, LIR is the reconstruction loss, θr is the network parameters, zr is the network input, and T is the image transformation function. The output of CFE in image re- construction is Icfe = fθ∗ r (zr). The function T varies based on the application under consideration. For example, T is an identity function for denoising and T is a down- sampling function for super-resolution [27]. IR model in Eq. 6 performs content feature enhancement by incorporat- ing the application-speciﬁc encoder-decoder architectures 1We have used φC={conv4 2} and φS={conv1 2, conv2 2, conv3 2} in our experiments. 4 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 4: Hazy Image Enhancement (indoor). The ﬁgure shows the image feature enhancement of the indoor scene. It could be observed that DILIE (ours) distributed image features with better perceptual quality (the images are best viewed after zooming). for f. The network architecture is observed to provide an implicit image prior for restoration [27]. 3.2. Style Feature Enhancement We described that CFE enhances the content features of I. SFE aims to improve style features and output the en- hanced image I∗given the CFE output Icfe. SFE transfer the style features to Icfe using style image S. We deﬁne SFE in Eq. 7. I∗= SFE(Icfe, S, f, φ, β). (7) Here, I∗is the enhanced image and S is the reference style image. β represents the type of feature enhancement, i.e., photo-realistic (β = 1) or painting style artistic (β = 2). The style features enhacement is performed using the content loss LC and style loss LS. The content loss LC is deﬁned between the content feature representations φC(Icfe) extracted from Icfe and the content feature rep- resentations φC(I∗) extracted from I∗. The content loss is given by LC = L  φC(Icfe), φC(I∗)  . The style loss LS is computed between the style feature representations φS(S) extracted from S and the style feature representation φS(I∗) of I∗. Formally, LS = L  φS(S), φS(I∗)  . We provide the detailed description of LC and LS in the sup- plementary material. SFE could be considered as photo-realistic or artistic fea- tures enhancement. The photo-realistic feature enhance- ment (PE) is aimed to minimize the distortion of object boundaries and preserve photo-realism using loss LP E. In contrast, the artistic feature enhancement (AE) allows small deformations to achieve an artistic look using loss LAE. 3.2.1 Photo-realistic Feature Enhancement The photo-realism characterization in the image is an un- solved problem [16]. The enhancement of the photo- realistic features is based on the observation that if the in- put image is photo-realistic, then those features could be retained with an afﬁne loss [16]. The image with lower per- ceptual errors is observed to be more photo-realistic [22]. The degree of photo-realism in the output I∗is measured by the perceptual error score PieAPP [22]. The total loss for PE is deﬁned as LP E = Lm + µ × LC + κ × LS, where µ and κ are the coefﬁcients for the content loss LC and the style loss LS. The afﬁne loss Lm preserves the object structure while transforming the style features. More speciﬁcally, afﬁne loss uses Matting Lapla- cian MIcfe of the input Icfe [16], where MIcfe represents the grayscale matte for the content features. Intuitively, the afﬁne loss function transforms the color distribution of I∗ while preserving the object structure. 3.2.2 Artistic Feature Enhancement We described that small image feature deformation could be present in the artistic style transfer. Therefore, the strategy is to match the distribution of the style and the content fea- tures and do not use the afﬁne loss to reduce deformations in I∗. The total loss for AE is deﬁned as LAE = µ × LC + κ × LS, where µ and κ are the coefﬁcients for the con- tent loss LC and the style loss LS. We use relaxed earth mover distance (EMD) to match the image feature distribu- tion [10]. The EMD loss preserves the distance between all the pairs of features extracted from the VGG19 φ to allow pixel value modiﬁcation for style features while preserving the structure of the objects. 4. Applications We perform image enhancement of hazy and noisy im- ages. 4.1. Hazy Image Enhancement Pictures taken in the hazy weather may lack scene infor- mation such as contrast, colors, and object structure. Haze is composed of small particles (e.g., dust) suspended in the 5 (a) Content & Style (b) Neural style [7] (c) DPST [16] (d) WCT2 [31] (e) STROTSS [10] (f) DILIE (ours) Figure 5: Noisy image enhancement. The ﬁgure shows that DILIE outputs images with better perceptual quality (see the cropped images). The style images are artistic images and content images contain noise with strength σ = 0.25. Table 1: The table shows SSIM comparison for dehazing of I-Haze and O-Haze dataset. DILIE outperforms other methods in comparison. AODNet [11] MSCNN [23] DcGAN [14] GFN [24] GCANet [3] PFFNet [20] DoubleDIP [6] DILIE (ours) I-Haze [1] 0.732 0.755 0.733 0.751 0.719 0.740 0.691 0.790 O-Haze [2] 0.539 0.650 0.681 0.671 0.645 0.669 0.643 0.705 Table 2: The table shows that DILIE (ours) performs image en- hancement with minimum perceptual error PieAPP [22]. Neural [7] DPST [16] WCT2 [31] STROTSS [10] DILIE I-Haze [1] 3.80 3.33 3.52 2.91 2.78 O-Haze [2] 3.00 2.71 2.88 2.81 2.55 Denoising 100 5.00 4.98 4.53 4.82 4.27 gas. We have discussed the pictorial representation for hazy image enhancement in Sec. 3 (Fig. 2). The image degrada- tion model for the hazy image is usually formulated using an atmospheric scattering model [29] as shown in Eq. 8. I(p) = ˆI(p) × M(p) + H(p) × (1 −M(p)). (8) Here, p is the pixel location and I is the degraded obser- vation. ˆI is the haze-free image and M is the transmission map. Intuitively, the hazy image I could be considered as a haze layer H superimposed on the true scene content ˆI. Image dehazing can be formulated as a layer decomposi- tion problem to separates the hazy image (I) into a haze-free image layer (Icfe) and a haze layer (H), where Icfe is the approximation of haze-free image ˆI. We have discussed the generalized image decomposition framework in Eq. 3. We show its applicability for hazy image enhancement in Eq. 9. (θ∗ c, θ∗ h, θ∗ m) = arg min (θc,θh,θm) LID  I; fθc, fθh, fθm  +LCL  I, φ; fθc  . (9) Here, LID is for image decomposition (Eq. 3) and LCL is for preserving image context (Eq. 5). θh represents the parameters for haze layer. The transmission map M = fθ∗m(zm) separates the haze-free image Icfe = fθ∗c (zc) and the atmospheric light H = fθ∗ h(zh). The joint framework is aimed to estimate ˆI and H preserving their relations. The main goal of Eq. 9 is to separate image features and haze features based on the semantics. The characteristics of haze particles in I are similar. Therefore, they accumulate into haze layer H. Similarly, the image features of I have similar characteristics and get separated into the haze-free image layer Icfe. We have discussed contextual content loss LCL given in Eq. 5 matches the contextual similarity between features. LCL improves the performance of the layer decomposition framework. Fig. 3 shows the image enhancement of outdoor images and Fig. 4 shows the enhancement of the indoor scenes. The outdoor scenes mostly contain clouds and trees and the in- door images mostly contain objects present in the house- hold. The image dehazing removes the haze from the input image and hazy image enhancement improves the quality of image features. Table 1 shows that DILIE achieves a good Structural Similarity Index (SSIM) for image dehazing. Table 2 shows that DILIE output images with better perceptual quality for hazy image enhancement2. It is interesting to observe that the generalisability of DILIE (ours) allows good perfor- mance for both content feature enhancement (image dehaz- ing) and style feature enhancement (hazy image enhance- ment). Fig. 6 shows that if the input image contains haze par- ticles, then the haze information gets incorporated into the output even when S does not include haze information. Ide- ally, the output should contain the content features from I 2We used implementation of Neural style provided in [26], Tensorﬂow implementation of DPS given in [15], contextual loss implementation in [18], STROTSS implementation in [21], and WCT2 implementation in [4]. We have provided more visual comparisons and implementation details of our method in the supplementary material. 6 (a) Haze-free Image ˆI (b) Hazy Image I (c) Style Image S (d) DPST [16] H: 0.455 (e) WCT2 [31] H: 0.943 (f) STROTSS [10] H: 0.876 (g) DILIE (ours) H: 0.350 Figure 6: Ablation Study. The ﬁgure highlights the corruption of image features due to the haze in the enhanced output images. The style features (color information) of the outputs get affected by haze even when the input style image does not contain haze particles. H denotes the relative perceptual error due to haze computed using PieAPP [22]. DILIE output image with the minimum perceptual error. It could also be observed visually that DILIE output has minimum effect from the haze. and style features from S. The image enhancement of hazy images highlight that preserving a perceptually good bal- ance between style and content features is very challenging. Our CFE module removes haze features so that the ﬁnal output I∗has less inﬂuence due to bad weather conditions. 4.2. Noisy Image Enhancement Denoising aims to recover a clean image from a noisy observation. The image degradation model for the noisy image is given as I = ˆI + ϵ. Here, I the noisy image, ˆI is the clean content image, and ϵ is the additive noise. Image denoising is formulated as image reconstruction, where an encoder-decoder f reconstructs the clear image Icfe from the noisy observation I. The network f provides a high impedance to noise and allows image features [27]. We have discussed the generalized framework for image re- construction using transformation T in Eq. 6. Image de- noising is performed by taking T to be identity function as given in Eq. 10. Icfe = fθ(z), where, θ∗= arg min θ ∥fθ(z) −I ∥. (10) Here, the restored image Icfe = fθ(z) is the approxima- tion of ˆI. The reconstruction loss given in Eq. 10 is itera- tively minimized, and early stopping is used to get the best possible outcome before the network over-learn the noisy features. We make noisy image enhancement more challenging by using the style and the content images containing noise with the strength σ = 0.25. We show the output images in Fig. 5. It could be observed that DILIE gets a better distribution of features with better clarity (see cropped images). We have shown a quantitative comparison in Table 2. It can be ob- served that DILIE outperforms other methods in compari- son. 5. Ablation Study Fig. 6 illustrates that DILIE output images with less en- vironmental noise. The quantitative comparison for haze corruption is described as follows. Consider the hazy im- age I, haze-free image ˆI, and the style image S (Fig. 6). The difference of image features between I and ˆI is due to the haze. Let ST(y, z) denote the style transfer of content y using style z. Fig. 6 shows that when performing ST be- tween I and S, the output image is observed to have haze corruption even when S does not have haze information. The goal is to minimize haze corruption. To quantify haze corruption, let E(w, x) denote the per- ceptual error [22] between image w and image x. The rel- ative error H = ∥E  ˆI, ST(ˆI, S)  −E  ˆI, ST(I, S)  ∥with reference to haze-free image ˆI measures the deformations caused by haze in ST(I, S) by comparing ST output of the clean image ˆI and the corrupted image I using perceptual error PieAPP [22]. Fig. 6 shows that DILIE output image with minimum perceptual error H. It could also be observed visually that in WCT2 [31] output contains haze corruption. DPST [16] and STROTSS [10] outputs also have haze effects when looking carefully. DILIE has the minimum haze effect 3. 6. Conclusion DILIE is a deep internal learning approach for image en- hancement. It is a generic framework for image restoration and image style transfer tasks for content feature enhance- ment (CFE) and style feature enhancement (SFE) models. The contextual content loss for image decomposition im- provised the performance of the image dehazing task. The interesting challenge here is that the degraded input image corrupts both style and content features. CFE and SFE to- gether lead to output images with a low perceptual error and good structure similarity. As future work, we propose to explore image enhancement for other image degradation models such as under-water scenes and snowfall. 3We discuss the ablation study more in the supplementary material. 7 References [1] Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe De Vleeschouwer. I-haze: a dehazing bench- mark with real hazy and haze-free indoor images. In arXiv:1804.05091v1, 2018. [2] Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe De Vleeschouwer. O-haze: a dehazing bench- mark with real hazy and haze-free outdoor images. In IEEE Conference on Computer Vision and Pattern Recognition, NTIRE Workshop, NTIRE CVPR’18, 2018. [3] Dongdong Chen, Mingming He, Qingnan Fan, Jing Liao, Li- heng Zhang, Dongdong Hou, Lu Yuan, and Gang Hua. Gated context aggregation network for image dehazing and derain- ing. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1375–1383. IEEE, 2019. [4] clovaai. https://github.com/clovaai/WCT2, 2019. [5] Raanan Fattal. Single image dehazing. ACM transactions on graphics (TOG), 27(3):1–9, 2008. [6] Yossi Gandelsman, Assaf Shocher, and Michal Irani. Double-dip”: Unsupervised image decomposition via cou- pled deep-image-priors. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), volume 6, page 2, 2019. [7] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im- age style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016. [8] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. IEEE transactions on pat- tern analysis and machine intelligence, 33(12):2341–2353, 2010. [9] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694–711. Springer, 2016. [10] Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and self-similarity. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 10051–10060, 2019. [11] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan Feng. Aod-net: All-in-one dehazing network. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 4770–4778, 2017. [12] Chongyi Li, Saeed Anwar, and Fatih Porikli. Underwater scene prior inspired deep underwater image and video en- hancement. Pattern Recognition, 98:107038, 2020. [13] Chongyi Li, Jichang Guo, Fatih Porikli, and Yanwei Pang. Lightennet: a convolutional neural network for weakly illu- minated image enhancement. Pattern Recognition Letters, 104:15–22, 2018. [14] Runde Li, Jinshan Pan, Zechao Li, and Jinhui Tang. Single image dehazing via conditional generative adversarial net- work. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8202–8211, 2018. [15] Yang Liu. https://github.com/LouieYang/ deep-photo-styletransfer-tf, 2017. [16] Fujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer. Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 6997–7005, 2017. [17] Indra Deep Mastan and Shanmuganathan Raman. Dcil: Deep contextual internal learning for image restoration and image retargeting. In The IEEE Winter Conference on Appli- cations of Computer Vision, pages 2366–2375, 2020. [18] Roey Mechrez. https://github.com/roimehrez/ contextualLoss, 2018. [19] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The contextual loss for image transformation with non-aligned data. European Conference on Computer Vision (ECCV), 2018. [20] Kangfu Mei, Aiwen Jiang, Juncheng Li, and Mingwen Wang. Progressive feature fusion network for realistic image dehazing. In Asian Conference on Computer Vision, pages 203–215. Springer, 2018. [21] Nkolkin13. https://github.com/nkolkin13/ STROTSS, 2019. [22] Ekta Prashnani, Hong Cai, Yasamin Mostoﬁ, and Pradeep Sen. Pieapp: Perceptual image-error assessment through pairwise preference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1808– 1817, 2018. [23] Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and Ming-Hsuan Yang. Single image dehazing via multi- scale convolutional neural networks. In European conference on computer vision, pages 154–169. Springer, 2016. [24] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang. Gated fusion network for single image dehazing. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 3253–3261, 2018. [25] Assaf Shocher, Shai Bagon, Phillip Isola, and Michal Irani. Ingan: Capturing and retargeting the “dna” of a natural im- age. In 2019 IEEE/CVF International Conference on Com- puter Vision (ICCV), pages 4491–4500. IEEE. [26] Cameron Smith. https://github.com/cysmith/ neural-style-tf, 2016. [27] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446– 9454, 2018. [28] Tianren Wang, Teng Zhang, and Brian C Lovell. Ebit: Weakly-supervised image translation with edge and bound- ary enhancement. Pattern Recognition Letters, 138:534–539, 2020. [29] Dong Yang and Jian Sun. Proximal dehaze-net: A prior learning-based deep network for single image dehazing. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 702–717, 2018. [30] Shibai Yin, Yibin Wang, and Yee-Hong Yang. A novel image-dehazing network with a parallel attention block. Pat- tern Recognition, 102:107255, 2020. 8 [31] Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha. Photorealistic style transfer via wavelet transforms. In International Conference on Com- puter Vision (ICCV), 2019. [32] Lei Zhang and Wangmeng Zuo. Image restoration: From sparse and low-rank priors to deep priors [lecture notes]. IEEE Signal Processing Magazine, 34(5):172–179, 2017. 9