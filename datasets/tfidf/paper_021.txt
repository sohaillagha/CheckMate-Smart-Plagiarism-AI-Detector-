Deep learning observables in computational ﬂuid dynamics Kjetil O. Lye ∗, Siddhartha Mishra †and Deep Ray ‡ December 17, 2019 Abstract Many large scale problems in computational ﬂuid dynamics such as uncertainty quantiﬁcation, Bayesian inversion, data assimilation and PDE constrained optimization are considered very chal- lenging computationally as they require a large number of expensive (forward) numerical solutions of the corresponding PDEs. We propose a machine learning algorithm, based on deep artiﬁcial neu- ral networks, that predicts the underlying input parameters to observable map from a few training samples (computed realizations of this map). By a judicious combination of theoretical arguments and empirical observations, we ﬁnd suitable network architectures and training hyperparameters that result in robust and eﬃcient neural network approximations of the parameters to observable map. Numerical experiments are presented to demonstrate low prediction errors for the trained network networks, even when the network has been trained with a few samples, at a computational cost which is several orders of magnitude lower than the underlying PDE solver. Moreover, we combine the proposed deep learning algorithm with Monte Carlo (MC) and Quasi- Monte Carlo (QMC) methods to eﬃciently compute uncertainty propagation for nonlinear PDEs. Under the assumption that the underlying neural networks generalize well, we prove that the deep learning MC and QMC algorithms are guaranteed to be faster than the baseline (quasi-) Monte Carlo methods. Numerical experiments demonstrating one to two orders of magnitude speed up over baseline QMC and MC algorithms, for the intricate problem of computing probability distributions of the observable, are also presented. 1 Introduction Many interesting ﬂuid ﬂows are modeled by so-called convection-diﬀusion equations [29] i.e, nonlinear partial diﬀerential equations (PDEs) of the generic form, Ut + divx(F(U)) = ν divx(D(U)∇xU), (x, t) ∈D ⊂Rds × R+, (1.1) with U ∈Rm denoting the vector of unknowns, F = (Fi)1⩽i⩽ds the ﬂux vector, D = (Dij)1⩽i,j⩽ds the diﬀusion matrix and ν a small scale parameter representing kinematic viscosity. Prototypical examples for (1.1) include the compressible Euler equations of gas dynamics, shallow water equations of oceanography and the magnetohydrodynamics (MHD) equations of plasma physics. These PDEs are hyperbolic systems of conservation laws i.e, special cases of (1.1) with ν = 0 and with the ﬂux Jacobian ∂U(F · n) having real eigenvalues for all normal vectors n. Another important example for (1.1) is provided by the incompressible Navier-Stokes equations, where 0 < ν << 1 and the ﬂux function is non-local on account of the divergence-free constraint on the velocity ﬁeld. It is well known that solutions to convection-diﬀusion equations (1.1) can be very complicated. These solutions might include singularities such as shock waves and contact discontinuities in the case of hy- perbolic systems of conservation laws. For small values of ν, these solutions can be generically unstable, even turbulent, and contain structures with a large range of spatio-temporal scales [29]. ∗Seminar for Applied Mathematics (SAM), D-Math ETH Z¨urich, R¨amistrasse 101, Z¨urich-8092, Switzerland †Seminar for Applied Mathematics (SAM), D-Math ETH Z¨urich, R¨amistrasse 101, Z¨urich-8092, Switzerland ‡Department of Computational & Applied Mathematics, Rice University, Houston, Texas, USA. 1 arXiv:1903.03040v2 [physics.comp-ph] 16 Dec 2019 Numerical schemes play a key role in the study of ﬂuid ﬂows and a large variety of robust and eﬃcient numerical methods have been designed to approximate them. These include (conservative) ﬁnite diﬀerence [31], ﬁnite volume [21, 24], discontinuous Galerkin (DG) ﬁnite element [24] and spectral (viscosity) methods [50]. These methods have been extremely successful in practice and are widely used in science and engineering today. The exponential increase in computational power in the last decades provides us with the opportunity to solve very challenging large scale problems in computational ﬂuid dynamics, such as uncertainty quantiﬁcation (UQ) [4, 19], (Bayesian) inverse problems [48] and real-time optimal control, design and PDE constrained (shape) optimization [6, 51]. In such problems, one is not always interested in computing the whole solution ﬁeld U of (1.1). Rather and in analogy with experimental measurements, one is interested in computing the so-called observables (functionals or quantities of interest) for the solution U of (1.1). These can be expressed in the generic form, L(U) = ∫ D×R+ ψ(x, t)g(U(x, t))dxdt. (1.2) Here ψ : D×R+ →R and g : Rm →R are suitable test functions. Prototypical examples of such observables (functionals) are provided by body forces, such as the lift and drag in aerodynamic simulations, and by the runup height (at certain probe points) in simulations of tsunamis. Moreover in practice, one is interested not just in a single value, but rather in the statistics of such observables. Typical statistical quantities of interest are the mean, variance, higher-moments and prob- ability density functions (pdfs) of the observable (1.2). Such statistical quantities quantify uncertainty in the solution, propagated from possible uncertainty in the underlying inputs i.e, ﬂuxes, diﬀusion co- eﬃcients, initial and boundary data. They might also stem from the presence of a statistical spread in design parameters such as those describing the geometry of the computational domain. It is customary to represent the resulting solution ﬁeld as U = U(y), with y ∈Y ⊂Rd, a possibly very high dimensional parameter space. Thus, the goal of many CFD simulations is to compute (statistics of) the so-called parameters to observable map y 7→L(U(y)). Calculating a single realization of this parameters to observable map might require a very expensive forward solve of (1.1) with a CFD solver and a quadrature to calculate (1.2). However, in UQ, Bayesian inversion or optimal design and control, one needs to evaluate a large number of instances of this map, necessitating a very high computational cost, even on state of the art HPC systems. As a concrete example, we consider a rather simple yet prototypical situation. We are interested in computing statistics of body forces such as lift and drag for a model two-dimensional RAE2822 airfoil [25] (see ﬁgure 2). A single forward solve of the underlying compressible Euler equations on this airfoil geometry on a mesh of high resolution (see ﬁgure 2), with a state of the art high-resolution ﬁnite volume solver [43], takes approximately 7 wall-clock hours on a HPC cluster (see table 5). This cost scales up in three space dimensions proportionately. However, a typical UQ problem such as Bayesian inversion with a state of the art MCMC algorithm, for instance the Random walk Metropolis-Hastings algorithm [48], might need upto 105 −106 such realizations, rendering even a two-dimensional Bayesian inversion infeasible ! This example illustrates the fact that the high-computational cost of the parameters to observable map makes problems such as forward UQ, Bayesian inversion, and optimal control and design very challenging. Hence, we need numerical methods which allow ultrafast (several order of magnitude faster than state of the art) computations of the underlying parameters to observable map. Machine learning, in the form of artiﬁcial neural networks (ANNs), has become extremely popular in computer science in recent years. This term is applied to methods that aim to approximate functions with layers of units (neurons), connected by (aﬃne) linear operations between units and nonlinear activations within units, [20] and references therein. Deep learning, i.e an artiﬁcial neural network with a large number of intermediate (hidden) layers has proven extremely successful at diverse tasks, for instance in image processing, computer vision, text and speech recognition, game intelligence and more recently in protein folding [15], see [30] and references therein for more applications of deep learning. A key element in deep learning is the training of tunable parameters in the underlying neural network by (approximately) minimizing suitable loss functions. The resulting (non-convex) optimization problem, on a very high dimensional underlying space, is customarily solved with variants of the stochastic gradient descent method [45]. 2 Deep learning is being increasingly used in the context of numerical solution of partial diﬀerential equations. Given that neural networks are very powerful universal function approximators [10, 26, 3, 36, 54], it is natural to consider the space of neural networks as an ansatz space for approximating solutions of PDEs. First proposed in [28] on an underlying collocation approach, it has been successfully used recently in diﬀerent contexts in [40, 41, 35, 12, 13, 23] and references therein. Given spatial and temporal locations as inputs, the (deep) neural networks, proposed by these authors, approximate the solution of the underlying PDE, by outputting function values. This approach appears to work quite well for problems with high regularity (smoothness) of the underlying solutions (see [46, 23]) and/or if the solution of the underlying PDE possesses a representation formula in terms of integrals [13, 23]. Given their low regularity, it is unclear if solutions of (1.1), realized as functions of space and time, can be eﬃciently learned by deep neural networks [49]. Several papers applying deep learning techniques in the context of CFD advocate embedding deep learning modules within existing CFD codes to increase their eﬃciency [49, 42, 33]. In this paper, we adopt a diﬀerent approach and propose to use deep neural networks to learn and predict the input parameters to observable map. Our algorithm will be based on fully connected networks (multi-layer preceptrons) which output values of the observable (1.2) for diﬀerent input parameters y ∈Y ⊂Rd with d >> 1. The network will be trained on data, generated from a few, say O(100), samples i.e, realizations of (1.2) with expensive CFD solvers. However, the task of designing deep neural networks that will approximate the parameters to observ- able map with reasonable accuracy is far from straightforward on account of the following issues, • Approximation results for deep neural networks [54, 5, 39], stipulate a network size of O  ε −d s  for attaining an error of O(ε), with s denoting the Sobolev regularity of the underlying map. However, and as explained in section 2.3, the parameters to observable map for most CFD problems is atmost Lipschitz continuous. Thus, we might require unrealistically large networks for approximating the underlying maps in this context. • Another relevant metric for estimating errors with deep neural network is the so-called generalization error of the trained network, (see section 2.3), that measures the errors of the network in predicting unseen data. The best available bounds for this error scale (inversely) with the square-root of the number of training samples. However, in this context, our training samples are generated from expensive CFD simulations. Hence, we might end up with large prediction errors in this data poor regime. Hence, it is quite challenging to ﬁnd deep neural networks that can accurately approximate maps of low Sobolev regularity in a data poor regime. Moreover, there are several hyperparameters that need to be speciﬁed in this framework, for instance size of the networks, choice of which variant of the stochastic gradient algorithm that one uses, choice of loss functions and regularizations thereof etc. A priori, the prediction errors can be sensitive to these choices. We propose an ensemble training procedure to search the hyperparameter space systematically and identify network architectures that are eﬃcient in our context. By a combination of theoretical considerations and rigorous empirical experimentation, we provide a recipe for ﬁnding appropriate deep neural networks to compute the parameters to observable map i.e, those network architectures which ensure a low generalization error, even for relatively few training samples. A second aim of this paper is to employ these trained neural networks, in conjunction with (Quasi- )Monte Carlo methods to compute statistical quantities of interest, particularly the underlying probability distributions of the observable and to demonstrate that these deep learning (quasi)-Monte Carlo methods signiﬁcantly outperform the baseline algorithms. The rest of the paper is organized as follows: the deep learning algorithm is presented in section 2. The deep learning (Quasi-)Monte Carlo algorithm for forward UQ is presented in section 3 and some details for the implementation of both sets of algorithms are provided in section 4. In section 5, we present numerical experiments illustrating the proposed algorithms and the results of the paper are summarized and discussed in section 6. 3 2 Deep learning algorithm 2.1 The problem We consider the following very general form of a parameterized convection-diﬀusion equation, ∂tU(t, x, y) + divx(F(y, U)) = ν divx(D(y, U)∇xU), ∀(t, x, y) ∈[0,T] × D(y) × Y, U(0, x, y) = U(x, y), ∀(x, y) ∈D(y) × Y, LbU(t, x, y) = Ub(t, x, y), ∀(t, x, y) ∈[0,T] × D(y) × Y (2.1) Here, Y is the parameter space and without loss of generality, we assume it to be Y = [0, 1]d, for some d ∈N. The spatial domain is labeled as y →D(y) ⊂Rds and U : [0,T] × D × Y →Rm is the vector of unknowns. The ﬂux vector is denoted as F = (Fi)1⩽i⩽ds : Rm × Y →Rm and D = (Dij)1⩽i,j⩽ds : Rm →Rm is the diﬀusion matrix. The operator Lb is a boundary operator that imposes boundary conditions for the PDE, for instance the no-slip boundary condition for incompressible Navier-Stokes equations or characteristic boundary conditions for hyperbolic systems of conservation laws. Additional conditions such as hyperbolicity for the ﬂux function F and positive-deﬁniteness of the diﬀusion matrix D might also be imposed, depending on the speciﬁc problem that is being considered. We remark that the parameterized PDE (2.1) will arise in the context of (both forward and inverse) UQ, when the underlying convection-diﬀusion equation (1.1) contains uncertainties in the domain, the ﬂux and diﬀusion coeﬃcients and in the initial and boundary data. This input uncertainty propagates into the solution. Following [4] and references therein, it is customary to model such random inputs and the resulting solution uncertainties by random ﬁelds. Consequently, one can parameterize the probability space on which the random ﬁeld is deﬁned in terms of a parameter space Y ⊂Rd, for instance by expressing random ﬁelds in terms of (truncated) Karhunen-Loeve expansions. By normalizing the resulting random variables, one may assume Y = [0, 1]d, with possibly a large value of the parameter dimension d. Moreover, there exists a measure, µ ∈Prob(Y), with respect to which the data from the underlying parameter space is drawn. We point out that the above framework is also relevant in problems of optimal control, design and PDE constrained optimization. In these problems, the parameter space Y represents the set of design or control parameters. For the parameterized PDE (2.1), we aim to compute observables of the following general form, Lg(y, U) := T ∫ 0 ∫ Dy ψ(x, t)g(U(t, x, y))dxdt, for µ a.e y ∈Y. (2.2) Here, ψ ∈L1 loc(Dy × (0,T)) is a test function and g ∈Cs(Rm), for s ⩾1. Most interesting observables encountered in experiments, such as the lift and the drag, can be cast in this general form. For ﬁxed functions ψ, g, we also deﬁne the parameters to observable map: L : y ∈Y →L(y) = Lg(y, U), (2.3) with Lg being deﬁned by (2.2). We also assume that there exist suitable numerical schemes for approximating the convection-diﬀusion equation (2.1) for every parameter vector y ∈Y. These schemes could be of the ﬁnite diﬀerence, ﬁnite volume, DG or spectral type, depending on the problem and on the baseline CFD code. Hence for any mesh parameter (grid size, time step) ∆, we are assuming that for any parameter vector y ∈Y, a high- resolution approximate solution U∆(y) ≈U(y) is available. Hence, there exists an approximation to the input to observable map L of the form, L : y ∈Y →L(y) = Lg(y, U∆), (2.4) 4 with the integrals in (2.2) being approximated to high accuracy by quadratures. Therefore, the original input parameters to observable map L is approximated by L∆to very high accuracy i.e, for every value of a tolerance ε > 0, there exists a ∆<< 1, such that ∥L(y) −L∆(y)∥Lp µ (Y) < ε, (2.5) for some 1 ⩽p ⩽∞and weighted norm, ∥f ∥Lp µ (Y) := ∫ Y | f (y)|pdµ(y)  1 p , for 1 ⩽p < ∞. The L∞ µ norm is analogously deﬁned. 2.2 Deep learning the parameters to observable map As stated earlier, it can be very expensive to compute the map L∆(y) for each single realization, y ∈Y, as a high-resolution CFD solver, possibly entailing a very large number of degrees of freedom, needs to be used. We propose instead, to learn this map by deep neural networks. This process entails the following steps, 2.2.1 Training set. As is customary in supervised learning [20] and references therein, we need to generate or obtain data to train the network. To this end, we select a set of parameters S = {yi}1⩽i⩽N, with each yi ∈Y. The points in S can be chosen as, (i.) randomly from Y, independently and identically distributed with the underlying probability distri- bution µ. (ii.) from a suitable set of quadrature points in Y, for instance the so-called low discrepancy sequences that arise in Quasi-Monte Carlo (QMC) quadrature algorithms [8]. Examples of such sequences include Sobol or Halton QMC quadrature points [8]. We emphasize that the generation of QMC points is very cheap, particularly for Sobol or Halton sequences. Moreover, these points are better spread out over the parameter space than a random selection of points and might provide more detailed information about it [8]. Hence, a priori QMC points might be a better choice for sampling the data. One can also replace QMC points with other hierarchical algorithms such as nodes of sparse grids (Smolyak quadrature points) [7] to form the set S. Once the training parameter set S is chosen, we perform a set of high-resolution CFD simulations to obtain L∆(y), for all y ∈S. As each high-resolution CFD simulation could be very expensive, we will require that N = #(S) will not be very large. It will typically be of at most O(100). Figure 1: An illustration of a (fully connected) deep neural network. The red neurons represent the inputs to the network and the blue neurons denote the output layer. They are connected by hidden layers with yellow neurons. Each hidden unit (neuron) is connected by aﬃne linear maps between units in diﬀerent layers and then with nonlinear (scalar) activation functions within units. 5 2.2.2 Neural network. Given an input vector, a feedforward neural network (also termed as a multi-layer perceptron), shown in ﬁgure 1, consists of layer of units (neurons) which compose of either aﬃne-linear maps between units (in successive layers) or scalar non-linear activation functions within units, culminating in an output [20]. In our framework, for any input vector z ∈Y, we represent an artiﬁcial neural network as, Lθ(z) = CK ⊙σ ⊙CK−1 . . . . . . . . . ⊙σ ⊙C2 ⊙σ ⊙C1(z). (2.6) Here, ⊙refers to the composition of functions and σ is a scalar (non-linear) activation function. A large variety of activation functions have been considered in the machine learning literature [20]. A very popular choice is the ReLU function, σ(z) = max(z, 0). (2.7) When, z ∈Rp for some p > 1, then the output of the ReLU function in (2.7) is evaluated componentwise. Moreover, for any 1 ⩽k ⩽K, we deﬁne Ckzk = Wkzk + bk, for Wk ∈Rdk+1×dk, zk ∈Rdk, bk ∈Rdk+1. (2.8) For consistency of notation, we set d1 = d and dK = 1. Thus in the terminology of machine learning (see also ﬁgure 1), our neural network (2.6) consists of an input layer, an output layer and (K −1) hidden layers for some 1 < K ∈N. The k-th hidden layer (with dk neurons) is given an input vector zk ∈Rdk and transforms it ﬁrst by an aﬃne linear map Ck (2.8) and then by a ReLU (or another) nonlinear (component wise) activation σ (2.7). Although the neural network consists of composition of very elementary functions, its complexity and ability to learn very general functions arises from the interactions between large number of hidden layers [20]. A straightforward addition shows that our network contains  d + 1 + K−1 Í k=2 dk  neurons. We denote, θ = {Wk, bk}, θW = {Wk} ∀1 ⩽k ⩽K, (2.9) to be the concatenated set of (tunable) weights for our network. It is straightforward to check that θ ∈Θ ⊂RM with M = K−1 Õ k=1 (dk + 1)dk+1. (2.10) Thus, depending on the dimensions of the input parameter vector and the number (depth) and size (width) of the hidden layers, our proposed neural network can contain a large number of weights. Moreover, the neural network explicitly depends on the choice of the weight vector θ ∈Θ, justifying the notation in (2.6). Although a variety of network architectures, such as convolutional neural networks or recurrent neural networks, have been proposed in the machine learning literature, [20] and references therein, we will restrict ourselves to fully connected architectures i.e, we do not a priori assume any sparsity structure for our set Θ. 2.2.3 Loss functions and optimization. For any y ∈S, we have already evaluated L∆(y) from the high-resolution CFD simulation. One can readily compute the output of the neural network Lθ(y) for any weight vector θ ∈Θ. We deﬁne the so-called loss function or mismatch function, as J(θ) := Õ y∈S |L∆(y) −Lθ(y)|p, (2.11) for some 1 ⩽p < ∞. The goal of the training process in machine learning is to ﬁnd the weight vector θ ∈Θ, for which the loss function (2.11) is minimized. The resulting optimization (minimization) problem might lead to 6 searching a minimum of a non-convex loss function. So, it is not uncommon in machine learning [20] to regularize the minimization problem i.e we seek to ﬁnd, θ∗= arg min θ ∈Θ (J(θ) + λR(θ)) . (2.12) Here, R : Θ 7→R is a regularization (penalization) term. A popular choice is to set R(θ) = ∥θW ∥q q for either q = 1 (to induce sparsity) or q = 2. The parameter 0 < λ << 1 balances the regularization term with actual loss J (2.11). The above minimization problem amounts to ﬁnding a minimum of a possibly non-convex function over a subset of RM for very large M. We can approximate the solutions to this minimization problem iteratively, either by a full batch gradient descent algorithm or by a mini-batch stochastic gradient descent (SGD) algorithm. A variety of SGD algorithms have been proposed in the literature and are heavily used in machine learning, see [45] for a survey. A generic step in a (stochastic) gradient method is of the form: θr+1 = θr −ηr∇θ (J(θk) + λR(θk)), (2.13) with ηr being the learning rate. The stochasticity arises in approximating the gradient in (2.13) by, ∇θ J(θk) ≈∇θ ©­­ « Õ y∈ˆSq |L∆(y) −Lθ(y)|pª®® ¬ , (2.14) and analogously for the gradient of the regularization term in (2.13). Here ˆSq ⊂S refers to the q-th batch, with the batches being shuﬄed randomly. Moreover, the SGD methods are initialized with a starting value θ0 = ¯θ ∈Θ. A widely used variant of the SGD method is the so-called ADAM algorithm [27]. For notational simplicity, we denote the (approximate, local) minimum weight vector in (2.12) as θ∗ and the underlying deep neural network Lθ∗will be our neural network surrogate for the parameters to observable map L (2.4). The algorithm for computing this neural network is summarized below, Algorithm 2.1. Deep learning of parameters to observable map. Inputs: Parameterized PDE (2.1), Observable (2.2), high-resolution numerical method for solving (2.1) and calculating (2.2). Goal: Find neural network Lθ∗for approximating the parameters to observable map L (2.4). Step 1: Choose the training set S and evaluate L∆(y) for all y ∈S by high-resolution CFD simulations. Step 2: For an initial value of the weight vector θ ∈Θ, evaluate the neural network Lθ (2.6), the loss function (2.12) and its gradients to initialize the (stochastic) gradient descent algorithm. Step 3: Run a stochastic gradient descent algorithm of form (2.13) till an approximate local minimum θ∗of (2.12) is reached. The map L∗= Lθ∗is the desired neural network approximating the parameters to observable map L. 2.3 Theory 2.3.1 Approximation with deep neural networks. Universal approximation theorems [3, 26, 10] prove that neural networks can approximate any continuous (in fact any measurable) function. However, these universality results are not constructive. More recent papers such as [54, 39] provide the following constructive approximation result: Theorem 2.2. Let f ∈Ws,p  [0, 1]d, R for some 1 ⩽p ⩽∞, such that ∥f ∥W s, p ⩽1, then for every ε > 0, there exists a neural network NN( f ) of the form (2.6) with the ReLU activation function, with O  1 + log   1 ε  layers and of size (number of weights) O  ε−d s  1 + log   1 ε  such that ∥f −NN( f )∥Lp ⩽ε, 1 ⩽p ⩽∞. (2.15) Given the above approximation result, it is essential to investigate regularity of the underlying pa- rameters to observeble map L in order to estimate the size of neural networks, needed to approximate them. 7 2.3.2 Regularity of the parameters to observable map (2.3) For simplicity, we assume that the underlying domain Dy = D ⊂BR for a. e. y ∈Y and BR is a ball of radius R. Moreover, we assume that ψ, gk ∈L∞, with gk denoting the k-th derivative of the map g in (2.2), for some k ⩾1. Then, it is straightforward to obtain the following upper bound, ∥L∥W k, p(Y) ⩽C∥U∥W k, p(D×(0,T)×Y), (2.16) with the constant C depending only on the initial data, Ψ and g. Thus, regularity in the parameters to observable map is predicated on the space-time and parametric regularity of the underlying solution ﬁeld U. Unfortunately, it is well-known that one cannot expect such space-time or parametric regularity for solutions of generic convection-diﬀusion equations (1.1). In particular, even for the simple case of scalar one-dimensional conservation laws, at best we can show that the solution U ∈L∞((0,T); BV(D × Y)). Therefore, one can show readily that ∥L∥BV(Y) ⩽C. (2.17) It can be rightly argued that deriving regularity estimates on L in terms of the underlying solution ﬁeld U can lead to rather pessimistic results and might overestimate possible cancellations. This is indeed the case, at least for scalar conservation laws with random initial data i.e, (2.1) with m = 1, ν = 0 and F(y, U) = F(U). In this case, we have the following theorem, Theorem 2.3. Consider the scalar conservation law, i.e parameterized convection-diﬀusion PDE (2.1) with m = 1 and ν = 0, in domain D = Rds and time period [0,T] and assume that the initial data u0 ∈W1,∞ Y; L1(D) and support of u0 is compact. Assume, furthermore that ψ ∈L∞(D × (0,T)) and g ∈W1,∞(R). Then, the parameters to observable map L, deﬁned in (2.3) satisﬁes, ∥L∥W 1,∞(Y) ⩽C. (2.18) for some constant C, depending on the initial data u0, ψ and g. Moreover, if the numerical solution u∆ is generated by a monotone numerical method, then the approximate parameters to observable map L∆ (2.4) satisﬁes ∥L∆∥W 1,∞(Y) ⩽C. (2.19) The proof of this theorem is a consequence of the L1 stability (contractivity) of the solution operator for scalar conservation laws [11]. In fact, for any y, y∗∈Y, a straightforward calculation using the deﬁnition (2.3) and the Lipschitz regularity of g yields, |L(y) −L(y∗)| ⩽∥ψ∥∞∥g∥Lip T ∫ 0 ∥u(t, ., y) −u(t, ., y∗)∥1dt, ⩽∥ψ∥∞∥g∥Lip T ∫ 0 ∥u0(., y) −u0(., y∗)∥1dt, by L1 −contractivity ⩽∥ψ∥∞∥g∥LipT∥u0∥W 1,∞(Y,L1(D)) ⩽C The above proof also makes it clear that bounds such as (2.18) and (2.19) will hold for systems of conservation laws and the incompressible Navier-Stokes equations as long as there is some stability of the ﬁeld with respect to the input parameter vector. On the other hand, we cannot expect any such bounds on the higher parametric derivatives of L, due to the lack of diﬀerentiability of the underlying solution ﬁeld. Given a bound such as (2.18) or (2.19), we can illustrate the diﬃculty of approximating the map L by considering a prototypical problem, namely that of learning the lift and the drag of the RAE2822 airfoil (see section 5.1). For this problem, the underlying parameter space is six-dimensional. Assuming that s = 1 in theorem 2.2 and requiring that the approximation error is at most one percent relative error i.e ε = 10−2 in (2.15), yields a neural network of size O(1012) tunable parameters and at least 6 layers. Such a large network is clearly unreasonable as it will be very diﬃcult to train and expensive to evaluate. 8 2.3.3 Trained networks and generalization error It can be argued that approximation theoretic results can severely overestimate errors with trained net- works. Rather, the relevant measure is the so-called generalization error i.e, once a (local) minimum θ∗ of (2.12) has been computed, we are interested in the following error, EG(θ∗) := ©­ « ∫ Y |L∆(y) −Lθ∗(y)|pdµ(y)ª® ¬ 1 p , 1 ⩽p ⩽∞. (2.20) In practice, one estimates the generalization error on a so-called test set i.e, T ⊂Y, with #(T) >> N = #(S). For instance, T could consist of i.i.d random points in Y, drawn from the underlying distribution µ. In this case, the generalization error (2.20) can be estimated by the prediction error, EP(θ∗) := ©­ « 1 #(T) Õ y∈T |L∆(y) −Lθ∗(y)|pª® ¬ 1 p (2.21) It turns out that one can estimate the generalization error (2.20) by using tools from machine learning theory [47] as, EG(θ∗) ⩽ r U N, (2.22) with N = #(S) being the number of training samples. The numerator U is typically estimated in terms of the Rademacher complexity or the Vapnik-Chervonenkis (VC) dimension of the network, see [47] and references therein for deﬁnitions and estimates. Unfortunately, such bounds on U are very pessimistic in practice and over estimate the generalization error by several (tens of) orders of magnitude [2, 55, 38]. Recent papers such as [2] provide far sharper bounds by estimating the compression i.e number of eﬀective parameters to total parameters in a network. Nevertheless, even if we make a stringent requirement of U ∼O(1), we might end up with unacceptably high prediction errors of 30 −100% for the O(102 −103) training samples that we can aﬀord to generate in practice. Summarizing, theoretical considerations outlined above indicate the challenges of ﬁnding deep neural networks to accurately learn maps of low regularity in a data poor regime. We illustrate these diﬃculties with a simple numerical example. 2.3.4 An illustrative numerical experiment. In this experiment we consider the parameter space Y = [0, 1]6 the following two maps, L1(y) = 6 Õ i=1 sin(4πyi), L2(y) = 6 Õ i=1 1 i3 sin(4πyi). (2.23) Both maps are inﬁnitely diﬀerentiable but with high amplitudes of the derivatives. We deﬁne a neural network of the form (2.6) and with the architecture speciﬁed in table 3. In order to generate the training set, we select the ﬁrst 128 Sobol points in [0, 1]6 and sample the functions (2.23) at these points. The training is performed with ADAM algorithm and hyperparameters deﬁned in table 4 (ﬁrst line). The performance of the neural network is ascertained by choosing the ﬁrst 8192 Sobol points in [0, 1]6 as the test set T. The results of evaluating the trained networks on the test set is shown in table 1 where we present the relative prediction error (2.21) as a percentage and also present the standard deviation (on the test set) of the prediction error. Clearly, the errors are unacceptably high, particularly for the unscaled sum of sines L1 in (2.23). Although still high, the errors reduce by an almost order of magnitude for the scaled sum of sines L2. This is to be expected as the derivatives for this map are smaller. This example illustrates the diﬃculty to approximating (even very regular) functions, in moderate to high dimensions, by neural networks when the training set is relatively small. 9 Map Error (Mean) in % Error (Std.) in % Í6 i=1 sin(4πxi) 133.95 417.83 Í6 i=1 sin(4πxi) i3 43.66 41.26 Table 1: Relative percentage Prediction errors (2.21) with p = 2, for the trained neural networks in the sum of sines experiment (2.23). 2.3.5 Pragmatic choice of network size We estimate network size based on the following very deep but easy to prove fact about training neural networks [55], Lemma 2.4. For the map L∆and for the training set S, with #(S) = N, there exists a weight vector ˆθ ∈Θ, and a resulting neural network of the form Lθ, with Θ ⊂RM and M = O(d + N), such that the following holds, L∆(z) = Lˆθ(z), ∀z ∈S. (2.24) The above lemma implies that there exists a neural network with weight vector ˆθ of size O(d + N) such that the training error deﬁned by, ET(ˆθ) := 1 #(S) Õ y∈S |L∆(y) −Lˆθ(y)|p = 0. (2.25) Thus, it is reasonable to expect that a network of size O(d + N) can be trained by a gradient descent method to achieve a very low training error. It is customary to monitor the generalization capacity of a trained neural network by computing a so-called validation set V ⊂Y with V ∩S = ∅, and evaluating the so-called validation loss, JV(θ) := 1 #(V) Õ y∈V ∥L∆(y) −Lθ(y)∥p p. (2.26) A low validation loss is observed to correlate with low generalization errors. In order to generate the validation set, one can set aside a small proportion, say 10 −20% of the training set as the validation set. 2.4 Hyperparameters and Ensemble training. Given the theoretical challenges of determining neural networks that can provide low prediction errors in our data poor regime, a suitable choice of the hyperparameters in the training process becomes imperative. To this end, we device a simple yet eﬀective ensemble training algorithm. To this end, we consider the set of hyperparameters, listed in table 2. A few comments regarding the listed hyperparameters are in order. We need to choose the number of layers and width of each layer such that the overall network size, given by (2.10) is O(N + d). The exponents p, q in the loss function and regularization terms (2.12) usually take the values of 1 or 2 and the regularization parameter λ is required to be small. The starting value ¯θ is chosen randomly. For each choice of the hyperparameters, we realize a single sample in the hyperparameter space. Then, the machine learning algorithm 2.1 is run with this sample and the resulting loss function minimized. Further details of this procedure are provided in section 4. We remark that this procedure has many similarities to the active learning procedure proposed recently in [56]. 3 Uncertainty quantiﬁcation in CFD with deep learning. We will employ the trained deep neural networks that approximate the parameters to observable map L∆, to eﬃciently quantify uncertainty in the underlying map. We are interested in computing the entire measure (probability distribution) of the observable. To this end, we assume that the underlying proba- bility distribution on the input parameters to the problem (2.1) is given by µ ∈Prob(Y). In the context of forward UQ, we are interested in how this initial measure is changed by the parameters to observable 10 1. Number of hidden layers (K −1) 2. Number of units in k-th layer (dk) 3. Exponent p in the loss function (2.11). 4. Exponent q in the regularization term in (2.12) 5. Value of regularization parameter λ in (2.12) 6. Choice of optimization algorithm (optimizer) – either standard SGD or ADAM. 7. Initial guess ¯θ in the SGD method (2.13). Table 2: Hyperparameters in the training algorithm map L (or its high-resolution numerical surrogate L∆). Hence, we consider the push forward measure ˆµ∆∈Prob(R) given by ˆµ∆:= L∆#µ, ⇒ ∫ R h(z)d ˆµ∆(z) = ∫ Y h(L∆(y))dµ(y), (3.1) for any µ-measurable function h : R 7→R. It should be emphasized that any statistical moment of interest can be computed by integrating an appropriate test function with respect to this measure ˆµ∆. For instance, letting h(w) = w in (3.1) yields the mean of the observable. Similarly, the variance can be computed from the mean and letting h(w) = w2 in (3.1). The task of eﬃciently computing this probability distribution is signiﬁcantly harder than just estimating the mean and variance of the underlying map. The simplest algorithm for computing this measure ˆµ∆is to use the Monte Carlo algorithm ([8] and references therein). It consists of selecting J samples i.e points {yj} with each yj ∈Y and 1 ⩽j ⩽J, that are independent and identically distributed (iid). Then, the Monte Carlo approximation of the push-forward measure is given by, ˆµmc = 1 J J Õ j=1 δL∆(yj) ⇒ ∫ R h(z)d ˆµmc(z) = 1 J J Õ j=1 h  L∆(yj)  . (3.2) It can be shown that ˆµmc ≈ˆµ∆with an error estimate that scales (inversely) as the square root of the number of samples J. Hence, the Monte Carlo algorithm can be very expensive in practice. 3.1 Quasi-Monte Carlo (QMC) methods 3.1.1 Baseline QMC algorithm. Quasi-Monte Carlo (QMC) methods are deterministic quadrature rules for computing integrals, [8] and references therein. The key idea underlying QMC is to choose a (deterministic) set of points on the domain of integration, designed to achieve some measure of equidistribution. Thus, the QMC algorithm is expected to approximate integrals with higher accuracy than MC methods, whose nodes are randomly chosen [8]. For simplicity, we set the underlying measure µ to be the scaled Lebesgue measure on Y. Our aim is to approximate the push forward measure ˆµ∆with the QMC algorithm. To this end, we choose a set of points Jq = {yj} ⊂Y with 1 ⩽j ⩽Jq = #(Jq). Then the measure ˆµ∆can be approximated by, ˆµqmc = 1 Jq Jd Õ j=1 δL∆(yj) ⇒ ∫ R h(z)d ˆµqmc(z) = 1 Jq Jq Õ j=1 h  L∆(yj)  , (3.3) for any measurable function h : Y →R. We want to estimate the diﬀerence between the measures ˆµ∆and ˆµqmc and need some metric on the space of probability measures, to do so. One such metric is the so-called Wasserstein metric [53]. To deﬁne this metric, we need the following, Deﬁnition 3.1. Given two measures, ν, σ ∈Prob(R), a transport plan π ∈Prob(R2) is a probability measure on the product space such that the following holds for all measurable functions F, G, ∫ R×R (F(u) + G(v))dπ(u, v) = ∫ R F(u)dν(u) + ∫ R G(v)dσ(v). (3.4) 11 The set of transport plans is denoted by Π(ν, σ) ■ Then the 1-Wasserstein distance [53] between ˆµ∆and ˆµqmc is deﬁned as, W1  ˆµ∆, ˆµqmc  := inf π∈Π(ˆµ∆, ˆµqmc) ∫ R×R |u −v|dπ(u, v) (3.5) It can be proved using the Koksma-Hlawka inequality and some elementary optimal transport techniques that the error in the Wasserstein metric behaves as, W1  ˆµ∆, ˆµqmc  ∼V∆D(Jq). (3.6) Here, V∆is some measure of variation of L∆. It can be bounded above by V∆⩽C ∫ Y ∂d ∂y1y2 · · · yd L∆(y)dy , (3.7) the Hardy-Krause variation of the function L∆. This upper bound requires some regularity for the underlying map in terms of mixed partial derivatives. However, it is well-known that the bound (3.7) is a large overestimate of the integration error [8]. The D(Jq) in (3.6) is the so-called discrepancy of the point sequence Jq, deﬁned formally in [8] and references therein. It measures how equally is the point sequence Jq spread out in Y. The whole objective in the development of quasi-Monte Carlo (QMC) methods is to design low discrepancy sequences. In particular, there exists many popular QMC sequences such as Sobol, Halton or Niederreiter [8] that satisfy the following bound on discrepancy, D(Jq) ∼(log(Jq))d Jq . (3.8) Thus, the integration error with QMC quadrature based on these rules behaves log-linearly with respect to the number of quadrature points. For simplicity, we assume that the dimension is moderate enough to replace the logarithms in (3.6) by a suitable power leading to W1  ˆµ∆, ˆµqmc  ∼ V∆ Jq α , (3.9) for some 1/2 < α ⩽1. Thus, for moderate dimensions, we see that the QMC algorithm outperforms the baseline MC algorithm. Requiring that the Wasserstein distance is of size O(ε) for some tolerance ε > 0, entails choosing Jq ∼V ∆ ε 1α and leads to a cost of ˆCqmc ∼CV∆ ε 1 α , (3.10) with C being the computational cost of a single CFD forward solve. This can be very expensive, partic- ularly for smaller values of α, as the cost of each forward solve is very high. 3.1.2 Deep learning quasi-Monte Carlo (DLQMC) In order to accelerate the baseline QMC method, we describe an algorithm that combines the QMC method with a deep neural network approximation of the underlying parameters to observable map. Algorithm 3.2. Deep learning Quasi-Monte Carlo (DLQMC). Inputs: Parameterized PDE (2.1), Observables (2.2), high-resolution numerical method for solving (2.1) and calculating (2.2). Goal: Approximate the push-forward measure ˆµ∆. 12 Step 1: For JL ∈N, select JL = {yj} ⊂Y, with 1 ⩽j ⩽JL, as the ﬁrst JL points from a Quasi-Monte Carlo low-discrepancy sequence such as Sobol, Halton etc. Step 2: For some N << JL, denote S = {yj} ⊂JL, with 1 ⩽j ⩽N i.e, ﬁrst N points in JL, and evaluate the map h(L∆(yj)), for all yj ∈Jd with the high-resolution CFD simulation. As a matter of fact, any consecutive set of N QMC points can serve as the training set. Step 3: With S as the training set, compute an optimal neural network L∗for the parameters to observable map L∆, by using algorithm 2.1. Step 4: Approximate the measure ˆµ∆by ˆµ∗ qmc = 1 JL JL Õ j=1 δL∗(yj), (3.11) The complexity of the DLQMC algorithm is analyzed in the following theorem. Theorem 3.3. For any given tolerance ε > 0 and under the assumption that the generalization error (2.20) is of size O(ε) and the baseline QMC estimator (3.3) follows the error estimate (3.9), the speedup Σdlqmc, deﬁned as the ratio of the cost of baseline QMC algorithm and the cost of the DLQMC algorithm 3.2 for approximating the push-forward measure ˆµ∆to an error of size O(ε) in the 1-Wasserstein metric, with the DLQMC algorithm 3.2, satisﬁes 1 Σdlqmc ∼N Jq + C∗ C V∗ V∆. (3.12) Here, C, C∗are the computational cost of computing L∆(y) (high resolution CFD simulation) and L∗(y) (deep neural network) for any y ∈Y and V∗= V(L∗) is an estimate on the variation that arises in a QMC estimate such as (3.9). Proof. We deﬁne the measure ˆµ∗= L∗#µ ∈Prob(R) and estimate, W1  ˆµ∆, ˆµ∗ qmc  ⩽W1  ˆµ∆, ˆµ∗ + W1  ˆµ∗, ˆµ∗ qmc  . We claim that assuming that the generalization error EG ∼ε implies that W1   ˆµ∆, ˆµ∗ ∼ε. To see this, we deﬁne a transport plan π∗∈Prob(R2) by π∗= ˆµ∆⊗ˆµ∗. Then, ∫ R2 |u −v|dπ∗(u, v) = ∫ R2 |u −v|d(L∆#µ × L∗#µ)(u, v), = ∫ Y |L∆(y) −L∗(y)|dµ(y) := EG ∼ε. This provides an upper bound on the Wasserstein distance (3.5) and proves the claim. Similarly, we use an estimate of the type (3.9) to obtain W1  ˆµ∗, ˆµ∗ qmc  ∼  V ∗ JL α . Requiring this error to be O(ε) yields JL ∼V ∗ ε 1α and leads to the following estimate on the total cost of the DLQMC algorithm, ˆCdlqmc ∼CN + C∗ V∗ ε 1 α , (3.13) with C, C∗being the computational cost of a single CFD forward solve and evaluation of the deep neural network L∗, respectively. Dividing (3.13) with (3.10) and using Jq ∼V ∆ ε 1α leads to the speedup estimate (3.12). □ Remark 3.4. In particular, we know that the cost C = O  ∆− 1 d+1  is very high for a small mesh size ∆. On the other hand the cost C∗= O(M) with M being the number of neurons (2.10) in the network. This cost is expected to be much lower than C. Moreover, it is reasonable to expect that V∗≈V∆. As long as we can guarantee that N << Jq, it follows from (3.12) that the speedup Σdlqmc, which measures the gain of using the deep learning based DLQMC algorithm 3.2 over the baseline Quasi-Monte Carlo method, can be quite substantial. ■ 13 4 Implementation of ensemble training In the following numerical experiments, we use fully connected neural networks with ReLU activation functions. Essential hyperparameters are listed in table 2. For each experiment, we use a reference network architecture i.e number of hidden layers and width per layer, such that the total network size is O(d + N). This ensures consistency with the prescriptions of lemma 2.4. We use two choices of the exponent p in the loss function (2.11) i.e either p = 1 or p = 2, denoted as mean absolute error (MAE) or mean square error (MSE), respectively. Similarly the exponent q in the regularization term (2.12) is either q = 1 or q = 2. In order to keep the ensemble size tractable, we chose four diﬀerent values of the regularization parameter λ in (2.12) i.e, λ = 7.8 × 10−5, 7.8 × 10−6, 7.8 × 10−7, 0, corresponding to diﬀerent orders of magnitude for the regularization parameter. For the optimizer of the loss function, we use either a standard SGD algorithm or ADAM. Both algorithms are used in the full batch mode. This is reasonable as the number of training samples are rather low in our case. A key hyperparameter is the starting value ¯θ in the SGD or ADAM algorithm (2.13). We remark that the loss function (2.12) is non-convex and we can only ensure that the gradient descent algorithms converge to a local minimum. Therefore, the choice of the initial guess is quite essential as diﬀerent initial starting values might lead to convergence to local minima with very diﬀerent loss proﬁles and generalization properties. Hence, it is customary in machine learning to retrain i.e, start with not one, but several starting values and run the SGD or ADAM algorithm in parallel. Then, one has to select one of the resulting trained networks. The ideal choice would be to select the network with the best generalization error. However, this quantity is not accessible with the data at hand. Hence, we rely on the following surrogates for predicting the best generalization error, 1. Best trained network. (train): We choose the network that leads to the lowest training error (2.25), once training has been terminated. 2. Best validated network. (val): We choose the network that leads to the lowest validation loss (2.26), once training has been terminated. One disadvantage of this approach is to sacriﬁce some training data for the validation set. 3. Best trained network wrt mean. (mean-train): We choose the network that has minimized the error in the mean of the training set i.e, Emean := 1 N N Õ j=1 L∆(yj) −1 N N Õ j=1 L∗(yj) , ∀yj ∈S. (4.1) 4. Best trained network wrt Wasserstein. (wass-train): We choose the network that has mini- mized the error in the Wasserstein metric with respect to the training set S: Ewass := W1 1 N N Õ j=1 δL∆(yj), 1 N N Õ j=1 δL∗(yj) ! , ∀yj ∈S. (4.2) We note that Wasserstein metric for measures on R can be very eﬃciently computed with the Hungarian algorithm [37]. Another hyperparameter (property) is whether we choose randomly distributed points (Monte Carlo) or more uniformly distributed points, such as quasi-Monte Carlo (QMC) quadrature points, to select the training set S. We use both sets of points in the numerical experiments below in order to ascertain which choice is to be preferred in practice. Other important hyperparameters in machine learning are the learning rate in (2.13) and the number of training epochs. Unless otherwise stated, we keep them ﬁxed to the values ηr == 0.01, ∀r, for both the SGD and ADAM algorithms and to 500000 epochs, respectively. 4.1 Code All the following numerical experiments use ﬁnite volume schemes as the underlying CFD solver. The machine learning and ensemble training runs are performed by a collection of Python scripts and Jupyter 14 notebooks, utilizing Keras [57] and Tensorﬂow [58] for machine learning and deep neural networks. The ensemble runs over hyperparameters are parallelized as standalone processes, where each process trains the network for one choice of hyperparameters. Jupyter notebooks for all the numerical experiments can be downloaded from https://github.com/kjetil-lye/learning_airfoils, under the MIT license. A lot of care is taken to ensure reproducability of the numerical results. All plots are labelled with the git commit SHA code, that produced the plot, in the upper left corner of the plot. (0,0) (1,0) SU(x) SL(x) (a) Airfoil shape (b) Primary mesh (c) Dual mesh Figure 2: The shape of the RAE airfoil and the underlying primary and voronoi dual meshes. 5 Numerical results In the following numerical experiments, we consider the compressible Euler equations of gas dynamics as the underlying PDE (1.1). In two dimensions, these equations are Ut + Fx(U)x + Fy(U)y = 0, U = ©­­­ « ρ ρu ρv E ª®®® ¬ , Fx(U) = ©­­­ « ρu ρv2 + p ρuv (E + p)u ª®®® ¬ , Fy(U) = ©­­­ « ρv ρuv ρv2 + p (E + p)v ª®®® ¬ (5.1) where ρ, u = (u, v) and p denote the ﬂuid density, velocity and pressure, respectively. The quantity E represents the total energy per unit volume E = 1 2 ρ|u|2 + p γ −1, where γ = cp/cv is the ratio of speciﬁc heats, chosen as γ = 1.4 for our simulations. Additional important variables associated with the ﬂow include the speed of sound a = p γp/ρ, the Mach number M = |u|/a and the ﬂuid temperature T evaluated using the ideal gas law p = ρRT, where R is the ideal gas constant. 5.1 Flow past an airfoil 5.1.1 Problem description. We consider ﬂow past an RAE 2822 airfoil, whose shape is shown in Figure 2 (Left). The ﬂow past this airfoil is a benchmark problem for UQ in aerodynamics [25]. The upper and lower surface of the airfoil are denoted at (x, SU(x)) and (x, SL(x)), with x ∈[0, 1]. We consider the following parametrized (free-stream) initial conditions and perturbed airfoil shapes T∞(y) = 1 + ε1G1(y), M∞(y) = 0.729(1 + ε1G2(y)), α(y) = [2.31(1 + G3(y))]◦, p∞(y) = 1 + ε1G4(y), ˆSU(x; y) = SU(x)(1 + ε2G5(y)), ˆSL(x; y) = SL(x)(1 + ε2G6(y)), (5.2) where α is the angle of attack, the gas constant R = 1, ε1 = 0.1, ε2 = 0.2, y ∈Y = [0, 1]6 is a parameter and Gk(y) = 2yk −1 for k = 1, ..., 6. 15 The total drag and lift experienced on the airfoil surface ˆS(y) = ˆSL(y) Ð ˆSU(y) are chosen as the observables for this experiment. These are expressed as Lift = L1(y) = 1 K(y) ∫ ˆS(y) p(y)(n(y) · ψl(y))ds, Drag = L2(y) = 1 K(y) ∫ ˆS(y) p(y)(n(y) · ψd(y))ds, (5.3) where K(y) = ρ∞(y)|u∞(y)|2/2 is the free-stream kinetic energy, while ψl(y) = [−sin(α(y)), cos(α(y))] and ψd(y) = [cos(α(y)), sin(α(y))]. Layer Width (Number of Neurons) Number of parameters Hidden Layer 1 12 84 Hidden Layer 2 12 156 Hidden Layer 3 10 130 Hidden Layer 4 12 132 Hidden Layer 5 10 130 Hidden Layer 6 12 156 Hidden Layer 7 10 130 Hidden Layer 8 10 110 Hidden Layer 9 12 132 Output Layer 1 13 1149 Table 3: Reference neural network architecture for the ﬂow past airfoil problem. The domain is discretized using triangles to form the primary mesh, while the control volumes are the voronoi dual cells formed by joining the circumcenters of the triangles to the face mid-points. A zoomed view of the primary and dual meshes are shown in Figure 2. The solutions are approximated using a second-order ﬁnite volume scheme, with the mid-point rule used as the quadrature to approximate the cell-boundary integrals. The numerical ﬂux is chosen as the kinetic energy preserving entropy stable scheme [43], with the solutions reconstructed at the cell-interfaces using a linear reconstruction with the minmod limiter . For details on ﬂux expression and the reconstruction procedure, we refer interested readers to [43]. The scheme is integrated using the SSP-RK3 time integrator. The contour plots for the Mach number with two diﬀerent realizations of the parameter y ∈[0, 1]6 are shown in Figure 3. It should be emphasized that the stochastic perturbations in (5.2) are rather strong, representing on an average a 10 −20 percent standard deviation (over mean). Hence, there is a signiﬁcant statistical spread in the resulting solutions. As shown in ﬁgure 3, one of the conﬁgurations lead to a transsonic shock around the airfoil while another leads to a shock-free ﬂow around the airfoil. 5.1.2 Generation of training data. In order to generate the training, validation and test sets, we denote Qsob as the ﬁrst 1001 Sobol points on the domain [0, 1]6. For each point yj ∈Qsob, the maps or (rather their numerical surrogate) L∆ 1,2(yj) is then generated from high-resolution numerical approximations U∆(y) and the corresponding lift and drag are calculated by a numerical quadrature . This forms the test set T. For this numerical experiment, we take the ﬁrst 128 Sobol points to generate the training set S and the next 128 points to generate the validation set V. 5.1.3 Results of the Ensemble training procedure. To set up the ensemble training procedure of the previous section, we select a fully connected network, with number of layers and size of each layer listed in table 3. Our network has 1149 tunable parameters and this choice is clearly consistent with the prescriptions of Lemma 2.4. Varying the hyperparameters as described in section 4 led to a total of 114 conﬁgurations and each was retrained 5 times with diﬀerent random initial choices of the weights and the biases. Thus, we trained a total of 570 networks in parallel. 16 (a) Sample 1 (b) Sample 2 Figure 3: Numerical solutions (Mach number) for the ﬂow past an RAE airfoil, with two diﬀerent realizations of the parameter y. For each conﬁguration, we select the retraining that minimizes the set selection criteria for the conﬁg- uration. Once the network with best retraining is selected, it is evaluated on the whole test set T = Qsob and the percentage relative L2 prediction error (2.21) computed. Histograms, describing the probability distribution over all hyperparameter samples for the lift and the drag are shown in ﬁgure 4. From this ensemble, we choose the hyperparameter conﬁguration that led to the smallest prediction error and list them in table 4. As seen from this table, there is a diﬀerence in the best performing network correspond- ing to lift and to drag. However, both networks use ADAM as the optimizer and regularize with the L2-norm of the weights in the loss function (2.12). We plot the training (and validation) loss, for these Obs Opt Loss L1-reg L2-reg Selection BP. Err mean (std) Lift ADAM MSE 0.0 7.8 × 10−6 wass-train 0.786 (0.010) Drag ADAM MAE 0.0 7.8 × 10−6 mean-train 1.847 (0.022) Table 4: The hyperparameter conﬁgurations that correspond to the best performing network (one with least mean prediction error) for the two observables of the ﬂow past airfoils. best performing networks with respect to the number of epochs of the ADAM algorithm in ﬁgure 5 (Top) and observe a three orders (for lift) and two orders (for drag) reduction in the underlying loss function during training. From ﬁgure 5 (Bottom) and table 4, we note that the prediction errors for the best performing networks are very small i.e, less than 1% relative error for the lift and less than 2% relative error for the drag, even for the relatively small number (128) of training samples. This is orders of magnitude less than the sum of sines regression problem (table 1). These low prediction errors are particularly impressive when viewed in term of the theory presented in section 2.3. A crude upper bound on the generalization error is given by q M N , with M, N being the number of parameters in the network and number of training samples respectively. Setting these numbers for this problem yields a relative error of approximately 300%. However, our prediction errors are 100−200 times (two orders of magnitude) less, demonstrating signiﬁcant compression in the trained networks [2]. These results are further contextualized, when considered with reference to the computational costs, shown in table 5. In this table, we present the costs of generating a single sample, the average training time (over a subset of hyperparameter conﬁgurations) and the time to evaluate a trained network. Each sample is generated on the EULER high-performance cluster of ETH- 17 (a) Lift (b) Drag Figure 4: Histograms depicting distribution of the percentage relative mean L2 prediction error (2.21) (X-axis) over number of hyperparameter conﬁgurations (samples, Y-axis) for the lift and the drag in the ﬂow past airfoil problem. Note that the range of X-axis is diﬀerent in the plots. (a) Lift, Best Performing (b) Drag, Best Performing (c) Lift, Best Performing (d) Drag, Best Performing Figure 5: Training loss (2.25) and Validation loss (2.26) (Top) and prediction errors (Ground truth vs. prediction) (Bottom) for the best performing (table 4) neural networks for the ﬂow past airfoils problem as a function of the number of epochs. 18 Time (in secs) Sample generation 24000 Training (Lift) 700 Evaluation (L1) 9 × 10−6 Training (Drag) 840 Evaluation (L2) 10−5 Table 5: Computational times (cost) of (single) sample generation, network training and (single) network evaluation, for the ﬂow past airfoils problem. Each sample was generated using a parallelized ﬁnite- volume solver on 16 Intel(R) Xeon(R) Gold 5118 @ 2.30GHz processor cores. The entire run took 1500 secs on the cluster and translates into a total wall clock time of 1500 × 16 = 24000 secs. The training and evaluation of the neural networks were all performed on a Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz machine. The training and evaluation times are approximations of the average runtimes over all hyperparameter conﬁgurations. Zurich. In this case, the training time is a very small fraction of the time to generate a single sample, let alone the whole training set whereas the computational cost of evaluating the neural networks is almost negligible as it is nine orders of magnitude less than evaluating a single training sample. A priori, it is unclear if the underlying problem has some special features that makes the compression, and hence high accuracy possible. In particular, learning an aﬃne map is relatively straightforward for a neural network. To test this possibility, we approximate the underlying parameters to observable map, for both lift and drag, with classical linear least squares algorithm i.e, Llsq j : R6 →R be aﬃne map deﬁned by L∗,lsq j = arg min   Õ y∈S ( f (y) −Lj(y))2 | f : R6 →R is aﬃne   , j = 1, 2. (5.4) The resulting convex optimization problem is solved by the standard gradient descent. The errors with the linear least squares algorithm are shown in ﬁgure 4. From this ﬁgure we can see that the prediction error with the linear least squares is approximately 4% for lift and 36% for drag. Hence, the best performing neural networks are approximately 5 times more eﬃcient for lift and 20 times more eﬃcient for drag, than the linear least squares algorithm. The gain over linear least squares implicitly measures how nonlinear the underlying map is. Clearly in this example, the drag is more nonlinear than the lift and thus more diﬃcult to approximate. 5.1.4 Network sensitivity to hyperparameters. We summarize the results of the sensitivity of network performance to hyperparameters below, • Overall sensitivity. The overall sensitivity to hyperparameters is depicted as histograms, represent- ing the distribution, over samples (hyperparameter conﬁgurations), shown in ﬁgure 4. As seen from the ﬁgure, the prediction errors with diﬀerent hyperparameters are spread over a signiﬁcant range for each observable. There are a few outliers which perform very poorly, particularly for the drag. On the other hand, a large number of hyperparameter conﬁgurations concentrate around the best performing network. Moreover all (most) of the hyperparameter choices led to better prediction errors for the lift (drag), than the linear least squares algorithm. • Choice of optimizer. The diﬀerence between ADAM and the standard SGD algorithm is shown in ﬁgure 6. As in the Sod shock tube problem, ADAM is clearly superior to the standard SGD algorithm. Moreover, there are a few outliers with the ADAM algorithm that perform very poorly for the drag. We call these outliers as the bad set of ADAM and they correspond to the conﬁgurations with MSE loss function and L1 regularization, or L2 regularization, with a regularization parameter λ > 10−5, or MAE loss function without any regularization. The diﬀerence between the bad set of ADAM and its complement i.e, the good set of ADAM is depicted in ﬁgure 7. 19 • Choice of loss function. The diﬀerence between L1 mean absolute error (MAE) and L2 root mean square error (MSE) i.e, exponent p = 1 or p = 2 in the loss function (2.11) is plotted in the form of a histogram over the error distributions in ﬁgure 8. The diﬀerence in performance, with respect to choice of loss function is minor. However, there is an outlier, based on MSE, for the drag. • Choice of type of regularization. The diﬀerence between choosing an L1 or L2 regularization term in (2.12) is shown in ﬁgure 9. Again, the diﬀerences are minor but the L2 regularization seems to be slightly better, particularly for the drag. • Value of regularization parameter. The variation of network performance with respect to the value of the regularization parameter λ is shown in ﬁgure 11. We observe from this ﬁgure that a little amount of regularization works better than no regularization. However, the variation in prediction errors wrt respect to the non-zero values of λ is minor. • Choice of selection criteria. The distribution of prediction errors with respect to the choice of selection criteria for retraining, namely train,val,mean-train,wass-train is shown in ﬁgure 10. There are very minor diﬀerences between mean-train and wass-train. On the other hand, both these criteria are clearly superior to the other two selection criteria. • Sensitivity to retrainings. The variation of performance with respect to retrainings i.e, diﬀerent random initial starting values for ADAM, is provided in table 6. In this table, we list the minimum, maximum, mean and standard deviation of the relative prediction error, over 5 retrainings, for the best performing networks for each observable. As seen from this table, the sensitivity to retraining is rather low for the lift. On the other hand, it is more signiﬁcant for the drag. • Variation of Network size. We consider the dependence of performance with respect to variation of network size by varying the width and depth for the best performing networks. The resulting prediction errors are shown in table 7 where a 3 × 3 matrix for the errors (rows represent width and columns depth) is tabulated. All the network sizes are consistent with the requirements of lemma 2.4. As observed from the table, the sensitivity to network size is greater for the drag than for the lift. It appears that increasing the width for constant depth results in lower errors for the drag. On the other hand, training clearly failed for some of the larger networks as the number of training samples appears to be not large enough to train these larger networks. 5.1.5 Network performance with respect to number of training samples. All the above results were obtained with N = 128 training samples. We have repeated the entire ensemble training with diﬀerent samples sizes i.e N = 32, 64, 256 also and present a summary of the results in ﬁgure 12, where we plot the mean (percentage relative) L2 prediction error with respect to the number of samples. For each sample size, three diﬀerent errors are plotted, namely the minimum (maximum) prediction errors and the mean prediction error over a subset of the hyperparameter space, that omits the bad set of ADAM as the optimizer. Moreover, we only consider errors with selected (optimal) retrainings in each case. As seen from the ﬁgure, the prediction error decreases with sample size N as predicted by the theory. The decay seems of the of form  Ui N αi with α1 = 0.81 and U1 = 0.77 for the lift and α2 = 0.92 and U2 = 3.42 for the drag. In both cases, the decay of error with respect to samples, is at a higher rate than that predicted by the theory in (2.22). Moreover, the constants are much lower than the number of parameters in the network. Both facts indicate a very high degree of compressibility (at least in this possible pre-asymptotic regime) for the trained networks and explains the unexpectedly low prediction errors. 5.1.6 UQ with deep learning Next, we approximate the underlying probability distributions (measures) (3.1), with respect to the lift and the drag in the ﬂow past airfoils problem with the DLQMC algorithm 3.2. A reference measure is computed from the whole test set of 1001 samples and the corresponding histograms are shown in ﬁgure 13 to visualize the probability distributions. In the same ﬁgure, we also plot corresponding histograms, 20 (a) Lift (b) Drag Figure 6: Histograms for the relative prediction errors comparing the ADAM and SGD algorithms on the ﬂow past airfoils. (a) Lift (b) Drag Figure 7: Histograms of the relative prediction errors, comparing Good ADAM and bad ADAM, for the ﬂow past airfoils. (a) Lift (b) Drag Figure 8: Histograms for the relative prediction errors, comparing the mean absolute error and mean square error loss functions, for the ﬂow past airfoils 21 (a) Lift (b) Drag Figure 9: Histograms for the relative prediction error, comparing L1 and L2 regularizations of the loss function, for the ﬂow past airfoils (a) Lift (b) Lift (c) Lift (d) Drag (e) Drag (f) Drag Figure 10: Network performance with respect to selection criteria for retrainings (see section 4 i.e, train,val,mean-train,wass-train for the ﬂow past airfoils problem. We compare wass-train with each of the other three criteria. Histograms for prediction error (X-axis) with number of conﬁgurations (Y-axis) are shown. Obs Err (min) Err (max) Err (mean) Err (std) Lift 0.786 1.336 1.004 0.187 Drag 1.847 8.016 3.841 2.332 Table 6: Sensitivity of the best performing networks (listed in table 4) to retrainings i.e starting values for ADAM for ﬂow past airfoil. All errors are relative mean L2 prediction error (2.21) in percentage and we list the minimum, maximum, mean and standard deviation of the error over 5 retrainings. 22 Width/Depth 6 12 24 4 [1.21, 4.37] [0.83, 1.74] [1.09, 2.48] 8 [0.84, 3.66] [0.86, 2.02] [0.81, 1.69] 16 [0.80, ¬] [¬, 74.72] [¬, 5.11] Table 7: Performance of trained neural networks with respect to the variation of network size for the ﬂow past airfoils problem. The rows represent variation with respect to Width (number of neurons per hidden layer) and the columns represent variation with respect to Depth (number of hidden layers). For each entry of the Width-Depth matrix, we tabulate the vector of relative percentage mean prediction error for the best performing networks. The components of each vector represent error in [Lift, Drag] and ¬ is used to indicate that the training procedure failed for this particular conﬁguration. (a) Lift (b) Drag Figure 11: Variation of prediction error (Y-axis) with respect to the size of the regularization parameter λ in (2.12) (X-axis) for the ﬂow past airfoils problems. The minimum and mean of prediction error (over all hyperparameter conﬁgurations) is shown (a) Lift (b) Drag Figure 12: Relative percentage mean square prediction error (2.21) (Y-axis) for the neural networks approximating the ﬂow past airfoils problem, with respect to number of training samples (X-axis). For each number of training samples, we plot the mean, minimum and maximum (over hyperparameter conﬁgurations) of the prediction error. Only the selected (optimal) retraining is shown. 23 computed with the best performing networks for each observable (listed in table 4) and observe very good qualitative agreement between the outputs of the DLQMC algorithm and the reference measure. In order to quantify the gain in computational eﬃciency with the DLQMC algorithm over the baseline QMC algorithm in approximating probability distributions, we will compute the speedup. To this end, we observe from ﬁgure 14 that the baseline QMC algorithm converges to the reference probability measure with respect to the Wasserstein distance (3.9), at a rate of α = 0.81, for both the lift and drag. Next, from table 5, we see that once the training samples have been generated, the cost of training the network and performing evaluations with the trained network is essentially free. Consequently, if the number of training samples is N, we compute a raw speed-up (for each observable) given by, σraw i,dlqmc := W1  ˆµi,qmc,N, ˆµi∆ W1  ˆµ∗ i,qmc, ˆµi∆ . (5.5) Here for i = Lift, Drag, ˆµ∆ i is the reference measure (probability distribution) computed from the test set T, ˆµi,qmc,N is the measure (3.3) with the baseline QMC algorithm and N QMC points, and ˆµ∗ i,qmc is the measure computed with the DLQMC algorithm (3.11). The real speed up is then given by, σreal i,dlqmc =  σraw i,dlqmc  1 αi , (5.6) in order to compensate for the rate of convergence of the baseline QMC algorithm. In other words, our real speed up compares the costs of computing errors in the Wasserstein metric, that are of the same magnitude, with the DLQMC algorithm vis a vis the baseline QMC algorithm. Henceforth, we denote σreal dlqmc as the speedup and list the speedups (for each observable) with the DLQMC algorithm for N = 128 training samples in table 8. We observe from this table that we obtain speedups ranging between half an order to an order of magnitude for the lift and the drag. The speedups for drag are slightly better than that for lift. In ﬁgure 15, we plot the speedup of the DLQMC algorithm (over the baseline QMC algorithm) as a function of the number of training samples in the deep learning algorithm 2.1. As seen from the ﬁgure, the best speed ups are obtained in case of 64 training samples for the lift and 128 training samples for the drag. This is on account of a complex interaction between the prediction error which decreases (rather fast) with the number of samples and the fact that the errors with the baseline QMC algorithm also decay with an increase in the number of samples. 5.1.7 Comparison with Monte Carlo algorithms We have computed a Monte Carlo approximation of the probability distributions of the lift and the drag by randomly selecting N = 128 points from the parameter domain Y = [0, 1]6 and computing the probability measure ˆµmc = 1 N ÍN j=1 δL∆(yj). We compute the Wasserstein error W1(ˆµmc, ˆµ∆), with respect to the reference measure ˆµ∆and divide it with the error obtained with the DLQMC algorithm to obtain a raw speedup of the DLQMC algorithm over MC. As MC errors converge as a square root of the number of samples, the real speedup of DLQMC over MC is calculated by squaring the raw speedup. We present this real speedup over MC in table 9 and observe from that the speedups of the DLQMC algorithm over the baseline MC algorithm are very high and amount to at least two orders of magnitude. This is not surprising as the baseline QMC algorithm signiﬁcantly outperforms the MC algorithm for this problem. Given that the DLQMC algorithm was an order of magnitude faster than the baseline QMC algorithm, the cumulative eﬀect leads to a two orders of magnitude gain over the baseline MC algorithm. 5.1.8 Choice of training set. For the sake of comparision with our choice of Sobol points to constitute the training set S in this problem, we chose N = 128 random points in Y as the training set. With this training set, we repeated the ensemble training procedure and found the best performing networks. The relative percentage mean L2-prediction error (2.21) (with respect to a test set of 320 random points) was computed and is presented 24 in table 10. As seen from this table, the prediction errors with respect to the best performing networks are considerably (an order of magnitude) higher than the corresponding errors obtained for the QMC training points (compare with table 4). Hence, at least for this problem, Sobol points are a much better choice for the training set than random points. An intuitive reason for this could lie in the fact that the Sobol points are better distributed in the parameter domain than random points. So on an average, a network trained on them will generalize better to unseen data, than it would if trained on random points. (a) Lift (b) Drag Figure 13: Empirical histograms representing the probability distribution (measure) for the lift and the drag in the ﬂow past airfoils problem. We compare the reference histograms (computed with the test set T) and the histograms computed with the DLQMC algorithm. (a) Lift (b) Drag Figure 14: Convergence of the Wasserstein distance W1  ˆµi,qmc, ˆµi∆ (Y-axis) for the of the lift and the drag for the ﬂow past airfoils, with respect to number of QMC points (X-axis) 5.2 A stochastic shock tube problem As a second numerical example, we consider a shock tube problem with the one-dimensional version of the compressible Euler equations (5.1). 25 Observable Speedup Lift 6.64 Drag 8.56 Table 8: Real speedups (5.6), for the lift and the drag in the ﬂow past airfoils problem, comparing DLQMC with baseline QMC algorithm. Observable Speedup over MC Lift 246.02 Drag 179.54 Table 9: Real speedups, for the lift and the drag in the ﬂow past airfoils problem, comparing DLQMC with the baseline MC algorithm. (a) Lift (b) Drag Figure 15: Real speedups (5.6) for the DLQMC algorithm over the baseline QMC algorithm (Y-axis) with respect to number of training samples (X-axis) for the ﬂows past airfoils problem. The maximum and mean speed up (over all hyperparameter conﬁgurations) are shown. Obs Opt Loss L1-reg L2-reg Selection BP. Err mean Lift SGD MAE 0.0 7.8 × 10−7 mean-train 8.487 Drag ADAM MAE 7.8 × 10−7 0.0 train 20.25 Table 10: The hyperparameter conﬁgurations that correspond the best performing network (one with least mean prediction error) for the two observables of the ﬂow past airfoils, trained on random (Monte Carlo) training set. 26 5.2.1 Problem description The underlying computational domain is [−5, 5] and initial conditions are prescibed in terms of left state (ρL, uL, pL) and a right state (ρR, uR, pR) at the initial discontinuity x0. These states are deﬁned as, ρL = 0.75 + 0.45G1(y) ρR = 0.4 + 0.3G2(y) wL = 0.5 + 0.5G3(y) wR = 0 pL = 2.5 + 1.6G4(y) pR = 0.375 + 0.325G5(y) x0 = 0.5G6(y). (5.7) y ∈Y = [0, 1]6 is a parameter and Gk(y) = 2yk −1 for k = 1, ..., 6. We use the Lebesgue measure on Y as the underlying measure. The observables for this problem are chosen as the average integral of the density over ﬁxed intervals Lj(y) = 1 |Ij| ∫ Ij ρ(x,Tf ; y)dx, j = 1, 2, 3, (5.8) where I1 = [−1.5, −0.5], I2 = [0.8, 1.8], I3 = [2, 3] are the underlying regions of interest. A priori, this problem appears to be simpler as it is only one-dimensional when compared to the two-dimensional airfoil. However, the diﬃculty stems from the large variance of the initial data. As seen from (5.7), the ratio of the standard deviation to the mean, for the underlying variables, is very high and can be as high as 100%. This is almost an order of magnitude larger than the corresponding initial stochasticity of the ﬂow past airfoil problem and we expect this large initial variance to be propagated into the solution. This is indeed borne out by the results shown in ﬁgure 16, where we plot the mean (and standard deviation) of the density, both initially and at time Tf = 1.5. Moreover, the initial stochastic inputs span a wide range of possible scenarios. Particular choices of the parameters i.e, (y1, y2, y3, y5, y6) = (7/9, 1/24, 0, 1/32, 1/13, 0) and (y1, y2, y3, y5, y6) = (0.1611, 2/3, 0.698, 0.8213, 0.8015, 0), correspond to the classical Sod shock tube and Lax shock tube problems [16]. Thus, we can think of this stochastic shock tube problem as more or less generic (scaled) Riemann problem for the Euler equations. In a recent paper [32], the authors have analyzed the role of the underlying variance in the general- ization errors for deep neural networks trained to regress functions. In particular, high variance can lead to large generalization errors. In this sense, this shock tube problem might be harder to predict with neural networks than the ﬂow past airfoil. (a) T = 0 (b) T = 1.5 Figure 16: Mean (± standard deviation) for the density of the stochastic shock tube problem. 5.2.2 Generation of training data. In order to generate the training and test sets, we denote Qsob as the ﬁrst 16256 Sobol points on the domain [0, 1]6. The training set S consists of the ﬁrst 128 of these Sobol points whereas as the test 27 set is T = Qsob. For each point yj ∈Qsob, the maps or (rather their numerical surrogate) L∆ 1,2,3(yj) are generated from high-resolution numerical approximations U∆(y) obtained using a second-order ﬁnite volume scheme. The domain is discretized into a uniform mesh of K = 2048 disjoint intervals, with the HLL numerical ﬂux and with the left and right cell-interface values obtained using a second-order WENO reconstruction. Time marching is performed using the second-order Runge-Kutta method with a CFL number of 0.475. Once the ﬁnite volume solution is computed, the corresponding observables (5.8) at each y are approximated as L∆ j (y) = Í i∈Λj ρi(y) #(Λj) , Λj = {i | xi ∈Ij}, j = 1, 2, 3, (5.9) where xi is the barycenter of the cell Ωi. 5.2.3 Results. We follow the ensemble training procedure outlined in section 4. The network architecture, speciﬁed in table 3 is used together with the 114 hyperparameter conﬁgurations of the previous section, with 5 retrainings for each hyperparameter conﬁguration. The best performing networks for each observable are identiﬁed as the ones with lowest prediction error (2.21) and these networks are presented in table 11. From the table, we observe very slight diﬀerences for the best performing networks for the three observables. However, these networks are indeed diﬀerent from the best performing networks for the ﬂow past airfoil (compare with table 4). Moreover, the predictions errors are larger than the ﬂow past airfoil case, ranging between 2% for the observable L1 to approximately 6% for the other two observables. This larger error is expected as the variance of the underlying maps is signiﬁcantly higher. Nevertheless, a satisfactory prediction accuracy is obtained for only 128 training samples. Results of the ensemble training Obs Opt Loss L1-reg L2-reg Selection BP. Err mean L1 ADAM MSE 7.8 × 10−6 0 wass-train 2.212 L2 ADAM MSE 7.8 × 10−6 0 wass-train 6.575 L3 ADAM MSE 7.8 × 10−5 0 mean-train 5.467 Table 11: The hyperparameter conﬁgurations that correspond to the best performing network (one with least mean prediction error) for the three observables of the stochastic shock tube problem. procedure are depicted in the form of histograms for the prediction errors in ﬁgure 17. From this ﬁgure, we observe that although there are some outliers, most of the hyperparameter conﬁgurations resulted in errors comparable to the best performing networks (see table 11). Furthermore, a large majority of the hyperparameters resulted in substantially (a factor of 3 −4) smaller prediction errors, when compared to the linear least squares regression (5.4). A sensitivity study (not presented here) indicates very similar robustness results as for the ﬂow past the airfoil. In ﬁgure 18, we plot the prediction error with respect to the number of samples and observe that the mean error (over hyperparameter conﬁgurations) decays with an exponent of 0.5 for the observables L2,3. On the other hand, the decay is a bit slower for L1. However, the low value of the constants still indicates signiﬁcant compression allowing us to approximate this rather intricate problem with accurate neural networks. Finally, we compute the underlying probability distributions for each observable with the DLQMC algorithm 3.2. In ﬁgure 19, we plot the maximum and mean real speedup (5.6), over hyperparameter conﬁgurations, for each observable, with respect to the number of training samples. From this ﬁgure, we observe maximum speedups of about 16 for the observables L1,2 and 5 for the observable L3, indicating that the DLQMC algorithm is signiﬁcantly superior to the baseline QMC algorithm, even for this problem. 28 (a) L1 (b) L2 (c) L3 Figure 17: Histograms depicting distribution of the percentage relative mean L2 prediction error (2.21) (X- axis) over number of hyperparameter conﬁgurations (samples, Y-axis) for the observables of the stochastic shock-tube problem. (a) L1 (b) L2 (c) L3 Figure 18: Relative percentage mean square prediction error (2.21) (Y-axis) for the neural networks approximating the stochastic shock tube problem, with respect to number of training samples (X-axis). For each number of training samples, we plot the mean, minimum and maximum (over hyperparameter conﬁgurations) of the prediction error. Only the selected (optimal) retraining is shown. (a) L1 (b) L2 (c) L3 Figure 19: Real speedups (5.6) for the DLQMC algorithm over the baseline QMC algorithm (Y-axis) with respect to number of training samples (X-axis) for the stochastic shock tube problem. The maximum and mean speed up (over all hyperparameter conﬁgurations) are shown. 29 6 Discussion Problems in CFD such as UQ, Bayesian Inversion, optimal design etc are of the many query type i.e, solving them requires evaluating a large number of realizations of the underlying input parameters to observable map (2.3). Each realization involves a call to an expensive CFD solver. The cumulative total cost of the many-query simulation can be prohibitively expensive. In this paper, we have tried to harness the power of machine learning by proposing using deep fully connected neural networks, of the form (2.6), to learn and predict the underlying parameters to observable map. However, a priori, this constitutes a formidable challenge. In section 2.3, we argue by a combination of theoretical considerations and numerical experiments that the crux of this challenge it to ﬁnd neural networks to learn maps of low regularity in a data poor regime. We are in this regime as the many parameters to observable maps (2.3) in ﬂuid dynamics, can be at best, Lipschitz continuous. Moreover, given the cost, we can only aﬀord to compute a few training samples. We overcome these diﬃculties with the following novel ideas, ﬁrst, we chose to focus on learning observables rather than the full solution of the parametrized PDE (2.1). Observables are more regular than ﬁelds (see section 2.3) and might have less underlying variance and are thus, easier to learn, see [32] for a more recent theoretical justiﬁcation. Second, we propose using low-discrepany sequences to generate the training set S. The equi-distribution property of these points might ensure lower generalization errors than those resulting from training data generated at random points, see the forthcoming paper [34] for further details. Finally, we propose an ensemble training procedure (see section 4) to systematically scan the hyperparameter space in order to winnow down the best performing networks as well as to test sensitivity of the trained networks to hyperparameter choices. We presented two prototypical numerical experiments for the compressible Euler equations in order to demonstrate the eﬃcacy of our approach. We observed from the numerical experiments that the trained networks were indeed able to provide low prediction errors, despite being trained on very few samples. Moreover, a large number of hyperparameter conﬁgurations were close in performance to the best per- forming networks, indicating a signiﬁcant amount of robustness to most hyperparameters. More crucially, the obtained generalization errors followed the theoretical predictions of section 2.3 and indicating a high degree of compression, explaining why the networks generalized well. It is essential to state that these low prediction errors are obtained even when the cost of evaluating the neural network is several orders of magnitude lower than the full CFD solve. As a concrete application of our deep learning algorithm 2.1, we combined it with a Quasi-Monte Carlo (QMC) method to propose a deep learning QMC algorithm 3.2, for performing forward UQ, by approximating the underlying probability distribution (3.1) of the observable. In theorem 3.3, we prove that the DLQMC algorithm is guaranteed to out-perform the baseline QMC algorithm as long as the underlying neural network generalizes well. This is indeed borne out in the numerical experiments, where we observed about an order of magnitude speedup of the DLQMC algorithm over the corresponding QMC algorithm and two orders of magnitude speedup over a Monte Carlo algorithm. Thus, we have demonstrated the viability of using neural networks to learn observables and to solve challenging problems of the many-query type. Our approach can be compared to a standard model order reduction algorithm [52]. Here, generating the training data and training the networks is the oﬄine step whereas evaluating the network constitutes the online step. Our results compare favorably with attempts to use Model order reduction for hyperbolic problems [1, 9], particularly when it comes to the low costs of training and evaluation. On the other hand, typical MOR algorithms provide a surrogate for the solution ﬁeld, where we only predict the observable of interest in the approach presented here. Our results can be extended in many directions; we can adapt this setting to learn the full solution ﬁeld, instead of the observables. However, this might be harder in practice as the ﬁeld is even less regular than the observable. Our approach is entirely data driven, in the sense that we do not use any speciﬁc information about the underlying parameterized PDE (2.1). Thus, the approach can be readily extended to other forms of (2.1), for instance the Navier-Stokes equations or other elliptic and parabolic PDEs. Finally, we plan to apply the trained neural networks in the context of Bayesian inverse problems and shape optimization under uncertainty, in forthcoming papers. 30 Acknowledgements The research of SM is partially support by ERC Consolidator grant (CoG) NN 770880 COMANFLO. A large proportion of computations for this paper were performed on the ETH compute cluster EULER. References [1] R. Abgrall and R. Crisovan. Model reduction using L1-norm minimization as an application to nonlinear hyperbolic problems. Internat. J. Numer. Methods Fluids 87 (12), 2018, 628651. [2] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 254-263. PMLR, Jul 2018. [3] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans. Inform. Theory, 39(3), 930-945, 1993. [4] H. Bijl, D. Lucor, S. Mishra and Ch. Schwab. (editors). Uncertainty quantiﬁcation in computational ﬂuid dynamics., Lecture notes in computational science and engineering 92, Springer, 2014. [5] H. B¨olcskei, P. Grohs, G. Kutyniok, and P. Petersen. Optimal approximation with sparsely connected deep neural networks. arXiv preprint, available from arXiv:1705.01714, 2017. [6] A. Borzi and V. Schulz. Computational optimization of systems governed by partial diﬀerential equations. SIAM (2012). [7] H. J. Bungartz and M. Griebel. Sparse grids. Acta Numer., 13, 2004, 147-269. [8] R. E. Caﬂisch. Monte Carlo and quasi-Monte Carlo methods. Acta. Numer., 1, 1988, 1-49. [9] Crisovan, R.; Torlo, D.; Abgrall, R.; Tokareva, S. Model order reduction for parametrized nonlinear hyperbolic problems as an application to uncertainty quantiﬁcation. J. Comput. Appl. Math. 348 (2019), 466489 [10] G. Cybenko. Approximations by superpositions of sigmoidal functions. Approximation theory and its applications., 9 (3), 1989, 17-28 [11] Constantine M. Dafermos. Hyperbolic Conservation Laws in Continuum Physics (2nd Ed.). Springer Verlag (2005). [12] W. E and B. Yu. The deep Ritz method: a deep learning-based numerical algorithm for solving variational problems. Commun. Math. Stat. 6 (1), 2018, 1-12. [13] W. E, J. Han and A. Jentzen. Deep learning-based numerical methods for high-dimensional parabolic partial diﬀerential equations and backward stochastic diﬀerential equations. Commun. Math. Stat. 5 (4), 2017, 349-380. [14] W. E, C. Ma and L. Wu. A priori estimates for the generalization error for two-layer neural networks. ArXIV preprint, available from arXiv:1810.06397, 2018. [15] R. Evans et. al. De novo structure prediction with deep learning based scoring. Google DeepMind working paper, 2019. [16] U. S. Fjordholm, S. Mishra and E. Tadmor, Arbitrarily high-order accurate entropy stable essentially nonoscillatory schemes for systems of conservation laws. SIAM J. Numer. Anal., 50 (2012), no. 2, 544573. [17] U. S. Fjordholm, R. K¨appeli, S. Mishra and E. Tadmor, Construction of approximate entropy measure valued solutions for hyperbolic systems of conservation laws. Found. Comput. Math., 17 (3), 2017, 763-827. 31 [18] U. S. Fjordholm, K. O. Lye, S. Mishra and F. R. Weber, Numerical approximation of statistical solutions of hyperbolic systems of conservation laws. In preparation, 2019. [19] R. Ghanem, D. Higdon and H. Owhadi (eds). Handbook of uncertainty quantiﬁcation, Springer, 2016. [20] I. Goodfellow. Y. Bengio and A. Courville. Deep learning, MIT Press, (2016), available from http://www.deeplearningbook.org [21] Edwige Godlewski and Pierre A. Raviart. Hyperbolic Systems of Conservation Laws. Mathematiques et Applications, Ellipses Publ., Paris (1991). [22] I. G. Graham, F. Y. Kuo, J. A. Nichols, R. Scheichl, Ch. Schwab and I. H. Sloan. Quasi-Monte Carlo ﬁnite element methods for elliptic PDEs with lognormal random coeﬃcients. Numer. Math. 131 (2), 2015, 329368. [23] J. Han. A. Jentzen and W. E. Solving high-dimensional partial diﬀerential equations using deep learning. PNAS, 115 (34), 2018, 8505-8510. [24] J S. Hesthaven. Numerical methods for conservation laws: From analysis to algorithms. SIAM, 2018. [25] C. Hirsch, D. Wunsch, J. Szumbarksi, L. Laniewski-Wollk and J. pons-Prats (editors). Uncertainty management for robust industrial design in aeronautics Notes on numerical ﬂuid mechanics and multidisciplinary design (140), Springer, 2018. [26] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi- mators. Neural networks, 2(5), 359-366, 1989. [27] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1-13, 2015. [28] I. E. Lagaris, A. Likas and D. I. Fotiadis. Artiﬁcial neural networks for solving ordinary and partial diﬀerential equations. IEEE Transactions on Neural Networks, 9 (5), 1998, 987-1000. [29] L. D. Landau and E. M. Lipschitz. Fluid Mechanics, 2nd edition, Butterworth Heinemann, 1987, 532 pp. [30] Y. LeCun, Y. Bengio and G. Hinton. Deep learning. Nature, 521, 2015, 436-444. [31] R. J. LeVeque. Finite diﬀerence methods for ordinary and partial diﬀerential equations, steady state and time dependent problems. SIAM (2007). [32] K. O. Lye, S. Mishra and R. Molinaro. A Multi-level procedure for enhancing accuracy of machine learning algorithms. Preprint, 2019, available from ArXiv:1909:09448. [33] S. Mishra. A machine learning framework for data driven acceleration of computations of diﬀerential equations, Math. in Engg., 1 (1), 2018, 118-146. [34] S. Mishra and T. K. Rusch. Enhacing accuracy of machine learning algorithms by training on low-discrepancy sequences. In preparation, 2020. [35] T. P. Miyanawala and R. K, Jaiman. An eﬃcient deep learning technique for the Navier-Stokes equations: application to unsteady wake ﬂow dynamics. Preprint, 2017, available from arXiv :1710.09099v2. [36] H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions, Neural Comput., 8 (1), 1996, 164-177. [37] J. Munkres. Algorithms for the Assignment and Transportation Problems Journal of the Society for Industrial and Applied Mathematics, 5 (1), 1957, 32–38. 32 [38] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018. [39] P. Petersen and F. Voightlaender. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. ArXIV preprint, available from arXiv:1709.05289, 2018. [40] M. Raissi and G. E. Karniadakis. Hidden physics models: machine learning of nonlinear partial diﬀerential equations. J. Comput. Phys., 357, 2018, 125-141. [41] M. Raissi, A. Yazdani and G. E. Karniadakis. Hidden ﬂuid mechanics: A Navier-Stokes informed deep learning framework for assimilating ﬂow visualization data. ArXIV preprint, available from arXiv:1808.04327, 2018. [42] D. Ray and J. S, Hesthaven. An artiﬁcial neural network as a troubled cell indicator. J. Comput. Phys., 367, 2018, 166-191 [43] D. Ray, P. Chandrasekhar, U. S. Fjordholm and S. Mishra. Entropy stable scheme on two-dimensional unstructured grids for Euler equations, Commun. Comput. Phys., 19 (5), 2016, 1111-1140. [44] D. Ray. Entropy-stable ﬁnite diﬀerence and ﬁnite volume schemes for compressible ﬂows, Doctoral thesis, 2017, available from https://deepray.github.io/thesis.pdf [45] S. Ruder. An overview of gradient descent optimization algorithms. Preprint, 2017, available from arXiv.1609.04747v2. [46] C. Schwab and J. Zech. Deep learning in high dimension. Technical Report 2017-57, Seminar for Applied Mathematics, ETH Z¨urich, 2017. [47] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo- rithms. Cambridge university press, 2014. [48] A. M. Stuart. Inverse problems: a Bayesian perspective. Acta Numerica, 19, 2010, 451-559. [49] J. Tompson, K. Schlachter, P. Sprechmann and K. Perlin. Accelarating Eulerian ﬂuid simulation with convolutional networks. Preprint, 2017. Available from arXiv:1607.03597v6. [50] L. N. Trefethen. Spectral methods in MATLAB, SIAM, (2000). [51] F. Troltzsch. Optimal control of partial diﬀerential equations. AMS, (2010). [52] A. Quateroni, A. Manzoni and F. Negri. Reduced basis methods for partial diﬀerential equations: an introduction, Springer Verlag (2015). [53] C. Villani. Topics in Optimal Transportation. American Mathematical Society, Graduate Studies in Mathematics, Vol. 58 (2013) [54] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94, 2017, 103-114 [55] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Represen- tations, 2017. [56] Linfeng Zhang, De-Ye Lin, Han Wang, Roberto Car, and Weinan E. Active Learning of Uni- formly Accurate Inter-atomic Potentials for Materials Simulation. ArXIV preprint, available from arXiv:1810.11890, 2018. [57] Chollet, Fran¸cois and others Keras https://keras.io, 2015. [58] MartinAbadi et al, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems https: //www.tensorflow.org/, 2015. 33