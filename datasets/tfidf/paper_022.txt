JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1 Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation Deep Shankar Pandey 1, Hyomin Choi 2, and Qi Yu 1 Abstract—Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty- aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective- Logic framework constrains evidence to be non-negative, re- quiring specific activation functions whose geometric proper- ties can induce activation-dependent learning-freeze behavior—a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classi- fication problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models. Index Terms—Evidential Deep Learning, Fine-Grained Uncer- tainty Quantification, Subjective Logic, Zero Evidence Region I. INTRODUCTION With recent growth in computational capabilities, availabil- ity of large-scale data, and algorithmic improvements, Deep Learning (DL) models have found great success in many real- world applications such as speech recognition [1], machine translation [2], and computer vision [3]. However, these highly expressive models can easily fit the noise in the training data, leading to overconfident predictions [4]. This challenge is compounded in specialized domains (e.g., medicine, public safety, and military operations) where labeled data is limited and costly to obtain. Accurate uncertainty quantification is essential for the successful application of DL models in these domains. To this end, DL models have been augmented to become uncertainty-aware [5], [6], [7]. However, commonly used extensions require expensive sampling operations [5], [6], which significantly increase the computational costs [8]. The recently developed evidential deep learning (EDL) models bring together evidential theory [9], [10] and deep neural architectures that turn a deterministic neural network 1 Deep Shankar Pandey and Qi Yu are with Rochester Institute of Technology, Rochester, NY, USA. Email: {dp7972, qyuvks}@rit.edu 2 Hyomin Choi is with AI Lab, InterDigital, CA, USA. Email: hyomin.choi@interdigital.com 2 This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. uncertainty-aware. By leveraging the learned evidence, eviden- tial models are capable of quantifying fine-grained uncertainty that helps to identify the sources of ‘unknowns’. Furthermore, since only lightweight modifications are introduced to existing DL architectures, additional computational costs remain mini- mal. Such evidential models have been successfully extended to classification [11], regression [12], meta-learning [13], and open-set recognition [14] settings. Fig. 1: Cifar-100 Result Despite the attractive uncertainty quantification capacity, evidential mod- els often achieve com- petitive predictive perfor- mance only on relatively simple learning problems. Their performance can de- grade on more complex, large-scale datasets even in standard classification settings. As shown in Figure 1, an evidential model using ReLU activation and an evidential MSE loss [11] achieves around 36% test accuracy on CIFAR-100, nearly 40% lower than a standard softmax model. In addition, many eviden- tial variants are sensitive to architecture or hyperparameter changes, requiring careful tuning for stable performance. The experiment section provides more details on these cases. To better understand this phenomenon, we perform a the- oretical analysis of evidential learning in the standard clas- sification setting. Our results identify an activation-induced learning-freeze behavior, where the interaction between non- negative evidence parameterization and specific activation functions can map samples into “zero-evidence regions” (re- gions of vanishing evidence gradients). Importantly, this behavior arises within the design choices of the EDL framework itself. EDL couples non-negative evidence parameterization with a KL-based prior that intentionally promotes high epistemic uncertainty at class boundaries and in regions far from the training distribution—an effect that helps prevent overconfident errors. Activation functions and regularizers determine how evidence accumulates under this framework. Our analysis shows that commonly used non- negative activations can inadvertently create “zero-evidence” regions where gradients become extremely small, making evidence updates for nearby samples ineffective. More specifically, EDL models acquire limited new ev- idence from samples mapped into these low-evidence re- gions because the corresponding evidence gradients approach zero. Moreover, the learning signal decreases proportionally arXiv:2512.23753v1 [cs.LG] 27 Dec 2025 JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2 Fig. 2: Intuitive visualization of a zero-evidence region for evidential models in the evidence space for binary classifica- tion. Samples mapped into such regions have extremely small gradients, leading to limited model update during training. GRED encourages larger gradients for ‘zero-evidence’ samples, enabling consistent learning across samples. as samples are mapped closer to the zero-evidence region, irrespective of supervised information. This activation-induced stagnation is illustrated in Figure 2 (with detailed discussion in Section IV-C). We analyze several existing evidential variants and observe this behav- ior consistently across models and settings. Motivated by these insights, we introduce a novel Generalized Regularized Evidential model (GRED) that employs positive evidence regularization to encourage evidence accumulation even in low-evidence regimes. A preliminary version of this work has been published as a conference paper [15]. Improving on RED, we propose gen- eralized regularized evidential models that mitigate learning stagnation across a family of evidential activations (Section IV). We theoretically show the effectiveness of the correct- evidence regularization (Theorem 3) and provide expanded analysis of evidential losses (Section IX). We further extend GRED to challenging few-shot classification and blind face restoration tasks, and carry out detailed uncertainty analysis, and demonstrate the broader utility of evidential uncertainty. Our major contributions can be summarized as follows: • We identify an activation-induced learning-freeze behavior in evidential models, wherein data samples mapped to “zero- evidence” regions receive vanishing evidence gradients. For these samples, the learning signal decreases proportionally as they are mapped closer to the zero-evidence region in the evidence space. • We theoretically show that evidential models with exp acti- vation produce stronger gradients near low-evidence regions compared to other activations. • We introduce a generalized evidence regularization strategy that encourages evidence updates across activation regimes, enabling more consistent learning from all samples. • We conduct extensive experiments across multiple settings including 4 benchmark classification tasks, few-shot classi- fication, and blind face restoration, validating the developed theory and demonstrating the effectiveness of our approach. II. RELATED WORKS a) Uncertainty Quantification in Deep Learning: Accu- rate quantification of predictive uncertainty is essential for de- velopment of trustworthy Deep Learning (DL) models. To this end, DL models have been augmented to become uncertainty- aware [16] using a variety of approaches such as deep ensem- ble–based approaches [8], [7], Bayesian neural networks [17], [5], [6], second-order distribution–based approaches [18], [19], [20], and credal-set–based approaches [21], [22], [23], [24], [25], [26]. Deep ensemble techniques [8], [7] construct an ensemble of neural networks and the agreement/disagreement across the ensemble components is used to quantify different uncertainties. Ensemble-based methods significantly increase the number of model parameters, and are computationally expensive at both training and test times. Bayesian neural networks [5][6][17] have been developed that consider a Bayesian formalism to quantify different un- certainties. For instance, Blundell et al. [6] use Bayes-by- backdrop to learn a distribution over neural network param- eters, whereas Gal et al. [5] enable dropout during inference phase to obtain predictive uncertainty. Bayesian methods resort to some form of approximation to address the intractability issue in marginalization of latent variables. Moreover, these methods are also computationally expensive as they require sampling for uncertainty quantification. Towards accurate UQ, Credal Bayesian deep learning (CBDL) models [21], [22], [27] have also been developed that use concepts from the imprecise probability theory [28] for comprehensive uncertainty quantification. These models aim to approximate the credal set of posterior distributions during training [21], based on which, the models infer the credal set of predictive distributions during inference. CBDL models are more robust to prior/likelihood distribution misspecification compared to BNN, have been effectively applied to continual learning settings [29], and provide more robust epistemic uncertainty quantification capabilities [22] especially in sit- uations of prior/likelihood misspecification. However, these models, due to the use of credal sets that require reasoning over multiple distributions, are computationally expensive compared to BNN, limiting their usage. In contrast, Evidential DL approaches only require a single forward pass through the deep learning models to quantify uncertainty, making them computationally lighter compared to BNN and CBDL models. b) Evidential Deep Learning: Uncertainty in be- lief/evidence theory [30], [31] and its neural extensions [32], [33] have been studied under Dempster–Shafer Theory [34], fuzzy logic [35], [36], and Subjective Logic [10]. Eviden- tial deep learning is closely related to second-order distri- bution–based uncertainty quantification [18], [19], [20] and frequently employs Subjective Logic [10] for uncertainty rea- soning. Evidential models introduce a conjugate higher-order evidential prior for the likelihood distribution that enables the model to capture the fine-grained uncertainties. For instance, JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3 Dirichlet prior is introduced over the multinomial likelihood for evidential classification [14], [37], [38], and the normal- inverse-gamma prior is introduced over the Gaussian likeli- hood [12], [39] for the evidential regression models. The robustness [40] and calibration [41] of evidential mod- els have been extensively studied. Usually, these models are trained with evidential losses and heuristically designed regu- larizers to guide uncertainty behavior [13], [42]. Some variants incorporate out-of-distribution (OOD) data into training [43], [44], but this assumption may not hold in real-world settings. A recent survey [45] provides a comprehensive overview. Recent works have examined the reliability of uncertainty measures in EDL. Wimmer et al. [46] and Bengs et al. [47] highlight cases where the decomposition of aleatoric and epis- temic uncertainty may be inconsistent. Jurgens et al. [48] study epistemic uncertainty behavior and report situations where evidential uncertainty can be misleading. Shen et al. [49] extend EDL by explicitly modeling additional forms of model uncertainty, while second-order approaches [18], [20] propose theoretically grounded uncertainty measures addressing limi- tations in earlier formulations. Orthogonal to these existing evidential works—which pri- marily investigate the interpretation or reliability of evidential uncertainty—we study the training dynamics of evidential models. Specifically, we characterize the activation-induced learning-freeze behavior: certain non-negative evidence ac- tivations can map samples into low-evidence regions where gradients become extremely small, limiting effective evidence updates. We introduce a theoretically justified regularization strategy that mitigates this stagnation and enables more con- sistent evidence accumulation across activation regimes. In this work, we focus on evidential classification models and consider settings without access to the out-of-distribution data during training, improving applicability in real-world scenarios. III. LEARNING IN EVIDENTIAL MODELS We first describe learning in standard classification models. We then describe evidential deep learning model basics for classification. Afterward, we analyze the gradient dynamics of evidential training to characterize the activation-induced learning-freeze behavior that arises when certain non-negative evidence activations map samples into low-evidence regions. A. Standard Classification Models Standard classification models use a softmax transformation on the output from the neural network FΘ for input x to obtain the class probabilities in a K-class classification prob- lem. Such models are trained with cross-entropy-based losses. These models have achieved state-of-the-art performance on many benchmark problems. a) Gradient Analysis: Consider a standard cross-entropy trained model for K−class classification. Let the overall network be represented by fΘ(·), and let o = fΘ(x) be the output from this network before the softmax layer for input x and one-hot ground truth label of y. The output at node i after the softmax layer is given by smi = exp(oi) PK k=1 exp(ok) = exp(oi) Sce (1) where Sce = PK k=1 exp(ok). For a given sample (x, y), the cross-entropy loss (Lce), and the gradient of this loss with respect to the pre-softmax values o are given by Lce = − K X k=1 yk log(smk) = log Sce − K X k=1 ykok (2) gradk = ∂Lce ∂ok =  1 Sce ∂Sce ∂ok −yk  = exp(ok) Sce −yk (3) = smk −yk (4) The gradient measures the error signal, and for standard classification models, it is bounded in the range [-1, 1] as 0 ≤smk ≤1 and yk ∈{0, 1}. The model is updated using gradient descent-based optimization objectives. For input x, the neural network outputs K values o1 to oK, and the corresponding ground truth is y, ygt = 1, y̸=gt = 0. When yi = 0, the gradient signal is gradi = smi and the model optimizes the parameters to minimize this value. Only when smi = 0, the gradient is zero, and the model is not updated. In all other cases when smi ̸= 0, there is a non-zero gradient dependent on smi, and the model is updated to minimize the smi as expected. When yi = 1, the gradient signal is gradi = smi −1 and the model optimizes the parameters to minimize this value. As smi ∈[0, 1], only when the model outputs a large logit on i (corresponding to the ground-truth class) and small logit for all other nodes, smi = 1, the gradient is zero, and the model is not updated. For the cases when smi < 1, there is a non-zero gradient dependent on smi and the model is updated to maximize the smi=gt and minimize all other smi̸=gt as expected. The gradient signal in standard classification models trained with standard cross-entropy loss is reasonable and enables learning from all the training samples. The above gradient analysis shows that standard classi- fication models trained with cross-entropy-based loss can effectively learn from all the training samples. Nevertheless, these models lack a systematic mechanism to quantify different sources of uncertainty, a highly desired property in many real- world problems. B. Evidential Deep Learning Models for Classification Fig. 3: Graphical model for Evidential Models Evidential deep learning models for classification formulate model training as an evidence acquisition process and consider a higher-order Dirichlet prior Dir(p|α) over the predictive Multinomial distribution Mult(y|p). Different from a stan- dard Bayesian formulation which optimizes Type-II Maximum JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4 Likelihood to learn the Dirichlet hyperparameter [50], eviden- tial models directly predict α using data features x and then generate the prediction y by marginalizing the Multinomial parameter p. Figure 3 describes this generative process. Such higher-order prior enables the model to systematically quantify different sources of uncertainty. It is worth noting that the uncertainty behavior of vanilla EDL [11] in low-evidence or boundary regions is an intentional design choice arising from its Dirichlet prior and KL-based regularization, which promote conservative (high epistemic) uncertainty away from training data. In evidential models, the Softmax in the standard neural networks for classification is replaced by a non-negative activation function A, where A(x) ≥0 ∀x ∈[−∞, ∞], such that for input x, the neural network model FΘ with parameters Θ can output evidence e for different classes. Dirichlet parameter α is evaluated as α = e + 1 to ensure α ≥1, where e = A(FΘ(x)) = A(o), which quantify fine- grained uncertainties in addition to the prediction y for the input x. Then, Dirichlet strength, S, for K−class classification problem is computed by S = K X k=1 (ek + 1) (5) The activation function A(·) assumes three common forms to transform the output into evidence: (1) ReLU(·) = max(0, ·), (2) SoftPlus(·) = log(1 + exp(·)), and (3) exp(·). Evidential models assign input sample to that class for which the output evidence is greatest. Moreover, they quantify the confidence in the prediction for K−class classification problem through vacuity ν = K S (i.e., measure of lack of confidence in the prediction.) For any training sample (x, y), the evidential models aim to maximize the evidence for the correct class, minimize the evidence for the incorrect classes, and output accurate confidence. To this end, three variants of evidential loss functions have been proposed [11]: 1) Bayes risk with the sum of squares loss, 2) Bayes risk with cross- entropy loss, and 3) Type-II Maximum Likelihood loss. Please refer to Eq. 28, Eq. 29, and Eq. 30 in the Appendix for the specific forms of these losses. Additionally, incorrect evidence regularization terms are introduced to guide the model to output low evidence for classes other than the ground truth class (See Appendix VIII for discussion on the regularization). With evidential training, evidential deep learning models are expected to output high evidence for the correct class, low evidence for all other classes, and output high vacuity for unseen/OOD samples. C. Theoretical Analysis of Evidential Classification Models To identify the underlying reason that causes the perfor- mance gap of evidential models as described earlier, we consider a K−class classification problem and a representative evidential model trained using Bayes risk with the sum of squares loss given in Eq. 28. We first provide important definitions that are critical for our theoretical analysis. Definition 1 (Zero Evidence Sample and Zero Evidence Region). A zero evidence sample is a data sample for which the model outputs zero evidence for all classes. A zero evidence region is the area in the evidence space that contains all the zero evidence samples. For a reasonable evidential model, novel data samples not yet seen during training, difficult data samples, and Out-Of- Distribution (OOD) samples should become zero evidence samples and should be mapped in the zero evidence region. Theorem 1. Given a training sample (x, y), if an evidential neural network outputs zero evidence e, then the gradients of the evidential loss evaluated on this training sample over the network parameters reduce to zero. Proof. Consider an input x with one-hot ground truth label y. Let the ground truth class index be gt, i.e., ygt = 1, with corresponding Dirichlet parameter αgt, and y̸=gt = 0. Moreover, let o, e, and α represent the neural network output vector before applying the activation A, the evidence vector, and the Dirichlet parameters respectively. In this evidential model, the loss is given by LMSE(x, y) = K X j=1 (yj −αj S )2 + αj(S −αj) S2(S + 1) (6) Now, the gradient of the loss with respect to the neural network output can be computed using the chain rule: ∂LMSE(x, y) ∂ok = ∂LMSE(x, y) ∂αk ∂ek ∂ok = 2αgt S2 −2yk S −2(S −αk) S(S + 1) + + 2(2S + 1) P i P j αiαj (S2 + S)2  × ∂ek ∂ok (7) Based on the actual form of A, we have three cases: • Case I: ReLU(·) to transform logits to evidence ek = ReLU(ok) =⇒∂ek ∂ok = ( 1 if ok > 0 0 otherwise (8) For a zero evidence sample, the logits ok satisfy the rela- tionship ok ≤0 ∀k =⇒ ∂ek ∂ok = 0 =⇒ ∂LMSE(x,y) ∂ok = 0 • Case II: SoftPlus(·) to transform logits to evidence ek = Softplus(ok) =⇒∂ek ∂ok = Sigmoid(ok) (9) For a zero evidence sample, the logits ok →−∞=⇒ Sigmoid(ok) →0 & ∂ek ∂ok →0. • Case III: exp(·) to transform logits to evidence ek = exp(ok) =⇒ ∂ek ∂ok = exp(ok) = αk −1 (10) For a zero evidence sample, αk →1 =⇒ ∂ek ∂ok →0. So, for all three instances of the evidential activation, ∂ek ∂ok → 0 as ek →0 & ek = 0 =⇒ ∂ek ∂ok = 0. Moreover, there is no term in the first part of the loss gradient in Eq. 7 to counterbalance these zero-approaching gradients. As the training sample is mapped to the region near the zero evidence region (i.e., ek →0), the evidence gradients ( ∂ek ∂ok ) approach to zero (i.e., ∂ek ∂ok →0), and the loss gradient (a.k.a., the learning JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 5 signal) also approaches zero (i.e., ∂LMSE(x,y) ∂ok →0) irrespective of the supervised learning signal. For zero evidence training samples, for any node k, ∂LMSE(x, y) ∂ok = 0 (11) For zero evidence training samples, since the gradient of the loss with respect to all the nodes is zero, there is no update to the model from such samples. This shows that the evidential models fail to learn from a zero evidence data sample. For completeness, we present the detailed proof of the evidential models trained using Bayes risk with the sum of squares error along with other evidential losses in Appendix VII, and impact of incorrect evidence regularization in Ap- pendix VIII. a) Remark: When an evidential model outputs zero evidence for all classes (i.e., a data sample that the model has never seen and for which the model accurately outputs “I don’t know”, i.e., ek = 0 ∀k ∈[1, K]), the gradients of standard evidential losses vanish, and the supervised in- formation in such samples cannot contribute to parameter updates. Such samples may naturally appear during training (for example, novel, ambiguous, or OOD-like inputs), but the model receives no learning signal from them because they lie in the zero evidence region. Similarly, samples mapped near the zero evidence region receive significantly diminished gradients: their learning signal becomes much weaker than that of samples with higher evidence, regardless of the strength of the supervised label. Corollary 1. Incorrect evidence regularization does not pro- vide a learning signal for zero evidence samples and therefore cannot induce parameter updates for such samples. Intuitively, the incorrect evidence regularization encourages the model to reduce evidence for non–ground-truth classes, but it does not increase the evidence of the ground-truth class. As a result, its gradients do not affect the zero-evidence condition. Consequently, the regularization can move samples closer to the “zero evidence region” in the evidence space, but it cannot create a non-zero gradient for samples already mapped to this region. Thus, incorrect evidence regularization does not supply the missing gradient needed for zero-evidence samples to contribute to learning. Theorem 2. For a data sample x, if an evidential model outputs logits ok ≤0 ∀k ∈[0, K], the exponential activation function leads to a larger gradient update on the model parameters than SoftPlus and ReLu. Proof. Consider an evidential loss L (formally defined in Eq. 28, Eq. 29, and Eq. 30) is used to train the evidential model. Let o, e ∈RK denote the neural network output vector before applying the activation A, and the evidence vector, respectively, for a network with weight w. For a data sample x, if the network outputs ok < 0, ∀k ∈[K], we have: 1. ReLU: ∂L ∂w  ReLU = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w = 0 (see Eq. 8) (12) 2. SoftPlus: ∂L ∂w  SoftPlus = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w (see Eq. 9) (13) = X k ∂L ∂ek ∂ok ∂w Sigmoid(ok) (14) 3. Exponential: ∂L ∂w  Exp = X k ∂L ∂ek ∂ek ∂ok ∂ok ∂w (see Eq. 10) (15) = X k ∂L ∂ek ∂ok ∂w exp(ok) (16) = X k ∂L ∂ek ∂ok ∂w {[1 + exp(ok)]Sigmoid(ok)} (17) Thus, we have  ∂L ∂w  Exp ≥  ∂L ∂w  SoftPlus ≥  ∂L ∂w  ReLU, which implies that A = exp leads to a larger update to the network than both SoftPlus and ReLU. This completes the proof. b) Remark: The above proof implies that the training of evidential models is most effective with the exponential acti- vation function as it has larger gradient update (and effectively stronger learning signal) for points near the zero evidence region i.e., for points with ok ≤0 ∀k ∈[0, K]. We now carry out additional analysis with a representative evidential model in K−class classification problem. We con- sider an input x with one-hot label of y, PK k=1 yk = 1. For this evidential framework, the Type-II Maximum Likelihood loss (LLog(x, y)) and its gradient with the logits o (Eq. 41) are given by LLog(x, y) = log S − K X k=1 yk log αk (18) gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk ∂ek ∂ok (19) Case I and II: ReLU(·) and SoftPlus(·) to transform logits to evidence. • Zero evidence region: For ReLU(·) based evidential models, if the logits value for class k i.e., ok is negative, then the corresponding evidence for class k i.e., ek = 0, ∂ek ∂ok = 0 & gradk = ∂LLog(x,y) ∂ok = 0. So, there is no update to the model through the nodes that output negative logits value. In the case of SoftPlus(·) based evidential models, there is no update to the model when training samples lie in zero evidence regions. This is possible in the condition of ok →−∞. In other cases, there will be some small finite small update in the accurate direction from the gradient. • Range of gradients: The range of gradients for both ReLU(·) and SoftPlus(·) based evidential models are JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 6 identical. Considering the gradient for the ground truth node i.e., yk = 1, the range of gradients is [ 1 K −1, 0]. For all other nodes other than the ground truth node i.e., yk = 0, the range of gradients is [0, 1 K ]. So, for classification problems with a large number of classes, the gradient updates to the nodes that do not correspond to the ground truth class will be bounded in a small range and is likely to be very small. • High incorrect evidence region: If the evidence for class k is very large i.e., ek →∞, then for ReLU(·), ∂ek ok = 1, and for SoftPlus(·), ∂ek ok = Sigmoid(ok) →1, 1 αk = 1 ek+1 →0, 1 S →0, & gradk = ∂LLog(x,y) ∂ok →0. For large positive model evidence, there is no update to the corre- sponding node of the neural network. The evidence can be further broken down into correct evidence (corresponding to the evidence for the ground truth class), and incorrect evidence (corresponding to the evidence for any other class other than the ground truth class). When the correct class evidence is large, the corresponding gradient is close to zero and there is no update to the model parameters which is desired. When the incorrect evidence is large, the model should be updated to minimize such incorrect evidence. However, the evidential models with ReLU and SoftPlus fail to minimize incorrect evidence when the incorrect evidence value is large. These necessities the need for incorrect evidence regularization terms. Case III: Exponential, exp(·), to transform logits to evi- dence. Considering Eq. 41 and Eq. 33, the gradient of the loss with respect to the logits becomes gradk = ∂LLog(x, y) ∂ok =  1 S −yk αk  (αk −1) (20) • Zero evidence region: In case of exp(·) based evidential models, except in the extreme cases of αk →∞, there will be some signal to guide the model. In cases outside the zero evidence region (i.e., outside αk →∞), there will be some finite small update in the accurate direction from the gradient. Moreover, for same evidence values, the gradient of exp based model is larger than the SoftPlus based evidential model by a factor of 1+exp(ok). Compared to SoftPlus models, the larger gradient is expected to help the model learn faster in low- evidence regions. • Range of gradients: For the ground truth node, i.e., , yk = 1, the range of gradients is [−1, 0]. For all nodes other than the ground truth node i.e.,, yk = 0, the range of gradients is [0, 1]. Thus, the gradients are expected to be more expressive and accurate in guiding the evidential model compared to ReLU and SoftPlus based evidential models. • High evidence region: If the evidence for class k is very high i.e., ek →∞, then αk −1 ≈αk and gradk = smk −yk. In other words, the model’s gradient updates become identical to the standard classification model (see Section III-A) without any learning issues. Due to smaller zero evidence region, more expressive gra- dients, and no issue of learning in high incorrect evidence region, the exponential-based evidential models are expected to be more effective compared to ReLU and SoftPlus based evidential models. As can be seen, the ReLU based activation completely destroys all the information in the negative logits and has the largest region in evidence space in which training data have zero evidence. SoftPlus activation improves over the ReLU, and compared to ReLU, has a smaller region in evidence space where training data have zero evidence. However, SoftPlus based evidential models fail to correct the acquired knowledge when the model has strong wrong evidence. Moreover, these models are likely to suffer from the vanishing gradients problem when the number of classes increases (i.e., classification problem becomes more challeng- ing). Finally, exponential activation has the smallest zero evidence region in the evidence space without suffering from the issues of SoftPlus based evidential models. Still, the learning signal for all evidential models reduces proportionally as the training data points become closer to zero evidence region, and the learning signal becomes zero for samples in zero evidence region of the evidence space irrespective of the supervised signal in the training data point. This problem exists for all the activation functions. IV. AVOIDING ZERO EVIDENCE REGIONS THROUGH CORRECT EVIDENCE REGULARIZATION We introduce a generalized correct evidence regularization for evidential classification models that provides a meaningful gradient for samples in low- or zero-evidence regions, while leaving standard evidential losses unchanged for high-evidence samples. A. Correct Evidence Regularization As shown in Section III-A, cross-entropy–trained softmax models naturally provide a strong gradient signal for the ground-truth class when its logit is highly negative. In evi- dential models, however, the gradients produced by standard evidential activations vanish as the evidence approaches zero. To encourage a learning behavior closer to that of cross- entropy models in these regions, we propose introducing a regularization term Lcor(x, y) that satisfies ∂Lcor(x, y) ∂ogt = −1 when K X k=1 ek = 0, ∂Lcor(x, y) ∂ogt →−1 as K X k=1 ek →0. Motivated by this analysis, we propose the following vacuity-guided regularization: Lcor(x, y) = −λcorogt, (21) where λcor = ν = K S denotes the vacuity produced by the evi- dential model. The vacuity behavior is characterized as λcor = 1 as PK k=1 ek = 0 & λcor →1 as PK k=1 ek →0. This choice ensures that the regularization magnitude ap- proaches 1 as the total evidence PK k=1 ek approaches zero. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 7 To allow the evidential losses to dominate learning for high- evidence samples, we introduce an evidence-dependent indi- cator function in the loss: Lcor(x, y) = ( −λcorogt if ogt < 0, 0 otherwise = −I λcorogt, (22) where I = 1(ogt < 0) disables the regularization once the model assigns sufficiently positive evidence to the ground-truth class. The term is active primarily in low-evidence regions and diminishes as the sample moves away from the zero- evidence region ,with the greatest magnitude achieved in the zero evidence region. Thus, it has the effect of pushing the samples away from the zero evidence region. This key property is summarized in the following theorem. Theorem 3. Correct evidence regularization provides a non- vanishing gradient signal for training samples mapped to zero- evidence regions. Proof. The regularization depends only on the logit ogt of the ground-truth class. Hence, ∂Lcor(x, y) ∂ok k̸=gt = 0, and non–ground-truth nodes receive no update. Because the indicator I = 1(ogt < 0) activates the term in low-evidence regions, we focus on this case. For ygt = 1, the regularization and its gradient become Lcor(x, y) = −λcorogt, (23) ∂Lcor(x, y) ∂ogt = −λcor. (24) The vacuity λcor = K S lies in [0, 1] and achieves its maximum of 1 when the evidence is zero. Thus, samples mapped to zero-evidence regions receive a gradient of −1, provide a meaningful update signal that promotes increased evidence for the ground-truth class. As evidence grows, vacuity decreases, and the influence of the regularization diminishes, allowing the standard evidential losses to guide learning for high-evidence samples. Hence, the proposed regularization ensures non-zero gradients for zero-evidence samples and restores gradient flow in regions where standard evidential losses alone produce vanishing updates. B. Generalized Regularized Evidential Models The correct evidence regularization term in Eq. 21 is ex- pressed in terms of the logit. When SoftPlus or Exponential activations are used, the regularization can also be written di- rectly in terms of the output evidence because these activations are invertible. This is not the case for ReLU, whose non- invertibility prevents an evidence-based formulation. Theo- rem 2 indicates that the Exponential activation provides strong gradients for samples near the zero-evidence region, but its output may grow rapidly for large positive logits, making optimization more difficult. To balance these behaviors we introduce a novel evidential activation function, referred to (a) Evidence-Logit Trend (b) Gradient Plot Fig. 4: Output Evidence and Gradient plot of different eviden- tial activations for different Logit values as Shifted Exponential Linear Unit (SELU) that generalizes existing activation functions with some appealing properties: ei = SELU(oi) = ( oi + 1 if oi > 0 exp(oi) otherwise (25) The activation behaves similarly to the Exponential activa- tion function for negative logits and has the largest gradient (compared to SoftPlus and ReLU) for samples close to the zero evidence region. For positive logits, the activation behaves linearly, and the evidence value does not explode as the logit value increases. The output evidence and the corre- sponding gradient plots from different activation functions are visualized in Figure 4. We present evidence-based formulation of the correct evidence regularization for different activation functions in Table I. TABLE I: Evidence-Based Regularization Activation Evidence Regularization (Lcor) ReLU ei = max(0, oi) N/A SoftPlus ei = log(exp(oi) + 1) −Iλcor log(exp(egt) −1) SELU see Eq. 25 −Iλcor log(egt) Exponential ei = exp(oi) −Iλcor log(egt) C. Evidential Model Training We formulate the overall objective used to train the proposed Generalized Regularized evidential model (RED). The model is trained to increase evidence for the ground-truth class, reduce evidence for incorrect classes, and ensure that samples in low- or zero-evidence regions receive a meaningful learning signal. The combined loss is L(x, y) = Levid(x, y) + η1Linc(x, y) + Lcor(x, y) (26) where Levid(x, y) is the loss based on the evidential frame- work given by Eq. 28, Eq. 30, or Eq. 29 (See Appendix VII), Linc(x, y) represents the incorrect evidence regularization (See Appendix Section VIII), Lcor(x, y) represents the pro- posed novel correct evidence regularization term in Eq. 22, and η1 = λ1 × min(1.0, epoch index/10) controls the impact of incorrect evidence regularization to the overall model training. In this work, we consider the forward-KL-based incorrect evidence regularization given in Eq. 49 based on [11]. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 8 Figure 2 provides an intuitive view of learning in the evidence space. Ideally, samples from Class 1 should lie in the blue region (high evidence for Class 1), samples from Class 2 in the green region, and unseen or OOD samples in the zero- evidence region. Training with Levid and Linc encourages these behaviors; however, when a sample is mapped to the zero-evidence region, the gradients of standard evidential losses vanish. Thus, although the true label is available, model does not update its knowledge when training such samples. Samples with low correct evidence and high incorrect evidence may also be driven toward this region (blue and green arrows in Figure 2), after which their update becomes inactive under standard evidential losses. This activation-dependent behavior occurs across evidential models. The GRED behavior is illustrated by the red arrows in Figure 2. The correct evidence regularization is weighted by vacuity and therefore contributes most strongly in the zero- evidence region, where Levid and Linc provide no gradients. As evidence increases and vacuity decreases, the influence of Lcor fades, and the standard evidential losses dominate the learning signal. In this way, GRED ensures that samples across all evidence levels contribute to parameter updates while preserving the intended behavior of evidential training for high-evidence regions. V. EXPERIMENTS We evaluate our method across a broad range of benchmarks and architectures to validate the theoretical analysis, demon- strate the effectiveness of correct evidence regularization, and assess generalization and uncertainty quantification. Our experiments span standard supervised classification, few-shot learning, and a real-world image restoration task. We consider MNIST [51], CIFAR-10 and CIFAR-100 [52], and Tiny-ImageNet [53] for classification; 100-way 1-shot and 100-way 5-shot CIFAR-100 for few-shot learning; and blind face restoration [54] using FFHQ [55] and CelebA [56]. To evaluate robustness across architectures, we employ LeNet[57] for MNIST, ResNet18 [58] for CIFAR experi- ments, and Swin-Transformer [59], [60] for Tiny-ImageNet. For few-shot learning, we use the transformer-based Visual Prompt Tuning (VPT) framework [61], which adapts large pretrained vision transformers using lightweight prompts—a setting that stresses uncertainty estimation due to extremely limited supervision. For blind face restoration, we use the VQGAN/Transformer-based CodeFormer model [54]. We first present experiments that empirically verify the gra- dient behavior characterized in Section III. We then evaluate the proposed correct evidence regularization on all datasets and architectures, followed by ablation studies analyzing the contribution of each evidential loss component and the im- pact on calibration and uncertainty metrics. We then extend the proposed evidential model to few-shot classification and blind face restoration, demonstrating consistent improvements across all settings. We then carry out out-of-distribution anal- ysis of the proposed GRED model with challenging few-shot classification setting. Unless noted otherwise, table mean/std results are averaged over three seeds; training curves show GT : 3 GT : 5 GT : 2 GT : 6 Fig. 5: Toy dataset with 4 data points 0 2 4 6 8 Iterations (× 10) 0.00 0.25 0.50 0.75 1.00 Accuracy Standard model Evidential model (a) Training Accuracy Trend 0 2 4 6 8 Iterations (× 10) 0 1 2 Loss Standard Model Evidential Model (b) Training Loss Trend Fig. 6: Training of standard and evidential models one representative run. Additional ablations, hyperparameter details, and clarifications are provided in the Appendix. A. Learning Dynamics and Failures in Evidential Models a) Sensitivity to the change of the architecture.: We first consider a toy illustrative experiment with two frameworks: (1) a standard Softmax model and (2) an evidential model. Both use a LeNet [57] architecture similar to that considered in EDL [11], with a minor modification to the architecture: dropout is removed. To construct the toy dataset, we randomly select 4 labeled data points from the MNIST training dataset, as shown in Figure 5. For the evidential model, we use ReLU to transform the network outputs to evidence and train the model with the MSE-based evidential loss [11] given in Eq. 28, without incorrect evidence regularization. We train both models using only these 4 training data points. Figure 6 compares the training accuracy and loss trends of the evidential model with the standard softmax model (trained with cross-entropy). Before training, both models have 0% accuracy and high loss, as expected. For the evidential model, in the first few iterations the accuracy increases to 50%, indicating that some samples are being fitted. Afterward, the accuracy plateaus: the evidential model maps two of the train- ing samples to the zero evidence region, where the gradients of standard evidential losses vanish. In this toy setting, the model therefore does not fully fit all four training points, empirically reflecting the behavior characterized in Theorem 1. It is also worth noting that the range of the evidential model’s loss is significantly smaller than that of the standard model, mainly due to the bounded nature of the evidential MSE loss (i.e., it lies in [0, 2]; see the Appendix for a detailed theoretical analysis). By contrast, the standard model trained with cross- entropy easily fits the trivial dataset, reaching near-zero loss and 100% accuracy after a few iterations. Additionally, we visualize the total evidence for each train- ing sample in this toy experiment. We plot the total evidence across the first 100 iterations in Figure 7. The evidential JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 9 0 25 50 75 100 Iteration 0 2 4 Evidence 3 5 2 6 Fig. 7: Zero evidence trend during model training model’s predictions are correct for the samples with ground- truth labels 3 and 6, and incorrect for the remaining two. After a few iterations, the latter two samples are mapped to the zero-evidence region, and their total evidence remains near zero. In this regime, the model receives no gradient from these samples, and the overall training accuracy stabilizes at 50% even after 100 iterations. In contrast, the standard model continues to update on all four samples and achieves 100% accuracy. This toy setting highlights how vanishing gradients in the zero-evidence region can affect evidential learning dynamics, even in simple cases. b) Sensitivity to hyperparameter tuning: Evidential mod- els are trained using evidential losses given in Eq. 28, Eq. 29, or Eq. 30 with incorrect evidence regularization to guide the model for accurate uncertainty quantification. We study the impact of the incorrect evidence regularization strength (i.e., hyperparameter λ1) on the evidential model’s performance using CIFAR-100 experiments. We consider the Type-II Maxi- mum Likelihood loss in Eq. 30 with different λ1 to control KL regularization. As shown in Figure 8, when some regulariza- tion is introduced, the evidential model’s test performance im- proves slightly. However, when large regularization is used, the model focuses strongly on minimizing the incorrect evidence, pushing a large number of training data samples to region near the zero evidence region. As can be seen, the generalization performance of evidential models is highly sensitive to λ1 values. A similar trend is seen across all the losses and settings (results on other loss functions and settings are presented in the Appendix). Such incorrect evidence regularization can cause the model to push many training samples into or close to the zero-evidence regions, thereby reducing the effective learning signal from those samples. At the same time, incorrect evidence regularization is essential to correct incorrect ac- quired evidence and improve uncertainty estimates. Therefore, choosing a reasonable regularization strength is important for achieving accurate uncertainty quantification, especially on challenging datasets and settings, which we present next. c) Challenging datasets and settings.: We next consider a standard cross-entropy-trained classification model for the CIFAR-100 dataset and construct evidential extensions using the Type-II Maximum Likelihood loss in Eq. 30 and the Bayes risk with cross-entropy loss in Eq. 29 without incorrect evidence regularization, using ReLU to transform logits to Fig. 8: Impact of different incorrect evidence regularization strengths to the test set accuracy on CIFAR-100 (a) Evid. Log Loss in Eq. 30 (b) Evid. CE Loss in Eq. 29 Fig. 9: Learning trends in CIFAR-100 for standard and evi- dential models with different evidential losses evidence. As shown in Figure 9, compared to the standard classification model, the evidential models exhibit lower pre- dictive performance (around 10%–20% lower for Eq. 30 and Eq. 29, and more than 30% lower for the MSE-based loss in Figure 1). This behavior coincides with many training samples being mapped into or near the zero evidence region, where the model expresses high vacuity and the gradients from standard evidential losses vanish. When incorrect evidence regularization is added, more samples can be driven toward the zero-evidence region, which may further reduce predictive accuracy if the regularization is too strong. In such cases, even though correct labels are available, the contribution of those samples to parameter updates becomes negligible once they reach near the zero-evidence region. d) Sub-Optimal Learning Caused by Incorrect Evidence Regularization: Existing evidential models are theoretically equipped to capture the fine-grained uncertainties through the higher-order conjugate prior distribution over the likelihood distribution. For classification, the evidential models introduce the Dirichlet prior over the multinomial likelihood distribution and train with evidential losses, such as Eq. 30. Additionally, these evidential models leverage incorrect evidence regulariza- tion given in Eq. 49 to ensure accurate uncertainty quantifi- cation, especially in the most challenging settings. To more clearly demonstrate the influence of the incorrect evidence regularization, we first consider the FGSM [62] adversarial attack applied to an evidential model with an Exponential activation function trained on CIFAR-100. We employ the JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 10 (a) Accuracy Trend (b) Vacuity Trend Fig. 10: Adversarial attack trends for different incorrect evi- dence regularization strengths evidential log loss, given in Eq. 30 and train the model for 200 epochs to study the accuracy-vacuity trends of the trained model for different strengths of the adversarial attack on the test set. As shown in Fig 10, as the attack strength increases, the overall accuracy of the model decreases. Since the evidential model is able to quantify the fine-grained uncertainty, we hope it can detect the attack through the predicted uncertainty. However, without the incorrect evidence regularization, the predicted vacuity remains low as the attack strength increases, as shown in Fig 10(b). With a larger incorrect evidence regu- larization, the model becomes aware of its lack of knowledge for adversarial samples and outputs higher vacuity. However, when the incorrect evidence regularization is high, the model’s learning capability becomes compromised: Fig 10(a) shows that a larger λ1 leads to a lower accuracy. Towards robust models, adversarial training methods have been developed and could be extended to evidential deep learning models [40]. However, we observe that adversarial training of evidential models is sensitive to incorrect evidence regularization values (section V-B, Figure 15), and adversarial training becomes ineffective. To further illustrate the need for incorrect evidence regu- larization, we next present the accuracy-uncertainty results for the 1024−class classification of the CodeFormer model for the FFHQ dataset. We consider the accuracy of the evidential transformer in the codebook prediction (details of the model are presented in the Appendix X-B4) We present the accuracy- vacuity curves for the evidential models trained with and without incorrect evidence regularization term in Figure 11. For a model with accurate uncertainty information, model’s accuracy should be higher on low vacuity predictions. In other words, the model should be accurate on its confident predictions. However, when no incorrect evidence regular- ization is used, the model is wrongly confident on all code predictions i.e., the uncertainty is not reliable. Moreover, the vacuity is not expressive and is bound on a narrow range of 0.005 to 0.0015. With a reasonable incorrect evidence regularization value, e.g., λ1 = 0.01 for the model training, the accuracy-vacuity curves become more reasonable. With a larger incorrect evidence regularization strength, model’s accuracy in codebook prediction increases with lower vacuity threshold: model is accurate on the most confident predictions. However, the incorrect evidence regularization tends to push training samples towards the zero evidence region, which hurts the model’s training data efficiency, and the generalization capability. 0.000 0.005 0.010 0.015 Vacuity Threshold 15.0 17.5 20.0 Accuracy Codebook Accuracy KL: 0.0 (a) λ1 = 0 0.2 0.4 0.6 0.8 Vacuity Threshold 25 50 75 100 Accuracy Codebook Accuracy KL: 0.01 (b) λ1 = 0.01 Fig. 11: (a) Without Incorrect Evidence Regularization (b) With Incorrect Evidence Regularization of λ1=0.01 B. Generalized Regularized Evidential Models (GRED) We now evaluate the proposed generalized regularized ev- idential models, which enable evidential networks to learn from all samples, including those mapped to the zero evidence region. We experiment with multiple activation functions and their regularized variants (i.e., trained with the correct evidence regularizer) using the Type-II evidential loss in Eq. 30. Across all datasets and architectures, introducing the cor- rect evidence regularization consistently improves generaliza- tion (Table II), validating its effectiveness. Figure 12 further shows that GRED remains stable even under strong incorrect evidence regularization, whereas baseline evidential models degrade because they cannot update on zero-evidence samples. Complete results and hyperparameter details are provided in the Appendix. We next examine training dynamics on MNIST with two ev- idential losses (Fig. 13). The exp activation performs strongest due to its minimal zero-evidence region, and adding correct evidence regularization further improves learning by ensuring that all samples contribute gradients. We further consider the evidential baseline model trained with Type-II Maximum Likelihood-based loss with incorrect evidence regularization strength of λ1 = 1.0 and 10.0. We 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Regularization Strength 0 20 40 60 80 Accuracy SoftPlus GRED-SoftPlus SELU GRED-SELU exp GRED-exp ReLU Fig. 12: Results for different regularization strengths for model trained with evidential log loss in Eq. 30 on CIFAR-100 JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 11 TABLE II: Evidential Classification Results. Mean and standard deviation are reported for each model after averaging across 3 runs. Bold values indicate the best performance for each dataset. Dataset ReLU SoftPlus GRED-SoftPlus SELU GRED-SELU exp GRED-exp MNIST 98.10±0.01 98.20±0.07 98.61±0.07 98.08±0.16 98.68±0.12 98.83±0.06 98.97±0.08 CIFAR-10 19.50±13.44 95.15±0.22 95.28±0.10 95.21±0.06 95.32±0.02 95.24±0.12 95.44±0.26 CIFAR-100 55.50±3.90 75.61±0.16 76.09±0.25 75.50±0.30 75.86±0.12 75.76±0.40 76.00±0.09 Tiny-ImageNet 33.08±0.94 90.25±0.02 90.63±0.06 90.15±0.04 90.58±0.03 90.01±0.06 90.63±0.05 (a) Evidential MSE loss (b) Evidential Log loss Fig. 13: Test accuracy with correct evidence regularization introduce the proposed novel correct evidence regularization to the evidential model. As can be seen in Figure 14, the model with correct-evidence regularization has superior gen- eralization performance compared to the baseline evidential models in all the cases. This is mainly due to the fact that with proposed correct evidence regularization, the evidential model can also learn from the zero evidence training samples to acquire new knowledge instead of ignoring it. Our proposed model considers knowledge from all the training data and aims to acquire new knowledge to improve its generalization instead of ignoring the samples for which it has no knowledge. Finally, as seen in Figure 14 (d), even though strong incorrect evidence regularization hurts the model’s generalization, the proposed model with correct evidence regularization is robust and generalizes better, empirically validating our Theorem 3. Limited by space, we present additional results in Appendix X-B2 (additional results of CIFAR-100 are presented in the Appendix). We evaluate the learning capabilities of evidential models under adversarial training. Specifically, we train an evidential model with an exp activation function using the eviden- tial Type-II Maximum Likelihood-based loss, and incorrect evidence regularization strength of λ1 = 0.1 and 1.0 on the CIFAR-100 dataset. For generating adversarial samples, we apply the FGSM method with an attack strength of ϵ = 0.05(additional details are provided in the Appendix X-A).The performance of the evidential models trained with incorrect evidence regularization values of 0.1 and 1.0 on the adversarial test set is presented in Fig. 15. The evidential model struggles to learn from all training samples, resulting in poor performance on the adversarial test set. Meanwhile, using the proposed evidence regularizer enables the model to learn from all samples, leading to decent performance on the test dataset (a) exp-based Model (b) SoftPlus-based Model (c) SELU-based Model (d) SoftPlus-based Model Fig. 14: Impact of correct evidence regularization to test accuracy for different evidential models (a) λ = 0.1 (b) λ = 1.0 Fig. 15: Impact of proposed regularization Lcor to Adversarial Training of evidential models C. Ablation Study a) Impact of loss function.: We first examine the effect of different evidential losses on MNIST using the exp activation and incorrect-evidence regularization strengths λ1 ∈{0, 1}. Training with the evidential MSE loss (Eq. 28) consistently yields lower test performance than the other two losses (Eq. 29 and Eq. 30). This behavior is expected because the evidential MSE loss is bounded in [0, 2], which restricts gradient magnitude and slows learning. Additional activation- wise comparisons and theoretical discussion are provided in the Appendix. Unless otherwise stated, subsequent experiments use the exp evidential activation with the Type-II Maximum Like- lihood loss (Eq. 30), which offers stable optimization and JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 12 a clearer probabilistic interpretation. A deeper comparison between Eq. 29 and Eq. 30 is left for future work. (a) Trend for λ1 = 0.0 (b) Trend for λ1 = 1.0 Fig. 16: Impact of evidential losses on test set accuracy b) Uncertainty vs Accuracy Trends: We next analyze the uncertainty behavior on CIFAR-100. Figure 17 shows accuracy–vacuity curves for different incorrect-evidence regu- larization strengths λ1. Vacuity reflects the lack of confidence in the predictions, and the accuracy of an effective evidential model should increase with a lower vacuity threshold. Without any incorrect evidence regularization (i.e., λ1 = 0), the evidential model is highly confident in its predictions and all test sample predictions are concentrated on the low vacuity region. As the incorrect evidence regularization strength is increased, the model outputs more accurate confidence in the predictions. Strong incorrect evidence regularization hurts the generalization over the test set as indicated by low accuracy when all test samples are considered (i.e., vacuity threshold of 1.0). Our correct-evidence regularized evidential model shows reasonable uncertainty behavior: the model’s test set accuracy increases as the vacuity threshold is decreased. Fig. 17: Accuracy-Vacuity curve We further evaluate accuracy on the top-K% most confident test predictions (Table III). For example, among the top 20% most confident samples, the proposed GRED model achieves 99.40% accuracy, outperforming all baseline models. Across all values of K, GRED matches or exceeds the strongest base- line, demonstrating that the improved uncertainty estimates translate into better ranking of prediction confidence. c) Calibration Analysis.: We evaluate calibration using Expected Calibration Error (ECE) on the CIFAR-100 test set. Figure 18 reports ECE trends across KL regularization strengths and the reliability diagrams. Across most settings, GRED achieves calibration on par with vanilla EDL, with TABLE III: Accuracy on Top-K% Confident Samples (%) Model 10% 20% 50% 70% 80% 100% ReLU 99.20 98.45 90.60 77.74 70.28 55.50 SELU 99.30 99.00 96.02 89.87 85.84 75.50 SoftPlus 98.70 98.65 95.94 89.91 85.80 75.61 Exp 99.40 99.20 96.54 90.46 86.19 75.76 GRED-Exp 99.60 99.40 96.40 90.46 86.26 76.00 marginal improvements in some hyperparameter values. As KL regularization increases, both models become increasingly underconfident, reflected by rising ECE values as seen in Figure 18(b)-(d) reliability diagrams. At large KL values (e.g., λ1 = 1.0), EDL [11] exhibits a deceptively low ECE (1.6%) due to model collapse: its accuracy drops to 34.3%, and the model outputs near-uniform predictions (“I don’t know”) for many test samples. In contrast, GRED maintains functional predictive performance (70.‘% accuracy, 26.4% ECE). This highlights how ECE alone can be misleading when a model collapses to high-vacuity predic- tions. Addressing the broader underconfidence trend observed in evidential models is an important direction for future work. (a) ECE vs KL strength (b) KL = 1.0 (c) KL = 0.5 (d) KL = 0.001 Fig. 18: Calibration analysis with log loss. (a) ECE trends (b)-(d) Reliability diagrams comparing GRED with EDL for different KL regularization values D. Extension to Blind Face Restoration Codeformer models [54] have been developed that intro- duce VQ-GAN-based networks with a transformer architecture leading to state-of-the-art blind face restoration performance. However, the blind face restoration problem is ill-posed by nature, and many of the restored faces are unlikely to be faithful to the true face images. Moreover, the blind face restoration problem, by design, introduces uncertainty in the downstream restoration task. In this section, we extend the JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 13 codeformer model using the ideas from our generalized reg- ularized evidential deep learning model to develop eviden- tial codeformers with fine-grained uncertainty-quantification capabilities. We use the fine-grained uncertainty information to improve the codebook lookup of the codeformer, which leads to significant improvement on the blind face restoration benchmark, demonstrating the potential of the uncertainty- aware model. The encoder-decoder based codeformer models [54] are trained in 3 stages: 1) Stage I with high-quality images to learn the codebook and train the decoder parameters, 2) Stage II with low-quality and high-quality image pairs to train the transformer classifier along with the encoder structure, and 3) an optional Stage III training of controllable feature trans- formation module to find good balance between the quality and fidelity of face restoration. We modify the transformer introduced in Stage II to an evidential transformer structure by replacing the softmax layer in the transformer with the evidential activation functions. We train the evidential trans- former on the FFHQ dataset based on Type II likelihood-based evidential loss in Eq. 30 with incorrect evidence regularization strength of λ1 = 1.0. The evidential transformer outputs the K-dimensional evidence vector to identify D-dimensional code item from the K × D shaped codebook for each of the M positions of the decoder input. For each position of the decoder input, based on the K-dimensional evidence vector e = (e1, e2, ..., eK)⊤, the vacuity ν, and K-dimensional belief vector b = (b1, b2, ..., bK)⊤can be computed. In all codeformer-based experiments, K = 1024, D = 256, and the decoder input is a 16 × 16 × 256 shaped tensor with M = 16×16 = 256. Additional model details are presented in the Appendix X-B4. The belief vector can be used to measure the model’s confusion (via dissonance [39]), and belief for each class can be used to make the code item prediction (the class prediction being the class for which the model outputs maximum belief). The vacuity represents the model’s lack of confidence in the prediction and can be used to identify the model’s confident predictions. If the evidential transformer outputs a highly confident prediction (indicated by low vacuity), the model is expected to be accurate, and such predictions can be trusted. In contrast, if the evidential classifier is not confident in the codebook pre- diction (e.g., due to the low quality of the input or insufficient knowledge of the model, then the code item selected at the decoder input is expected to be incorrect. In this case, instead of relying on the transformer’s top predicted code item, we could consider the evidential model’s beliefs to obtain more accurate code for the decoder. Based on this insight, we intro- duce a novel uncertainty-guided Top−t belief-based codebook selection scheme for inference. For the decoder input positions that the evidential transformer is confident (indicated by a low vacuity ν ≤νthr), we trust the evidential model and select the code item from the codebook for which the model outputs the maximum belief. In contrast, for the decoder input positions that the evidential model is not confident (indicated by a high vacuity ν > νthr), we consider the top t code items of the codebook for which the model’s belief is the largest. We then consider a belief-weighted combination of the predicted codes to obtain the final code item for the decoder input. Mathematically, given the codebook C = {c1, c2, ..., cK}, the decoder input dm for each position in the M−dimensional decoder input is obtained as: dm = ( cmax i if ν ≤νthr Pt j=1 bmax j cj max Pt l=1 bmax l Otherwise (27) where ν represents the vacuity predicted at mth decoder input position, cmax i represents the ith code item in the codebook C such that the evidential model’s belief is maximum for class i, bmax 1 , ..., bmax t represent the t greatest belief values among the K beliefs, and cmax 1 , ...cmax t are the corresponding code items of codebook with the t greatest belief values. When t = 1 or the vacuity threshold vthr = 1, the above inference scheme simplifies to the standard evidential model. When t = K, the model considers all the codebook items and weights them by the predicted belief to obtain the decoder input. With t > 1, and reasonable vacuity threshold values, the inference scheme considers multiple code items for blind face restoration and uses its belief to weight the code items. Evidential Model PSNR↑ MSE↓ L1 Loss↓ CodeFormer [54] 21.90 446.82 13.72 Evid. CodeFormer-ReLU 6.62 15436.89 102.38 Evid. CodeFormer-SELU 21.31 514.82 14.97 GRED-SELU 21.84 451.93 13.69 Evid. CodeFormer-Softplus 21.17 528.47 15.41 GRED-Softplus 21.81 454.03 13.86 Evid. CodeFormer-Exp 21.46 491.01 14.63 GRED-Exp 21.79 456.25 13.76 GRED-Exp(t = 5) 22.27 409.64 12.93 GRED-Exp(t = 10) 22.33 403.69 12.86 TABLE IV: CelebA Blind Face Restoration Results We now carry out blind face restoration experiments using the CelebA dataset and present the overall results in Table IV, where we consider t values of 5 and 10. We consider a set of metrics, including Peak Signal to Noise Ratio (PSNR), Mean Squared Error (MSE), and the L1 loss between the generated image and the ground truth image for evaluation. We observe that the ReLU based evidential model fails to achieve reasonable performance due to its sub-optimal learning (as indicated by low PSNR, and high MSE and L1 loss values in Table IV). The evidential codeformer models with proposed correct evidence regularization (i.e., the GRED variants for SELU-based , softplus-based, and exponential-based evidential models) consistently improve compared to the corresponding evidential codeformer models as the GRED regularization enables the evidential models to learn from all the training samples. Moreover, using the novel uncertainty-guided Top-k strategy leads to significant improvements in terms of PSNR (with a boost of around 0.43 db), L1 loss (decrease of around 0.42 units), and MSE (decrease by around 43 units). We carry out additional ablations to show the superiority of using belief weighting compared to uniform weighting, the impact of the t value, and the vacuity threshold in the Appendix. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 14 E. Extension to Few-Shot Classification Few-shot learning operates in a regime of extremely lim- ited supervision, making uncertainty awareness particularly important for trustworthiness and robustness [63], [64]. Prior works have explored evidential methods for uncertainty-aware few-shot learning [39], [65], but they suffer from the same zero-evidence learning limitations identified in Section III. We improve upon these approaches by incorporating our proposed correct-evidence regularization into a modern few-shot frame- work. We build an evidential Visual Prompt Tuning (VPT) model by modifying the transformer classifier in VPT [61] to output evidential activations instead of softmax probabilities. The model is trained using the full evidential objective in Eq. 26 with incorrect evidence regularization strength λ1 = 1.0, and we evaluate both with and without the proposed correct-evidence regularization across multiple activation func- tions. Additional implementation details, along with results for other λ1 values, are provided in Appendix X-B3. Table V summarizes performance on the challenging 100- way 1-shot and 100-way 5-shot CIFAR-100 benchmarks. Stan- dard evidential models perform poorly in these extreme low- data settings. In contrast, the proposed GRED variants con- sistently outperform their non-regularized counterparts across all activation functions, demonstrating improved generaliza- tion from limited supervision. Importantly, this improvement comes with no additional computational overhead and natu- rally yields high-quality uncertainty estimates. The benefit of uncertainty is further illustrated in the ac- curacy–vacuity curves in Fig. 19. As the vacuity threshold increases, we retain only the model’s most confident predic- tions, and accuracy rises sharply. For example, at a vacuity threshold of 0.6, the accuracy in the 100-way 1-shot setting improves from roughly 50% to above 90%. This highlights the practical advantage of evidential uncertainty for filtering reliable predictions in few-shot scenarios. Evidential Model 100-Way 1-Shot 100-way 5-Shot ReLU 1.00±0.00 1.00±0.00 SELU 36.81±5.69 49.39±3.51 GRED-SELU 45.88±1.31 76.09±0.78 Softplus 38.85±3.15 47.87±4.78 GRED-Softplus 46.63±1.59 74.08±1.61 Exp 37.49±3.18 49.05±5.51 GRED-Exp 46.54±1.45 77.00±1.01 TABLE V: Few-Shot CIFAR-100 Classification Results (a) 100-way 1-shot trend (b) 100-way 5-shot trend Fig. 19: Few-Shot CIFAR-100 Accuracy-Vacuity curves VI. OUT-OF-DISTRIBUTION DETECTION We evaluate the ability of evidential models to assign high epistemic uncertainty to out-of-distribution (OOD) inputs. Following standard practice, CIFAR-100 test samples serve as in-distribution (ID) and SVHN as OOD. We fine-tune the few- shot models on 1-shot and 5-shot CIFAR-100 with three KL regularization strengths (0.0, 1.0, 100.0), using the uncertainty score 1 −max p(y) for ID–OOD discrimination. AUROC is used as the evaluation metric. Figure 20 shows AUROC results across all settings. GRED consistently improves OOD separability, with the largest gains under moderate KL regularization. In the 5-shot scenario with KL= 1.0, GRED boosts AUROC from 0.633 to 0.882, demonstrating substantially improved epistemic uncertainty modeling. Large KL values (e.g., 100.0) yield diminishing returns, but GRED remains superior to the baseline. Overall, correct evidence regularization is crucial for robust OOD behavior in few-shot regimes. (a) 1-shot learning (b) 5-shot learning Fig. 20: AUROC-OOD detection trends. GRED consistently improves OOD separability across KL strengths. VII. CONCLUSION In this work, we presented a theoretical analysis revealing a fundamental limitation of evidential deep learning models: their gradients vanish in the zero-evidence region, preventing the model from learning from precisely the samples where supervision is most needed. We showed that exponential activations provide stronger learning signals in this regime and introduced a correct-evidence regularization term that restores meaningful gradients for low- and zero-evidence samples. This yields GRED, a generalized regularized evidential model that learns from all training examples. Extensive experiments across classification, few-shot learning, adversarial evalua- tion, OOD detection, and blind face restoration demonstrate consistent gains in generalization and uncertainty reliabil- ity. GRED mitigates activation-induced learning freeze and advances the development of trustworthy, uncertainty-aware neural networks. REFERENCES [1] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech recognition, vol. 84. Springer, 2019. [2] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain, “Machine translation using deep learning: An overview,” in 2017 in- ternational conference on computer, communications and electronics (comptelix), pp. 162–167, IEEE, 2017. [3] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep learning for computer vision: A brief review,” Computational intelligence and neuroscience, vol. 2018, 2018. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 15 [4] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 427–436, 2015. [5] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing model uncertainty in deep learning,” in international conference on machine learning, pp. 1050–1059, PMLR, 2016. [6] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight uncertainty in neural network,” in International conference on machine learning, pp. 1613–1622, PMLR, 2015. [7] T. Pearce, F. Leibfried, and A. Brintrup, “Uncertainty in neural networks: Approximately bayesian ensembling,” in International conference on artificial intelligence and statistics, pp. 234–244, PMLR, 2020. [8] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive uncertainty estimation using deep ensembles,” Advances in neural information processing systems, vol. 30, 2017. [9] G. Shafer, A mathematical theory of evidence, vol. 42. Princeton university press, 1976. [10] A. Jøsang, Subjective logic, vol. 3. Springer, 2016. [11] M. Sensoy, L. Kaplan, and M. Kandemir, “Evidential deep learning to quantify classification uncertainty,” Advances in neural information processing systems, vol. 31, 2018. [12] A. Amini, W. Schwarting, A. Soleimany, and D. Rus, “Deep eviden- tial regression,” Advances in Neural Information Processing Systems, vol. 33, pp. 14927–14937, 2020. [13] D. S. Pandey and Q. Yu, “Multidimensional belief quantification for label-efficient meta-learning,” in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pp. 14391– 14400, June 2022. [14] W. Bao, Q. Yu, and Y. Kong, “Evidential deep learning for open set action recognition,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13349–13358, 2021. [15] D. S. Pandey and Q. Yu, “Learn to accumulate evidence from all training samples: Theory and practice,” in International Conference on Machine Learning, pp. 26963–26989, PMLR, 2023. [16] E. H¨ullermeier and W. Waegeman, “Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods,” Machine learning, vol. 110, no. 3, pp. 457–506, 2021. [17] A. Mobiny, P. Yuan, S. K. Moulik, N. Garg, C. C. Wu, and H. Van Nguyen, “Dropconnect is effective in modeling uncertainty of bayesian deep networks,” Scientific reports, vol. 11, no. 1, pp. 1–14, 2021. [18] Y. Sale, V. Bengs, M. Caprio, and E. H¨ullermeier, “Second-order uncertainty quantification: A distance-based approach,” 2024. [19] Y. Ren, A. Bahamou, and D. Goldfarb, “Kronecker-factored quasi- newton methods for deep learning,” arXiv preprint arXiv:2102.06737, 2021. [20] Y. Sale, P. Hofman, L. Wimmer, E. H¨ullermeier, and T. Nagler, “Second-order uncertainty quantification: Variance-based measures,” arXiv preprint arXiv:2401.00276, 2023. [21] M. Caprio, S. Dutta, K. J. Jang, V. Lin, R. Ivanov, O. Sokolsky, and I. Lee, “Credal bayesian deep learning,” arXiv e-prints, pp. arXiv–2302, 2023. [22] M. Caprio, M. Sultana, E. Elia, and F. Cuzzolin, “Credal learning theory,” arXiv preprint arXiv:2402.00957, 2024. [23] F. G. Cozman, “Credal networks,” Artificial intelligence, vol. 120, no. 2, pp. 199–233, 2000. [24] M. Zaffalon, “The naive credal classifier,” Journal of statistical planning and inference, vol. 105, no. 1, pp. 5–21, 2002. [25] M. Caprio, Y. Sale, E. H¨ullermeier, and I. Lee, “A novel bayes’ theorem for upper probabilities,” in International Workshop on Epistemic Uncertainty in Artificial Intelligence, pp. 1–12, Springer, 2023. [26] Y. Sale, M. Caprio, and E. H¨ollermeier, “Is the volume of a credal set a good measure for epistemic uncertainty?,” in Uncertainty in Artificial Intelligence, pp. 1795–1804, PMLR, 2023. [27] B. Ristic, C. Gilliam, M. Byrne, and A. Benavoli, “A tutorial on uncer- tainty modeling for machine reasoning,” Information Fusion, vol. 55, pp. 30–44, 2020. [28] T. Augustin, F. P. Coolen, G. De Cooman, and M. C. Troffaes, Intro- duction to imprecise probabilities, vol. 591. John Wiley & Sons, 2014. [29] P. Lu, M. Caprio, E. Eaton, and I. Lee, “Ibcl: Zero-shot model generation for task trade-offs in continual learning,” arXiv preprint arXiv:2305.14782, 2023. [30] T. Denœux, D. Dubois, and H. Prade, “Representations of uncertainty in artificial intelligence: Probability and possibility,” A Guided Tour of Artificial Intelligence Research: Volume I: Knowledge Representation, Reasoning and Learning, pp. 69–117, 2020. [31] T. Denœux and L. M. Zouhal, “Handling possibilistic labels in pat- tern classification using evidential reasoning,” Fuzzy sets and systems, vol. 122, no. 3, pp. 409–424, 2001. [32] T. Denoeux, “A neural network classifier based on dempster-shafer theory,” IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, vol. 30, no. 2, pp. 131–150, 2000. [33] T. Denœux, “An evidential neural network model for regression based on random fuzzy numbers,” in International Conference on Belief Functions, pp. 57–66, Springer, 2022. [34] G. Shafer, “Dempster-shafer theory,” Encyclopedia of artificial intelli- gence, vol. 1, pp. 330–331, 1992. [35] T. Denœux, “Quantifying prediction uncertainty in regression using random fuzzy sets: the ennreg model,” IEEE Transactions on Fuzzy Systems, vol. 31, no. 10, pp. 3690–3699, 2023. [36] C. W. De Silva, Intelligent control: fuzzy logic applications. CRC press, 2018. [37] X. Zhao, F. Chen, S. Hu, and J.-H. Cho, “Uncertainty aware semi- supervised learning on graph data,” Advances in Neural Information Processing Systems, vol. 33, pp. 12827–12836, 2020. [38] B. Charpentier, D. Z¨ugner, and S. G¨unnemann, “Posterior network: Uncertainty estimation without ood samples via density-based pseudo- counts,” Advances in Neural Information Processing Systems, vol. 33, pp. 1356–1367, 2020. [39] D. S. Pandey and Q. Yu, “Evidential conditional neural processes,” arXiv preprint arXiv:2212.00131, 2022. [40] A.-K. Kopetzki, B. Charpentier, D. Z¨ugner, S. Giri, and S. G¨unnemann, “Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based models reliable?,” in International Conference on Ma- chine Learning, pp. 5707–5718, PMLR, 2021. [41] C. Tomani and F. Buettner, “Towards trustworthy predictions from deep neural networks with fast adversarial calibration,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 9886–9896, 2021. [42] W. Shi, X. Zhao, F. Chen, and Q. Yu, “Multifaceted uncertainty estima- tion for label-efficient deep learning,” Advances in neural information processing systems, vol. 33, 2020. [43] A. Malinin and M. Gales, “Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness,” Advances in Neural Information Processing Systems, vol. 32, 2019. [44] A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,” Advances in neural information processing systems, vol. 31, 2018. [45] D. Ulmer, “A survey on evidential deep learning for single-pass uncer- tainty estimation,” arXiv preprint arXiv:2110.03051, 2021. [46] L. Wimmer, Y. Sale, P. Hofman, B. Bischl, and E. H¨ullermeier, “Quan- tifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures?,” in Uncertainty in Artificial Intelligence, pp. 2282–2292, PMLR, 2023. [47] V. Bengs, E. H¨ullermeier, and W. Waegeman, “Pitfalls of epistemic un- certainty quantification through loss minimisation,” Advances in Neural Information Processing Systems, vol. 35, pp. 29205–29216, 2022. [48] M. J¨urgens, N. Meinert, V. Bengs, E. H¨ullermeier, and W. Waegeman, “Is epistemic uncertainty faithfully represented by evidential deep learning methods?,” arXiv preprint arXiv:2402.09056, 2024. [49] M. Shen, J. J. Ryu, S. Ghosh, Y. Bu, P. Sattigeri, S. Das, and G. W. Wornell, “Are uncertainty quantification capabilities of evidential deep learning a mirage?,” arXiv e-prints, pp. arXiv–2402, 2024. [50] C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning. Springer, 2006. [51] Y. LeCun, “The mnist database of handwritten digits,” http://yann. lecun. com/exdb/mnist/, 1998. [52] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” -, 2009. [53] Y. Le and X. Yang, “Tiny imagenet visual recognition challenge,” CS 231N, vol. 7, no. 7, p. 3, 2015. [54] S. Zhou, K. Chan, C. Li, and C. C. Loy, “Towards robust blind face restoration with codebook lookup transformer,” Advances in Neural Information Processing Systems, vol. 35, pp. 30599–30611, 2022. [55] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019. [56] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability, and variation,” arXiv preprint arXiv:1710.10196, 2017. JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 16 [57] Y. LeCun, P. Haffner, L. Bottou, and Y. Bengio, “Object recognition with gradient-based learning,” in Shape, contour and grouping in computer vision, pp. 319–345, Springer, 1999. [58] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. [59] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012–10022, 2021. [60] E. Huynh, “Vision transformers in 2022: An update on tiny imagenet,” arXiv preprint arXiv:2205.10660, 2022. [61] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, “Visual prompt tuning,” in European Conference on Computer Vision, pp. 709–727, Springer, 2022. [62] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014. [63] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., “Matching net- works for one shot learning,” Advances in neural information processing systems, vol. 29, 2016. [64] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126– 1135, JMLR. org, 2017. [65] D. S. Pandey and Q. Yu, “Multidimensional belief quantification for label-efficient meta-learning,” in Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pp. 14391–14400, 2022. [66] K. Knopp, “Weierstrass’s factor-theorem,” in Theory of Functions: Part II, pp. 1–7, Dover, 1996. [67] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. Deep Shankar Pandey received his Bachelor’s de- gree in Electronics and Communication Engineering from Institute of Engineering, Pulchowk Campus, Tribhuvan University, Nepal in 2017. He completed his PhD degree in Computing and Information Sci- ences at Rochester Institute of Technology with Dr. Qi Yu as his PhD advisor in 2025 where he worked on Uncertainty Aware Meta Learning for Learning from Limited Data. He is currently working as Applied Scientist at Amazon, and this work was done prior to joining Amazon. He has authored several publications in top-tier venues, including NeurIPS, CVPR, ICML, and AAAI. His research focuses on developing trustworthy uncertainty-aware machine learning models with an emphasis on real-world applications. Hyomin Choi received the Ph.D. degree in engineer- ing science from Simon Fraser University, Burnaby, BC, Canada, in 2022. He is a senior staff engineer at the AI Lab, InterDigital, in Los Altos, CA, USA. He was a research engineer at the System IC Research Center, LG Electronics, Seoul, Korea between 2012 and 2016. He received the 2017 Vanier Canada Graduate Scholarship, the 2023 Governor General’s Gold Medal from Simon Fraser University, and the 2023 IEEE Transactions on Circuits and Systems for Video Technology Best Paper Award. He is currently a Member of the IEEE-CAS Multimedia Systems and Applications Technical Committee. His research interests encompass end-to-end learning- based image/video coding, video coding for machines, and machine learning with applications in multimedia processing. Qi Yu is a Professor in the School of Informa- tion (iSchool) at Rochester Institute of Technology (RIT). He earned his PhD in Computer Science from Virginia Polytechnic Institute and State University (Virginia Tech). Dr. Yu’s primary research interests are in artificial intelligence (AI) and machine learn- ing (ML). He has authored over 150 publications, with many appearing in top-tier venues, including NeurIPS, ICML, ICLR, AAAI, IJCAI, AISTATS, CVPR, ICCV, and ECCV. Dr. Yu actively contributes to the research community as an area chair or senior program committee member for major conferences in AI, ML, and computer vision. Additionally, he serves as an Associate Editor for the IEEE Transactions on Services Computing and the IEEE Transactions on Cognitive and Developmental Systems.