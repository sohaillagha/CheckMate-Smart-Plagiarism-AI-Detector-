Continual Learning for Recurrent Neural Networks: an Empirical Evaluation Andrea Cossua,b,1,∗, Antonio Cartaa,1, Vincenzo Lomonacoa, Davide Bacciua aUniversity of Pisa, Largo B. Pontecorvo, 3, 56127, Pisa, Italy bScuola Normale Superiore, Piazza dei Cavalieri, 7, 56126, Pisa, Italy Abstract Learning continuously during all model lifetime is fundamental to deploy ma- chine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like nat- ural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-speciﬁc and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Net- works in class-incremental scenario, by testing their ability to mitigate forgetting with a number of diﬀerent strategies which are not speciﬁc to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear speciﬁcation of the CL scenario. Keywords: continual learning, recurrent neural networks, benchmarks, evaluation 1. Introduction Continual Learning (CL) refers to the ability “to learn over time by accom- modating new knowledge while retaining previously learned experiences” [91]. Traditionally, Machine Learning (ML) models are trained with a large amount of data to solve a ﬁxed task. The assumption is that, at test time, the model will encounter similar data. Unfortunately, real world scenarios do not satisfy this assumption. Non stationary processes produce gradual or abrupt drifts in the data distribution [31, 45] and the models must be continuously adapted to ∗Corresponding author 1Equal contribution Preprint submitted to Elsevier August 3, 2021 arXiv:2103.07492v4 [cs.LG] 2 Aug 2021 the new changes. The objective of incrementally learn new concepts [116] may result in the Catastrophic Forgetting (CF) of previous knowledge [84, 83, 51]. CF causes a deterioration of the performance on old tasks when acquiring new knowledge and it is a direct consequence of the stability-plasticity dilemma [51, 43]: the need to maintain adaptive parameters both stable enough to pre- serve information and plastic enough to adapt to new tasks. Today, CF is considered one of the main problems faced by CL algorithms [108, 87, 30]. Machine Learning models endowed with CL capabilities would radically change the way a model is deployed, removing the need for a separate oﬄine training phase and allowing continuous training during the entire model lifetime without forgetting previous knowledge. In this paper we focus on continual learning with Recurrent Neural Networks (RNNs) [99]. These are adaptive models that can capture the input history within an internal hidden state which is updated iteratively. These models are widely used for Sequential Data Processing (SDP) tasks [48], where the input is a sequence of items. Recurrent models are not the only solution for sequential data processing: time- delayed neural networks [114] and, more recently, Transformers [112] are feed- forward neural networks able to handle sequential patterns. Transformers have not been investigated much in the context of Continual Learning (with one no- table exception [105]), while feedforward models use a ﬁxed context (e.g. sliding windows) to emulate temporal coherence. This may be challenging to imple- ment in Continual Learning, since there may not be the opportunity to collect enough patterns to build the context before making a prediction. Ultimately, recurrent models show important diﬀerences with respect to both convolutional and feedforward approaches (including Transformers). Recurrent models implement a variable number of hidden layers, due to the unrolling through the entire length of the input sequence. This is not the case for feed- forward and convolutional models, whose number of layers is usually ﬁxed a priori. Also, recurrent models leverage weight sharing across time steps. This inﬂuences the trajectory of the learning path of recurrent models when training with backpropagation through time. We hypothesize that these two diﬀerences may impact on the application of CL strategies on recurrent models. Sequential data is widespread in fundamental machine learning applications such as Natural Language Processing [121], Human Activity Recognition [7, 68], Speech recognition [48], and Robotics [21]. All these environments are highly non stationary and represent perfect examples of real world CL applications where data comes in the form of sequences. For example, a robot that learns to walk on diﬀerent terrains or to grasp objects with diﬀerent shapes will re- ceive input patterns from its sensors as time series. During its deployment, the robot will encounter novel environments/objects and will need to adapt to these novel settings continuously. It is impossible to pretrain the robot in advance on all possible environments/objects. What if, once deployed, it is necessary to adapt to a new terrain or to grasp a new object? Retraining from scratch is an expensive, unnecessary solution since most of the knowledge needed is already 2 contained in the model. Continual Learning represents a better solution which can be combined with SDP techniques to appropriately address sequential data. Human activity recognition [55] is another application in which sequential data processing and continual learning interact. The recognition of new classes of activities is typically performed from sequential data (e.g. videos, sensors ob- servations). Therefore, it is fundamental to study how this setting may impact on existing CL strategies. As the examples show, sequential data processing may have a strong impact on Continual Learning, both at the methodological and application level. A true continual learner should be able to deal with temporally correlated patterns, since this is the case in many real world scenarios. This advocates for a more thorough understanding of the behavior of recurrent models in Continual Learn- ing. Currently, most of the Continual Learning literature focuses on Computer Vision and Reinforcement Learning problems [91, 71], while the study of se- quential data processing remains under-documented. The few existing works introduce new approaches tailored to speciﬁc tasks and use a diverse set of learning scenarios and experimental protocols. Therefore: i) it remains unclear whether previously existing CL strategies could still work well in SDP environ- ments and ii) the heterogeneity of learning scenarios in the literature makes it diﬃcult to compare experimental results among diﬀerent papers. In fact, spe- ciﬁc continual settings can have diﬀerent complexity or require domain-speciﬁc solutions. In this paper, we provide a systematization and categorization of current litera- ture to highlight the diﬀerent characteristic of each speciﬁc CL strategy and the peculiarities of the data and the continual environment. We adopt the class- incremental scenario [111] as a common test-bed for CL strategies. This sce- nario involves a stream of data where new steps gradually introduce new classes. Class-incremental scenarios allow to develop and test generic CL strategies for recurrent networks. In our experiments, we verify whether or not CL strate- gies which are not speciﬁcally tailored for recurrent models are still suﬃcient to endow them with continual learning capabilities. To summarize, our main contributions of this work are: a) the ﬁrst comprehensive review of the literature on continual learning in RNNs, including a precise categorization of current strategies, datasets and continual environments adopted by the literature (Section 3); b) the proposal of two novel benchmarks in the class-incremental setting: Syn- thetic Speech Recognition, based on speech recognition data, and Quickdraw, based on hand-drawn sketches (Section 4.2); c) an extensive experimental evaluation of 6 continual learning strategies on sev- eral CL environments for sequence classiﬁcation. To the best of our knowl- edge, this is currently the most extensive evaluation of CL strategies for recurrent models (Section 5); d) an experimental comparison that highlights the eﬀect of the recurrence and 3 Step 1 Task 1 Step 2 Task 1 Step 3 Task 1 (a) Single Incremental Task Step 1 Task 1 Step 2 Task 2 Step 3 Task 3 (b) Multi Task Figure 1: In Single Incremental Task, each step belongs to the same task label (which is the same as not providing a task label). In Multi Task, each step belongs to a diﬀerent task label. Such information can be used to select diﬀerent heads for diﬀerent tasks both at training and test time. Best viewed in color. sequence length on catastrophic forgetting (Section 6). Also, a comparison between single-head and multi-head models (Section 6.3) which justiﬁes our choice of class-incremental scenarios. 2. Continual Learning framework In the rest of the paper, we will refer to the notation and the Continual Learning framework deﬁned in this section. 2.1. Notation and fundamental concepts For a formal deﬁnition of continual learning environments we follow the framework proposed in [71]. For simplicity, here we focus on supervised con- tinual learning on a sequence of steps S = (S1, S2, ...), where each step Si has its own data distribution Di and belongs to a speciﬁc task with label ti. The patterns drawn from each Di are input-target pairs (xj, yj). We study sequence classiﬁcation problems in which yj is the target class corresponding to the input sequence xj. Each sequence xj is composed of T ordered vectors (x1 j , ..., xT j ), xi j ∈Rd. In sequence classiﬁcation, the target is provided only at the end of the entire sequence, that is after seeing the last vector xT j . Also, the sequence length T may vary from one sequence to another, depending on the speciﬁc application. Figure 2 provides a representation of a sequence classiﬁca- tion task. At step i, a new batch of data becomes available. A CL algorithm A executed at step i leverages the current model hi to learn the new training data TRi, drawn from Di. The CL algorithm may also use a memory buﬀer Mi and a task label ti associated to the training patterns. The model is then updated using the new data: Ai : < hi−1, TRi, Mi−1, ti > →< hi, Mi >, ∀Di ∈(D1, D2, ...). (1) Notice that the buﬀer Mi and task label ti are optional and not always available. For example, it may be impossible to store previous data due to privacy con- cerns, while the task labels may not be available in many real-world scenarios. Notice that the following deﬁnition does not pose any computational constraint to continual learning algorithms. However, we are often interested in eﬃcient 4 algorithms. As a results, most algorithms assume to have bounded memory and computational resources. Trivial strategies, such as retraining from scratch using Si i=1 TRi as training data, are unfeasible due to their excessive compu- tational cost and bad scaling properties with respect to the number of steps. Unfortunately, training sequentially on TR1, . . . , TRn will suﬀer from catas- trophic forgetting. Figure 2: Sequence classiﬁcation example. The input sequence xj is composed by T vectors (circles). The target label (square) is provided only after the last vector xT j . Best viewed in color. A Continual Learning scenario de- ﬁnes the properties of the data stream, such as the distinction be- tween diﬀerent tasks and the proper- ties of new data samples. The work of [82] and, subsequently, [71] introduces a classiﬁcation for continual learn- ing scenarios according to two prop- erties: the task label sequence and the content of incoming data. The authors distinguished between Multi Task (MT) scenarios, where each incoming task is diﬀerent from the previous ones, Single Incremental Task (SIT) in which the task is always the same, and Multi Incremental Task (MIT) in which new and old tasks may be interleaved and presented again to the model. Each task, identiﬁed by its task label, can be composed by one or more steps. Figure 1 compares SIT and MT scenarios with respect to the task label information. In addition, data from new steps may provide New Classes (NC), New Instances (NI) of previously seen classes or a mix of both, that is New Instances and Classes (NIC). A previous categorization, proposed in [111], focused on three main scenarios: Task incremental, Domain incremental and Class incremental which can ﬁnd their place in the framework of [82, 71]. Table 1 summarizes and compare these diﬀerent classiﬁcations. Task-incremental (MT, MIT) scenarios assume the availability of task labels for each sample. Unfortunately, in many real-world applications it is diﬃcult to obtain explicit task labels for each sample. While sometimes it may be possible to label the data for training, at test time task labels are often not available. This distinction is fundamental since the availability of task labels simpliﬁes the learning problem (see Section 6.3). For example, in multi-task scenarios it is possible to use a multi-head model, in which each task has its own output layer (head). At test time, the model uses the task label to choose the appropriate head and to compute the output. Most CL systems deployed in the real world operate without task labels, and therefore it is important to develop algorithms that do not require them. Single incremental task scenarios, where the task label is always the same, re- quire to use single-head output layers [38]. Single-head models do not partition the output space, thus they are more susceptible to catastrophic forgetting (see Section 6.3 for an experimental comparison). Even without the availability of task label, a model could still leverage a multi-head output layer, but then it 5 Multi Task Single Incremental Task Multi Incremental Task New Classes Task incremental Class incremental New Instances — Domain incremental New Instances and Classes — Table 1: Comparison of diﬀerent CL scenarios classiﬁcation. The table highlights the fact that the three scenarios for CL introduced by [111] are not the only possible ones in CL. In fact, the scenario classiﬁcation in [82] provides 7 scenarios. Dashes indicate unrealistic scenarios: Multi Task is only compatible with New Classes, since new instances would belong to previously encountered tasks. Empty cells indicate scenarios which could occur but are not included in the classiﬁcation of [111]. would have to infer at test time which head to use [27]. Figure 3 shows the diﬀerence between multi and single headed models. Head 1 MODEL Head 2 Head 3 MODEL Single Head Figure 3: Multi-head model (left) and single- head model (right). In the multi-head model, the output layer allocates a diﬀerent set of out- put units (head) for each task. The single-head model use the same output layer for each task. Best viewed in color. The classiﬁcation proposed above assumes task boundaries are exact and provided by the environment. Task-free or online CL [101, 53, 5, 123] removes such assumption by fo- cusing on a stream of data in which a task label is not provided (SIT) and examples are seen one (or a very small number) at a time. Incre- mental or online learning (see [31] for a review) shares similarities with continual learning. However, online learning usually focuses less on catas- trophic forgetting and more on fast learning of new data. In this scenario, old knowledge must be preserved to help with the new step. However, the old steps are not necessarily revisited. This settings is common to many time series applications, such as stock price prediction, or weather and energy consumption prediction [31]. 2.2. Taxonomy of Continual Learning Strategies Continual learning strategies are traditionally divided into three main fam- ilies [91]: regularization strategies, architectural strategies and replay (or re- hearsal) strategies. 6 Regularization strategies balance plasticity and stability by adding a reg- ularization term to the loss function. For example, Elastic Weight Consolidation (EWC) [64] and Synaptic Intelligence [122] estimate the parameters importance and foster the model stability by penalizing large changes in important parame- ters. Importance values can be updated after each step, as in EWC and its more eﬃcient version [102], or online after each minibatch, as in Synaptic Intelligence, Memory Aware Synapses [3] and EWC++ [19]. The eﬀectiveness of importance-based regularization approaches has been ques- tioned in [72] for class incremental scenarios. The authors showed that this family of strategies suﬀers from complete forgetting. Learning without Forgetting (LwF) [75] is a regularization strategy which is not based on importance values. LwF retains the knowledge of previous steps by using knowledge distillation [58] to encourage the current model to behave similarly to the models trained on the previous steps. Architectural strategies increment the model plasticity with dynamic mod- iﬁcations to its architecture, for example by adding new components [98, 120]. Forgetting may be mitigated by freezing previous components [98], by reorga- nizing the architecture via pruning [61] or by using sparse connections [104]. Due to the very general idea behind this family of strategies, it is quite chal- lenging to organize the existing body of works. Nonetheless, most members of the architectural family share some commonalities: an increasing computational cost with the number of steps due to the incremental model expansion and the need to choose a technique to prevent forgetting on the old model components. Replay strategies leverage an external memory to store previous examples by selecting them randomly or with more ad-hoc criteria [97, 109, 4]. Since the amount of storage required for the memory could grow without bounds, several approaches use generative replay to avoid the need to explicitly store patterns [110, 115]. A generative model replaces the memory, thus bounding the mem- ory cost to the storage of the generative model weights. However, catastrophic forgetting must be still taken into consideration when training the generative model. The three aforementioned families of CL strategies do not capture the en- tirety of the proposals in CL. Bayesian approaches are increasingly used to mitigate CF [123, 86, 73, 37] and they have been successfully combined with regularization [33, 2], architectural approaches [85] and replay [67]. Bayesian methods in CL consider an initial distribution on the model parameters and iteratively approximate its posterior when new tasks arrive. The approximation usually includes components able to mitigate forgetting of previous tasks: as an example, the learning trajectory can be controlled by the uncertainty as- sociated to each model parameter, computed from its probability distribution. If a parameter has a large uncertainty value, it is deemed not to be relevant for the current task. Therefore, to protect important parameters and mitigate forgetting, the allowed magnitude in parameter change is set to be proportional to its uncertainty [2, 33]. Another line of research explores the use of Sparse distributed represen- 7 tations [1]. Machine learning models often produce dense, entangled activa- tions, where a slight change in one parameter may aﬀect the entire represen- tation, causing catastrophic forgetting. In contrast, sparse distributed repre- sentations naturally create independent substructures which do not aﬀect each other and are less prone to CF [44]. As a result, methods that encourage spar- sity produce disentangled representations which do not interfere with each other [47, 90, 25, 6]. The main problem is that sparsity by design is not suﬃcient: the model has to learn how to use sparse connections during training. Since one of the objectives of CL is to quickly learn new information, it seems natural to combine CL and Meta Learning [60]. There are already eﬀorts in this direction [12, 62, 16], with a focus on online and task-free scenarios [39, 57, 54]. Finally, CL has also been studied through the lens of graph- structured data: expressing similarities in the input patterns through graphs may help in mitigating forgetting [106]. Also, deep graph networks have been successfully combined with CL approaches on a number of class-incremental graph benchmarks [17]. In the remainder of this section, we present in more details the methods that we decided to use in our experiments. We focus on regularization and replay approaches for recurrent networks. Architectural approaches are a promising avenue of research but they cannot be easily adapted to diﬀerent model archi- tectures. As a result, none of them has emerged as a standard in the literature. Therefore, we decided to focus on model-agnostic methods that can be easily adapted to a wide range of models and CL scenarios. Elastic Weight Consolidation. Elastic Weight Consolidation (EWC) [64] is one of the most popular CL strategies. It belongs to the regularization family and it is based on the estimation of parameters importance. At the end of each step, EWC computes an importance vector Ωn Θ for parameters Θ. The importance vector is computed on the training set D at step n by approximating the diagonal elements of the Fisher Information Matrix: Ωn Θ = E(x,y)∈D  diag((∇Θ log pΘ(y|x)) (∇Θ log pΘ(y|x))T )  = E(x,y)∈D  (∇Θ log pΘ(y|x))2 , (2) where pθ is the output of the model parameterized by Θ and (x, y) is the input- target pair. By strictly following Eq. 2, the expectation operator would require to compute the squared gradient on each data sample and then average the result. Since this procedure may be quite slow, a minibatch approach is usually taken. No major diﬀerence is experienced between these two versions. During training, the loss function is augmented with a regularization term R which keeps important parameters close to their previous value: R(Θ, Ω) = λ n−1 X t=1 X θ∈Θ Ωt θ(θt −θn)2. (3) The hyperparameter λ controls the amount of regularization. 8 Memory Aware Synapses. Memory Aware Synapses (MAS) [3] is similar to EWC since it is an importance-based regularization method. Unlike EWC, MAS computes importances online in an unsupervised manner. Therefore, MAS keeps only a single importance vector ΩΘ and updates it with a running average computed after each pattern: ΩΘN+1 = NΩΘN + ∇Θ∥pΘ(xk)∥2 2 N + 1 , (4) where N indexes the number of patterns seen so far. To make the update faster, the running average in Eq. 4 can be computed after each minibatch. The penalization during training is the same of EWC (Eq. 3). Learning without Forgetting. Learning without Forgetting (LwF) [75] is a regu- larization approach based on knowledge distillation [58]. At the end of each step, a copy of the current model is saved. During training, the previous model pro- duces its outputs on the current step input. The learning signal for the current model is regularized by a function measuring the distance between the current model output and the previous model output. The objective is to keep the two output distributions close to each other. Formally, when training on step t, the total loss Lt is the sum of the classiﬁcation loss CE (e.g. cross entropy) and the distillation penalization: Lt(xt, yt; Θt, Θt−1) = CEΘt(xt, yt) + + λ KL[pΘt(xt)||pΘt−1(xt)], (5) where KL[p || q] is the KL-divergence between p and q. The softmax temperature T used in the ﬁnal layer of the previous model can be tuned to control the prediction conﬁdence. Gradient Episodic Memory. Gradient Episodic Memory (GEM) [79] is a CL strategy which mitigates forgetting by projecting the gradient update along a direction which does not interfere with previously acquired knowledge. The av- erage gradient for each step is computed using a small buﬀer of samples collected from previous steps. The steps gradients are used to deﬁne a set of constraints which cannot be violated by the update direction. Finding the optimal direction requires solving a quadratic programming problem: min z 1 2 ∥g −z∥2 2 (6) subject to Gz ≥γ, (7) where G is a matrix in which each column is the gradient computed step-wise on the memory samples (one column per previous step). The vector g is the gradient update and γ is the margin within which the constraints must be respected. GEM can work also in online scenarios [79]. 9 Averaged Gradient Episodic Memory. Average-GEM (A-GEM) [20] is a more eﬃcient version of GEM where the quadratic program solution is approximated over a random sample from the memory, thus removing the need to compute a constraint for each previous step. In this way the constraints may be broken for some step, resulting in worse performance. The projected gradient ˆg is computed by ˆg = g −gT gref gT refgref gref, (8) where gref is the gradient computed on the random memory sample and g is the proposed gradient update. A-GEM has also been tested in online scenarios [20]. Replay. Replay strategies store a subset of input patterns from previous steps in a separate replay memory. To support the generality of our experiments, in this paper we selected patterns at random from the training set, without using any speciﬁc selection procedure. The memory accepts up to K patterns per class. During training, each minibatch is augmented with P patterns per previous class, selected at random from the replay memory. We keep P to very small values in order to make the approach reasonable in a real CL environment. The CL strategies presented above belong to diﬀerent families and have dif- ferent characteristics. In order to choose which strategy is more suitable to a speciﬁc application, one has to understand their advantages and limitations. Regularization strategies like EWC, LwF and MAS are very eﬃcient since they operate without keeping a memory of previous pattern. However, their per- formance usually deteriorates when observing a large number of steps [72]. In contrast, Replay remains eﬀective even on long streams of steps [56], even if the storage of previous patterns may be unfeasible for applications with strict memory bounds. Finally, GEM and A-GEM share the storage problems of re- play, since they keep a buﬀer of previous samples. However, their performance is usually superior to the regularization strategies in class-incremental settings [79]. 3. Continual Learning with Recurrent Models The literature on continual learning focuses on feedforward and convolutional models, with experiments in Computer Vision and, to a lesser extent, Reinforce- ment Learning applications. Recently, there has been a growing interest towards continual learning applications with sequential data. In this section, we provide the ﬁrst comprehensive review of the literature on recurrent neural networks in continual learning settings. In addition, we describe the main datasets and benchmarks for sequential data processing used in Continual Learning. 10 Paper Deep RNN Application CL scenario Large Comparison seminal [95] × - - × [42] × - SIT × [9] × - SIT × [8] × - SIT × [26] × - SIT × NLP [10] ✓ domain adaptation SIT × [74] ✓ instruction learning, NMT SIT × [118] ✓ language modeling - × [66] × language modeling Online × [81] ✓ sentiment analysis - × [107] ✓ NMT - × bio-inspired [28] × - Online × [88] × - Online, SIT × [92] × vision - × [65] × motor control MT × [89] × - Online × deep learning [119] ✓ ASR - × [103] ✓ - - × [100] ✓ synthetic - × [27] ✓ generic SIT × [32] ✓ neuroscience - × [34] ✓ - MT ✓ this work ✓ - SIT ✓ Table 2: Overview of the literature based on our categorization. Deep RNN refers to the use of a learning model attributable to the family of deep recurrent networks (e.g. LSTM). Application-agnostic papers have a dash in the application column. A dash in the CL scenario indicates that the paper does not provide its clear indication. Large comparison refers to experiments with at least 3 CL baselines on two diﬀerent application domains. 3.1. Survey of Continual Learning in Recurrent Models We propose a new classiﬁcation in which each paper is assigned to one of 4 macro-areas: seminal work, natural language processing applications, bio- inspired and alternative recurrent models, and deep learning models. Table 2 provides a ﬁne-grained description of our review. For each paper we highlight 5 diﬀerent properties: the group to which the paper belongs to, the use of popular deep learning architectures, the application domain, the type of CL scenarios and if the paper provides a large experimental comparison. Table 2 shows that we are the only one to provide a large scale evaluation of recurrent models in SIT+NC scenario. The details of our categorization are discussed in the following. 11 Seminal Work. Historically, interest in CL and Sequential Data Processing traces back to [95] and [42]. The CHILD model [95] represents one of the ﬁrst attempts to deal with sequential data in reinforcement learning environments like mazes. Although CHILD lacks an explicit recurrent structure, the author discussed about the possibility to include a more powerful memory component into the model (i.e. a recurrent neural network) to address tasks with longer temporal dependencies. However, the author also recognizes the diﬃculties in this process, due to the challenge of learning very long sequences. Moreover, the ﬁxed structure of a RNN compared with a growing model like CHILD high- lighted the need for approaches based on model expansion. In the same period, French introduced the pseudo-recurrent connectionist net- work [42, 41], a model which makes use of pseudo replay [96] (replay based on random, but ﬁxed, input patterns), but did not address sequential data process- ing tasks. Later on, the pseudo recurrent network together with pseudo-replay inspired the Reverberating Simple Recurrent Network (RSRN) [9, 8]. This is a dual model composed by two auto-associative recurrent networks which ex- change information by means of pseudo patterns. In the awake state, the per- formance network learns a new pattern through backpropagation. The storage network generates a random pseudo pattern and presents it to the performance network, interleaved with the real one. Once the loss falls below a certain threshold, the system enters the sleep state, in which the performance network generates a pseudo pattern and the storage network learns from it. In this way, pseudo patterns produced in the awake state carry information about previous sequences to the performance network, while pseudo patterns produced in the sleep state carry information about the recently acquired knowledge to the stor- age network. The authors showed the presence of forgetting on toy sequences and the beneﬁcial eﬀect of pseudo patterns in mitigating it. However, from their experiments it is not possible to assess the eﬀectiveness of RSRN on more realistic benchmarks. From the very beginning, sparsity has been a recurring theme in Continual Learning [43, 40]. The Fixed Expansion Layer [25] introduced the use of a large, sparse layer to disentangle the model activations. Subsequently, the Fixed Ex- pansion Layer has been applied to recurrent models [26]. However, in order to build the sparse layer in an optimal way, the model requires to solve a quadratic optimization problem (feature-sign search algorithm) which can be problematic in real world problems (as we discuss in Section 6). Natural Language Processing. Natural Language Processing is becoming one of the main test-beds for continual and online settings in sequential data processing [13]. Most proposals in this area used modern recurrent architectures and fo- cus on speciﬁc problems and strategies. Moreover, they rarely compare against existing CL techniques. It is therefore challenging to draw general conclusions on recurrent networks in CL from this kind of experiments. Examples of ap- plications are online learning of language models where new words are added incrementally [74, 118, 66], continual learning in neural machine translation on multiple languages [107] and sentiment analysis on multiple domains [81]. 12 The use of attention mechanisms [11], now widespread in NLP, may provide ap- proaches which are widely applicable, since there is no assumptions on the type of information to attend to. As an example, the Progressive Memory Banks [10] augments a recurrent neural network with an external memory. The memory grows in time to accommodate for incoming information, while at the same time freezing previous memory cells successfully prevents forgetting. In addition, the author showed that ﬁnetuning old cells, instead of freezing them, increases for- ward transfer on future data. Their experiments are executed on incremental domain adaptation tasks, where the distribution of words shifts as new domains are introduced. Bio-inspired and Alternative Recurrent Models. There are a number of con- tributions that propose customized architectures to address continual learning problems and mitigate catastrophic forgetting. Spiking models for CL, such as the Hierarchical Temporal Memory [28] and the Spiking Neural Coding Net- work [88], are naturally designed to tackle sequential data processing tasks. More importantly, they adopt learning algorithms which allow to control more eﬃciently the stability-plasticity trade-oﬀ. While promising, these approaches are still missing a thorough empirical validation on real world CL benchmarks. Echo State Networks [80] are RNNs whose recurrent component, called reser- voir, is not trained. Therefore, they are appealing for CL since the reservoir cannot suﬀer from forgetting. One of the drawbacks may be the fact that the static connections must be able to learn diﬀerent tasks without adapting their values. The work in [65] tries to overcome the problem by employing a frac- tal reservoir combined with an external task vector (based on the task label) representing the current task. Diﬀerent reservoir chunks process diﬀerent pat- terns based on their associated task vector. While the ﬁnal performance on diﬀerent motor commands for reinforcement learning environments validated the approach, the requirement of multi task scenarios limits its applicability. The Backpropagation Through Time (BPTT) is the most used algorithm to train recurrent networks. The Parallel Temporal Neural Coding Network [89] introduced a new learning algorithm which is less susceptible to forgetting than BPTT and other variants in sequential benchmarks like MNIST and language modelling. Temporal information may also arrive from videos. This is particularly impor- tant since it allows to exploit the vast literature on CL and Computer Vision. However, it is also possible to develop speciﬁc solutions, as it has been done with recurrent Self Organizing Maps (SOM) [92]. The authors incorporate temporal information into the recurrent SOM and perform object detection from short videos with CORe50 dataset [77]. Deep Learning Models. Recently, there have been a number of papers that stud- ied CL applications in sequential domains using recurrent architectures widely used in the deep learning literature, such as Elman RNNs [36] and LSTMs [59]. The advantage of this generic approach is that it can be easily adapted to spe- 13 cialized models to solve any sequential problem. As expected, vanilla recurrent models such as Elman RNNs and LSTMs suﬀer from catastrophic forgetting in CL scenarios [103, 100]. The combination of existing CL strategies, like Gradient Episodic Memory [79] and Net2Net [22], with RNNs has already showed promising results [103]. Con- trary to vanilla LSTM networks, their model was able to mitigate forgetting in three simple benchmarks. This important result supports the need for an exten- sive evaluation of RNNs and CL strategies not speciﬁcally tailored to sequential data processing problems. Recurrent networks are also inclined to be combined with architectural strate- gies, since most of them are model agnostic. The idea behind Progressive net- works [98] has been applied to recurrent models [27] and also improved by removing the need for task labels at test time with a separate set of LSTM au- toencoders, able to recognize the distribution from which the pattern is coming from. The resulting model is multi-headed but it is able to automatically select the appropriate head at test time in class-incremental scenarios. Hypernetworks (previously used for CL in [113]) are able to mitigate forgetting when combined with RNNs [34]: this work was the ﬁrst to provide an extensive comparison of traditional CL techniques in several sequential domains. Diﬀerently from this paper, they use multi-task scenarios with a multi-head (see Section 6.3 for a comparison between single-head and multi-head models). Preserving the space spanned by the connections from being corrupted by the weight update appears to be beneﬁcial also to CL [32]. Finance [94] and Automatic Speech Recogni- tion [119] applications have been explored as candidate application domains for online and continual learning strategies. 3.2. Sequential Data Processing Datasets for Continual Learning Due to the diﬀerent application domains and diﬀerent research communities interested in continual learning for SDP domains, there are no standard bench- marks used to evaluate CL strategies. Existing benchmarks vary greatly in terms of complexity. Furthermore, diﬀerent application domains use a slightly diﬀerent language. In this section, we provide a review of the diﬀerent datasets and continual learning scenarios, following the classiﬁcation described in Table 1. We believe that this review can favor the cross-pollination between classic CL techniques and sequential domains and between diﬀerent sequential domains. Table 3 provides an overview of the diﬀerent datasets in literature. For each dataset we highlight previous work that used the datasets, the application domain of the data, and the CL scenario according to Table 1. Clearly, most datasets in literature are used by few, or even just one, paper. This is due to the diﬀerent research questions each paper tries to answer: since few works are spread over very diﬀerent areas, it is natural to ﬁnd diﬀerent benchmarks and evaluation protocols. Unfortunately, these diﬀerences in the experimental setups make it impossible to compare diﬀerent models in the literature or to deduce general and task-independent conclusions. Diﬀerent synthetic benchmarks have been adapted to continual scenar- ios. The Copy Task [50], a benchmark used to test the short-term memory of 14 Dataset Application Scenario Copy Task [103, 34] synthetic MT+NI Delay/Memory Pro/Anti [32] synthetic, neuroscience MT+NI Seq. Stroke MNIST [103, 34] stroke classiﬁcation SIT+(NI/NC) Quick, Draw! † stroke classiﬁcation SIT+NC MNIST-like [27] [26] † object classiﬁcation SIT+(NI/NC) CORe50 [92] object recognition SIT+(NI/NC) MNLI [10] domain adaptation SIT+NI MDSD [81] sentiment analysis SIT+NI WMT17 [14] NMT MT+NC OpenSubtitles18 [76] NMT MT+NC WIPO COPPA-V2 [63] [107] NMT MT+NC CALM [66] language modeling Online WikiText-2 [118] language modeling SIT+NI/NC Audioset [27, 34] sound classiﬁcation SIT+NC LibriSpeech, Switchboard [119] speech recognition (SIT/MT)+NC Synthetic Speech Commands † sound classiﬁcation SIT+NC Acrobot [65] reinforcement learning MT+NI Table 3: Datasets used in continual learning for sequential data processing. The scenario column indicates in which scenario the dataset has been used (or could be used when the related paper does not specify this information). Datasets used on this paper are marked with †. recurrent models, incrementally increases the sequence lengths in the continual setting (SIT+NI). However, the data generating distribution remains constant, which means that the drift between the diﬀerent steps is limited. Pixel-MNIST is another popular benchmark for RNN models [69] where MNIST digits are presented one pixel at a time, either with the original order or using a ﬁxed permutation. Continual learning scenarios based on pixel-MNIST include new classes (SIT+NC in [27] and this paper or MT+NC in [35]) or new permutations (SIT+NI in this work). Sequential Stroke MNIST [29] represents MNIST digits [70] as a sequence of pen strokes, with pen displacements as features (plus pen up/down bit). The dataset is adapted to a continual data stream by increasing the sequence length (SIT+NI) or by creating new classes (SIT+NC in [27] and this paper, or MT+NC in [35]). More realistic CL benchmarks for computer vision, like CORe50 [77], can be used also in sequential contexts to leverage temporal correlated information from videos. In the Natural Language Processing domain a common scenario is the domain-incremental setting, where new instances from diﬀerent topics are grad- ually introduced (SIT+NI). Examples of applications are natural language in- ference [117, 10], sentiment analysis [81] and machine translation in a MT+NC scenario [107]. Alternatively, [118] studies the problem of online learning a lan- guage model. In this work, EWC is used to keep the previous knowledge when the recurrent model is trained with a single sequence. CALM is a benchmark 15 speciﬁcally designed for online CL in language modeling [66] which provides a realistic environment in which to test NLP models. The most common CL scenario in the audio signal processing domain is the incremental classiﬁcation (MT/SIT+NC), where new classes are gradually introduced [27]. For example, AudioSet [46] is a dataset of annotated audio events. The raw audio signals are already preprocessed, generating sequences of 10 timesteps with 128 features each. Diﬀerently from Sequential Stroke MNIST or Copy Task, AudioSet is a real-world application. However, the small sequence length may conceal possible problems when working with RNNs. Other datasets in the literature refer to speciﬁc applications, like reinforcement learning [65], or neuroscience problems [32]. 4. Evaluating Continual Learning Strategies In our review, we showed that literature on CL strategies for SDP domains is still in its early stages. The use of heterogeneous benchmarks and of diﬀerent jargon is detrimental to the comparison of the various approaches in litera- ture. Most of the works focus on customized strategies, often ignoring popular techniques widely employed in other domains where CL is more mature, e.g. computer vision. Furthermore, benchmarks have large variations in terms of complexity. It is for these reasons that we believe of fundamental importance to put forward a solid and articulated evaluation of existing CL strategies on RNN architectures. To this end, we design an experimental setup which is easily reproducible and general enough to be used with all recurrent architectures. In this section, we focus on two main points related to our evaluation pro- tocol: the choice of the CL scenarios and the application-agnostic nature of the benchmarks. 4.1. Deﬁning the Continual Learning Scenario We rely on class-incremental classiﬁcation (SIT+NC) of sequences as a stan- dard and generic scenario to test novel CL techniques for recurrent models. Incremental classiﬁcation is a challenging task - at least in the SIT scenario without explicit task labels - and it is directly applicable to many real-world problems, such as incremental audio classiﬁcation. We deliberately avoid the use of Multitask scenarios (MT, MIT), in which the task label is explicitly provided for each pattern, both at training and test time. Although this assumption may be reasonable in very speciﬁc applications (e.g. machine translation), in the vast majority of the cases such information is not available. We also experimented with a popular benchmark in the Domain in- cremental scenario (SIT+NI). This represents a simpler CL problem than class incremental. Still, it is compatible with our setup since it does not use knowl- edge about task labels. 16 4.2. Benchmarks We believe there is a necessity for generic CL benchmarks that measure ba- sic CL capabilities in sequential domains. This paper contributes to this goal by adapting the Synthetic Speech Commands dataset [15] and Quick, Draw! dataset [52] to the incremental classiﬁcation scenario. These are two tasks which are quite easy in an oﬄine setting (especially the Synthetic Speech Commands) but become challenging in a continual setting, making them ideal as a bench- mark. Moreover, they are suitable to evaluate recurrent models since they have a sequential nature: feedforward counterparts, although applicable, are more expensive in terms of adaptive parameters since there is no weight sharing and they do not incorporate any kind of temporal knowledge. In addition to the data above, our experimental protocol also includes the Permuted MNIST and Split MNIST datasets, since they are two of the most used benchmarks in the papers surveyed in our review. Table 4 summarizes the main characteristics of the benchmarks used in the experiments, while Figure 4 provides a graphical representation of their patterns. Figure 5 highlights how our class-incremental benchmarks were created starting from the dataset classes. Synthetic Speech Commands (SSC). SSC is a dataset of audio recordings of spoken words [15]. Each audio represents a single word, taken from a vocabulary of 30 words, sampled at 16kHz. We preprocessed each sample to extract 40 Mel coeﬃcients using a sliding window length of 25 ms, with 10 ms stride. The resulting sequences have a ﬁxed length of 101 timesteps. We did not apply any further normalization. We create a Class Incremental scenario (SIT+NC) from SSC by learning two classes at a time, for a total number of 10 steps. For model selection and assessment, we used a total of 16 classes out of the 30 available ones. Additional details on the use of SSC in the experiments are reported in Appendix B.1. Quick, Draw! (QD). QD is a dataset of hand-drawn sketches, grouped into 345 classes [52]. Each drawing is a sequence of pen strokes, where each stroke has 3 features: x and y displacements with respect to the previous stroke and a bit indicating if the pen has been lifted. We adopted a Class-incremental scenario (SIT+NC) by taking 2 classes at a time for a total number of 10 steps. For model selection and assessment, we used a total of 16 classes out of the 345 available ones. Appendix B.2 provides details about the classes used in the experiments. Diﬀerently from the previous datasets Quick, Draw! sequences have variable length. Since this is often the case in real-world applications, it is important to assess the performance of recurrent models in this conﬁguration. Permuted MNIST (PMNIST). PMNIST is heavily used to train recurrent neu- ral networks on long sequences [69]. Each MNIST image is treated as a sequence of pixels and shuﬄed according to a ﬁxed permutation. Each step of the data stream uses a diﬀerent permutation to preprocess the images and uses all the 10 MNIST classes. This allows to easily create an unlimited number of steps. This conﬁguration corresponds to a Domain incremental scenario (SIT+NI). 17 SSC QD SMNIST PMNIST CL Steps 10 10 5 10 Classes per step 2 2 2 2 Sequence length 101 variable (8-211) 28/196 28/196 Total number of classes 30 345 10 10 Input size 40 3 28/4 28/4 Table 4: Benchmarks used in the experimental evaluation. QuickD, Draw has patterns with variable sequence length, from a minimum of 8 steps to a maximum of 211 steps . Experiments with SMNIST and PMNIST have been conducted with diﬀerent sequence lengths by taking 28 or 4 pixels at a time, resulting in sequences of length 28 and 196, respectively. Split MNIST (SMNIST). SMNIST is another popular continual adpatation of sequential MNIST. In this scenario, MNIST sequences are taken two classes at a time. First, all 0s and 1s are fed to the model. Then, all 2s and 3s. And so on and so forth up to the last pair (8 and 9). This scenario consists of only 5 steps. The conﬁguration corresponds to a Class Incremental benchmark (SIT+NC). 5. An Experimental Protocol for CL in SDP Domains In the following section we describe the experimental settings that we pro- pose to robustly assess CL strategies within SDP tasks and that are used in the experiments discussed in the following2. Model Selection. Hyperparameter tuning is a critical phase when building solid machine learning based predictors. This process is made considerably more diﬃcult in CL scenarios, since we cannot assume to have a separate validation set comprising the entire stream of data in advance. As a result, it is not possible to build the standard train-validation-split used in model selection. To solve this problem, we follow the model selection procedure for CL introduced in [20]. We separate the data stream into validation and test steps. In particular, we hold out the ﬁrst 3 steps from our data stream and use them to perform model selection. After selecting the best hyperparameters, we use the remainder of the stream (the test stream, composed of 10 unseen steps in our experiments), to perform model assessment. Since Split MNIST has a limited number of steps (5), we decided to perform model selection and model assessment on the same data stream for this particular benchmark. Appendix A reports the parameters used in the model selection procedure and the selected conﬁgurations for both feedforward and recurrent models. 2We publish the code and conﬁguration ﬁles needed to reproduce our experiments at this link https://github.com/AndreaCossu/ContinualLearning_RecurrentNetworks 18 5 10 15 20 5 10 15 (a) SSC (b) Quick, Draw! (c) Row MNIST (d) Permuted Row MNIST Figure 4: Graphical representation of two patterns for each benchmark used in the experi- ments. Fig. 4a SSC uses 40 Mel features (columns) for each of the 101 steps (rows). Both plots use log values. Fig. 4b provides sketches of images (cat and airplane). Fig. 4c shows MNIST digits, which are provided to the model one row at a time in the Row MNIST version. Permuted MNIST (Fig. 4d) permutes images row-wise. Best viewed in color. Data Preprocessing. Preprocessing a dataset in a CL setting suﬀers from the same limitations highlighted for the model selection phase. Computing global statistics such as the mean and variance require the entire stream, which is not available beforehand. As a result, we cannot operate any kind of normalization that needs access to a global statistic, since do not have access to the entire stream in advance. For MNIST data we simply divided the pixel value for 255, which does not require any prior knowledge about the data stream (only about the limits of the data generating device). Models. The reference recurrent model that we use in our experiments is the popular Long Short-Term Memory network (LSTM) [59] (PyTorch implemen- tation [93]). LSTMs are among the most used recurrent models [24] and their performance is comparable to other alternatives, like Gated Recurrent Units [23]. Therefore, we decided to use LSTM as general representatives of recurrent architectures. To provide a means of comparison between feedforward and recurrent models, we also consider and experiment with Multi Layer Perceptrons (MLP) with ReLU activations. Notice that the feedforward models take as input the entire sequence, therefore, they can be viewed as recurrent networks which receive se- quences of length 1. This alternative view over these models is useful since we 19 MODEL 1 MODEL 2 MODEL 3 Step 1 Step 2 Step 3 Figure 5: Class-incremental benchmark with 3 steps starting from the patterns of Quick, Draw! dataset. Each step is composed by 2 classes. experimented with multiple sequence lengths. On PMNIST and SMNIST, we ran experiments with LSTM which takes as input 4 pixels at a time (LSTM-4), 16 pixels at a time (LSTM-16) and 28 pixels at a time (ROW-LSTM). Performance Metrics. In order to assess the performance of the continual learn- ing strategies we chose to focus on catastrophic forgetting by measuring the av- erage accuracy over all steps at the end of training on last step T. This metric, called ACC [79], can been formalized as: ACC = 1 T T X t=1 RT,t, (9) where RT,t is the accuracy on step t after training on step T. In Appendix A.1 we also compare execution times in order to assess the computational eﬃciency of diﬀerent strategies. Strategies. We compare the performances of our models in combination with 6 CL strategies: EWC [64], MAS [3], LwF [75], GEM [79], A-GEM [20] and Replay. In addition to the continual learning strategies, we provide two baselines. Naive trains the model on the sequence without applying any measure against forget- ting. It is useful to provides a lower-bound to any reasonable CL strategy. Joint Training trains the model by considering the concatenation of the full stream of sequential tasks in a single batch3. Notice that this is not a CL strategy since it assumes access to the entire data stream. Joint Training can be used to highlight the eﬀect of the continuous training on the model performance. 3Joint Training is often used as an upper bound performance for CL. However, in the presence of a strong positive forward and backward transfer this may not be true. 20 PMNIST MLP ROW-LSTM LSTM-4 EWC 0.92±0.02 0.58±0.09 0.29±0.05 LWF 0.82±0.03 0.77±0.05 0.35±0.04 MAS 0.58±0.04 0.64±0.07 0.31±0.04 GEM 0.94±0.01 0.90±0.01 0.71±0.02 A-GEM 0.80±0.03 0.58±0.08 0.20±0.03 REPLAY 0.92±0.00 0.92±0.01 0.82±0.01 NAIVE 0.31±0.04 0.61±0.06 0.30±0.03 Joint Training 0.97±0.00 0.96±0.00 0.94±0.01 SMNIST MLP ROW-LSTM LSTM-4 EWC 0.21±0.01 0.21±0.02 0.19±0.00 LWF 0.70±0.02 0.31±0.07 0.26±0.06 GEM 0.91±0.01 0.93±0.03 0.66±0.04 A-GEM 0.20±0.00 0.20±0.00 0.13±0.02 MAS 0.20±0.00 0.20±0.00 0.19±0.00 REPLAY 0.82±0.02 0.89±0.01 0.61±0.06 NAIVE 0.20±0.00 0.20±0.00 0.19±0.00 Joint Training 0.95±0.00 0.97±0.00 0.95±0.01 Table 5: Mean ACC and standard deviation over 5 runs on PMNIST and SMNIST bench- marks. 6. Results We ran a set of experiments aimed at verifying 1) whether general CL strate- gies are able to mitigate forgetting in recurrent models and 2) the eﬀect of se- quence length on the catastrophic forgetting. Table 5 reports the mean ACC and its standard deviation (over 5 runs) computed on PMNIST and SMNIST. Table 6 reports the same results computed on SSC and QD. 6.1. Catastrophic Forgetting with CL strategies In Class-incremental scenarios, importance-based regularization strategies are subjected to a complete forgetting, as also showed by [72] for feedforward models. We conﬁrmed their results also for recurrent models.The eﬀectiveness of replay in SIT scenario is also conﬁrmed by our experiments. GEM emerges as one of the best performing approaches not only for SM- NIST and PMNIST, but also for the more complex SSC and QD. However, its computational cost remains very large (as showed in Appendix A.1), due to the quadratic optimization requested to ﬁnd a projecting direction. Unfortunately, its more eﬃcient version A-GEM results in complete forgetting. This is caused by the fact that A-GEM is not constrained by each previous steps, but instead, it computes an approximation of the constraints by sampling patterns randomly 21 SSC MLP LSTM EWC 0.10±0.00 0.10±0.00 LWF 0.05±0.00 0.12±0.01 MAS 0.10±0.00 0.10±0.00 GEM 0.55±0.00 0.53±0.01 A-GEM 0.05±0.00 0.09±0.01 REPLAY 0.81±0.03 0.73±0.04 NAIVE 0.10±0.00 0.10±0.00 Joint Training 0.93±0.00 0.89±0.02 QD LSTM EWC 0.12±0.02 LWF 0.12±0.01 MAS 0.10±0.00 GEM 0.47±0.03 A-GEM 0.10±0.00 REPLAY 0.49±0.02 NAIVE 0.10±0.00 Joint Training 0.96±0.00 Table 6: Mean ACC and standard deviation over 5 runs on Synthetic Speech Commands and Quick, Draw! benchmarks. from the memory. While this may be suﬃcient in a MT scenario, it does not work appropriately in a SIT scenario. LwF has been the most diﬃcult strategy to use, requiring a careful tuning of hyperparameters which questions its applicability to real-world environment. While it works on SMNIST, its performance rapidly decreases on more complex benchmarks like SSC and QD, where it suﬀers from complete forgetting. 6.2. Sequence Length aﬀects Catastrophic Forgetting Our experiments on PMNIST and SMNIST also allow to draw some con- clusions on the eﬀect of sequence length on forgetting. Fig. 6 shows mean ACC results for diﬀerent sequence lengths. The decreasing trends highlights a clear phenomenon: long sequences cause more forgetting in recurrent networks. Regularization strategies suﬀer the most when increasing sequence length. Figure 7 oﬀers some insights also on replay: although more robust than the other strategies in terms of ﬁnal performance, replay needs more patterns to recover from forgetting as sequences become longer. The results on PMNIST with MAS may seem to contradict the sequence- length eﬀect, since ROW-LSTM performs surprisingly better than MLP. How- ever, the performance has to be measured relatively to the Naive performance, in which no CL strategy is applied. When compared to this value, the ROW-LSTM 22 MLP (1) ROW-LSTM (28) LSTM-16 (49) LSTM-4 (196) Model (Sequence Length) 0.0 0.2 0.4 0.6 0.8 1.0 ACC lwf gem agem mas ewc replay-10 replay-1 (a) Permuted MNIST MLP (1) ROW-LSTM (28) LSTM-16 (49) LSTM-4 (196) Model (Sequence Length) 0.0 0.2 0.4 0.6 0.8 1.0 ACC lwf gem replay-10 replay-1 (b) Split MNIST Figure 6: Average ACC on all steps for diﬀerent sequence lengths and diﬀerent CL strategies. Sequence length causes a decrease in performances among all strategies. Best viewed in color. does not perform any better than MLP. Instead it achieves a worse accuracy with respect to the Naive, as expected. The MAS-Naive ratio is 1.87 for MLP, but only 1.05 for MAS. 6.3. Comparison between Multi-Head and Single-Head Models Although in this work we focused on Class Incremental and Domain Incre- mental scenarios, we also provide a comparison with Task-Incremental scenarios (MT + NC). Figure 8 shows results for the same experimental conﬁguration in Section 6, but with a multi-head model instead of a single-head. We did not test multi-head models in PMNIST since this benchmark always uses the same output units for all tasks. Appendix C reports Table C.13 with detailed results. There is a clear diﬀerence between single and multi-head performances. A multi-head setting easily prevents CF: even a simple Naive procedure is able to retain most of the knowledge in both class-incremental scenarios. The inﬂuence of sequence length in single-headed models is not reﬂected in the multi-head case. In fact, the LSTM performs better than MLP on SSC: this could be related to the fact that, given the already large mitigation of forgetting with multi-head, the sequential nature of the problem favours a recurrent network. We concluded that MT scenarios with multi-head models should be used only when there is the need to achieve high-level performances in environments where tasks are easily distinguishable. However, task label information should never be used to derive general conclusions about the behavior and performance of CL algorithms. 23 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM-ROW LSTM-16 LSTM-4 (a) Permuted MNIST 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM-ROW LSTM-16 LSTM-4 (b) Split MNIST 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP LSTM (c) SSC 1 10 20 Replay patterns per minibatch 0.0 0.2 0.4 0.6 0.8 1.0 ACC LSTM (d) QD Figure 7: Average ACC for diﬀerent replay patterns per minibatch. The longer the input se- quence, the more the patterns needed to recover the performance. Quick, Draw! performance does not change signiﬁcantly with respect to replay patterns in the minibatch. Best viewed in color. 7. Research Directions The problem of learning continuously on sequential data processing tasks is far from being solved. Fortunately, the recent interest in the topic is fostering the discovery of new solutions and a better understanding of the phenomena at play. We believe that our work could play an important role in sketching promising research directions. Leveraging existing CL strategies. From our experiments, it is clear that some families of CL strategies are better suited to the Class-incremental scenario than others (e.g. GEM mitigates forgetting across all benchmarks while EWC per- forms poorly in class-incremental settings). Instead of designing entirely new approaches, it could be better to start building from the existing ones. Replay oﬀers a simple and eﬀective solution, but storing previous patterns may not be possible in all environments. It would be interesting to extend LwF with knowl- edge distillation algorithms speciﬁcally designed for recurrent networks. De- pending on the eﬀectiveness of the distillation, results could drastically change. Even if architectural strategies have not been part of our study, they can be an important part of the research. In particular, they may be a valid option when 24 ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP multi-head LSTM-4 multi-head MLP single-head LSTM-4 single-head (a) Split MNIST ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC MLP multi-head LSTM multi-head MLP single-head LSTM single-head (b) Synthetic Speech Commands ewc agem naive 0.0 0.2 0.4 0.6 0.8 1.0 ACC LSTM multi-head LSTM single-head (c) Quick, Draw! Figure 8: Bar plot comparing multi-head and single-head performances. Best viewed in color. all other approaches struggle. In fact, task-speciﬁc components or freezing of previous parameters can remove forgetting by design. The usually large compu- tational cost can be mitigated by pruning. Combining pruning techniques for RNNs with dynamic architectures may open a new topic of research in CL. Improving recurrent models. The role of sequence length in continuous learning processes can be investigated under diﬀerent points of view. From the point of view of dynamical systems, the learning trajectory followed by a recurrent model can be studied in terms of diﬀerent regimes (chaotic, edge-of-chaos, attractors) [18]. Longer sequences would produce longer trajectories which could make the Continual Learning problem more diﬃcult. Inspecting and interpreting these trajectories could in turn foster the design of new regularization strategies to constrain the learning path. Instead, from an experimental point of view, the optimization problem that has to be solved by recurrent networks is highly dependent on the sequence length. For example, the Backpropagation Through Time has to propagate information from the end of the sequence to the very beginning in order to compute gradients. Some of the obstacles encountered by many strategies in our experimental analysis may be overcome by alternative learning algorithms, for example Truncated Backpropagation. Design of realistic scenarios. The complexity of the CL environments is another important topic to address in future researches. Providing target labels to the system each time a new pattern arrives is often too expensive, even unrealistic. Instead, sequences are able to work with sparse targets [49]. Sequential data processing may lead to more autonomous systems in which the required level of 25 supervision is drastically reduced. Our proposed benchmarks could contribute to realize such scenario through a stream of realistic patterns, resembling what an agent would experience in the real world. To build this setting, it could be required to concatenate multiple commands from SSC or multiple drawings from QD, possibly interleaved with noisy patterns. This online scenario would free the model from the necessity to learn over multiple passes over the data. Instead, the environment itself would provide new instances and new classes (SIT+NIC) which can be experienced one (or few) at a time. With this achievement, CL models could seamlessly be integrated in many real world applications. 8. Conclusion In this work we focused on Continual Learning in Recurrent Neural Network models. First, we reviewed the literature and proposed a taxonomy of contri- butions. We also analysed and organized the benchmarks and datasets used in CL for sequential data processing. The number of existing benchmarks compared to the relatively small number of contributions revealed that most papers proposed their own model and tested it on datasets which are almost never reused in subsequent works. Therefore, we discussed the opportunity to build standard benchmarks and, in addition, we proposed two new CL benchmarks based on existing datasets: Synthetic Speech Commands (SSC) and Quick, Draw! (QD). Both datasets are speciﬁ- cally tailored to Sequential Data Processing and are general enough to be model agnostic. From the literature review, we also found out that all available works focus on new CL strategies with customized architectures, often for a specialized domain. The diﬀerent experimental setups prevent a comparison of multiple approaches and there is little information about how recurrent networks behave with gen- eral CL strategies. To ﬁll this gap, we ran an extensive experimental analysis with single-head mod- els focused on Class-Incremental scenarios. We compared traditional LSTMs against feedforward networks on 6 popular CL strategies. Our main ﬁnding shows increasing forgetting as sequences are made artiﬁcially longer without modifying their content. Even replay strategies, which are the most robust among the considered methods, are subjected to this phenomenon. This holds also for Domain-incremental scenarios with Permuted MNIST. The eﬀect of sequence length on the ﬁnal performance may be questioned by results presented in [34]: the authors found the working memory (the amount of features needed to address a task) to be the main driving factor in the per- formance of recurrent models. In our work, we showed that, with ﬁxed working memory, sequence length plays a role. These two conclusions are not directly in contrast with each other, because the experiments of [34] have been conducted on either a MT+NC scenarios with multi-head models or on simpler scenarios like Domain-incremental. This diﬃculty in comparing results conﬁrms the need for our large-scale evaluation. 26 Finally, to justify our choice of Class-incremental scenarios, we ran additional experiments with multi-head models in Task-incremental scenarios. We veriﬁed that a multi-head approach greatly simpliﬁes the problem in both feedforward and recurrent architectures. However, this comes at the cost of a strong as- sumption on task label knowledge. Concluding, we entered into a discussion of interesting research paths inspired by our ﬁndings on the topic of Continual Learning in sequential domains. We believe that addressing such challenges will make Continual Learning more ef- fective in realistic environments, where sequential data processing is the most natural way to acquire and process new information across a lifetime. Acknowledgments We thank the ContinualAI association and all of its members for the fruitful discussions. Funding This work has been partially supported by the European Community H2020 programme under project TEACHING (grant n. 871385). References [1] Subutai Ahmad and JeﬀHawkins. How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites. arXiv, pages 1–23, 2016. [2] Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon. Uncertainty-based Continual Learning with Adaptive Regularization. In NeurIPS, pages 4392–4402, 2019. [3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory Aware Synapses: Learning what (not) to forget. In The European Conference on Computer Vision (ECCV), 2018. [4] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online Continual Learning with Maximal Interfered Retrieval. In NeurIPS, pages 11849– 11860. Curran Associates, Inc., 2019. [5] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-Free Continual Learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [6] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess Sequen- tial Learning. In ICLR, 2019. 27 [7] Giuseppe Amato, Davide Bacciu, Stefano Chessa, Mauro Dragone, Clau- dio Gallicchio, Claudio Gennaro, Hector Lozano, Alessio Micheli, Gregory M. P. O’Hare, Arantxa Renteria, and Claudio Vairo. A benchmark dataset for human activity recognition and ambient assisted living. In Helena Lindgren, Juan F. De Paz, Paulo Novais, Antonio Fern´andez-Caballero, Hyun Yoe, Andres Jim´enez Ram´ırez, and Gabriel Villarrubia, editors, Ambient Intelligence- Software and Applications – 7th International Sym- posium on Ambient Intelligence (ISAmI 2016), pages 1–9. Springer Inter- national Publishing, 2016. [8] Bernard Ans, St´ephane Rousset, Robert M. French, and Serban Musca. Self-refreshing memory in artiﬁcial neural networks: Learning temporal sequences without catastrophic forgetting. Connection Science, 16(2):71– 99, 2004. [9] Bernard Ans, Stephane Rousset, Robert M. French, and Serban C. Musca. Preventing Catastrophic Interference in MultipleSequence Learning Using Coupled Reverberating Elman Networks. In Proceedings of the 24th An- nual Conference of the Cognitive Science Society, 2002. [10] Nabiha Asghar, Lili Mou, Kira A Selby, Kevin D Pantasdo, Pascal Poupart, and Xin Jiang. Progressive Memory Banks for Incremental Do- main Adaptation. In ICLR, 2019. [11] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR, 2015. [12] Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, JeﬀClune, and Nick Cheney. Learning to Continually Learn. In ECAI, 2020. [13] Magdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-juss`a. Continual Lifelong Learning in Natural Language Processing: A Survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6523–6541, Barcelona, Spain (Online), 2020. Interna- tional Committee on Computational Linguistics. [14] Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Ru- bino, Lucia Specia, and Marco Turchi. Findings of the 2017 Conference on Machine Translation (WMT17). In Proceedings of the Second Con- ference on Machine Translation, pages 169–214, Copenhagen, Denmark, 2017. Association for Computational Linguistics. [15] Johannes Buchner. Synthetic Speech Commands: A public dataset for single-word speech recognition. Kaggle Dataset, 2017. 28 [16] Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez, and Laurent Charlin. Online Fast Adaptation and Knowl- edge Accumulation: A New Approach to Continual Learning. arXiv, 2020. [17] Antonio Carta, Andrea Cossu, Federico Errica, and Davide Bacciu. Catas- trophic Forgetting in Deep Graph Networks: An Introductory Benchmark for Graph Classiﬁcation. The 2021 Web Conference (WWW) Workshop on Graph Benchmarks Learning (GLB), 2021. [18] Andrea Ceni, Peter Ashwin, and Lorenzo Livi. Interpreting Recurrent Neural Networks Behaviour via Excitable Network Attractors. Cognitive Computation, 12(2):330–356, 2020. [19] Arslan Chaudhry, Puneet K. Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian Walk for Incremental Learning: Under- standing Forgetting and Intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pages 532–547, 2018. [20] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mo- hamed Elhoseiny. Eﬃcient Lifelong Learning with A-GEM. In ICLR, 2019. [21] Dechao Chen, Shuai Li, and Liefa Liao. A recurrent neural network ap- plied to optimal motion control of mobile robots with physical constraints. Applied Soft Computing, 85:105880, December 2019. [22] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2Net: Accelerat- ing Learning via Knowledge Transfer. In ICLR, 2016. [23] Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the Properties of Neural Machine Translation: En- coder–Decoder Approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103– 111, Doha, Qatar, 2014. Association for Computational Linguistics. [24] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Ben- gio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS Workshop on Deep Learning, 2014. [25] Robert Coop and Itamar Arel. Mitigation of catastrophic interference in neural networks using a ﬁxed expansion layer. In 2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 726–729. IEEE, 2012. [26] Robert Coop and Itamar Arel. Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer. In The 2013 International Joint Conference on Neural Networks (IJCNN), pages 1–7, Dallas, TX, USA, 2013. IEEE. 29 [27] Andrea Cossu, Antonio Carta, and Davide Bacciu. Continual Learning with Gated Incremental Memories for sequential data processing. In Pro- ceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020), 2020. [28] Yuwei Cui, Subutai Ahmad, and JeﬀHawkins. Continuous Online Se- quence Learning with an Unsupervised Neural Network Model. Neural Computation, 28(11):2474–2504, 2016. [29] Edwin D. de Jong. Incremental Sequence Learning. arXiv: 1611.03068 [cs], 2016. [30] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation tasks. arXiv, 2019. [31] Gregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar. Learn- ing in Nonstationary Environments: A Survey. IEEE Computational In- telligence Magazine, 10(4):12–25, 2015. [32] Lea Duncker, Laura N Driscoll, Krishna V Shenoy, Maneesh Sahani, and David Sussillo. Organizing recurrent network dynamics by task- computation to enable continual learning. In NeurIPS, volume 33, 2020. [33] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided Continual Learning with Bayesian Neu- ral Networks. In ICLR, 2020. [34] Benjamin Ehret, Christian Henning, Maria R Cervera, Alexander Meule- mans, Johannes von Oswald, and Benjamin F Grewe. Continual Learning in Recurrent Neural Networks. arXiv, 2020. [35] Benjamin Ehret, Christian Henning, Maria R. Cervera, Alexander Meule- mans, Johannes von Oswald, and Benjamin F. Grewe. Continual Learning in Recurrent Neural Networks with Hypernetworks. arXiv:2006.12109 [cs, stat], 2020. [36] Jeﬀrey L. Elman. Finding Structure in Time. Cognitive Science, 14(2):179–211, 1990. [37] Sebastian Farquhar and Yarin Gal. A Unifying Bayesian View of Continual Learning. In NeurIPS Bayesian Deep Learning Workshop, 2018. [38] Sebastian Farquhar and Yarin Gal. Towards Robust Evaluations of Con- tinual Learning. In Privacy in Machine Learning and Artiﬁcial Intelligence Workshop, ICML, 2019. [39] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online Meta-Learning. In ICML, 2019. 30 [40] Robert French. Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks. In In Proceedings of the 13th Annual Cognitive Science Society Conference, pages 173–178. Erlbaum, 1991. [41] Robert French. Pseudo-recurrent Connectionist Networks: An Approach to the ’Sensitivity-Stability’ Dilemma. Connection Science, 9(4):353–380, 1997. [42] Robert French. Using Pseudo-Recurrent Connectionist Networks to Solve the Problem of Sequential Learning. In Proceedings of the 19th Annual Cognitive Science Society Conference, 1997. [43] Robert French. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4):128–135, 1999. [44] Robert M. French. Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks. In In Proceedings of the 13th Annual Cognitive Science Society Conference, pages 173–178. Erlbaum, 1991. [45] Jo˜ao Gama, Indr˙e ˇZliobait˙e, Albert Bifet, Mykola Pechenizkiy, and Ab- delhamid Bouchachia. A survey on concept drift adaptation. ACM Com- puting Surveys (CSUR), 46(4):44:1–44:37, 2014. [46] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter. Audio Set: An ontology and human-labeled dataset for audio events. In 2017 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), pages 776– 780, 2017. [47] Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual Learning via Neural Pruning. arXiv, 2019. [48] Alex Graves. Sequence Transduction with Recurrent Neural Networks. arXiv, 2012. [49] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmid- huber. Connectionist temporal classiﬁcation: Labelling unsegmented se- quence data with recurrent neural networks. In ICML, ICML ’06, pages 369–376, New York, NY, USA, 2006. Association for Computing Machin- ery. [50] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv:1410.5401 [cs], 2014. [51] Stephen Grossberg. How does a brain build a cognitive code? Psycholog- ical Review, 87(1):1–51, 1980. 31 [52] David Ha and Douglas Eck. A Neural Representation of Sketch Drawings. In ICLR, 2018. [53] Michael Bonnell Harries, Claude Sammut, and Kim Horn. Extracting Hidden Context. Machine Learning, 32(2):101–126, 1998. [54] James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone. Con- tinuous meta-learning without tasks. arXiv, 2019. [55] Mahmudul Hasan and Amit K. Roy-Chowdhury. A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models. IEEE Transactions on Multimedia, 17(11):1909–1922, 2015. [56] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory Ef- ﬁcient Experience Replay for Streaming Learning. IEEE International Conference on Robotics and Automation (ICRA), 2018. [57] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan Pascanu. Task Agnostic Continual Learning via Meta Learning. arxiv, 2019. [58] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀrey Dean. Distilling the Knowl- edge in a Neural Network. In NIPS Deep Learning and Representation Learning Workshop, 2015. [59] Sepp Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9:1735–1780, 1997. [60] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-Learning in Neural Networks: A Survey. arXiv:2004.05439 [cs, stat], 2020. [61] Steven C Y Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi- Ming Chan, and Chu-Song Chen. Compacting, Picking and Growing for Unforgetting Continual Learning. In NeurIPS, pages 13669–13679, 2019. [62] Khurram Javed and Martha White. Meta-Learning Representations for Continual Learning. In NeurIPS, 2019. [63] Marcin Junczys-Dowmunt, Bruno Pouliquen, and Christophe Mazenc. COPPA V2.0: Corpus of parallel patent applications. Building large par- allel corpora with GNU make. In Proceedings of the 4th Workshop on Challenges in the Management of Large Corpora, Portoroˇz, Slovenia, May 23-28, 2016, 2016. [64] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guil- laume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ra- malho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. PNAS, 114(13):3521–3526, 2017. 32 [65] Taisuke Kobayashi and Toshiki Sugino. Continual Learning Exploiting Structure of Fractal Reservoir Computing. In Igor V Tetko, Vˇera K˚urkov´a, Pavel Karpov, and Fabian Theis, editors, Artiﬁcial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions, vol- ume 11731, pages 35–47, Cham, 2019. Springer International Publishing. [66] Germ´an Kruszewski, Ionut-Teodor Sorodoc, and Tomas Mikolov. Evalu- ating Online Continual Learning with CALM. arXiv, 2020. [67] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan G¨unnemann. Continual Learning with Bayesian Neural Networks for Non-Stationary Data. In ICLR, 2020. [68] Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma. FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network. arXiv:1901.02358 [cs, stat], January 2019. [69] Quoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. A simple way to initialize recurrent networks of rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015. [70] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient- Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [71] Timoth´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia D´ıaz-Rodr´ıguez. Continual learning for robotics: Deﬁnition, framework, learning strategies, opportunities and challenges. Information Fusion, 58:52–68, 2020. [72] Timoth´ee Lesort, Andrei Stoian, and David Filliat. Regularization Short- comings for Continual Learning. arXiv, 2020. [73] HongLin Li, Payam Barnaghi, Shirin Enshaeifar, and Frieder Ganz. Con- tinual Learning Using Bayesian Neural Networks. arXiv, 2019. [74] Yuanpeng Li, Liang Zhao, Kenneth Church, and Mohamed Elhoseiny. Compositional Language Continual Learning. In ICLR, 2020. [75] Zhizhong Li and Derek Hoiem. Learning without Forgetting. In European Conference on Computer Vision, Springer, pages 614–629, 2016. [76] Pierre Lison, J¨org Tiedemann, and Milen Kouylekov. OpenSubtitles2018: Statistical Rescoring of Sentence Alignments in Large, Noisy Parallel Cor- pora. In Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC 2018), pages 1742–1748. European Lan- guage Resources Association (ELRA), 2018. 33 [77] Vincenzo Lomonaco and Davide Maltoni. CORe50: A New Dataset and Benchmark for Continuous Object Recognition. In Sergey Levine, Vin- cent Vanhoucke, and Ken Goldberg, editors, Proceedings of the 1st An- nual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pages 17–26. PMLR, 2017. [78] Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graﬃeti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Sub- utai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni. Avalanche: An End-to-End Library for Continual Learning. In CLVision Workshop at CVPR, 2021. [79] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient Episodic Memory for Continual Learning. In NIPS, 2017. [80] Mantas Lukoˇseviˇcius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. Computer Science Review, 3(3):127– 149, 2009. [81] Avinash Madasu and Vijjini Anvesh Rao. Sequential Domain Adaptation through Elastic Weight Consolidation for Sentiment Analysis. arXiv, 2020. [82] Davide Maltoni and Vincenzo Lomonaco. Continuous Learning in Single- Incremental-Task Scenarios. Neural Networks, 116:56–73, 2019. [83] James L. McClelland, Bruce L. McNaughton, and Andrew K. Lampinen. Integration of new information in memory: New insights from a comple- mentary learning systems perspective. Philosophical Transactions of the Royal Society B: Biological Sciences, 375(1799):20190637, 2020. [84] Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Con- nectionist Networks: The Sequential Learning Problem. In Gordon H. Bower, editor, Psychology of Learning and Motivation, volume 24, pages 109–165. Academic Press, 1989. [85] Nikhil Mehta, Kevin J Liang, and Lawrence Carin. Bayesian Nonpara- metric Weight Factorization for Continual Learning. arXiv, pages 1–17, 2020. [86] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational Continual Learning. In ICLR, 2018. [87] Hung Nguyen, Xuejian Wang, and Leman Akoglu. Continual Rare-Class Recognition with Emerging Novel Subclasses. In ECML, 2019. 34 [88] Alexander Ororbia. Spiking Neural Predictive Coding for Continual Learning from Data Streams. arXiv, 2020. [89] Alexander Ororbia, Ankur Mali, C Lee Giles, and Daniel Kifer. Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations. arXiv, 2019. [90] Alexander Ororbia, Ankur Mali, Daniel Kifer, and C Lee Giles. Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively. arXiv, pages 1–11, 2019. [91] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019. [92] German I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter. Life- long Learning of Spatiotemporal Representations With Dual-Memory Re- current Self-Organization. Frontiers in Neurorobotics, 12, 2018. [93] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. NeurIPS, 2019. [94] Daniel Philps, Artur d’Avila Garcez, and Tillman Weyde. Making Good on LSTMs’ Unfulﬁlled Promise. arXiv, 2019. [95] Mark B Ring. CHILD: A First Step Towards Continual Learning. Machine Learning, 28(1):77–104, 1997. [96] Anthony Robins. Catastrophic Forgetting; Catastrophic Interference; Sta- bility; Plasticity; Rehearsal. Connection Science, 7(2):123–146, 1995. [97] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P Lillicrap, and Greg Wayne. Experience Replay for Continual Learning. In NeurIPS, pages 350–360, 2019. [98] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Had- sell. Progressive Neural Networks. arXiv, 2016. [99] Anton Maximilian Sch¨afer and Hans Georg Zimmermann. Recurrent Neu- ral Networks Are Universal Approximators. In Stefanos D. Kollias, An- dreas Stafylopatis, W lodzis law Duch, and Erkki Oja, editors, Artiﬁcial Neural Networks – ICANN 2006, Lecture Notes in Computer Science, pages 632–640. Springer Berlin Heidelberg, 2006. 35 [100] Monika Schak and Alexander Gepperth. A Study on Catastrophic For- getting in Deep LSTM Networks. In Igor V Tetko, Vˇera K˚urkov´a, Pavel Karpov, and Fabian Theis, editors, Artiﬁcial Neural Networks and Ma- chine Learning – ICANN 2019: Deep Learning, Lecture Notes in Com- puter Science, pages 714–728. Springer International Publishing, Cham, 2019. [101] Jeﬀrey C. Schlimmer and Richard H. Granger. Incremental learning from noisy data. Machine Learning, 1(3):317–354, 1986. [102] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & Compress: A scalable framework for continual learning. In ICML, pages 4528–4537, 2018. [103] Shagun Sodhani, Sarath Chandar, and Yoshua Bengio. Toward Training Recurrent Neural Networks for Lifelong Learning. Neural Computation, 32(1):1–35, 2019. [104] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. SpaceNet: Make Free Space For Continual Learning. arXiv, 2020. [105] Fan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. LAMOL: LAnguage MOdeling for Lifelong Language Learning. In ICLR, 2020. [106] Binh Tang and David S. Matteson. Graph-Based Continual Learning. In ICLR, 2020. [107] Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2062–2068, Minneapolis, Minnesota, 2019. As- sociation for Computational Linguistics. [108] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoﬀrey J Gordon. An Empirical Study of Example Forgetting during Deep Neural Network Learning. In ICLR, 2019. [109] Gido M. van de Ven, Hava T. Siegelmann, and Andreas S. Tolias. Brain- inspired replay for continual learning with artiﬁcial neural networks. Na- ture Communications, 11, 2020. [110] Gido M. van de Ven and Andreas S. Tolias. Generative replay with feed- back connections as a general strategy for continual learning. arXiv, 2018. [111] Gido M van de Ven and Andreas S Tolias. Three scenarios for continual learning. In Continual Learning Workshop NeurIPS, 2018. 36 [112] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer- gus, S. Vishwanathan, and R. Garnett, editors, NIPS, pages 5998–6008. Curran Associates, Inc., 2017. [113] Johannes von Oswald, Christian Henning, Jo&#xE3, O Sacramento, and Benjamin F. Grewe. Continual learning with hypernetworks. In ICLR, 2019. [114] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(3):328–339, 1989. [115] Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis, and Lau- rent Charlin. Continual Learning of New Sound Classes using Generative Replay. arXiv, 2019. [116] Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts. Machine Learning, 23(1):69–101, 1996. [117] Adina Williams, Nikita Nangia, and Samuel Bowman. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Pro- ceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana, 2018. Association for Computational Linguistics. [118] Thomas Wolf, Julien Chaumond, and Clement Delangue. Continuous Learning in a Hierarchical Multiscale Neural Network. In ACL, 2018. [119] Jiabin Xue, Jiqing Han, Tieran Zheng, Xiang Gao, and Jiaxing Guo. A Multi-Task Learning Framework for Overcoming the Catastrophic Forget- ting in Automatic Speech Recognition. arXiv, 2019. [120] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong Learning With Dynamically Expandable Networks. In ICLR, 2018. [121] T Young, Devamanyu Hazarika, S Poria, and E Cambria. Recent Trends in Deep Learning Based Natural Language Processing. IEEE Computational Intelligence Magazine, 13:55–75, 2018. [122] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual Learning Through Synaptic Intelligence. In ICML, pages 3987–3995, 2017. [123] Chen Zeno, Itay Golan, Elad Hoﬀer, and Daniel Soudry. Task Agnostic Continual Learning Using Online Variational Bayes. In NeurIPS Bayesian Deep Learning Workshop, 2018. 37 Appendix A. Experiment conﬁguration Here we report additional details on the experiments conﬁguration. Each step is divided into a training set and a test set. We used the default train-test split of Torchvision for MNIST-based datasets. For SSC we randomly generate a train-test split for each class, with 20% of patterns for the test set. Quick, Draw! already provides separate splits for each class. We changed ran- dom seed on each run. To make training stabler with RNNs, we used a ﬁxed permutation on SMNIST which does not change across steps. This uniformly spreads the information along the sequence. Following the original paper, A-GEM concatenates a one-hot task vector to the input patterns. Since we adopt class-incremental scenarios, we concatenate the vector only at training time. On SSC, we leveraged the Avalanche library [78] for the GEM implementation. We tested a wide range of hyperparameters for the grid search, as showed by Table A.7 for PMNIST, Table A.8 for SMNIST, Table A.9 for SSC and Table A.10 for QD. The tables use the following abbreviations: number of layers is nl, number of hidden units is hs, minibatch size is mbs, optimizer is opt, learning rate is lr, patterns per step in GEM memory is pps, sample size from A-GEM memory is ss, softmax temperature in LwF is T. MLPs are with ReLU activa- tions. LSTMs have 1 layer for MNIST-based benchmarks, 2 layers for SSC and Quick, Draw!. LSTM with QD uses 512 hidden units. Multi-head experiments use the same conﬁguration. All models use gradient clipping at norm 5. The number of epochs is set to guarantee convergence except for online methods (GEM, A-GEM) which use 2 epochs and minibatches of 10 patterns. The default optimizer is Adam with no regularization. Appendix A.1. Computational Complexity We monitored execution times for recurrent architectures with diﬀerent CL strategies. We ran experiments on a single V100 GPU, with 3 threads at a time on a Intel® Xeon® Gold 6140M CPU with 2.30GHz frequency. Our results clearly show that the cost of GEM is quite large and may be pro- hibitive in realistic applications. Appendix B. Benchmarks Description Appendix B.1. Synthetic Speech Commands We introduced the Synthetic Speech Commands (SSC) [15] benchmark4 and adapted it for a class-incremental setting. We took 2 classes at a time and we 4The Synthetic Speec Commands Dataset is available at https://www.kaggle.com/ jbuchner/synthetic-speech-commands-dataset 38 PMNIST LSTM MLP EWC λ=(0.1, 1, 10, 100), hs=(256, 512) λ=(0.1, 1, 10, 100), nl=(1,2), hs=512 MAS λ=(0.1, 1, 10, 100), mbs=(64,128), hs=(256, 512) λ=(0.1, 1, 10), mbs=(32,64,128), lr=(1e-2, 1e-3), nl=1, hs=512 GEM pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-2, 1e-3), hs=256 pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-1, 1e-2, 1e-3), nl=(1,2), hs=512 A-GEM patterns per step i memory=(64, 128, 256), ss=(256, 512), lr=(1e-2, 1e-3), hs=256 pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), nl=(1,2), hs=5120 LwF α=(0.1, 1), hs=256, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4) α=(0.1, 1), nl=(1,2), hs=512, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay lr=(1e-3, 1e-4), hs=(256, 512) lr=(1e-3, 1e-4), hs=(128,256), nl=1 Naive hs=(256, 512), lr=(1e-3, 1e-4) hs=(256,512), nl=1 Joint Training lr=(1e-2, 1e-3), mbs=128, hs=512 hs=(256, 512), nl=1 Table A.7: Hyperparameter selection on PMNIST. Grid search has been performed for all the combinations in parenthesis. Bold notation indicates best hyperparameter value. See the text in Appendix A for explanation of the abbreviations used in this table. created sequences of 3 steps for model selection and sequences of 10 steps for model assessment. We preprocessed each audio with a MelSpectrogram with 10 ms hop length, 25 ms window size and 40 MFCCs features. Table B.11 following, we report statistics for all the 30 classes in the dataset. Appendix B.2. Quick, Draw! Quick, Draw! dataset has been downloaded from Google Cloud Console, in the sketchrnn folder5. All classes have 70, 000 patterns for training and 2, 500 5Quick, Draw! details are available at https://github.com/googlecreativelab/ quickdraw-dataset 39 SMNIST LSTM MLP EWC λ=(0.1, 1, 10, 100, 1000), hs=(128, 256) λ=(0.1, 1, 10, 100, 1000), nl=(1,2) hs=128 MAS λ=(0.1, 1, 10, 100, 1000), hs=(128, 256) λ=(0.1, 1, 10, 100), lr=(1e-2, 1e-3), nl=1, hs=128 GEM pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-2, 1e-3), hs=128 pps=(64, 128, 256), γ=(0.5, 1), lr=(1e-1, 1e-2, 1e-3), hs=128, nl=1 A-GEM pps=(128, 256, 512), ss=(256, 512), lr=(1e-2, 1e-3), hs=128 pps=(128, 256, 512), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), hs=512, nl=1 LwF α=([0, 1/2, 2*(2/3), 3*(3/4), 4*(4/5)], 1), hs=(256,512), T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4), mbs=(64,128) α=([0, 1/2, 2*(2/3), 3*(3/4), 4*(4/5)], 1), nl=(1,2), hs=256, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay lr=(1e-3, 1e-4), hs=(128, 256) hs=128, nl=(1,2) Naive hs=128, mbs=(32,64), lr=(1e-3, 1e-4) hs=(128,256), nl=1 Joint Training hs=512, mbs=128, lr=(1e-3, 1e-4) hs=128, nl=(1,2) Table A.8: Hyperparameter selection on SMNIST. Grid search has been performed for all the combinations in parenthesis. Bold notation indicates best hyperparameter value. See the text in Appendix A for explanation of the abbreviations used in this table. for test. Table B.12 summarizes the classes used in the experiments. Appendix C. Additional Results We provide a set of paired plots showing the relationship between the train- ing accuracy at the end of each step and the ﬁnal test accuracy at the end of training on all steps. These plots are useful to compare the relative performance of diﬀerent CL strategies and diﬀerent architectures. Table C.13 shows the mean ACC and its standard deviation for diﬀerent CL strategies with multi-head models in MT+NC scenarios. A-GEM concatenates the one-hot task vector to each train and test input pattern. 40 SSC LSTM MLP EWC λ=(0.1, 1, 10, 100, 1000), hs=512 λ=(0.1, 1, 10, 100, 1000), lr=(1e-2, 1e-3) nl=1, hs=1024 MAS λ=(0.1, 1, 10, 100), lr=(1e-3, 1e-4), hs=512, mbs=(32, 64, 128) λ=(0.1, 1, 10, 100), mbs=(32,64,128), lr=(1e-2, 1e-3), nl=1, hs=1024 GEM pps=(256, 512), γ=(0, 0.5, 1) pps=(256, 512), γ=(0, 0.5, 1) A-GEM pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), hs=512 pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2, 1e-3), nl=1, hs=1024 LwF α=(0.1, 1), hs=(512, 1024), T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=(1e-3, 1e-4), mbs=(64,128) α=(0.1, 1), nl=(1,2), hs=1024, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), mbs=(64, 128), lr=(1e-2, 1e-3) Replay hs=(512, 1024) lr=(1e-2, 1e-3), hs=1024, nl=(1,2) Naive hs=(512, 1024) hs=1024, nl=(1,2,3), lr=(1e-2, 1e-3) Joint Training lr=(0.01, 0.001), hs=(512, 1024), layers=2, mbs=128 hs=1024, nl=(1,2), weight decay=(0, 1e-4), lr=1e-3, mbs=128 Table A.9: Hyperparameter selection on SSC. Grid search has been performed for all the combinations in parenthesis. Bold notation indicates best hyperparameter value. See the text in Appendix A for explanation of the abbreviations used in this table. 41 QD LSTM EWC λ=(1, 10, 100) MAS λ=(1, 10, 100) GEM pps=(32, 64), γ=(0, 0.5), lr=(1e-2, 1e-3) A-GEM pps=(64, 128, 256), ss=(256, 512), lr=(1e-1, 1e-2) LwF α=(0.1, 1), hs=512, T=(0.5, 1, 1.5, 2), opt=(sgd, adam), lr=1e-4, mbs=(64,128) Replay hs=512, lr=(1e-3, 1e-4) Naive hs=512, layers=(1,2), directions=(1, 2) Joint Training hs=(256, 512), layers=(1,2), bidirectional=(true, false), mbs=512, lr=1e-4 Table A.10: Hyperparameter selection on QD. Grid search has been performed for all the combinations in parenthesis. Bold notation indicates best hyperparameter value. See the text in Appendix A for explanation of the abbreviations used in this table. ewcagemgem mas lwfnaive replay-10 0 20000 40000 60000 80000 100000 120000 Time (s) LSTM-ROW LSTM-4 (a) Permuted MNIST ewcagemgem mas lwfnaive replay-10 0 250 500 750 1000 1250 1500 1750 2000 Time (s) LSTM-ROW LSTM-4 (b) Split MNIST ewc agem mas lwf naive replay-10 0 500 1000 1500 2000 2500 3000 Time (s) LSTM (c) SSC ewcagemgem mas lwf naive replay-10 0 20000 40000 60000 80000 100000 Time (s) LSTM (d) QD Figure A.9: Average execution time of recurrent models for diﬀerent CL strategies. On SSC, we did not include the GEM strategy since it has been executed with a diﬀerent framework. Best viewed in color. 42 Class name patterns use Class name patterns use bed 1356 S1 oﬀ 2244 A3 bird 1346 S1 one 1276 A4 cat 1378 S2 on 2228 A4 dog 1474 S2 right 1276 A5 down 1188 S3 seven 1411 A5 eight 1113 S3 sheila 1463 A6 ﬁve 1092 - six 1485 A6 four 2400 - stop 1485 A7 go 960 - three 1188 A7 happy 1481 - tree 1188 A8 house 2382 A1 two 902 A8 left 1485 A1 up 1187 A9 marvel 1253 A2 wow 957 A9 nine 1144 A2 yes 1244 A10 no 957 A3 zero 1306 A10 Table B.11: Details of SSC dataset classes: name of the class, total number of patterns, how it has been used. S for model selection, A for model assessment, the following number indicates which step uses the class. Dash indicates that the class has not been used in our experiments. Class name use Class name use hot dog S1 octopus A4 palm tree S1 cloud A5 moon S2 bicycle A5 envelope S2 swan A6 dumbbell S3 picture frame A6 microwave S3 shorts A7 drill A1 ﬂying saucer A7 telephone A1 basketball A8 airplane A2 harp A8 dishwasher A2 beard A9 chair A3 binoculars A9 grass A3 tiger A10 rhinoceros A4 book A10 Table B.12: Details of Quick, Draw! dataset classes: name of the class, how it has been used. S for model selection, A for model assessment, the following number indicates which step uses the class. 43 SMNIST MLP LSTM-4 EWC 0.99±0.00 0.64±0.16 A-GEM 0.73±0.05 0.20±0.06 NAIVE 0.97±0.02 0.72±0.11 SSC MLP LSTM EWC 0.77±0.09 0.80±0.06 A-GEM 0.57±0.02 0.62±0.07 NAIVE 0.72±0.10 0.67±0.10 QD LSTM EWC 0.98±0.00 A-GEM 0.56±0.16 NAIVE 0.71±0.09 Table C.13: Multi-head experiments showing average ACC and standard deviation. Naive strategy already recovers large part of the performance. 44 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (a) PMNIST + naive MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (b) PMNIST + ewc MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (c) PMNIST + mas MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (d) PMNIST + lwf MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (e) PMNIST + gem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (f) PMNIST + agem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (g) PMNIST + replay-1 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (h) PMNIST + replay-10 45 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (i) SMNIST + naive MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (j) SMNIST + ewc MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (k) SMNIST + mas MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (l) SMNIST + lwf MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (m) SMNIST + gem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (n) SMNIST + agem MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (o) SMNIST + replay-1 MLP LSTM-ROW LSTM-4 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 mean (p) SMNIST + replay-10 46 MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (q) SSC + naive MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (r) SSC + ewc MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (s) SSC + mas MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (t) SSC + lwf MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (u) SSC + gem MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (v) SSC + agem MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (w) SSC + replay-1 MLP LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (x) SSC + replay-10 47 LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (y) QD + naive LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (z) QD + ewc LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (aa) QD + mas LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ab) QD + lwf LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ac) QD + gem LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ad) QD + agem LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (ae) QD + replay-1 LSTM 0.0 0.2 0.4 0.6 0.8 1.0 ACC random S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 mean (af) QD + replay-10 Figure C.10: Paired plots comparing accuracy at the end of training on each step (left point) and accuracy after training on all steps (right point). Each column represents a diﬀerent model. Red asterisk represents mean performance across all steps. Horizontal dotted line represents the performance of a random classiﬁer. The more vertical the line, the larger the forgetting. Best viewed in color. 48