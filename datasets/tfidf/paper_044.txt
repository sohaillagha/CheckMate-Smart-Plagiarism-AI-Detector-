MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion Raktim Kumar Mondol1, Ewan K.A. Millar2, Arcot Sowmya1, and Erik Meijering1,* 1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia 2Department of Anatomical Pathology, NSW Health Pathology, St. George Hospital *Correspondence: erik.meijering@unsw.edu.au Abstract Survival risk stratification is an important step in clinical decision making for breast cancer manage- ment. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes. Keywords Multimodal Fusion · Breast Cancer · Whole Slide Images · Deep Neural Network · Survival Prediction 1 Introduction The inherent heterogeneity of breast cancer poses challenges for prediction of prognosis and treatment decisions particularly in post-menopausal estrogen receptor positive (ER+) breast cancer, with previous studies reporting conflicting results on the survival difference between Luminal A and B metastatic breast cancer patients [1, 2]. A common critical clinical dilemma is the se- lection of those early ER+ breast cancer patients at high risk of recurrence who may benefit from the addition of chemotherapy to endocrine therapy. Traditional risk prediction methods, based mainly on clinicopathological factors, may not entirely account for the complex biology of cancer [3]. To address this issue, molecular markers and gene expression profiles have emerged as a potential addition to current decision making tools [4]. Therefore, combining histopathology image, molecular and clin- icopathological factors (see Figure 1 adapted from [5]) into a single risk prediction model could potentially enhance risk prog- nostication in the clinic [6]. Here we propose a multimodal risk prediction model for ER+ breast cancer and compare its perfor- mance to existing methods. The significance of our work lies in potentially refining ER+ breast cancer survival risk prediction, facilitating informed therapeutic decision making [7]. 2 Related Works Breast cancer accounts for about 30% of all cancer cases in women, with varying incidence rates across regions and notably higher rates in developed countries [8]. The ER status, specif- ically ER+ breast cancer, which encompasses Luminal A and Luminal B subtypes, is crucial in shaping treatment strategies and predicting prognosis [3]. While Luminal A tumours are usually low-grade with a favorable prognosis (Ki-67 < 14%), Luminal B tumours are usually higher grade and pose a higher Patient Organ Tissue Genomics Clinical Information Mammogram RNA Sequence Histopathological Image Figure 1: Multimodal Data for Breast Cancer Characterization: This figure illustrates the diverse data types at multiple biological levels to comprehensively characterize breast cancer. It encom- passes clinical data at the patient level, mammogram images at the organ level, histopathology images at the tissue level, and gene expression data (e.g., RNA Sequencing) at the molecular level, thereby providing a holistic view of the cancer’s nature and behavior. recurrence risk and worse outcome (Ki-67 ≥14%) [3, 9]. Tra- ditional prediction models primarily centred on clinical and pathological factors fall short, especially in capturing the intri- cacies and heterogeneity of ER+ breast cancer subtypes [10]. Transitioning from traditional methods, early research in sur- vival risk prediction using histopathology images shifted from reliance on hand-crafted features like texture and shape to lever- aging deep learning for automatic extraction of complex, high- level features, resulting in enhanced accuracy [11]. In recent developments, multimodal approaches that combine imaging, clinical and molecular data have been shown to enhance predic- tion accuracy [12–17]. Comprehensive multimodal approaches, arXiv:2402.11788v1 [cs.CV] 19 Feb 2024 2 Patch Generation Annotation Colour Transformation Tumour Expert Pathologist QuPath Annotation Tool Whole Slide (ER+ Sample) Figure 2: Preprocessing Methodology for Histopathology Images: Whole Slide Images (WSIs) are first annotated using the QuPath annotation tool by expert pathologists. From these annotated regions, non-overlapping patches are systematically extracted. Subsequently, a color transformation is applied to ensure uniformity in color representation across datasets. especially those that fuse whole slide images, clinical data and genetic information, could pave the way for superior predictive models. 3 Data Preparation Our study uses the hematoxylin and eosin (H&E)-stained formalin-fixed paraffin-embedded (FFPE) digital slides of The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset [18], sourcing 249 whole-slide images (WSIs) from the Ge- nomic Data Commons (GDC) Portal with a focus on Luminal A (149 samples) and Luminal B (100 samples) molecular sub- types. Slide annotations were obtained manually by an expert breast pathologist using QuPath [19], prioritising tumour local- isation including stroma and tumour-infiltrating lymphocytes (TILs) and excluding necrotic areas. H&E-stained tissues un- derwent downsampling to a resolution of 0.25 µm/pixel, with tissue masks created to exclude artifacts and non-tissue sections, and tumour regions split into 224 × 224 pixel patches. To coun- teract staining inconsistencies, singular value decomposition- based normalisation was applied on the images (See Figure 2) using Macenko method [20]. Furthermore, we processed RNA- sequencing data with RSEM and selected PAM50 genes for anal- ysis, given their significance in breast cancer subtyping [21, 22]. Pertinent clinical variables, encompassing tumour grade, size, patient age and lymph node status, were meticulously prepared to enhance the predictive accuracy of the subsequent deep learn- ing model evaluations. For experimental purposes, we adopted a five-fold cross-validation strategy, allocating 200 samples for training and 50 for validation while maintaining the distribution of Luminal subtypes and survival statuses. 4 Proposed Method 4.1 Histopathological Feature Extraction We utilised vision transformers (ViTs) and convolutional neu- ral networks (CNNs) to extract features from histopathological images. Specifically, the MaxViT model, a prominent vision transformer, was employed for its effectiveness in capturing global image information and leveraging self-attention for com- prehensive tumour analysis [23]. For the purpose of comparison, ResNet50, a CNN-based model, was selected to extract local patterns within the images [24]. 4.2 Patch-to-Patient Aggregation Using Self-Attention In our framework, the self-attention mechanism crucially ag- gregates patch-level features into a holistic patient-level repre- sentation. Notably, the same image patches act as the Key (K), Query (Q), and Value (V) within the attention paradigm, aiming to discern intricate interdependencies among patches from a singular histopathological image. Mathematically, self-attention processes a sequence of image patches K1, Q1, V1, . . . , Kn, Qn, Vn to yield a patient-level fea- ture vector y. The mechanism computes a weighted sum of the Value vectors V j based on attention scores derived from the scoring function s(Qi, K j): yi = n X j=1 softmax  s(Qi, K j)  Vj. (1) In this formulation, the softmax function normalises scores, facilitating the model’s emphasis on specific patches. The re- sultant yi is a weighted blend of patch-level Value vectors V j, representing the patient-level feature. Two primary advantages emerge: 1. Inherent Feature Importance: Using image patches for Key, Query and Value enables the model to natu- rally highlight significant regions in histopathological images, capturing inherent sample heterogeneity. 2. Inter-Patch Contextualisation: The mechanism grasps the context among distinct patches, contextual- ising each patch with respect to others for a compre- hensive patient-level representation. The enriched patient-level vector, emphasising both context and focus, is pivotal for subsequent survival risk prediction tasks. 3 [ No. of Patches (N) × No. of Features (F)] [ Each Patient × No. of Embedding (E)] . . . . . . . . . 500 × 3 × 224 ×224 500 × 512 Preprocessed Histopathology Image Skip Connection Pretrained Feature Extractor: MaxViT Key Query Value Softmax = Matrix Multiplication Aggregated Output Self-Attention Module Output Image Features (Patient Level) Image Patches Average Pooling ... Figure 3: The proposed MM-SurvNet architecture. It utilises a pretrained MaxViT to extract features from histopathology images, which are then processed through a self-attention network to aggregate patch-level features into a patient-level representation. Table 1: Performance Comparison using C-index on TCGA Data. CV Fold Multimodal (Imaging+Genetic+Clinical) Imaging+Genetic Imaging Clinical MaxViT ResNet50 MaxViT ResNet50 MaxViT ResNet50 CoxPH 1 0.70 0.64 0.72 0.61 0.45 0.47 0.50 2 0.57 0.49 0.44 0.58 0.47 0.47 0.24 3 0.66 0.62 0.63 0.65 0.55 0.57 0.34 4 0.62 0.55 0.50 0.48 0.60 0.60 0.57 5 0.70 0.50 0.57 0.51 0.62 0.65 0.71 Mean 0.64 ± 0.06 0.56 ± 0.06 0.57 ± 0.10 0.56 ± 0.06 0.53 ± 0.08 0.55 ± 0.08 0.47 ± 0.17 4.3 Multimodal Fusion Using Dual Cross-Attention Fusing data from diverse sources is paramount in multimodal learning. In our architecture, we deploy a dual cross-attention mechanism to achieve a refined interaction between histopathol- ogy image features and genetic expression profiles. Specifically, this encompasses two attention operations: 1. Image features serve as the Query (Q) while genetic features act as both the Key (K) and Value (V). 2. Conversely, genetic features form the Query, with im- age features becoming the Key and Value. Let I = {i1, i2, . . . , in} be the set of image feature vectors and G = {g1, g2, . . . , gm} denote the genetic feature vectors. The enriched feature vectors for the kth image and genetic patches are: yI k = m X j=1 softmax  s(ik, g j)  g j, yG k = n X i=1 softmax (s(gk, ii)) ii. Here, the scoring function s(.) gauges the relevance between Queries and Keys. These weighted sums, after being normalised by the softmax function, are then concatenated to form a unified feature representation: yk = Concatenate(yI k, yG k ) The dual cross-attention offers two primary advantages: 1. Bidirectional Contextualisation: The mechanism captures intermodal dependencies in both directions— image to genetic and vice versa—providing a holistic contextualised representation. 2. Enhanced Selective Attention: By alternating the role of Query, the architecture gleans specific features from both modalities that are crucial for each other, ensuring compact and informative fused feature vectors. The resultant vectors yk are thus contextually enriched and opti- mised for tasks such as survival risk stratification. 4.4 MM-SurvNet Model Training The MM-SurvNet model (Fig. 3) employs either a pretrained Vision Transformer (MaxViT) or a CNN (ResNet50) for 4 [ Each Patient × Image Embedding (E)] Key Query Value Softmax = Matrix Multiplication Dual Cross Attention Module [ Each Patient × No. of Genes (PAM50)] Key Query Value Softmax + + = Concatenation 1 ... ≈≈ ... ≈≈ Fully-connected 128 ... ≈≈ ... ≈≈ Risk Score 256 32 + 4 [ Each Patient × Clinical Features (4)] Image as Query Gene as Query + 2 Fully-connected Figure 4: The proposed MM-SurvNet architecture employs a multimodal deep learning framework for survival risk prediction in cancer patients. Image embedding is concatenated with genetic data using dual cross-attention mechanisms and further integrated with clinical data in the final layer. histopathological image feature extraction. The multimodal data is subsequently incorporated into the MM-SurvNet model (Fig. 4) to enhance prediction accuracy. The Adam optimiser with decoupled weight decay (AdamW) with a learning rate of 0.001 and a mini-batch size of 12 governs the optimisation, which includes an early stopping criterion set at a patience of 5 and a maximum of 150 epochs [25]. The primary loss function is the negative log-likelihood (NLL), aligning with our survival analysis objectives, and optimisation targets the concordance index (C-index) as the key performance metric[26]. The NLL loss is calculated as: Loss = − P i  log(hi) −log P j∈Ri exp(log(hj))  · di P i di (2) where h = exp(log(h)) are the hazards, R is the risk set, and d represents the event. The optimisation, subject to early stopping criteria, ranged from 20 to 50 minutes on an NVIDIA Tesla V100 32GB GPU. The best model weights, based on the minimal NLL loss (2) were retained for testing. 5 Experimental Results In our study on survival risk stratification for ER+ breast can- cer patients, we integrated multimodal data—histopathological images, genetic information, and clinical data—resulting in en- hanced predictive accuracy. Our method achieved a notable C-index of 0.64, superior to models leveraging a single data modality (Table 1). For instance, models based on histopatho- logical image data only or on clinical information only, yielded C-indices of 0.53 and 0.47, respectively. An ablation study (Ta- ble 1) highlighted the significance of our holistic, multimodal approach. When integrating all feature sets, the MaxViT model Figure 5: Mean survival curves aggregated from all five cross- validation (CV) tests with 95% confidence intervals. In the depicted curves, the log-rank test p<0.05. surpassed ResNet50, emphasising the advantages of combining clinical, imaging and genetic data for more nuanced and accurate risk predictions. Kaplan-Meier survival analysis (Fig. 5) further demonstrates our method’s efficacy in differentiating outcomes between risk groups (log-rank test p<0.05), with an Integrated Brier Score of 0.11 for 5 years and 0.16 for 10 years [27]. 6 Discussion The findings of our study assessing multimodal data integra- tion for survival risk stratification in ER+ breast cancer patients underscore the superiority of multimodal approaches. Our pro- 5 posed model, amalgamating histopathological images, genetic data and clinical parameters, achieved a significant C-index of 0.64, outshining single-modal counterparts. Specifically, the MaxViT architecture consistently surpassed ResNet50 in han- dling this multimodal data. While the Kaplan-Meier analysis reinforced our model’s aptitude in patient risk stratification, there was notable performance variability across cross-validation folds, suggesting potential dataset nuances or imbalances. More- over, the traditional CoxPH model’s C-index of 0.47 highlighted the advantages of deep learning in capturing cancer prognosis’ intricate nature. The analysis not only substantiated our model’s proficiency in stratifying patients but also highlighted significant survival differences between the high-risk and low-risk groups, as revealed in the survival curve plots. Future work will encompass broader data types and probe the model’s utility across diverse cancer subtypes. While our study yielded promising results, it suffers from certain limitations. Our sample size, though adequate for the current modeling exercise, might benefit from expansion to ensure broader generalisability. The study’s concentration on ER+ breast cancer means that its applicability to other cancer subtypes remains as yet untested. The variability in performance across different cross-validation folds suggests potential dataset imbalances or unidentified influ- encing factors. Lastly, the model’s dependence on three types of data may pose challenges in settings where one or more of these data modalities is unavailable. 7 Conclusion Our study highlights the significance of multimodal data inte- gration in advancing survival risk stratification for ER+ breast cancer patients. This integrative methodology enhances predic- tion accuracy and sets the foundation for personalised treatment strategies in oncology. Compliance with Ethical Standards This research study was conducted retrospectively using hu- man subject data made available in open access by The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset, accessible through the National Cancer Institute’s Genomic Data Commons (GDC) portal. Ethical approval was not required for this study, in accordance with the ethical policies set forth by The Cancer Genome Atlas program. Conflicts of Interest No funding was received for conducting this study. The authors have no relevant financial or non-financial interests to disclose. Acknowledgement This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI), which is supported by the Australian Government. Addi- tionally, data preprocessing was performed using the computa- tional cluster Katana, which is supported by Research Technol- ogy Services at UNSW Sydney. References [1] H. Lindman, F. Wiklund, and K. K. Andersen, “Long-term treat- ment patterns and survival in metastatic breast cancer by intrinsic subtypes—an observational cohort study in Sweden,” BMC Can- cer, vol. 22, p. 1006, 2022. [2] Y. Han, J. Wang, and B. Xu, “Clinicopathological characteristics and prognosis of breast cancer with special histological types: A surveillance, epidemiology, and end results database analysis,” The Breast, vol. 54, pp. 114–120, 2020. [3] C. Shuai, F. Yuan, Y. Liu, C. Wang, J. Wang, and H. He, “Estrogen receptor—positive breast cancer survival prediction and analysis of resistance–related genes introduction,” PeerJ, vol. 9, p. e12202, 2021. [4] X. Li, L. Liu, G. J. Goodall, A. W. Schreiber, T. Xu, J. Li, and T. D. Le, “A novel single-cell based method for breast cancer prognosis,” PLoS Computational Biology, vol. 16, p. e1008133, 2020. [5] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multi- modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics,” 11 2021. [6] C. Nero, F. Ciccarone, A. Pietragalla, S. Duranti, G. Daniele, G. Scambia, and D. Lorusso, “Adjuvant treatment recommenda- tions in early-stage endometrial cancer: What changes with the introduction of the integrated molecular-based risk assessment,” Frontiers in Oncology, vol. 11, p. 612450, 2021. [7] W. Guo, W. Liang, Q. Deng, and X. Zou, “A multimodal affin- ity fusion network for predicting the survival of breast cancer patients,” Frontiers in Genetics, vol. 12, p. 709027, 2021. [8] Y. B. Shvetsov, L. R. Wilkens, K. K. White, M. Chong, A. Buyum, G. Badowski, R. T. L. Guerrero, and R. Novotny, “Prediction of breast cancer risk among women of the Mariana Islands: The BRISK retrospective case—control study,” BMJ Open, vol. 12, p. e061205, 2022. [9] K. Holli-Helenius, A. Salminen, I. Rinta-Kiikka, I. Koskivuo, N. Brück, P. Boström, and R. Parkkola, “MRI texture analysis in differentiating luminal A and luminal B breast cancer molecular subtypes—a feasibility study,” BMC Medical Imaging, vol. 17, p. 69, 2017. [10] K. Yao, E. Schaafsma, B. Zhang, and C. Cheng, “Tumor cell in- trinsic and extrinsic features predict prognosis in estrogen receptor positive breast cancer,” PLOS Computational Biology, vol. 18, pp. 1–22, 03 2022. [11] S. C. Wetstein, V. M. d. Jong, N. Stathonikos, M. Opdam, G. M. H. E. Dackus, J. P. W. Pluim, P. J. v. Diest, and M. Veta, “Deep learning-based breast cancer grading and survival analysis on whole-slide histopathology images,” Scientific Reports, vol. 12, 2022. [12] T. Wei, X. Yuan, R. Gao, L. J. Johnston, J. Zhou, Y. Wang, W. Kong, Y. Xie, Y. Zhang, D. Xu, and Z. Yu, “Survival predic- tion of stomach cancer using expression data and deep learning models with histopathological images,” Cancer Science, vol. 114, pp. 690–701, 2022. [13] L. Chen, H. Zeng, X. Yu, Y. Huang, Y. Luo, and X. Ma, “Histopathological images and multi-omics integration predict molecular characteristics and survival in lung adenocarcinoma,” Frontiers in Cell and Developmental Biology, vol. 9, 2021. [14] X. Wu, Y. Shi, M. Wang, and A. Li, “CAMR: cross-aligned multimodal representation learning for cancer survival prediction,” Bioinformatics, vol. 39, p. btad025, 2023. [15] Y. He, B. Hu, C. Zhu, W. Xu, X. Hao, B. Dong, X. Chen, Q. Dong, and X. Zhou, “A novel multimodal radiomics model for predicting prognosis of resected hepatocellular carcinoma,” Frontiers in Oncology, vol. 12, p. 745258, 2022. 6 [16] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multimodal fusion using sparse CCA for breast cancer survival prediction,” IEEE International Symposium on Biomedical Imaging, pp. 1429– 1432, 2021. [17] R. Vanguri, J. Luo, A. Aukerman, J. V. Egger, C. J. Fong, N. Hor- vat, A. Pagano, J. d. A. B. Araújo-Filho, L. Geneslaw, H. Rizvi, R. E. Sosa, K. M. Boehm, S. Yang, F. M. Bodd, K. Ventura, T. J. Hollmann, M. S. Ginsberg, J. Gao, M. D. Hellmann, J. L. Sauter, and S. P. Shah, “Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer,” Nature Cancer, vol. 3, no. 10, pp. 1151–1164, 2022. [18] W. Lingle, B. J. Erickson, M. L. Zuley, R. Jarosz, E. Bonac- cio, J. Filippini, J. M. Net, L. Levi, E. A. Morris, G. G. Figler, P. Elnajjar, S. Kirk, Y. Lee, M. Giger, and N. Gruszauskas, “The cancer genome atlas breast invasive carcinoma collection (TCGA- BRCA),” The Cancer Imaging Archive, 2016. [19] P. Bankhead, M. B. Loughrey, J. A. Fernández, Y. Dombrowski, D. G. McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray, H. G. Coleman, J. A. James, M. Salto-Tellez, and P. W. Hamil- ton, “QuPath: Open source software for digital pathology image analysis,” Scientific Reports, vol. 7, p. 16878, 2017. [20] M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley, X. Guan, C. Schmitt, and N. E. Thomas, “A method for normalizing histology slides for quantitative analysis,” IEEE International Symposium on Biomedical Imaging (ISBI), pp. 1107– 1110, 2009. [21] B. Li and C. N. Dewey, “RSEM: Accurate transcript quantification from RNA-Seq data with or without a reference genome,” BMC Bioinformatics, vol. 12, p. 323, 2011. [22] J. S. Parker, M. E. Mullins, M. C. Cheang, S. Leung, V. David, T. L. Vickery, S. R. Davies, C. Fauron, X. He, Z. Hu, J. Quacken- bush, I. J. Stijleman, J. Palazzo, J. S. Marron, A. B. Nobel, E. R. Mardis, T. O. Nielsen, M. J. Ellis, C. M. Perou, and P. S. Bernard, “Supervised risk predictor of breast cancer based on intrinsic sub- types,” Journal of Clinical Oncology, vol. 27, pp. 1160–1167, 2009. [23] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li, “MaxViT: Multi-axis vision transformer,” European Con- ference on Computer Vision (ECCV), p. 459–479, 2022. [24] A. D. Jones, J. P. Graff, M. A. Darrow, A. D. Borowsky, K. Ol- son, R. Gandour-Edwards, A. Mitra, D. Wei, G. Gao, B. Durbin- Johnson, and H. H. Rashidi, “Impact of pre-analytical variables on deep learning accuracy in histopathology,” Histopathology, vol. 75, pp. 39–53, 2019. [25] I. Loshchilov and F. Hutter, “Decoupled weight decay regular- ization,” International Conference on Learning Representations (ICLR), 2019. [26] E. Longato, M. Vettoretti, and B. Di Camillo, “A practical per- spective on the concordance index for the evaluation and selection of prognostic time-to-event models,” Journal of Biomedical Infor- matics, vol. 108, p. 103496, 2020. [27] K. J. Jager, P. C. van Dijk, C. Zoccali, and F. W. Dekker, “The analysis of survival data: The Kaplan–Meier method,” Kidney International, vol. 74, no. 5, pp. 560–565, 2008. Raktim Kumar Mondol is a PhD candidate in Computer Science and Engineering, spe- cializing in computer vision and bioinformat- ics. He completed his MEng in Engineering with High Distinction from RMIT University, Australia. Mondol’s research interests in- clude histopathological image analysis, clini- cal prognosis prediction, and enhancing clini- cal understanding through the interpretability of computational models. Ewan Millar is a Senior Staff Specialist Histopathologist with NSW Health Pathology at St George Hospital Sydney with expertise in breast cancer pathology and translational research and a strong interest in AI and digi- tal pathology applications. Arcot Sowmya is Professor in the School of Computer Science and Engineering, UNSW. Her major research interest is in the area of Machine Learning for Computer Vision and includes learning object models, fea- ture extraction, segmentation and recogni- tion based on computer vision, machine learning and deep learning. In recent years, applications in the broader health area are a focus, including biomedical informatics and rapid diagnostics in the real world. All of these areas have been supported by com- petitive, industry and government funding. Erik Meijering (Fellow, IEEE), is a Profes- sor of Biomedical Image Computing in the School of Computer Science and Engineer- ing. His research focusses on the develop- ment of innovative computer vision and ma- chine learning (in particular deep learning) methods for automated quantitative analysis of biomedical imaging data.