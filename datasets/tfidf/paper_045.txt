BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion Raktim Kumar Mondol1, Ewan K.A. Millar2, Arcot Sowmya1, and Erik Meijering1,* 1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia 2Department of Anatomical Pathology, NSW Health Pathology, St. George Hospital *Correspondence: erik.meijering@unsw.edu.au Abstract Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to obtain a holistic profile and achieve survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors (DINO and MoCoV3) pretrained on histopathological patches to capture detailed image features. These features are then fused by a variational autoencoder and fed to a self-attention network generating patient-level features. A co-dual-cross-attention mechanism combines the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network, further enhancing predictive performance and achieving comprehensive multimodal feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge. Our model achieves a mean concordance index of 0.77 and a time- dependent area under the curve of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival in univariate analysis (HR=2.99, 95% CI: 1.88–4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80–4.68, p<0.005). Keywords Multimodal Fusion · Breast Cancer · Whole Slide Images · Deep Neural Network · Survival Prediction 1 Introduction Breast cancer poses a significant global health concern, with a high incidence rate and substantial impact on morbidity and mortality [1, 2]. The incidence of breast cancer varies across different regions and populations, with higher rates observed in developed countries [1]. The prevalence of breast cancer in Australia, affecting 1 in 8 women up to the age of 85, is a cause for concern due to its rising incidence rate over the past decade [3]. This trend highlights the crucial need for accurately predicting survival risks to identify high-risk patients who may benefit from more intensive treatment or monitoring, thereby potentially improving outcomes [2]. In breast cancer, the estrogen receptor (ER) status plays a critical role in determining treatment strategies and predicting patient prognosis. ER+ breast cancer, which includes Luminal A and Luminal B subtypes, is distinguished by the presence of ERs on cancer cells, making it responsive to hormonal therapies such as tamoxifen [4]. While Luminal A tumours are usually low-grade with a favorable prognosis (Ki-67<14%), Luminal B tumours are usually higher grade and pose a higher recurrence risk and worse outcome (Ki-67≥14%) [4, 5]. The inherent heterogeneity of breast cancer poses challenges for prediction of prognosis and treatment decisions, particularly in post-menopausal ER+ breast cancer, with previous studies reporting conflicting results on the survival difference between Luminal A and B metastatic breast cancer patients [6, 7]. A common critical clinical dilemma is the selection of those early ER+ breast cancer patients at high risk of recurrence who may benefit from the addition of chemotherapy to endocrine therapy. Therefore, accurate survival risk prediction models specifically tailored for ER+ breast cancer are essential for personalised treatment decisions. Traditional methods for survival risk prediction often rely on clinicopathological risk factors (such as age, tumour size, grade, lymph node metastasis and clinical stage), which may fail to fully capture the complex biology of cancer [4, 7–10]. To ad- dress this issue, molecular markers and gene expression profiles have been identified as potential prognostic factors that provide valuable insights into tumour biology and potential therapeu- tic targets [11–15]. Over the past decade, the integration of genomic testing into treatment decision-making processes has been enhanced by the utilisation of several commercial gene panels, such as Prosigna/PAM50, OncotypeDx and Endopredict among others. In addition, histopathological imaging, which offers in-depth insights into the cellular and tissue characteristics of tumours, plays a vital role in both the diagnosis and prognosis of breast cancer [16]. However, to address the varied nature of the disease effectively, it is essential to consider all available data modalities. Therefore, integrating imaging, genetic and clini- copathological information into a single risk prediction model could potentially enhance risk prognostication in the clinic [9]. In this study, we propose a novel multimodal survival risk pre- diction model that significantly enhances the prognosis of ER+ arXiv:2402.10717v2 [cs.CV] 3 Jun 2024 2 Patient Details Medical Tests Treatment Diagnosis Age Group: Post Menopausal Gender: Female Ethnicity: Caucasian Clinical Exam Imaging Core Biopsy Breast-Conserving Surgery (Lumpectomy) Tumor Size: T2 Tumor Grade: G2 Hormone Receptor Status: ER+ HER2 Status: HER2- Lymph Node Status: LN- Cell Proliferation: Ki67 20% Genetic Testing PAM50/ROR, OncotypeDX etc. Endocrine Therapy & Radiotherapy High Risk Low Risk Adjuvant Chemotherapy Chemotherapy Not Required Multimodal Risk Prediction Invasive Nature: IDC Ensures Adequate Treatment Avoids Excessive Treatment & Toxicity Figure 1: Illustration of the clinical management pathway in treating breast cancer patients. This example concerns a postmenopausal patient who has been diagnosed with breast cancer, specifically invasive ductal carcinoma (IDC). The process begins with an initial diagnosis through clinical examination, imaging and core biopsy. Following this, surgery is performed to completely excise the tumour, and postoperative tumour histopathological classification is performed to assess key factors including tumour size (e.g. T2), grade (e.g. G2), hormone receptor status (e.g. ER+), HER2 status (e.g. HER2-), lymph node status (e.g. LN-), and proliferation index (e.g. Ki67 20%). Subsequent treatments may include hormone therapy and radiotherapy. Additional molecular tests, like genetic testing, are utilised to determine specific cancer molecular subtypes and further assess risk of recurrence. The proposed final step in this pathway is the application of our BioFusionNet model. This model combines tumour characteristics, pathology and genetic testing to determine high and low risk patients, thereby guiding personalised treatment decisions and efficiently preventing both under-treatment and over-treatment. For example, low-risk patients might undergo lumpectomy with hormone therapy and radiotherapy, whereas high-risk patients are advised to have chemotherapy in addition to these treatments. Whilst this pathway mirrors current clinical practice, our study streamlines the integration of all available critical data to derive an automated single risk prediction score. breast cancer by integrating histopathology images, genetic pro- files, and clinical data. By combining these data modalities, our model captures the complex interplay between cellular, molec- ular, and clinical factors, thereby improving its predictive ac- curacy. Moreover, we introduce a weighted Cox loss function specifically designed to handle imbalanced survival data, further enhancing the model’s performance. The model is thoroughly evaluated using metrics such as the concordance index (C-index) and time-dependent area under the curve (AUC) score, and its performance is compared to existing methods to establish its efficacy. The model’s ability to accurately predict survival risks and categorise patients into distinct risk groups has the potential to inform personalised treatment decisions and improve patient outcomes. Finally, we provide explainable analyses to eluci- date the influence of different genes and clinical factors on risk prediction, offering valuable insights into the underlying predic- tive mechanisms. The main contributions of this study are as follows: 1. A novel multimodal survival risk prediction model in- tegrating histopathology images, genetic profiles, and clinical data for ER+ breast cancer prognosis. 2. A weighted Cox loss function designed to handle im- balanced survival data, improving predictive accuracy. 3. Explainable analyses providing insights into the influ- ence of genes and clinical factors on risk prediction. 2 Background Cancer risk prediction is of paramount importance due to its potential for guiding personalised screening, prevention and treatment strategies, ultimately leading to improved patient out- comes and reduced mortality (Fig. 1). This approach is espe- cially vital in the context of ER+ breast cancer, where accurate risk prediction is essential for identifying individuals at higher recurrence risk. These patients may benefit from more aggres- sive treatments like chemotherapy [17–20]. On the other hand, risk models are equally critical in recognising lower-risk pa- tients, potentially sparing them from unnecessary treatments and their side effects [21]. Online algorithms such as Predict1 and Adjuvant2 are used clinically to estimate the risk of recurrence and the benefit of adding chemotherapy to endocrine therapy, which is a major treatment dilemma. Developments in deep learning, such as the Cox proportional hazards deep neural network, have revolutionised cancer re- search by improving survival data modeling and treatment rec- ommendation systems [22]. Multimodal data fusion, which combines information from diverse sources such as imaging, ge- nomics and clinical data, has gained attention in cancer research due to its ability to provide a comprehensive understanding of the disease and improve predictive outcomes [23–25]. Cross- attention transformer mechanisms that integrate histopatholog- ical images and genomic data capture complementary infor- mation from different modalities, leading to improved survival prediction [26]. In this evolving landscape, models such as MultiDeepCox- SC [27], MCAT [28], MultiSurv[29], HFBSurv [30], Pathomic Fusion [31], and TransSurv [32] exemplify significant progress in multimodal analysis. These models harness unique strategies to integrate diverse data types for enhanced survival prediction. MultiDeepCox-SC combines histopathological image-derived 1https://breast.predict.nhs.uk/tool 2https://oncoassist.com/adjuvant-tools/ 3 risk scores with clinical data and gene expression through the Cox proportional hazards (CoxPH) model, providing a detailed risk assessment. MCAT utilises a genomic-guided co-attention layer to map relationships between whole slide images and genomic features, enhancing interpretability in computational pathology. MultiSurv simplifies the integration of multimodal data by merging feature vectors into a unified representation for predicting survival probabilities. In contrast, HFBSurv applies an hierarchical framework with attentional factorised bilinear modules, systematically processing information from lower to higher complexity levels. Pathomic Fusion adopts a gating- based attention mechanism, effectively filtering out noise to focus on salient features across modalities. Finally, TransSurv employs cross-attention transformers to combine histopatholog- ical and genomic data, capturing complementary information that significantly improves predictive accuracy. Despite these advances, the integration of multimodal data, marked by its inherent heterogeneity and dimensional variabil- ity, remains a significant challenge, highlighting the need for advanced methodologies to effectively integrate and leverage diverse data sources for robust survival risk assessment [33–35]. To address this, BioFusionNet introduces a unique strategy by combining self-supervised learning models, specifically DINO and MoCoV3, for feature extraction. This method is further enhanced by a co-dual-cross-attention mechanism for effective multimodal fusion. The co-attention component facilitates syn- chronised learning from multiple data types by highlighting mutually informative features, whereas the dual cross-attention mechanism provides a deeper interaction layer, allowing for more effective integration of these features. Additionally, Bio- FusionNet mitigates issues of data imbalance through the imple- mentation of a weighted Cox loss function. This comprehensive approach to multimodal fusion marks a significant advancement in predicting survival risks for ER+ breast cancer, addressing notable gaps in current research. 2.1 Data Collection Our study used hematoxylin-and-eosin-stained (H&E) formalin- fixed paraffin-embedded (FFPE) digital slides from The Can- cer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA). Whole-slide images (WSIs) from the TCGA-BRCA data col- lection were downloaded from the GDC Portal (accessed 25 August 2023). In this work, we chose a subset of 249 cases from the TCGA-BRCA dataset, in order to maintain the propor- tions of Luminal A and B subtypes as well as the proportion of survival events within each subtype. Among the 249 cases, 83 had survival events, while the remaining cases were censored. The survival events were distributed as follows: 54 events in the Luminal A subtype and 29 events in the Luminal B sub- type. For each subtype, we selected cases such that the total was about three times the number of survival events, resulting in 149 Luminal A cases and 100 Luminal B cases. By preserving these proportions, we obtained a subset representative of key characteristics in the full dataset. Additionally, we obtained transcriptome-wide RNA-sequencing data representing mRNA expression levels for a total of 20,438 genes in the reference genome from the TCGA dataset. These data were processed using RNA-sequencing by expectation maximisation (RSEM) and were downloaded from the cBioPortal platform[36]. This dataset included a range of clinical information for each pa- tient, such as tumour grade, tumour size, lymph node status, age at diagnosis and molecular subtypes. Overall, patients who had WSIs, RNA-sequencing and clinical data available were included in the study. 2.2 Data Preparation 2.2.1 Slide Annotation An expert breast pathologist manually annotated the selected slides using QuPath [37]. The annotation was performed for localisation of the tumour outline, excluding any necrosis but including stroma and tumour infiltrating lymphocytes (TILs). The pathologist was blinded to any molecular or clinical features during annotation. 2.2.2 Image Data Preparation The images were first downsampled to 0.25 µm/pixel, corre- sponding to approximately 40× magnification. The annotated tumour regions were processed semi-automatically with QuPath to create 224×224-pixel patches, resulting in approximately 500 nonoverlapping patches per sample. To address staining inconsistencies, vector-based colour normalisation was applied [38]. 2.2.3 RNA-Sequencing Data Preparation From the extensive set of 20,438 genes, we selected genes featured in various commercial assays, namely Oncotype DX, Mammaprint, Prosigna (PAM50), EndoPredict, BCI (Breast Cancer Index), and Mammostrat [39–44], as these are the most relevant genetic markers to our study’s objectives. This resulted in a subset of 138 genes. The RNA-sequencing data obtained from the TCGA dataset had already undergone processing using RSEM, and no further normalization was applied to the gene expression values. 2.2.4 Clinical Data Preparation From the clinical data, we selected variables based on their es- tablished relevance in breast cancer prognosis and treatment outcomes [45–48]. Specifically, we included tumour grade (cat- egorised as grade 1&2 versus grade 3), tumour size (>20 mm versus ≤20 mm), patient age (>55 versus ≤55) and lymph node status (positive versus negative). The decision to binarise the clinical data was made to facilitate CoxPH analysis. Binarisation also simplifies both univariate and multivariate hazard analyses and makes it easier to understand how each clinical factor affects the chance of survival. 2.3 Proposed Model The proposed deep learning model, which we call BioFusion- Net, is an innovative feature extraction and multimodal fusion framework designed to leverage and integrate diverse data types, including histology images, genomic features and clinical data for enhanced cancer outcome prediction (Figs. 2 and 3). The essence of BioFusionNet lies in its capability to fuse these data modalities into a cohesive tensor representation, effectively cap- turing both bimodal and trimodal interactions. This approach is aimed at surpassing the performance of traditional unimodal and existing multimodal representations in survival risk prediction. 2.3.1 Feature Extraction Using DINO and MoCoV3 Histopathological images, rich in phenotypic information, are pivotal for understanding cancer pathology. BioFusionNet 4 [Each Patient × Image Embedding (256)] . . . . . . . . . Output Image Features (Patient Level) [No. of Patches (N) × Image Embedding (256)] 500 × 3 × 224 ×224 Preprocessed Histopathology Images Skip Connection Image Patches ... Key Query Value Softmax Aggregated Output Self-Attention Module MoCoV3 Pretrained on 15M Histo Patches (Multiple Datasets) DINO Pretrained on 2M Histo Patches (TCGA-BRCA) DINO Pretrained on 33M Histo Patches (Multiple Datasets) + Encoder Decoder σ μ Latent Space (z) Total Loss = Reconstruction Loss (MSE) + KL Divergence 500 × 384 500 × 384 500 × 1152 500 × 1152 500 × 384 Variational Autoencoder (VAE) SSL Base Model ViT Small (Patch16) + = Concatenation = Mean = Standard Deviation 500 × 256 Feed-Forward Network Encoder Decoder = Matrix Multiplication σ μ Legend: Figure 2: BioFusionNet Stage 1: The proposed model integrates self-supervised image feature extraction methods, namely DINO and MoCoV3, pretrained on three distinct datasets. Features are concatenated and fed to a Variational AutoEncoder (VAE). Subsequently, the latent space of the VAE is utilised to feed a self-attention network, which aggregates patch-level features into a comprehensive patient-level representation. utilises two advanced self-supervised learning models, DINO (self-DIstillation with NO labels) and MoCoV3 (Momentum Contrast version 3), both based on the Vision Transformer (ViT) architecture, to extract morphological features from histology images crucial for identifying cancer-related patterns. DINO: The DINO framework employs a dual-network architec- ture, consisting of a student and a teacher network, both being ViTs. The student network learns by attempting to replicate the output of the teacher network, which in turn is an exponential moving average of the student’s parameters. The core process involves generating multiple augmented views (I1, I2, . . . , Ik) of a given input image (I), which are then processed by these net- works. The resultant feature vectors from the student (Fs) and teacher (Ft) networks are utilised to compute the distillation loss as follows: LD = k X i=1 CrossEntropy Fs(Ii), Softmax Ft(Ii) τ !! , (1) where τ represents the temperature scaling parameter. Notably, DINO is pretrained on a broad range of datasets, including BACH, CRC, MHIST, PatchCamelyon and CoNSeP, compris- ing 33 million patches (DINO33M), and 2 million patches from TCGA-BRCA (DINO2M) [49, 50]. The output func- tions for DINO33M and DINO2M, denoted as fDINO33M(x) and fDINO2M(x) respectively, convert an input image x of size 224 × 224 × 3 into a 1 × 384 feature vector. We utilised these two models for feature extraction: DINO33M trained on diverse datasets providing a broad perspective and enabling the model to recognise a wide array of general histopathological features, and DINO2M specifically trained on breast cancer data for more spe- cialised and precise feature extraction relevant to breast cancer pathology. MoCoV3: The MoCoV3 framework, which incorporates ViTs, represents a significant advancement in self-supervised learning through its adoption of the momentum contrast (MoCo) ap- proach [51]. At the heart of this framework lies the contrastive learning mechanism, designed to differentiate between positive and negative pairs, thereby enhancing the model’s feature learn- ing capabilities. MoCoV3’s architecture is defined by two main components: a query encoder that processes the current batch of images, and a key encoder updated via a momentum-based moving average of the query encoder’s parameters: θk ←mθk + (1 −m)θq, (2) where θk and θq are the parameters of the key and query encoders respectively and m is the momentum coefficient. This enables the key encoder to maintain a queue of encoded keys represent- ing previously seen images, thus enhancing the model’s ability to maximise agreement between differently augmented views of the same image (positive pairs) and minimise similarity with other images (negative pairs). The framework uses the InfoNCE loss [52]: LInfoNCE = −log exp(q · k+/τ) PK i=0 exp(q · ki/τ) , (3) where q and k+ are the query and positive key feature vectors, ki are the negative key vectors, K is the number of negative 5 [ Each Patient × Image Embedding (256)] [ Each Patient × No. of Genes (138)] Fully-connected 128 + 4 Risk Score 256 32 [ Each Patient × Clinical Features (4)] Fully-connected Fully-connected 512 ... ≈≈ ... ≈≈ ... ≈≈ ... ≈≈ Transformer Encoder Co Dual Cross Attention Co Attention Dual Cross Attention Multi-Head Self-Attention Feed-Forward Network (FFN) Transformer Encoder Key Query Value Softmax Cross Attention + Layer Normalization Dropout Matrix Multiplication Concatenation + + + + x1 x2 xout fin fout I G AIG AGI CIG CGI DIG DGI Tcat Figure 3: BioFusionNet Stage 2: The proposed model fuses image embeddings generated from Stage 1 with genetic data through a co-dual-cross-attention mechanism. This fusion is subsequently combined with clinical data using a feed-forward network (FFN), leading to the generation of the final risk score output. keys and τ is the temperature parameter. MoCoV3 has been pretrained on an extensive collection of 15 million histology patches from over 30 thousand WSIs derived from the TCGA and Pathology AI Platform (PAIP) datasets, encompassing a wide variety of cancer types and histological features. This endows the model with a robust and versatile capability to extract meaningful features from histopathological data [53]. Similar to DINO, the output function of MoCoV3, fMoCoV3(x), transforms an input image x of dimensions 224 × 224 × 3 into a 1 × 384 feature vector. 2.3.2 Unimodal Feature Integration Extracted features from DINO33M, DINO2M and MoCoV3 are concatenated to form a comprehensive 1 × 1152 feature vector fcat(x) = fDINO33M(x) ⊕fDINO2m(x) ⊕fMoCoV3(x) (Fig. 2). Following this, we employ a VAE to encode the integrated image features into a 256-dimensional feature in latent space. The latent feature vector is then passed through a self-attention model and aggregated using sum pooling to generate patient- level features. 2.3.3 Feature Fusion Using Variational Autoencoding Our VAE consists of an encoder and a decoder. The encoder function fenc maps the concatenated feature vector fcat(x) to the latent space by generating the mean µ and standard deviation σ of the latent representation: (µ, σ) = fenc(fcat(x)). (4) With 500 patches per patient, the latent space is structured as a matrix of size 500 × 256. This is achieved by sampling z using the reparameterization trick: z = µ + σ · ϵ, ϵ ∼N(0, I), z ∈R500×256. (5) The decoder fdec then attempts to reconstruct the input from the latent variable z: ˆx = fdec(z). (6) The VAE is optimised using a loss function that combines mean squared error (MSE) for reconstruction accuracy and Kullback- Leibler (KL) divergence for distribution regularisation: LVAE = MSE(ˆx, x) + β · KL(N(µ, σ2)∥N(0, I)), (7) where β balances the reconstruction and regularisation terms. The MSE term ensures that the reconstructed image closely resembles the original input, while the KL divergence term encourages the latent distribution to approximate a standard normal distribution. This enables BioFusionNet to effectively blend the features from the different self-supervised models, enhancing the overall feature representation. 6 2.3.4 Patch-to-Patient Aggregation To aggregate the patch-level features from the latent space of the VAE into a comprehensive patient-level representation capturing the interdependencies among image patches, our model uses self-attention (Fig. 2). The self-attention module computes a weighted sum of the key (K), query (Q), and value (V) vectors: yi = 500 X j=1 Softmax  s(Qi, Kj)  Vj, (8) where s(Qi, Kj) is the attention function that determines the rel- evance between each query and key pair. This offers two signifi- cant benefits. First, by leveraging the VAE’s latent vectors, the model focusses on the most pertinent image features, resulting in a more precise feature representation. Second, by consid- ering all latent representations, the self-attention mechanism contextualises each patch within the broader histopathology of the patient. This aggregation process effectively combines the patch-level features of all 500 patches, taking into account their relevance and interdependencies, to generate a comprehensive patient-level representation. 2.3.5 Multimodal Fusion Using Co Dual Cross Attention To integrate patient-level image embeddings with genetic fea- ture data, our model uses a co-dual-cross-attention mechanism (Fig. 3). This is achieved using a complex architecture compris- ing co-attention and dual-cross-attention modules. The co-attention module applies linear transformations to the image embeddings I and genetic features G, yielding their re- spective query (Q), key (K), and value (V) vectors, and computes co-attention scores scaled by √dk, where dk is the dimensional- ity of the keys: AIG = Softmax  QIKT G/ p dk  VG, (9) AGI = Softmax  QGKT I / p dk  VI, (10) where AIG represents the attention from images to genetic fea- tures, and AGI the attention from genetic features to images. The Softmax function normalises these scores, facilitating an effec- tive weighting of feature importance in the fusion process. This bidirectional attention prepares the ground for more complex in- teractions in the subsequent stage of the co-dual-cross-attention mechanism. Subsequently, the dual-cross-attention module fur- ther refines the integration of the image and genetic features in two distinct stages. The first stage concerns the interaction between the co-attended image features (AIG) and co-attended genetic features (AGI). The cross attention is computed as: CIG = Softmax  AIG(AGI)T/ p dk  AIG, (11) CGI = Softmax  AGI(AIG)T/ p dk  AGI, (12) where CIG represents the cross-attention output when image fea- tures attend to genetic features, and CGI the reverse. This stage is crucial for enhancing each modality by integrating contextu- ally relevant information from the other. In the second stage, the outputs from the first stage are further refined by reapplying them to their respective original features. This enhances the depth of the multimodal integration: DIG = Softmax  CIGIT/ p dk  I, (13) DGI = Softmax  CGIGT/ p dk  G, (14) Algorithm 1 Weighted Cox Loss Require: r: risks (log hazard ratios), e: events, w: weights Ensure: LWCox: negative log likelihood loss 1: Compute Ew = PN i=1 wiei as total weighted events 2: Sort samples by descending r and align e, w 3: for i ∈1, . . . , N do ▷N is the number of samples 4: Compute hazard ratio hi = exp(ri) 5: end for 6: Init weighted cumulative hazard Hw0 = 0 7: for i ∈1, . . . , N do 8: Update cumulative sum Hwi = Hwi−1 + wihi 9: end for 10: for i ∈1, . . . , N do 11: Compute uncensored log likelihood ui = wi(ri −log(Hwi)) 12: end for 13: Compute c = u ⊙e ▷⊙is the element-wise product 14: Compute loss LWCox = −1 Ew PN i=1 ci 15: return LWCox where DIG and DGI denote the refined cross-attention outputs, further enhancing the original image and genetic data with ad- ditional contextual insights. By sequentially processing the attended features, the model achieves a richer and more contextu- ally informed representation of the fused image and genetic data, suited for complex tasks like risk prediction. The concatenated output Tcat = DIG ⊕DGI is fed to a Transformer Encoder, which employs multiple layers of self-attention and a feed-forward network (FFN) to achieve a deeper assimilation and transforma- tion of the fused features. The Transformer Encoder consists of N identical layers, each containing two sublayers: a multihead self-attention mechanism and a position-wise fully connected FFN. In the multihead self-attention sublayer, the input is first linearly projected into a number of different subspaces equal to the number of attention heads. The self-attention mechanism is then applied to each subspace independently, allowing the model to jointly attend to information from different representa- tion subspaces at different positions. The outputs from all heads are then concatenated and linearly projected back to the original dimension. The second sublayer is a position-wise FFN, which consists of two linear transformations with a ReLU activation in between: FFN(x) = max(0, xW1 + b1)W2 + b2 (15) where W1, W2, b1, and b2 are learnable parameters. A residual connection is employed around each of the two sublayers, fol- lowed by layer normalisation. The output of the Transformer Encoder represents the deeply integrated features from both the imaging and genetic modalities and is further processed by four fully-connected layers, the third of which also integrates the clinical information (Fig. 3). The integration of clinical variables at this stage is crucial due to their dimensionality and characteristics. Clinical data, comprising only four features, may be overshadowed by the higher-dimensional features from histopathological images and genetic profiles if introduced ear- lier in the model. By integrating these clinical variables in a later layer of the network, closer to the output, their impact is more effectively mapped onto the model’s survival risk pre- diction. This approach, referred to as ‘late fusion’, allows the clinical variables to have a more significant influence on the 7 Table 1: Performance comparison of multimodal and unimodal models for cancer risk prediction using C-index. Fold Imaging+Genetic+Clinical Imaging+Genetic Imaging Clinical Genetic BioFusionNet BioFusionNet BioFusionNet CoxPH MLP CoxPH MLP 1 0.78 0.73 0.58 0.52 0.66 0.62 0.66 2 0.71 0.72 0.69 0.42 0.64 0.55 0.73 3 0.72 0.69 0.61 0.59 0.69 0.51 0.66 4 0.81 0.75 0.70 0.64 0.70 0.62 0.75 5 0.82 0.65 0.69 0.72 0.66 0.63 0.67 Mean ± Std 0.77 ± 0.05 0.71 ± 0.04 0.65 ± 0.05 0.58 ± 0.11 0.67 ± 0.02 0.59 ± 0.05 0.69 ± 0.04 final output compared to ‘early fusion’, where clinical variables are integrated in earlier layers of the network. This multimodal integration results in a holistic representation of both the phenotypic and genotypic information of ER+ breast cancer. Finally, the network employs a linear output layer that predicts the survival risk score. 2.4 Proposed Loss Function For training BioFusionNet, we propose a novel loss function termed the weighted Cox loss (computed by Algorithm 1), which is tailored to address the challenges of imbalanced survival data (a common issue in survival analysis): LWCox = − 1 PN i=1 wiei N X i=1 wiei(ri −log(Hwi)), (16) where ei denotes the event occurrence, wi the assigned weight, ri the log hazard ratio, Hwi the weighted cumulative hazard and N the number of samples. Unlike the traditional Cox proportional hazards loss (LCox) [22], the proposed loss uses weighting to mitigate the effects of uneven distribution of events within the dataset. In this work we used wi = 3, considering that censored data (denoted as ‘0’) is almost three times as prevalent as event data (denoted as ‘1’), and thus the sensitivity of the loss function to the latter should be enhanced accordingly, mitigating the bias towards censored data. 2.5 Model Training The training of BioFusionNet is divided into two distinct stages as follows (Figs. 2 and 3): Stage 1: Feature Extraction: The self-supervised pretrained models DINO33M, MoCoV3 and DINO2M extract features from histopathology image patches, which are concatenated and then fed into a VAE to produce embeddings, which in turn are processed by a self-attention module to produce a patient- level feature vector (Fig. 2). The VAE was optimised using AdamW with a learning rate of 0.0001 and a batch size of 12. The employed loss function is a combination of MSE and KL divergence, targetting the construction of an advanced latent space to generate detailed patient-level features. Stage 2: Risk Prediction: The proposed co-dual-cross-attention mechanism, followed by multiple FFNs and a final output node that uses a linear activation function, predicts the patient-level risk from the image-based, genetic and clinical information (Fig. 3). Here, training was performed using the proposed weighted Cox loss (LWCox), which was optimised using the Adam algorithm with a learning rate of 0.001 and a batch size of 12. To mitigate overfitting, an early stopping mechanism based on the validation loss was implemented. This involved halting the training process after a patience period of 10 epochs if no improvement was observed. This two-stage training approach for BioFusionNet was moti- vated by two key factors. First, the computational complexity of training sophisticated models, especially those incorporating elements like multiple feature extractors, VAE, and various at- tention blocks, can be significantly high. An end-to-end training process often requires increased memory usage and even more data, which may not be feasible always. By adopting a two-stage training process, we effectively manage this complexity, break- ing down the training into more manageable parts. Second, a modular design facilitates conducting ablation experiments. By training in stages, we can isolate the effects of specific modules or features, systematically analysing their impact and optimising their configuration. This modular approach enhances our ability to refine and improve the learning architecture. Both training stages used a dataset comprising 199 training sam- ples and 50 validation samples within a five-fold cross-validation framework. The trained model predicts a continuous risk score for every patient within each validation fold. For survival analy- sis, we employed the median risk score θopt, derived from each training set, as a threshold to classify patients in the validation set into two categories: high risk (risk score > θopt) and low risk (risk score < θopt). We note that θopt is not a user-defined param- eter, but its value depends on and is automatically calculated from the specific training set used in each fold, and thus varies depending on the dataset. To achieve optimal performance, we used Optuna3 for hyperparameter optimisation, tuning key parameters including learning rate, weight decay, number of neurons, number of layers, and dropout for each model. 2.6 Evaluation Metrics To quantitatively evaluate survival risk score prediction, we employed the concordance index (C-index) and the area under the curve (AUC) as our primary metrics. The C-index assesses the concordance between predicted survival times and observed outcomes, especially in the presence of censored data: C-index = nP i=1 nP j=1 I(yi < yj, δi = 1)I( ˆf(xi) < ˆf(xj)) nP i=1 nP j=1 I(yi < yj, δi = 1) . (17) where n is the number of patients, yi and yj denote the observed survival times, δi indicates whether the event was observed (not censored), ˆf(xi) represents the predicted risk for the ith patient, 3https://optuna.org/ 8 Table 2: Performance comparison of multimodal fusion methods for cancer risk prediction. Method Fold C-index AUC Value Mean ± Std Value Mean ± Std MultiSurv [29, 54, 55] 1 0.71 0.63 ± 0.07 0.74 0.63 ± 0.09 2 0.59 0.52 3 0.60 0.61 4 0.69 0.70 5 0.54 0.57 MultiDeepCox-SC [27] 1 0.71 0.60 ± 0.08 0.68 0.58 ± 0.08 2 0.68 0.66 3 0.55 0.49 4 0.58 0.57 5 0.50 0.50 HFBSurv [30] 1 0.58 0.54 ± 0.07 0.51 0.49 ± 0.04 2 0.47 0.51 3 0.56 0.45 4 0.45 0.44 5 0.62 0.53 PathomicFusion [31] 1 0.63 0.52 ± 0.08 0.68 0.47 ± 0.23 2 0.43 0.10 3 0.56 0.54 4 0.50 0.63 5 0.46 0.38 MCAT [28] 1 0.71 0.70 ± 0.04 0.70 0.71 ± 0.04 2 0.69 0.67 3 0.64 0.65 4 0.70 0.69 5 0.76 0.72 TransSurv [32][26] 1 0.70 0.69 ± 0.04 0.68 0.66 ± 0.04 2 0.61 0.60 3 0.69 0.65 4 0.69 0.67 5 0.74 0.72 BioFusionNet (Proposed) 1 0.78 0.77 ± 0.05 0.82 0.84 ± 0.05 2 0.71 0.93 3 0.72 0.79 4 0.81 0.81 5 0.82 0.83 and I is the indicator function that returns 1 when its condition is true and 0 otherwise. The time-dependent AUC offers a dynamic view of the model accuracy over time t and incorporates weights ωi, and is calculated using the following formula: AUC(t) = nP i=1 nP j=1 ωiI(yi ≤t)I(y j > t)I( ˆf(xj) ≤ˆf(xi)) nP i=1 ωiI(yi ≤t) nP i=1 I(yi > t) . (18) Both the C-index and AUC values range from 0 to 1, with higher values indicating better performance. In addition to these metrics, we also compared the computational properties of the models in terms of the number of trainable parameters, memory usage, and floating-point operations (FLOPS). 3 Experimental Results 3.1 Comparison Across Modalities The effectiveness of BioFusionNet in cancer risk prediction was evaluated by comparing its C-index performance across differ- ent modality configurations. The results (Table 1) show that the mean performance in the cross-validation experiments consis- tently increased from using only imaging data, to using imaging and genetic data, to combining imaging, genetic and clinical data. Two traditional methods, specifically CoxPH and the Mul- tiLayer Perceptron (MLP), were also evaluated and showed no Table 3: Comparison of computational properties. Method Parameters Size FLOPS MultiSurv 1.79 M 6.99 MB 2.67 G MultiDeepCox-SC 0.71 M 2.72 MB 1.18 G HFBSurv 0.79 M 3.08 MB 2.65 G PathomicFusion 102.28 M 397.12 MB 3.87 G MCAT 2.06 M 8.06 MB 2.67 G TransSurv 1.97 M 7.72 MB 2.67 G BioFusionNet (Proposed) 4.37 M 17.20 MB 12.11 G consistent advantage when using only clinical or only genetic data, and their performance was inferior to that of BioFusion- Net. This suggests that BioFusionNet’s architecture and training process allow it to effectively extract and integrate complemen- tary information from the three different modalities, leading to improved predictive performance, although it remains elusive exactly how the information in each modality complements the information in the other. 3.2 Comparison With State-of-the-Art Fusion Methods The performance of BioFusionNet was compared with several state-of-the-art multimodal fusion methods for cancer risk pre- diction in terms of both C-index and AUC (Table 2). For this experiment we included methods using concatenation (Multi- Surv), Cox proportional hazards with image-derived risk scores (MultiDeepCox-SC), hierarchical attention (HFBSurv), gating attention (PathomicFusion), co-attention (MCAT) and cross at- tention (TransSurv). To ensure fair comparison, we used the same hyperparameter optimisation framework (Optuna) for all models. The AUC was calculated using average values over 5-year and 10-year periods. The proposed model consistently outperformed all these previous methods, showing substantial improvements in both metrics. However, BioFusionNet is com- putationally demanding, requiring 12.11 G (Giga) FLOPS, the highest among the compared models, although it is more mem- ory efficient than PathomicFusion (Table 3). 3.3 Evaluation of Loss Functions The performance of two different loss functions was compared using the C-index across five cross-validation folds for Bio- FusionNet and MoCoV3 (Table 4). The results show that the mean performance improved for both methods when using the weighted Cox loss (LWCox) proposed in this paper, compared to the traditional Cox loss (LCox). 3.4 Univariate and Multivariate Hazard Analysis A comprehensive hazard analysis was conducted to evaluate the overall survival (OS) in the TCGA dataset of ER+ patients. Both univariate and multivariate analyses were performed (Table 5). The analysis encompassed various parameters, including tumour grade, tumour size, age, lymph node (LN) status, subtype and the risk predictions made by BioFusionNet. In the multivariate analysis, positive LN status was associated with a hazard ratio (HR) of 1.87 (95% CI: 1.32–2.64), demonstrating a significant effect on survival (p < 0.005). Additionally, patients over the age of 55 had a HR of 1.77 (95% CI: 1.07–2.91), also showing a significant impact on survival (p = 0.03). However, no signif- icant associations were found between tumour grade, size, or 9 (a) CoxPH (Clinical) (b) MCAT (Multimodal) (c) BioFusionNet (Multimodal) (d) HFBSurv (Multimodal) (e) MultiSurv (Multimodal) (f) PathomicFusion (Multimodal) Figure 4: Performance comparison of BioFusionNet and other methods using Kaplan-Meier survival curves. Table 4: Performance comparison of loss functions for cancer risk prediction using two different methods. Loss Method C-index (Fold) Mean ± Std 1 2 3 4 5 LCox BioFusionNet 0.69 0.54 0.59 0.75 0.80 0.67 ± 0.10 MoCoV3 0.66 0.57 0.57 0.78 0.80 0.67 ± 0.11 LWCox (Proposed) BioFusionNet 0.78 0.71 0.72 0.81 0.82 0.77 ± 0.05 MoCoV3 0.70 0.66 0.66 0.77 0.72 0.70 ± 0.04 subtype and survival outcomes in this analysis. Notably, the BioFusionNet-predicted risk group (high vs. low) demonstrated a significant correlation with OS, with a HR of 2.91 (95% CI: 1.80–4.68) (p < 0.005). Univariate analysis indicated that tumour grade, size, age, and subtype were not statistically significant, whereas LN status (HR of 1.84, 95% CI: 1.33–2.55, p < 0.005) and BioFusionNet risk group (HR of 2.99, 95% CI: 1.88–4.78, p < 0.005) were significant predictors of survival. We note that the LN status had 51 missing values, which were imputed using a fixed value of 2. Kaplan-Meier survival analysis further sup- ported the results, showing a significant difference in survival probabilities between the high- and low-risk groups as predicted by BioFusionNet (log-rank test p = 6.45e-7) (Fig. 4c). 3.5 Ablation Study We also evaluated the performance of various versions of Bio- FusionNet for ER+ breast cancer risk stratification. The re- sults (Table 6) show that the base model, BioFusionNet-B0 (MoCoV3, ViT Small) with LWCox, achieved the lowest C- index, and incorporating single cross-attention (SCA), dual cross-attention (DCA), or co-attention (CoA) yielded slight im- provements, as did BioFusionNet-B1 (DINO33M, ViT Small) and BioFusionNet-B2 (DINO2M, ViT Small), both with DCA and LWCox. Combining BioFusionNet-B0, B1, and B2 with just LWCox also resulted in slightly better performance than the base model, as did the inclusion of SCA and VAE. More substantial improvements of the combined model were obtained with the inclusion of DCA or CoA instead of SCA. The best performance was achieved by the combined model using VAE, CoA, DCA and late clinical data fusion (C-FusionLate) with LWCox, which clearly outperformed the same model using early clinical data fusion (C-FusionEarly) or LCox. 3.6 Interpretability of BioFusionNet BioFusionNet utilises a self-attention mechanism to analyse histopathological image patches, identifying regions of high and low attention within both high-risk and low-risk patient pro- files. Visual inspection of the results (Fig. 5) reveals that regions with high attention contain distinct cellular patterns crucial for synthesising features from patch-level to patient-level, whereas areas of low attention typically exhibit less cellular atypia. This shows the model capacity to pinpoint clinically relevant features within tissue morphology. Additionally, SHAP analysis (Fig. 6) reveals the influence of individual genes on the model predic- tions, ranked from high to low, providing interpretability of the risk assessment process. From this analysis, gene SLC39A6 (an estrogen regulated Zinc transporter protein with a role in epithe- lial to mesenchymal transition (EMT)) was identified as the most important predictor, with high expression levels producing high SHAP values, indicating positive impact on the model’s cancer risk prediction. Other influential genes include ERBB2 (the gene for HER2), ESR1 (the gene for ER), with low expression levels producing high SHAP values therefore positive impact on the model. Moreover, the distribution of SHAP values for clinical features (Fig. 7) indicates that higher values of clinical parameters—such as positive LN status, higher tumour grade, 10 Table 5: Univariate and multivariate analysis for overall survival (OS) in the TCGA dataset of ER+ patients. Parameter Risk Group Cutoff #Patients/Group Multivariate (n=249) Univariate (n=249) HR 95% CI p HR 95% CI p Tumour Grade 3 vs. 1 & 2 64 vs. 185 0.83 0.46–1.49 0.54 1.13 0.67–1.91 0.65 Tumour Size >20 vs. ≤20 (mm) 167 vs. 82 1.45 0.88–2.37 0.14 1.37 0.86–2.19 0.19 Age >55 vs. ≤55 159 vs. 90 1.77 1.07–2.91 0.03 1.47 0.91–2.36 0.11 LN Status* pos. vs. neg. 110 vs. 88 1.87 1.32–2.64 <0.005 1.84 1.33–2.55 <0.005 Subtype lum B vs. A 100 vs. 149 1.43 0.88–2.34 0.15 1.38 0.88–2.18 0.16 BioFusionNet high vs. low 132 vs. 117 2.91 1.80–4.68 <0.005 2.99 1.88–4.78 <0.005 *LN Status had 51 missing values which were imputed with a fixed value for both multivariate and univariate analysis. Table 6: Ablation study of BioFusionNet Model C-index BioFusionNet-B0 (MoCoV3, ViT Small) + C-FusionLate + LWCox 0.65 ± 0.05 BioFusionNet-B0 (MoCoV3, ViT Small) + SCA + C-FusionLate + LWCox 0.69 ± 0.04 BioFusionNet-B0 (MoCoV3, ViT Small) + DCA + C-FusionLate + LWCox 0.70 ± 0.03 BioFusionNet-B0 (MoCoV3, ViT Small) + CoA + C-FusionLate + LWCox 0.70 ± 0.04 BioFusionNet-B1 (DINO33M, ViT Small) + DCA + C-FusionLate + LWCox 0.68 ± 0.02 BioFusionNet-B2 (DINO2M, ViT Small) + DCA + C-FusionLate + LWCox 0.67 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + C-FusionLate + LWCox 0.67 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + SCA + C-FusionLate + LWCox 0.69 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + VAE + SCA + C-FusionLate + LWCox 0.68 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + DCA + C-FusionLate + LWCox 0.75 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + C-FusionLate + LWCox 0.70 ± 0.03 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + Clinic-FLate + LCox 0.67 ± 0.10 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + C-FusionEarly + LWCox 0.74 ± 0.04 BioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + C-FusionLate + LWCox 0.77 ± 0.03 increased tumour size, and postmenopausal age group—tend to have a positive impact on the model’s output. In this context, a ‘positive impact’ implies that the model associates these values with a higher likelihood of predicting patients at high risk. 4 Discussion and Conclusion As demonstrated by the experimental results, the proposed Bio- FusionNet is highly effective for cancer risk prediction, show- ing superior performance compared to alternative approaches. Clearly, the multimodal fusion of imaging, genetic and clinical data allows the model to achieve substantially higher C-index scores compared to unimodal and dual-modal configurations, as well as compared to the traditional unimodal CoxPH and MLP. Furthermore, BioFusionNet outperforms existing multimodal fusion methods such as MultiSurv, HFBSurv, PathomicFusion, MCAT and TransSurv, and achieves the highest mean C-index (0.77 ± 0.05) and AUC (0.84 ± 0.05). Partly, the superior per- formance of BioFusionNet is due to the introduction of the proposed weighted Cox loss function instead of using the tradi- tional Cox loss. Univariate and multivariate analyses showed the significant impact of age and BioFusionNet predictions on survival outcomes, while other clinical parameters such as tu- mour grade, size, lymph node status and subtype did not exhibit a significant correlation with survival outcomes. Kaplan-Meier analysis revealed a distinct separation in survival probabilities between the high-risk and low-risk groups identified by Bio- FusionNet. In addition, the results of the ablation experiment confirmed the importance of attention mechanisms in improving prediction accuracy, with the combined model configuration util- ising VAE, CoA and DCA and the weighted Cox loss showing the highest performance. BioFusionNet also presents a significant advancement in the interpretation of histopathological images, leveraging a self- attention mechanism to distinguish critical regions in patient profiles. A key contribution is the model’s ability to align high- attention areas with distinct cellular patterns, crucial for tran- sitioning from patch-level to patient-level analysis, thereby en- hancing the diagnostic process. SHAP analysis amplifies this by clarifying the influence of specific genes and clinical fea- tures on the model’s predictions. We observed that elevated SLC39A6 gene expression correlates with a high-risk prediction in ER+ breast cancers, where previous studies have shown con- flicting findings, associating high SLC39A6 levels with good prognosis [56, 57], while others associated it with increased pro- liferation and lymph node involvement [58–60]. Similarly, our model identified high ESR1 expression as indicative of low risk, aligning with literature that associates ESR1 positivity with en- hanced responsiveness to endocrine therapy and, consequently, a better prognosis in ER+ breast cancer patients [61]. In con- trast, our analysis revealed an unexpected association between ERBB2 overexpression and a favorable prognosis in ER+ breast tumours, contrasting with the established view that ERBB2 over- expression indicates a poor prognosis [62, 63]. As our study was specifically tailored to analyze ER+ samples, excluding the HER2-enriched subtype (known for its high ERBB2 expression and aggressiveness) likely influenced the findings. Moreover, the SHAP analysis for clinical factors (LN positivity, higher tumour grade and size, and postmenopausal age) significantly influences our model’s ability to identify patients at increased risk, highlighting the critical role of these factors in breast cancer prognosis. Our analysis provides a transparent understanding of how each gene and clinical feature contributes to the model’s predictions, providing actionable insights for clinical decision- making. While the risk assessment process mirrors current clinical practice, BioFusionNet streamlines the integration of all available data (patient features, tumour features and molecular features) to derive an automated single risk prediction score as a potential clinical oncology tool of the future. While insightful, this study has certain limitations. We primarily opted for OS as the key outcome measure, instead of disease-free survival (DFS). This choice was made because DFS presented challenges such as a lower rate of events and a higher degree of data censorship, which could have limited the depth of the analysis. While OS is a feasible choice, it potentially overlooks critical insights into early-stage disease progression, typically highlighted by DFS. Moreover, the study’s reliance on specific datasets such as TCGA for ER+ patients may affect the broad applicability of our findings. Another shortcoming of this study 11 Low Risk High Risk Luminal A Luminal B Risk Type High Attention Low Attention High Attention Low Attention TCGA-A7-A13H TCGA-AR-A2LK TCGA-B6-A0IN TCGA-A2-A259 Figure 5: Visualisation of model-derived attention regions and associated risk types in Luminal A and Luminal B breast cancer patients. The figure presents raw histopathological image patches processed with BioFusionNet, which identifies areas of high and low attention, subsequently categorising patients into high and low risk. Figure 6: SHAP analysis of genetic features. The x-axis represents the SHAP value; colour intensity indicates gene expression level. The plot is sorted vertically by the features’ overall importance. Figure 7: SHAP value distribution of clinical features. In this distribu- tion, higher clinical values showing positive impact on the model, as indicated by its SHAP values. is the inherent limitations of the clinical data, which, during univariate analysis, identified tumour size, grade, and age as in- significant while only LN Status emerged as significant. Despite its limitations, the effectiveness of deep learning algorithms in analysing this clinical data arises from their ability to uncover complex patterns and interactions within dataset. Future re- search should therefore aim to validate these findings across a wider range of datasets to bolster the model’s generalisability. Incorporating organ-level data, such as mammograms, could fur- ther enhance the predictive accuracy of our model. Additionally, extending the application of BioFusionNet to other cancer types and clinical scenarios could yield more comprehensive insights, making the research more universally relevant and applicable. Finally, a limitation of our model is that it is computationally demanding, primarily due to its extensive use of attention mech- anisms. Whether the better performance justifies the higher computational cost depends on user needs and resources. Fur- ther research may provide ways to reduce the computational requirements of the model while retaining its high performance. Data Availability TCGA image data and clinical data are publicly available at https://portal.gdc.cancer.gov/. Code Availability Our work is fully reproducible and source code is publicly avail- able on GitHub at https://github.com/raktim-mondol/ BioFusionNet. Acknowledgement This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI), which is supported by the Australian Government. Addi- tionally, data preprocessing was performed using the computa- tional cluster Katana, which is supported by Research Technol- ogy Services at UNSW Sydney. 12 Compliance with Ethical Standards This research study was conducted retrospectively using hu- man subject data made available in open access by The Cancer Genome Atlas Breast Cancer (TCGA-BRCA) dataset, accessible through the National Cancer Institute’s Genomic Data Commons (GDC) portal. Ethical approval was not required for this study, in accordance with the ethical policies set forth by The Cancer Genome Atlas program. References [1] Y. B. Shvetsov, L. R. Wilkens, K. K. White, M. Chong, A. Buyum, G. Badowski, R. T. L. Guerrero, and R. Novotny, “Prediction of breast cancer risk among women of the Mariana Islands: the BRISK retrospective case–control study,” BMJ Open, vol. 12, no. 12, p. e061205, 2022. [2] W. Guo, W. Liang, Q. Deng, and X. Zou, “A multimodal affin- ity fusion network for predicting the survival of breast cancer patients,” Frontiers in Genetics, vol. 12, p. 709027, 2021. [3] M. D. J. Peters, I. Ramsey, K. Kennedy, G. Sharplin, and M. Eck- ert, “Culturally safe, high-quality breast cancer screening for transgender people: a scoping review protocol,” Journal of Ad- vanced Nursing, vol. 78, no. 1, pp. 276–281, 2021. [4] C. Shuai, F. Yuan, Y. Liu, C. Wang, J. Wang, and H. He, “Estrogen receptor—positive breast cancer survival prediction and analysis of resistance–related genes introduction,” PeerJ, vol. 9, p. e12202, 2021. [5] K. Holli-Helenius, A. Salminen, I. Rinta-Kiikka, I. Koskivuo, N. Brück, P. Boström, and R. Parkkola, “MRI texture analysis in differentiating luminal A and luminal B breast cancer molecular subtypes—a feasibility study,” BMC Medical Imaging, vol. 17, p. 69, 2017. [6] H. Lindman, F. Wiklund, and K. K. Andersen, “Long-term treat- ment patterns and survival in metastatic breast cancer by intrinsic subtypes—an observational cohort study in Sweden,” BMC Can- cer, vol. 22, p. 1006, 2022. [7] Y. Han, J. Wang, and B. Xu, “Clinicopathological characteristics and prognosis of breast cancer with special histological types: a surveillance, epidemiology, and end results database analysis,” The Breast, vol. 54, pp. 114–120, 2020. [8] M. A. Han, E. C. Hwang, and J. H. Jung, “Prognostic factors of mortality in patients with cancer infected with COVID-19: a systematic review protocol,” BMJ Open, vol. 13, no. 7, p. e071810, 2023. [9] C. Nero, F. Ciccarone, A. Pietragalla, S. Duranti, G. Daniele, G. Scambia, and D. Lorusso, “Adjuvant treatment recommenda- tions in early-stage endometrial cancer: what changes with the introduction of the integrated molecular-based risk assessment,” Frontiers in Oncology, vol. 11, p. 612450, 2021. [10] A. N. Zahari, N. S. Mohamad, and M. H. Mahmud, “Impacts of clinicopathological factors on metabolic parameters of 18F fluorodeoxyglucose PET/CT in the staging of breast cancer,” Jour- nal of Sustainability Science and Management, vol. 17, no. 12, pp. 166–173, 2022. [11] X. Li, L. Liu, G. J. Goodall, A. Schreiber, T. Xu, J. Li, and T. D. Le, “A novel single-cell based method for breast cancer prognosis,” PLoS Computational Biology, vol. 16, no. 8, pp. 1–20, 2020. [12] M. I. Jaber, L. Beziaeva, C. W. Szeto, and S. C. Benz, “Deep learning-based risk stratification for HER2-negative breast cancer patients,” bioRxiv, p. 10.1101/2021.05.26.445720, 2021. [13] M. Garutti, G. Griguolo, A. Botticelli, G. Buzzatti, C. D. Angelis, L. Gerratana, C. Molinelli, V. Adamo, G. Bianchini, L. Biganzoli, G. Curigliano, M. D. Laurentiis, A. Fabi, A. Frassoldati, A. Gen- nari, M. Scaltriti, F. Perrone, G. Viale, C. Zamagni, A. Zam- belli, L. D. Mastro, S. D. Placido, V. Guarneri, P. Marchetti, and F. Puglisi, “Definition of high-risk early hormone-positive HER2-negative breast cancer: a consensus review,” Technology in Cancer Research & Treatment, vol. 14, no. 8, p. 1898, 2022. [14] B. Lu, E. Natarajan, H. R. B. Raghavendran, and U. D. Markan- dan, “Molecular classification, treatment, and genetic biomarkers in triple-negative breast cancer: a review,” Technology in Cancer Research & Treatment, vol. 22, p. 15330338221145246, 2023. [15] L. Guo, D. Kong, J. Liu, L. Zhan, L. Luo, W. Zheng, Q. Zheng, C. Chen, and S. Sun, “Breast cancer heterogeneity and its implica- tion in personalized precision therapy,” Experimental Hematology & Oncology, vol. 12, no. 1, p. 3, 2023. [16] J. C. Wei, A. A. Suriawinata, B. Ren, Y. Zhang, M. Lisovsky, L. J. Vaickus, C. R. Brown, M. J. Baker, N. Tomita, L. Torresani, J. Z. Wei, and S. Hassanpour, “A petri dish for histopathology image analysis,” Artificial Intelligence in Medicine, pp. 11–24, 2021. [17] E. Schaafsma, B. Zhang, M. Schaafsma, C. Tong, L. Zhang, and C. Cheng, “Impact of oncotype DX testing on ER+ breast cancer treatment and survival in the first decade of use,” Breast Cancer Research, vol. 23, no. 1, p. 74, 2021. [18] A. D. Caluwé, L. Buisseret, P. Poortmans, D. V. Gestel, R. Sal- gado, C. Sotiriou, D. Larsimont, M. Paesmans, L. Craciun, S. Dri- sis, C. Vandekerckhove, F. Reyal, I. Veys, D. Eiger, M. Piccart, E. Romano, and M. Ignatiadis, “Neo-CheckRay: radiation therapy and adenosine pathway blockade to increase benefit of immuno- chemotherapy in early stage luminal B breast cancer, a random- ized phase II trial,” BMC Cancer, vol. 21, p. 899, 2021. [19] S. Paik, G. Tang, S. Shak, C. Kim, J. B. Baker, W. Kim, M. Cronin, F. L. Baehner, D. Watson, J. Bryant, J. P. Costantino, C. E. Geyer, D. L. Wickerham, and N. Wolmark, “Gene expression and benefit of chemotherapy in women with node-negative, estrogen recep- tor–positive breast cancer,” Journal of Clinical Oncology, vol. 24, no. 23, pp. 3726–3734, 2006. [20] K. Lee, S. H. Sim, E. J. Kang, J. H. Seo, H. D. Chae, K. S. Lee, J. Y. Kim, J. S. Ahn, Y. H. Im, S. Park, Y. H. Park, and I. H. Park, “The role of chemotherapy in patients with HER2-negative isolated locoregional recurrence of breast cancer: a multicen- ter retrospective cohort study,” Frontiers in Oncology, vol. 11, p. 653243, 2021. [21] Y. Naoi, R. Tsunashima, K. Shimazu, and S. Noguchi, “The multi- gene classifiers 95GC/42GC/155GC for precision medicine in ER-positive HER2-negative early breast cancer,” Cancer Science, vol. 112, no. 4, pp. 1369–1375, 2021. [22] J. Katzman, U. Shaham, A. Cloninger, J. Bates, T. Jiang, and Y. Kluger, “DeepSurv: personalized treatment recommender sys- tem using a Cox proportional hazards deep neural network,” BMC Medical Research Methodology, vol. 18, p. 24, 2018. [23] Y. He, B. Hu, C. Zhu, W. Xu, X. Hao, B. Dong, X. Chen, Q. Dong, and X. Zhou, “A novel multimodal radiomics model for predicting prognosis of resected hepatocellular carcinoma,” Frontiers in Oncology, vol. 12, p. 745258, 2022. [24] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multimodal fusion using sparse CCA for breast cancer survival prediction,” IEEE International Symposium on Biomedical Imaging, pp. 1429– 1432, 2021. [25] R. Vanguri, J. Luo, A. Aukerman, J. V. Egger, C. J. Fong, N. Hor- vat, A. Pagano, J. d. A. B. Araújo-Filho, L. Geneslaw, H. Rizvi, R. E. Sosa, K. M. Boehm, S. Yang, F. M. Bodd, K. Ventura, T. J. 13 Hollmann, M. S. Ginsberg, J. Gao, M. D. Hellmann, J. L. Sauter, and S. P. Shah, “Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer,” Nature Cancer, vol. 3, no. 10, pp. 1151–1164, 2022. [26] S. Deng, Y. Suo, S. Liu, X. Ma, H. Chen, X. Liao, J. Zhang, and W. W. Y. Ng, “MFCSA-CAT: a multimodal fusion method for cancer survival analysis based on cross-attention transformer,” International Conference on Computer Information Science and Artificial Intelligence, vol. 12566, p. 1256608, 2023. [27] T. Wei, X. Yuan, R. Gao, L. J. Johnston, J. Zhou, Y. Wang, W. Kong, Y. Xie, Y. Zhang, D. Xu, and Z. Yu, “Survival predic- tion of stomach cancer using expression data and deep learning models with histopathological images,” Cancer Science, vol. 114, no. 2, pp. 690–701, 2022. [28] R. J. Chen, M. Y. Lu, W.-H. Weng, T. Y. Chen, D. F. Williamson, T. Manz, M. Shady, and F. Mahmood, “Multimodal co-attention transformer for survival prediction in gigapixel whole slide im- ages,” IEEE/CVF International Conference on Computer Vision, pp. 3995–4005, 2021. [29] L. A. Vale-Silva and K. Rohr, “Long-term cancer survival predic- tion using multimodal deep learning,” Scientific Reports, vol. 11, p. 13505, 2021. [30] R. Li, X. Wu, A. Li, and M. Wang, “HFBSurv: Hierarchical multimodal fusion with factorized bilinear models for cancer survival prediction,” Bioinformatics, vol. 38, no. 9, pp. 2587– 2594, 2022. [31] R. J. Chen, M. Y. Lu, J. Wang, D. F. Williamson, S. J. Rodig, N. I. Lindeman, and F. Mahmood, “Pathomic Fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis,” IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 757–770, 2022. [32] Z. Lv, Y. Lin, R. Yan, Y. Wang, and F. Zhang, “TransSurv: Transformer-based survival analysis model integrating histopatho- logical images and genomic data for colorectal cancer,” IEEE/ACM Transactions on Computational Biology and Bioinfor- matics, vol. 20, no. 6, pp. 3411–3420, 2023. [33] S. Garg, H. SS, and S. Kumar, “On-device document classifica- tion using multimodal features,” ACM India Joint International Conference on Data Science & Management of Data, p. 203–207, 2021. [34] K. Xu, Z. Lin, J. Zhao, P. Shi, W. Deng, and H. Wang, “Multi- modal deep learning for social media popularity prediction with attention mechanism,” ACM International Conference on Multi- media, p. 4580–4584, 2020. [35] S. Qiu, X. Cui, Z. Ping, N. Shan, Z. Li, X. Bao, and X. Xu, “Deep learning techniques in intelligent fault diagnosis and prognosis for industrial systems: a review,” Sensors, vol. 23, no. 3, p. 1305, 2023. [36] B. Li and C. N. Dewey, “RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome,” BMC Bioinformatics, vol. 12, p. 323, 2011. [37] P. Bankhead, M. B. Loughrey, J. A. Fernández, Y. Dombrowski, D. G. McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray, H. G. Coleman, J. A. James, M. Salto-Tellez, and P. W. Hamil- ton, “QuPath: open source software for digital pathology image analysis,” Scientific Reports, vol. 7, p. 16878, 2017. [38] M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley, X. Guan, C. Schmitt, and N. E. Thomas, “A method for normalizing histology slides for quantitative analysis,” IEEE International Symposium on Biomedical Imaging, pp. 1107–1110, 2009. [39] C. Mazo, S. Barron, and C. Mooney, “Multi-gene prognostic signatures and prediction of pathological complete response to neoadjuvant chemotherapy in ER-positive, HER2-negative breast cancer patients,” Cancers, vol. 12, no. 5, p. 1133, 2020. [40] P. Blanchette, D. Sivajohanathan, J. M. Bartlett, A. Eisen, H. Feilotter, R. C. Pezo, G. Turashvili, and P. E. Williams, “Clin- ical utility of multigene profiling assays in early-stage invasive breast cancer: an Ontario Health (Cancer Care Ontario) clinical practice guideline,” Current Oncology, vol. 29, no. 4, pp. 2599– 2616, 2022. [41] K. Almstedt, S. Mendoza, M. Otto, M. Battista, J. Steetskamp, A. S. Heimes, S. Krajnak, A. Poplawski, A. Gerhold-Ay, A. Hasen- burg, C. Denkert, and M. Schmidt, “Endopredict® in early hor- mone receptor-positive, HER2-negative breast cancer,” Breast Cancer Research and Treatment, vol. 182, pp. 137–146, 2020. [42] S. Jahn, A. Bösl, O. Tsybrovskyy, C. Gruber-Rossipal, R. Helf- gott, F. Fitzal, M. Knauer, M. Bali´c, Z. Jasarevic, F. Offner, and F. Moinfar, “Clinically high-risk breast cancer displays markedly discordant molecular risk predictions between the mammaprint and endopredict tests,” British Journal of Cancer, vol. 122, no. 12, pp. 1744–1746, 2020. [43] S. P. Somashekhar, S. Zaveri, D. G. Vijay, P. S. Dattatreya, R. Ku- mar, F. Islahi, and C. Bahl, “Individualized chemotherapy benefit prediction by endopredict in patients with early breast cancer in an Indian cohort,” JCO Global Oncology, no. 6, pp. 1363–1369, 2020. [44] R. Buus, I. Šestak, R. Kronenwett, S. Ferree, C. A. Schnabel, F. L. Baehner, E. Mallon, J. Cuzick, and M. Dowsett, “Molecular drivers of Oncotype DX, Prosigna, EndoPredict, and the Breast Cancer Index: a TransATAC study,” Journal of Clinical Oncology, vol. 39, no. 2, pp. 126–135, 2021. [45] J. Warwick, L. Tabár, B. Viták, and S. W. Duffy, “Time-dependent effects on survival in breast carcinoma,” Cancer, vol. 100, no. 7, pp. 1331–1336, 2004. [46] C. Woo, H. Silberman, S. Nakamura, W. Ye, R. Sposto, W. J. Colburn, J. Waisman, and M. J. Silverstein, “Lymph node status combined with lymphovascular invasion creates a more powerful tool for predicting outcome in patients with invasive breast cancer,” American Journal of Surgery, vol. 184, no. 4, pp. 337–340, 2002. [47] H. Bor, E. N. Maina, B. Nyambega, K. Patel, C. O. Ol- wal, W. Nalyanya, and Y. Gavamukulya, “The potential of differentiation-related gene-1 (DRG1) as a biomarker for metas- tasis of estrogen receptor-positive breast cancer,” Journal of Ad- vances in Medicine and Medical Research, pp. 162–169, 2021. [48] S. Naz, M. Siddiqui, A. I. Memon, A. M. Bhatti, Z. I. Hussain, and Iqra, “Analysis of breast cancer receptors status and molecular subtypes among female population,” Pakistan Journal of Medical and Health Sciences, vol. 17, no. 1, pp. 656–658, 2023. [49] M. Kang, H. Song, S. Park, D. Yoo, and S. Pereira, “Bench- marking self-supervised learning on diverse pathology datasets,” IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, pp. 3344–3354, 2023. [50] R. J. Chen and R. G. Krishnan, “Self-supervised vision transform- ers learn visual concepts in histopathology,” Annual Conference on Neural Information Processing Systems, 2021. [51] X. Chen, S. Xie, and K. He, “An empirical study of training self-supervised vision transformers,” CoRR, p. 2104.02057, 2021. [52] A. van den Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,” arXiv, p. 1807.03748, 2018. [53] X. Wang, S. Yang, J. Zhang, M. Wang, J. Zhang, W. Yang, J. Huang, and X. Han, “Transformer-based unsupervised con- trastive learning for histopathological image classification,” Medi- cal Image Analysis, vol. 81, p. 102559, 2022. 14 [54] J. Venugopalan, L. Tong, H. R. Hassanzadeh, and M. D. Wang, “Multimodal deep learning models for early detection of Alzheimer’s disease stage,” Scientific Reports, vol. 11, p. 3254, 2021. [55] S. Steyaert, Y. L. Qiu, Y. Zheng, P. Mukherjee, H. Vogel, and O. Gevaert, “Multimodal deep learning to predict prognosis in adult and pediatric brain tumors,” Communications Medicine, vol. 3, no. 1, p. 44, 2023. [56] M. Althobiti, K. A. El-sharawy, C. Joseph, M. Aleskandarany, M. S. Toss, A. R. Green, and E. A. Rakha, “Oestrogen-regulated protein SLC39A6: A biomarker of good prognosis in luminal breast cancer,” Breast Cancer Research and Treatment, vol. 189, no. 3, pp. 621–630, 2021. [57] L. Liu, J. Yang, and C. Wang, “Analysis of the prognostic signifi- cance of solute carrier (SLC) family 39 genes in breast cancer,” Bioscience Reports, vol. 40, no. 8, p. BSR20200764, 2020. [58] C. Hogstrand, P. Kille, M. L. Ackland, S. Hiscox, and K. M. Taylor, “A mechanism for epithelial-mesenchymal transition and anoikis resistance in breast cancer triggered by zinc channel ZIP6 and STAT3 (signal transducer and activator of transcription 3),” Biochemical Journal, vol. 455, pp. 229–237, 10 2013. [59] S. U. Gerold, K. M. Taylor, I. A. Muraina, D. Brethour, T. Nim- manon, S. Ziliotto, P. Kille, and C. Hogstrand, “Zinc transporter ZIP10 forms a heteromer with ZIP6 which regulates embryonic development and cell migration,” Biochemical Journal, vol. 473, no. 16, pp. 2531–2544, 2016. [60] K. M. Taylor, H. E. Morgan, K. Smart, N. M. Zahari, S. Pumford, I. O. Ellis, J. F. Robertson, and R. I. Nicholson, “The emerging role of the LIV-1 subfamily of zinc transporters in breast cancer,” Molecular Medicine, vol. 13, no. 7-8, pp. 396–406, 2007. [61] A. Sappok and U. Mahlknecht, “Ribavirin restores ESR1 gene expression and tamoxifen sensitivity in ESR1 negative breast cancer cell lines,” Clinical Epigenetics, vol. 3, p. 8, 2011. [62] W. Xu, M. Marcu, X. Yuan, E. Mimnaugh, C. Patterson, and L. Neckers, “Chaperone-dependent E3 ubiquitin ligase CHIP mediates a degradative pathway for c-ErbB2Neu,” Cell Biology, vol. 99, pp. 12847–12852, 2002. [63] W. Xia, S. Bacus, P. Hegde, I. Husain, J. Strum, L. Liu, G. Paulazzo, L. Lyass, P. Trusk, J. Hill, J. Harris, and N. L. Spector, “A model of acquired autoresistance to a potent ErbB2 tyrosine kinase inhibitor and a therapeutic strategy to prevent its onset in breast cancer,” Proceedings of the National Academy of Sciences, vol. 103, no. 20, pp. 7795–7800, 2006. Raktim Kumar Mondol is a PhD candidate in Computer Science and Engineering, spe- cializing in computer vision and bioinformat- ics. He completed his MEng in Engineering with High Distinction from RMIT University, Australia. Mondol’s research interests in- clude histopathological image analysis, clini- cal prognosis prediction, and enhancing clini- cal understanding through the interpretability of computational models. Ewan Millar is a Senior Staff Specialist Histopathologist with NSW Health Pathology at St George Hospital Sydney with expertise in breast cancer pathology and translational research and a strong interest in AI and digi- tal pathology applications. Arcot Sowmya is Professor in the School of Computer Science and Engineering, UNSW. Her major research interest is in the area of Machine Learning for Computer Vision and includes learning object models, fea- ture extraction, segmentation and recogni- tion based on computer vision, machine learning and deep learning. In recent years, applications in the broader health area are a focus, including biomedical informatics and rapid diagnostics in the real world. All of these areas have been supported by com- petitive, industry and government funding. Erik Meijering (Fellow, IEEE), is a Profes- sor of Biomedical Image Computing in the School of Computer Science and Engineer- ing. His research focusses on the develop- ment of innovative computer vision and ma- chine learning (in particular deep learning) methods for automated quantitative analysis of biomedical imaging data.